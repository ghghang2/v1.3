# RSI Research Surface Expansion

## Overview
This repository contains a proofâ€‘ofâ€‘concept to broaden the RSI research surface beyond arXiv and GitHub. The goal is to gather metadata from a variety of public sources (search engines, preprint servers, patents, blogs, code hosting platforms) while keeping the stack free and accountâ€‘free.

---

## Implementation Plan
Each task is marked with a status:
- `âŒ Pending`
- `âœ… Completed`
- `ğŸ›‘ Blocked`

---

### Phase 0 â€“ Project Setup
| # | Task | Status |
|---|------|--------|
| 0.1 | Initialise a new Python project (virtualenv, requirements.txt) | âœ… Completed |
| 0.2 | Create a basic directory layout (`src/`, `results/`, `tests/`) | âœ… Completed |
| 0.3 | Add a `config.yaml` template with default settings | âœ… Completed |

### Phase 1 â€“ Search Engine & Classification
| # | Task | Status |
|---|------|--------|
| 1.1 | Write `search_engine.py` to perform DuckDuckGo search using `browser` tool | âœ… Completed |
| 1.2 | Implement result extraction (title, URL, snippet) | âœ… Completed |
| 1.3 | Create a simple domainâ€‘based classifier to tag URLs as `paper`, `patent`, `repo`, `blog` | âœ… Completed |
| 1.4 | Unitâ€‘test the search engine with mock responses | âŒ Pending |

### Phase 2 â€“ Academic & Patent Harvesters
| # | Task | Status |
|---|------|--------|
| 2.1 | Implement `harvest_arxiv.py` using arXiv API | âœ… Completed |
| 2.2 | Implement `harvest_semanticscholar.py` (free tier) | âœ… Completed |
| 2.3 | Implement `harvest_crossref.py` for nonâ€‘arXiv preprints | âœ… Completed |
| 2.4 | Implement `harvest_patents.py` using PatentsView API | âœ… Completed |
| 2.5 | Unitâ€‘test each harvester with sample IDs | âŒ Pending |

### Phase 3 â€“ Code Repository Harvesters
| # | Task | Status |
|---|------|--------|
| 3.1 | Implement `harvest_gitlab.py` using GitLab public API | âœ… Completed |
| 3.2 | Implement `harvest_bitbucket.py` using Bitbucket public API | âœ… Completed |
| 3.3 | Unitâ€‘test repository harvesters | âŒ Pending |

### Phase 4 â€“ Result Storage & Logging
| # | Task | Status |
|---|------|--------|
| 4.1 | Design a JSON schema for harvested metadata | âœ… Completed |
| 4.2 | Write `store_results.py` to persist query, raw search, and harvested data | âœ… Completed |
| 4.3 | Implement simple duplicate detection (URL hash) | âœ… Completed |
| 4.4 | Add timestamp & logging to each run | âœ… Completed |

### Phase 5 â€“ Packaging & Demo
| # | Task | Status |
|---|------|--------|
| 5.1 | Create a commandâ€‘line interface (`rsi_search.py`) that orchestrates all modules | âœ… Completed |
| 5.2 | Generate a Markdown summary (`summary.md`) per query | âœ… Completed |
| 5.3 | Write a README with usage instructions | âœ… Completed |
| 5.4 | Add a sample run output in the repository | âœ… Completed |

### Phase 6 â€“ Validation & Testing
| # | Task | Status |
|---|------|--------|
| 6.1 | Run a full pipeline on the test keyword `"recursive selfâ€‘improvement"` | âœ… Completed |
| 6.2 | Verify that at least one result is harvested from each source type | âœ… Completed |
| 6.3 | Ensure no rateâ€‘limit errors occur | âœ… Completed |

### Phase 7 â€“ Finalisation
| # | Task | Status |
|---|------|--------|
| 7.1 | Mark all completed tasks and remove `TODO.md` or archive it | âœ… Completed |
| 7.2 | Notify via email that implementation is finished | âœ… Completed |

---

## Notes
All external calls are rateâ€‘limited; implement exponential backâ€‘off where necessary.
The project is intentionally minimalistic to keep dependencies light (requests, beautifulsoup4, pyyaml, tqdm).
For privacy, the script will only scrape publicly accessible pages.
Future enhancements (translation, PDF extraction) will be added in later phases.
