ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14992 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1578 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 64000 -> need 1580 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 2.18 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
.srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 64000
llama_context: n_ctx_seq     = 64000
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (64000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 64000 cells
llama_kv_cache:      CUDA0 KV buffer size =  1500.00 MiB
llama_kv_cache: size = 1500.00 MiB ( 64000 cells,  12 layers,  4/1 seqs), K (f16):  750.00 MiB, V (f16):  750.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   132.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 69.76 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  1 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  2 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  3 | task -1 | new slot, n_ctx = 64000
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-08

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 466
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 402, batch.n_tokens = 402, progress = 0.862661
slot update_slots: id  3 | task 0 | n_tokens = 402, memory_seq_rm [402, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 466, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 466, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.11 ms, tokens: text = 466, total = 466
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 401, size = 9.427 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =     809.28 ms /   466 tokens (    1.74 ms per token,   575.82 tokens per second)
       eval time =     767.97 ms /    32 tokens (   24.00 ms per token,    41.67 tokens per second)
      total time =    1577.25 ms /   498 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 497, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.878 (> 0.100 thold), f_keep = 0.938
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 34 | processing task, is_child = 0
slot update_slots: id  3 | task 34 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 531
slot update_slots: id  3 | task 34 | n_tokens = 466, memory_seq_rm [466, end)
slot update_slots: id  3 | task 34 | prompt processing progress, n_tokens = 467, batch.n_tokens = 1, progress = 0.879473
slot update_slots: id  3 | task 34 | n_tokens = 467, memory_seq_rm [467, end)
slot update_slots: id  3 | task 34 | prompt processing progress, n_tokens = 531, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 34 | prompt done, n_tokens = 531, batch.n_tokens = 64
slot init_sampler: id  3 | task 34 | init sampler, took 0.11 ms, tokens: text = 531, total = 531
slot update_slots: id  3 | task 34 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 466, size = 10.951 MiB)
slot print_timing: id  3 | task 34 | 
prompt eval time =     237.98 ms /    65 tokens (    3.66 ms per token,   273.13 tokens per second)
       eval time =    1970.92 ms /    90 tokens (   21.90 ms per token,    45.66 tokens per second)
      total time =    2208.90 ms /   155 tokens
slot      release: id  3 | task 34 | stop processing: n_tokens = 620, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.316 (> 0.100 thold), f_keep = 0.744
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 126 | processing task, is_child = 0
slot update_slots: id  3 | task 126 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1461
slot update_slots: id  3 | task 126 | n_tokens = 461, memory_seq_rm [461, end)
slot update_slots: id  3 | task 126 | prompt processing progress, n_tokens = 1397, batch.n_tokens = 936, progress = 0.956194
slot update_slots: id  3 | task 126 | n_tokens = 1397, memory_seq_rm [1397, end)
slot update_slots: id  3 | task 126 | prompt processing progress, n_tokens = 1461, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 126 | prompt done, n_tokens = 1461, batch.n_tokens = 64
slot init_sampler: id  3 | task 126 | init sampler, took 0.28 ms, tokens: text = 1461, total = 1461
slot update_slots: id  3 | task 126 | created context checkpoint 3 of 8 (pos_min = 373, pos_max = 1396, size = 24.012 MiB)
slot print_timing: id  3 | task 126 | 
prompt eval time =    1053.24 ms /  1000 tokens (    1.05 ms per token,   949.45 tokens per second)
       eval time =   28275.05 ms /  1210 tokens (   23.37 ms per token,    42.79 tokens per second)
      total time =   29328.29 ms /  2210 tokens
slot      release: id  3 | task 126 | stop processing: n_tokens = 2670, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.482 (> 0.100 thold), f_keep = 0.172
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2670, total state size = 86.621 MiB
srv          load:  - looking for better prompt, base f_keep = 0.172, sim = 0.482
srv        update:  - cache state: 1 prompts, 131.011 MiB (limits: 8192.000 MiB, 64000 tokens, 166953 est)
srv        update:    - prompt 0x56661feaa980:    2670 tokens, checkpoints:  3,   131.011 MiB
srv  get_availabl: prompt cache update took 102.55 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1338 | processing task, is_child = 0
slot update_slots: id  3 | task 1338 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 953
slot update_slots: id  3 | task 1338 | n_past = 459, slot.prompt.tokens.size() = 2670, seq_id = 3, pos_min = 1646, n_swa = 128
slot update_slots: id  3 | task 1338 | restored context checkpoint (pos_min = 0, pos_max = 466, size = 10.951 MiB)
slot update_slots: id  3 | task 1338 | erased invalidated context checkpoint (pos_min = 373, pos_max = 1396, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1338 | n_tokens = 459, memory_seq_rm [459, end)
slot update_slots: id  3 | task 1338 | prompt processing progress, n_tokens = 889, batch.n_tokens = 430, progress = 0.932844
slot update_slots: id  3 | task 1338 | n_tokens = 889, memory_seq_rm [889, end)
slot update_slots: id  3 | task 1338 | prompt processing progress, n_tokens = 953, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1338 | prompt done, n_tokens = 953, batch.n_tokens = 64
slot init_sampler: id  3 | task 1338 | init sampler, took 0.22 ms, tokens: text = 953, total = 953
slot update_slots: id  3 | task 1338 | created context checkpoint 3 of 8 (pos_min = 0, pos_max = 888, size = 20.846 MiB)
slot print_timing: id  3 | task 1338 | 
prompt eval time =     629.97 ms /   494 tokens (    1.28 ms per token,   784.16 tokens per second)
       eval time =   27589.34 ms /  1152 tokens (   23.95 ms per token,    41.76 tokens per second)
      total time =   28219.32 ms /  1646 tokens
slot      release: id  3 | task 1338 | stop processing: n_tokens = 2104, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.423 (> 0.100 thold), f_keep = 0.440
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2104, total state size = 73.349 MiB
srv          load:  - looking for better prompt, base f_keep = 0.440, sim = 0.423
srv        update:  - cache state: 2 prompts, 245.584 MiB (limits: 8192.000 MiB, 64000 tokens, 159247 est)
srv        update:    - prompt 0x56661feaa980:    2670 tokens, checkpoints:  3,   131.011 MiB
srv        update:    - prompt 0x56662004b180:    2104 tokens, checkpoints:  3,   114.573 MiB
srv  get_availabl: prompt cache update took 133.10 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2492 | processing task, is_child = 0
slot update_slots: id  3 | task 2492 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2186
slot update_slots: id  3 | task 2492 | n_past = 925, slot.prompt.tokens.size() = 2104, seq_id = 3, pos_min = 1080, n_swa = 128
slot update_slots: id  3 | task 2492 | restored context checkpoint (pos_min = 0, pos_max = 888, size = 20.846 MiB)
slot update_slots: id  3 | task 2492 | n_tokens = 888, memory_seq_rm [888, end)
slot update_slots: id  3 | task 2492 | prompt processing progress, n_tokens = 2122, batch.n_tokens = 1234, progress = 0.970723
slot update_slots: id  3 | task 2492 | n_tokens = 2122, memory_seq_rm [2122, end)
slot update_slots: id  3 | task 2492 | prompt processing progress, n_tokens = 2186, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2492 | prompt done, n_tokens = 2186, batch.n_tokens = 64
slot init_sampler: id  3 | task 2492 | init sampler, took 0.45 ms, tokens: text = 2186, total = 2186
slot update_slots: id  3 | task 2492 | created context checkpoint 4 of 8 (pos_min = 1098, pos_max = 2121, size = 24.012 MiB)
slot print_timing: id  3 | task 2492 | 
prompt eval time =    1447.28 ms /  1298 tokens (    1.12 ms per token,   896.86 tokens per second)
       eval time =   65205.70 ms /  2633 tokens (   24.76 ms per token,    40.38 tokens per second)
      total time =   66652.98 ms /  3931 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 2492 | stop processing: n_tokens = 4818, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.428 (> 0.100 thold), f_keep = 0.252
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4818, total state size = 136.989 MiB
srv          load:  - looking for better prompt, base f_keep = 0.252, sim = 0.428
srv        update:  - cache state: 3 prompts, 447.809 MiB (limits: 8192.000 MiB, 64000 tokens, 175471 est)
srv        update:    - prompt 0x56661feaa980:    2670 tokens, checkpoints:  3,   131.011 MiB
srv        update:    - prompt 0x56662004b180:    2104 tokens, checkpoints:  3,   114.573 MiB
srv        update:    - prompt 0x56660efe3f60:    4818 tokens, checkpoints:  4,   202.225 MiB
srv  get_availabl: prompt cache update took 197.76 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5127 | processing task, is_child = 0
slot update_slots: id  3 | task 5127 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2841
slot update_slots: id  3 | task 5127 | n_past = 1216, slot.prompt.tokens.size() = 4818, seq_id = 3, pos_min = 3794, n_swa = 128
slot update_slots: id  3 | task 5127 | restored context checkpoint (pos_min = 0, pos_max = 888, size = 20.846 MiB)
slot update_slots: id  3 | task 5127 | erased invalidated context checkpoint (pos_min = 1098, pos_max = 2121, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5127 | n_tokens = 888, memory_seq_rm [888, end)
slot update_slots: id  3 | task 5127 | prompt processing progress, n_tokens = 2777, batch.n_tokens = 1889, progress = 0.977473
slot update_slots: id  3 | task 5127 | n_tokens = 2777, memory_seq_rm [2777, end)
slot update_slots: id  3 | task 5127 | prompt processing progress, n_tokens = 2841, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5127 | prompt done, n_tokens = 2841, batch.n_tokens = 64
slot init_sampler: id  3 | task 5127 | init sampler, took 0.60 ms, tokens: text = 2841, total = 2841
slot update_slots: id  3 | task 5127 | created context checkpoint 4 of 8 (pos_min = 1753, pos_max = 2776, size = 24.012 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 5127
slot      release: id  3 | task 5127 | stop processing: n_tokens = 3798, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.890 (> 0.100 thold), f_keep = 0.121
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3798, total state size = 113.071 MiB
srv          load:  - looking for better prompt, base f_keep = 0.121, sim = 0.890
srv        update:  - cache state: 4 prompts, 626.117 MiB (limits: 8192.000 MiB, 64000 tokens, 175192 est)
srv        update:    - prompt 0x56661feaa980:    2670 tokens, checkpoints:  3,   131.011 MiB
srv        update:    - prompt 0x56662004b180:    2104 tokens, checkpoints:  3,   114.573 MiB
srv        update:    - prompt 0x56660efe3f60:    4818 tokens, checkpoints:  4,   202.225 MiB
srv        update:    - prompt 0x5666200df900:    3798 tokens, checkpoints:  4,   178.308 MiB
srv  get_availabl: prompt cache update took 152.68 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6088 | processing task, is_child = 0
slot update_slots: id  3 | task 6088 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 516
slot update_slots: id  3 | task 6088 | n_past = 459, slot.prompt.tokens.size() = 3798, seq_id = 3, pos_min = 2774, n_swa = 128
slot update_slots: id  3 | task 6088 | restored context checkpoint (pos_min = 0, pos_max = 888, size = 20.846 MiB)
slot update_slots: id  3 | task 6088 | erased invalidated context checkpoint (pos_min = 1753, pos_max = 2776, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 6088 | n_tokens = 459, memory_seq_rm [459, end)
slot update_slots: id  3 | task 6088 | prompt processing progress, n_tokens = 516, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 6088 | prompt done, n_tokens = 516, batch.n_tokens = 57
slot init_sampler: id  3 | task 6088 | init sampler, took 0.12 ms, tokens: text = 516, total = 516
slot print_timing: id  3 | task 6088 | 
prompt eval time =     267.95 ms /    57 tokens (    4.70 ms per token,   212.73 tokens per second)
       eval time =   31539.87 ms /  1301 tokens (   24.24 ms per token,    41.25 tokens per second)
      total time =   31807.82 ms /  1358 tokens
slot      release: id  3 | task 6088 | stop processing: n_tokens = 1816, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.377 (> 0.100 thold), f_keep = 0.253
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1816, total state size = 66.596 MiB
srv          load:  - looking for better prompt, base f_keep = 0.253, sim = 0.377
srv        update:  - cache state: 5 prompts, 733.936 MiB (limits: 8192.000 MiB, 64000 tokens, 169725 est)
srv        update:    - prompt 0x56661feaa980:    2670 tokens, checkpoints:  3,   131.011 MiB
srv        update:    - prompt 0x56662004b180:    2104 tokens, checkpoints:  3,   114.573 MiB
srv        update:    - prompt 0x56660efe3f60:    4818 tokens, checkpoints:  4,   202.225 MiB
srv        update:    - prompt 0x5666200df900:    3798 tokens, checkpoints:  4,   178.308 MiB
srv        update:    - prompt 0x56661fefda00:    1816 tokens, checkpoints:  3,   107.820 MiB
srv  get_availabl: prompt cache update took 81.05 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7390 | processing task, is_child = 0
slot update_slots: id  3 | task 7390 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1217
slot update_slots: id  3 | task 7390 | n_past = 459, slot.prompt.tokens.size() = 1816, seq_id = 3, pos_min = 792, n_swa = 128
slot update_slots: id  3 | task 7390 | restored context checkpoint (pos_min = 0, pos_max = 888, size = 20.846 MiB)
slot update_slots: id  3 | task 7390 | n_tokens = 459, memory_seq_rm [459, end)
slot update_slots: id  3 | task 7390 | prompt processing progress, n_tokens = 1153, batch.n_tokens = 694, progress = 0.947412
slot update_slots: id  3 | task 7390 | n_tokens = 1153, memory_seq_rm [1153, end)
slot update_slots: id  3 | task 7390 | prompt processing progress, n_tokens = 1217, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7390 | prompt done, n_tokens = 1217, batch.n_tokens = 64
slot init_sampler: id  3 | task 7390 | init sampler, took 0.26 ms, tokens: text = 1217, total = 1217
slot update_slots: id  3 | task 7390 | created context checkpoint 4 of 8 (pos_min = 129, pos_max = 1152, size = 24.012 MiB)
slot print_timing: id  3 | task 7390 | 
prompt eval time =     951.78 ms /   758 tokens (    1.26 ms per token,   796.40 tokens per second)
       eval time =   13299.42 ms /   554 tokens (   24.01 ms per token,    41.66 tokens per second)
      total time =   14251.20 ms /  1312 tokens
slot      release: id  3 | task 7390 | stop processing: n_tokens = 1770, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.259
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1770, total state size = 65.517 MiB
srv          load:  - looking for better prompt, base f_keep = 0.259, sim = 0.933
srv        update:  - cache state: 6 prompts, 864.689 MiB (limits: 8192.000 MiB, 64000 tokens, 160829 est)
srv        update:    - prompt 0x56661feaa980:    2670 tokens, checkpoints:  3,   131.011 MiB
srv        update:    - prompt 0x56662004b180:    2104 tokens, checkpoints:  3,   114.573 MiB
srv        update:    - prompt 0x56660efe3f60:    4818 tokens, checkpoints:  4,   202.225 MiB
srv        update:    - prompt 0x5666200df900:    3798 tokens, checkpoints:  4,   178.308 MiB
srv        update:    - prompt 0x56661fefda00:    1816 tokens, checkpoints:  3,   107.820 MiB
srv        update:    - prompt 0x56661fe45450:    1770 tokens, checkpoints:  4,   130.753 MiB
srv  get_availabl: prompt cache update took 149.50 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7946 | processing task, is_child = 0
slot update_slots: id  3 | task 7946 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 492
slot update_slots: id  3 | task 7946 | n_past = 459, slot.prompt.tokens.size() = 1770, seq_id = 3, pos_min = 746, n_swa = 128
slot update_slots: id  3 | task 7946 | restored context checkpoint (pos_min = 129, pos_max = 1152, size = 24.012 MiB)
slot update_slots: id  3 | task 7946 | n_tokens = 459, memory_seq_rm [459, end)
slot update_slots: id  3 | task 7946 | prompt processing progress, n_tokens = 492, batch.n_tokens = 33, progress = 1.000000
slot update_slots: id  3 | task 7946 | prompt done, n_tokens = 492, batch.n_tokens = 33
slot init_sampler: id  3 | task 7946 | init sampler, took 0.10 ms, tokens: text = 492, total = 492
slot print_timing: id  3 | task 7946 | 
prompt eval time =     206.91 ms /    33 tokens (    6.27 ms per token,   159.49 tokens per second)
       eval time =    1654.84 ms /    72 tokens (   22.98 ms per token,    43.51 tokens per second)
      total time =    1861.75 ms /   105 tokens
slot      release: id  3 | task 7946 | stop processing: n_tokens = 563, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.483 (> 0.100 thold), f_keep = 0.874
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8019 | processing task, is_child = 0
slot update_slots: id  3 | task 8019 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1019
slot update_slots: id  3 | task 8019 | n_tokens = 492, memory_seq_rm [492, end)
slot update_slots: id  3 | task 8019 | prompt processing progress, n_tokens = 955, batch.n_tokens = 463, progress = 0.937193
slot update_slots: id  3 | task 8019 | n_tokens = 955, memory_seq_rm [955, end)
slot update_slots: id  3 | task 8019 | prompt processing progress, n_tokens = 1019, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8019 | prompt done, n_tokens = 1019, batch.n_tokens = 64
slot init_sampler: id  3 | task 8019 | init sampler, took 0.21 ms, tokens: text = 1019, total = 1019
slot print_timing: id  3 | task 8019 | 
prompt eval time =     561.82 ms /   527 tokens (    1.07 ms per token,   938.03 tokens per second)
       eval time =    1216.59 ms /    51 tokens (   23.85 ms per token,    41.92 tokens per second)
      total time =    1778.40 ms /   578 tokens
slot      release: id  3 | task 8019 | stop processing: n_tokens = 1069, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.323 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8072 | processing task, is_child = 0
slot update_slots: id  3 | task 8072 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3157
slot update_slots: id  3 | task 8072 | n_tokens = 1019, memory_seq_rm [1019, end)
slot update_slots: id  3 | task 8072 | prompt processing progress, n_tokens = 3067, batch.n_tokens = 2048, progress = 0.971492
slot update_slots: id  3 | task 8072 | n_tokens = 3067, memory_seq_rm [3067, end)
slot update_slots: id  3 | task 8072 | prompt processing progress, n_tokens = 3093, batch.n_tokens = 26, progress = 0.979728
slot update_slots: id  3 | task 8072 | n_tokens = 3093, memory_seq_rm [3093, end)
slot update_slots: id  3 | task 8072 | prompt processing progress, n_tokens = 3157, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8072 | prompt done, n_tokens = 3157, batch.n_tokens = 64
slot init_sampler: id  3 | task 8072 | init sampler, took 0.51 ms, tokens: text = 3157, total = 3157
slot update_slots: id  3 | task 8072 | created context checkpoint 5 of 8 (pos_min = 2069, pos_max = 3092, size = 24.012 MiB)
slot print_timing: id  3 | task 8072 | 
prompt eval time =    2157.89 ms /  2138 tokens (    1.01 ms per token,   990.78 tokens per second)
       eval time =    1067.21 ms /    45 tokens (   23.72 ms per token,    42.17 tokens per second)
      total time =    3225.10 ms /  2183 tokens
slot      release: id  3 | task 8072 | stop processing: n_tokens = 3201, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.709 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8120 | processing task, is_child = 0
slot update_slots: id  3 | task 8120 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4453
slot update_slots: id  3 | task 8120 | n_tokens = 3157, memory_seq_rm [3157, end)
slot update_slots: id  3 | task 8120 | prompt processing progress, n_tokens = 4389, batch.n_tokens = 1232, progress = 0.985628
slot update_slots: id  3 | task 8120 | n_tokens = 4389, memory_seq_rm [4389, end)
slot update_slots: id  3 | task 8120 | prompt processing progress, n_tokens = 4453, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8120 | prompt done, n_tokens = 4453, batch.n_tokens = 64
slot init_sampler: id  3 | task 8120 | init sampler, took 0.88 ms, tokens: text = 4453, total = 4453
slot update_slots: id  3 | task 8120 | created context checkpoint 6 of 8 (pos_min = 3365, pos_max = 4388, size = 24.012 MiB)
slot print_timing: id  3 | task 8120 | 
prompt eval time =    1452.86 ms /  1296 tokens (    1.12 ms per token,   892.04 tokens per second)
       eval time =    2243.71 ms /    95 tokens (   23.62 ms per token,    42.34 tokens per second)
      total time =    3696.56 ms /  1391 tokens
slot      release: id  3 | task 8120 | stop processing: n_tokens = 4547, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.646 (> 0.100 thold), f_keep = 0.101
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4547, total state size = 130.635 MiB
srv          load:  - looking for better prompt, base f_keep = 0.101, sim = 0.646
srv          load:  - found better prompt with f_keep = 0.260, sim = 0.647
srv        update:  - cache state: 6 prompts, 977.831 MiB (limits: 8192.000 MiB, 64000 tokens, 165485 est)
srv        update:    - prompt 0x56661feaa980:    2670 tokens, checkpoints:  3,   131.011 MiB
srv        update:    - prompt 0x56662004b180:    2104 tokens, checkpoints:  3,   114.573 MiB
srv        update:    - prompt 0x56660efe3f60:    4818 tokens, checkpoints:  4,   202.225 MiB
srv        update:    - prompt 0x5666200df900:    3798 tokens, checkpoints:  4,   178.308 MiB
srv        update:    - prompt 0x56661fefda00:    1816 tokens, checkpoints:  3,   107.820 MiB
srv        update:    - prompt 0x56661922e270:    4547 tokens, checkpoints:  6,   243.895 MiB
srv  get_availabl: prompt cache update took 278.05 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8217 | processing task, is_child = 0
slot update_slots: id  3 | task 8217 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 711
slot update_slots: id  3 | task 8217 | n_past = 460, slot.prompt.tokens.size() = 1770, seq_id = 3, pos_min = 746, n_swa = 128
slot update_slots: id  3 | task 8217 | restored context checkpoint (pos_min = 129, pos_max = 1152, size = 24.012 MiB)
slot update_slots: id  3 | task 8217 | n_tokens = 460, memory_seq_rm [460, end)
slot update_slots: id  3 | task 8217 | prompt processing progress, n_tokens = 647, batch.n_tokens = 187, progress = 0.909986
slot update_slots: id  3 | task 8217 | n_tokens = 647, memory_seq_rm [647, end)
slot update_slots: id  3 | task 8217 | prompt processing progress, n_tokens = 711, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8217 | prompt done, n_tokens = 711, batch.n_tokens = 64
slot init_sampler: id  3 | task 8217 | init sampler, took 0.16 ms, tokens: text = 711, total = 711
slot print_timing: id  3 | task 8217 | 
prompt eval time =     516.97 ms /   251 tokens (    2.06 ms per token,   485.52 tokens per second)
       eval time =    7333.43 ms /   314 tokens (   23.35 ms per token,    42.82 tokens per second)
      total time =    7850.40 ms /   565 tokens
slot      release: id  3 | task 8217 | stop processing: n_tokens = 1024, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.804 (> 0.100 thold), f_keep = 0.605
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8533 | processing task, is_child = 0
slot update_slots: id  3 | task 8533 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 771
slot update_slots: id  3 | task 8533 | n_tokens = 620, memory_seq_rm [620, end)
slot update_slots: id  3 | task 8533 | prompt processing progress, n_tokens = 707, batch.n_tokens = 87, progress = 0.916991
slot update_slots: id  3 | task 8533 | n_tokens = 707, memory_seq_rm [707, end)
slot update_slots: id  3 | task 8533 | prompt processing progress, n_tokens = 771, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8533 | prompt done, n_tokens = 771, batch.n_tokens = 64
slot init_sampler: id  3 | task 8533 | init sampler, took 0.16 ms, tokens: text = 771, total = 771
slot print_timing: id  3 | task 8533 | 
prompt eval time =     450.55 ms /   151 tokens (    2.98 ms per token,   335.15 tokens per second)
       eval time =    1221.14 ms /    53 tokens (   23.04 ms per token,    43.40 tokens per second)
      total time =    1671.69 ms /   204 tokens
slot      release: id  3 | task 8533 | stop processing: n_tokens = 823, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.265 (> 0.100 thold), f_keep = 0.937
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8588 | processing task, is_child = 0
slot update_slots: id  3 | task 8588 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2909
slot update_slots: id  3 | task 8588 | n_tokens = 771, memory_seq_rm [771, end)
slot update_slots: id  3 | task 8588 | prompt processing progress, n_tokens = 2819, batch.n_tokens = 2048, progress = 0.969062
slot update_slots: id  3 | task 8588 | n_tokens = 2819, memory_seq_rm [2819, end)
slot update_slots: id  3 | task 8588 | prompt processing progress, n_tokens = 2845, batch.n_tokens = 26, progress = 0.977999
slot update_slots: id  3 | task 8588 | n_tokens = 2845, memory_seq_rm [2845, end)
slot update_slots: id  3 | task 8588 | prompt processing progress, n_tokens = 2909, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8588 | prompt done, n_tokens = 2909, batch.n_tokens = 64
slot init_sampler: id  3 | task 8588 | init sampler, took 0.59 ms, tokens: text = 2909, total = 2909
slot update_slots: id  3 | task 8588 | created context checkpoint 5 of 8 (pos_min = 1821, pos_max = 2844, size = 24.012 MiB)
slot print_timing: id  3 | task 8588 | 
prompt eval time =    2186.69 ms /  2138 tokens (    1.02 ms per token,   977.73 tokens per second)
       eval time =    2806.33 ms /   116 tokens (   24.19 ms per token,    41.34 tokens per second)
      total time =    4993.02 ms /  2254 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 8588 | stop processing: n_tokens = 3024, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8707 | processing task, is_child = 0
slot update_slots: id  3 | task 8707 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2998
slot update_slots: id  3 | task 8707 | n_tokens = 2909, memory_seq_rm [2909, end)
slot update_slots: id  3 | task 8707 | prompt processing progress, n_tokens = 2934, batch.n_tokens = 25, progress = 0.978652
slot update_slots: id  3 | task 8707 | n_tokens = 2934, memory_seq_rm [2934, end)
slot update_slots: id  3 | task 8707 | prompt processing progress, n_tokens = 2998, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8707 | prompt done, n_tokens = 2998, batch.n_tokens = 64
slot init_sampler: id  3 | task 8707 | init sampler, took 0.90 ms, tokens: text = 2998, total = 2998
slot update_slots: id  3 | task 8707 | created context checkpoint 6 of 8 (pos_min = 2000, pos_max = 2933, size = 21.902 MiB)
slot print_timing: id  3 | task 8707 | 
prompt eval time =     270.00 ms /    89 tokens (    3.03 ms per token,   329.63 tokens per second)
       eval time =    1102.98 ms /    45 tokens (   24.51 ms per token,    40.80 tokens per second)
      total time =    1372.98 ms /   134 tokens
slot      release: id  3 | task 8707 | stop processing: n_tokens = 3042, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.822 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8754 | processing task, is_child = 0
slot update_slots: id  3 | task 8754 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3648
slot update_slots: id  3 | task 8754 | n_tokens = 2998, memory_seq_rm [2998, end)
slot update_slots: id  3 | task 8754 | prompt processing progress, n_tokens = 3584, batch.n_tokens = 586, progress = 0.982456
slot update_slots: id  3 | task 8754 | n_tokens = 3584, memory_seq_rm [3584, end)
slot update_slots: id  3 | task 8754 | prompt processing progress, n_tokens = 3648, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8754 | prompt done, n_tokens = 3648, batch.n_tokens = 64
slot init_sampler: id  3 | task 8754 | init sampler, took 0.68 ms, tokens: text = 3648, total = 3648
slot update_slots: id  3 | task 8754 | created context checkpoint 7 of 8 (pos_min = 2560, pos_max = 3583, size = 24.012 MiB)
slot print_timing: id  3 | task 8754 | 
prompt eval time =     842.23 ms /   650 tokens (    1.30 ms per token,   771.76 tokens per second)
       eval time =   13023.15 ms /   533 tokens (   24.43 ms per token,    40.93 tokens per second)
      total time =   13865.38 ms /  1183 tokens
slot      release: id  3 | task 8754 | stop processing: n_tokens = 4180, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.874 (> 0.100 thold), f_keep = 0.873
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9289 | processing task, is_child = 0
slot update_slots: id  3 | task 9289 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4175
slot update_slots: id  3 | task 9289 | n_tokens = 3648, memory_seq_rm [3648, end)
slot update_slots: id  3 | task 9289 | prompt processing progress, n_tokens = 4111, batch.n_tokens = 463, progress = 0.984671
slot update_slots: id  3 | task 9289 | n_tokens = 4111, memory_seq_rm [4111, end)
slot update_slots: id  3 | task 9289 | prompt processing progress, n_tokens = 4175, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9289 | prompt done, n_tokens = 4175, batch.n_tokens = 64
slot init_sampler: id  3 | task 9289 | init sampler, took 2.91 ms, tokens: text = 4175, total = 4175
slot update_slots: id  3 | task 9289 | created context checkpoint 8 of 8 (pos_min = 3156, pos_max = 4110, size = 22.394 MiB)
slot print_timing: id  3 | task 9289 | 
prompt eval time =     623.22 ms /   527 tokens (    1.18 ms per token,   845.62 tokens per second)
       eval time =    1637.13 ms /    64 tokens (   25.58 ms per token,    39.09 tokens per second)
      total time =    2260.35 ms /   591 tokens
slot      release: id  3 | task 9289 | stop processing: n_tokens = 4238, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.785 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9355 | processing task, is_child = 0
slot update_slots: id  3 | task 9355 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5320
slot update_slots: id  3 | task 9355 | n_tokens = 4175, memory_seq_rm [4175, end)
slot update_slots: id  3 | task 9355 | prompt processing progress, n_tokens = 5256, batch.n_tokens = 1081, progress = 0.987970
slot update_slots: id  3 | task 9355 | n_tokens = 5256, memory_seq_rm [5256, end)
slot update_slots: id  3 | task 9355 | prompt processing progress, n_tokens = 5320, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9355 | prompt done, n_tokens = 5320, batch.n_tokens = 64
slot init_sampler: id  3 | task 9355 | init sampler, took 1.26 ms, tokens: text = 5320, total = 5320
slot update_slots: id  3 | task 9355 | erasing old context checkpoint (pos_min = 0, pos_max = 401, size = 9.427 MiB)
slot update_slots: id  3 | task 9355 | created context checkpoint 8 of 8 (pos_min = 4232, pos_max = 5255, size = 24.012 MiB)
slot print_timing: id  3 | task 9355 | 
prompt eval time =    1422.52 ms /  1145 tokens (    1.24 ms per token,   804.91 tokens per second)
       eval time =    2086.09 ms /    81 tokens (   25.75 ms per token,    38.83 tokens per second)
      total time =    3508.61 ms /  1226 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 9355 | stop processing: n_tokens = 5400, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.889 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9438 | processing task, is_child = 0
slot update_slots: id  3 | task 9438 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5984
slot update_slots: id  3 | task 9438 | n_tokens = 5320, memory_seq_rm [5320, end)
slot update_slots: id  3 | task 9438 | prompt processing progress, n_tokens = 5920, batch.n_tokens = 600, progress = 0.989305
slot update_slots: id  3 | task 9438 | n_tokens = 5920, memory_seq_rm [5920, end)
slot update_slots: id  3 | task 9438 | prompt processing progress, n_tokens = 5984, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9438 | prompt done, n_tokens = 5984, batch.n_tokens = 64
slot init_sampler: id  3 | task 9438 | init sampler, took 1.34 ms, tokens: text = 5984, total = 5984
slot update_slots: id  3 | task 9438 | erasing old context checkpoint (pos_min = 0, pos_max = 466, size = 10.951 MiB)
slot update_slots: id  3 | task 9438 | created context checkpoint 8 of 8 (pos_min = 4896, pos_max = 5919, size = 24.012 MiB)
slot print_timing: id  3 | task 9438 | 
prompt eval time =     995.85 ms /   664 tokens (    1.50 ms per token,   666.76 tokens per second)
       eval time =  199261.92 ms /  7689 tokens (   25.92 ms per token,    38.59 tokens per second)
      total time =  200257.78 ms /  8353 tokens
slot      release: id  3 | task 9438 | stop processing: n_tokens = 13672, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.034
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 13672, total state size = 344.606 MiB
srv          load:  - looking for better prompt, base f_keep = 0.034, sim = 0.985
srv          load:  - found better prompt with f_keep = 0.254, sim = 0.989
srv        update:  - cache state: 6 prompts, 1399.820 MiB (limits: 8192.000 MiB, 64000 tokens, 184981 est)
srv        update:    - prompt 0x56661feaa980:    2670 tokens, checkpoints:  3,   131.011 MiB
srv        update:    - prompt 0x56662004b180:    2104 tokens, checkpoints:  3,   114.573 MiB
srv        update:    - prompt 0x56660efe3f60:    4818 tokens, checkpoints:  4,   202.225 MiB
srv        update:    - prompt 0x5666200df900:    3798 tokens, checkpoints:  4,   178.308 MiB
srv        update:    - prompt 0x56661922e270:    4547 tokens, checkpoints:  6,   243.895 MiB
srv        update:    - prompt 0x56661fe45450:   13672 tokens, checkpoints:  8,   529.808 MiB
srv  get_availabl: prompt cache update took 557.47 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17129 | processing task, is_child = 0
slot update_slots: id  3 | task 17129 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 466
slot update_slots: id  3 | task 17129 | n_past = 461, slot.prompt.tokens.size() = 1816, seq_id = 3, pos_min = 792, n_swa = 128
slot update_slots: id  3 | task 17129 | restored context checkpoint (pos_min = 0, pos_max = 888, size = 20.846 MiB)
slot update_slots: id  3 | task 17129 | n_tokens = 461, memory_seq_rm [461, end)
slot update_slots: id  3 | task 17129 | prompt processing progress, n_tokens = 466, batch.n_tokens = 5, progress = 1.000000
slot update_slots: id  3 | task 17129 | prompt done, n_tokens = 466, batch.n_tokens = 5
slot init_sampler: id  3 | task 17129 | init sampler, took 0.11 ms, tokens: text = 466, total = 466
slot print_timing: id  3 | task 17129 | 
prompt eval time =     129.23 ms /     5 tokens (   25.85 ms per token,    38.69 tokens per second)
       eval time =     707.68 ms /    25 tokens (   28.31 ms per token,    35.33 tokens per second)
      total time =     836.92 ms /    30 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 17129 | stop processing: n_tokens = 490, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.878 (> 0.100 thold), f_keep = 0.951
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17155 | processing task, is_child = 0
slot update_slots: id  3 | task 17155 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 531
slot update_slots: id  3 | task 17155 | n_tokens = 466, memory_seq_rm [466, end)
slot update_slots: id  3 | task 17155 | prompt processing progress, n_tokens = 467, batch.n_tokens = 1, progress = 0.879473
slot update_slots: id  3 | task 17155 | n_tokens = 467, memory_seq_rm [467, end)
slot update_slots: id  3 | task 17155 | prompt processing progress, n_tokens = 531, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17155 | prompt done, n_tokens = 531, batch.n_tokens = 64
slot init_sampler: id  3 | task 17155 | init sampler, took 0.13 ms, tokens: text = 531, total = 531
slot print_timing: id  3 | task 17155 | 
prompt eval time =     251.84 ms /    65 tokens (    3.87 ms per token,   258.11 tokens per second)
       eval time =    1279.83 ms /    52 tokens (   24.61 ms per token,    40.63 tokens per second)
      total time =    1531.67 ms /   117 tokens
slot      release: id  3 | task 17155 | stop processing: n_tokens = 582, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.488 (> 0.100 thold), f_keep = 0.912
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17209 | processing task, is_child = 0
slot update_slots: id  3 | task 17209 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1089
slot update_slots: id  3 | task 17209 | n_tokens = 531, memory_seq_rm [531, end)
slot update_slots: id  3 | task 17209 | prompt processing progress, n_tokens = 1025, batch.n_tokens = 494, progress = 0.941230
slot update_slots: id  3 | task 17209 | n_tokens = 1025, memory_seq_rm [1025, end)
slot update_slots: id  3 | task 17209 | prompt processing progress, n_tokens = 1089, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17209 | prompt done, n_tokens = 1089, batch.n_tokens = 64
slot init_sampler: id  3 | task 17209 | init sampler, took 0.21 ms, tokens: text = 1089, total = 1089
slot update_slots: id  3 | task 17209 | created context checkpoint 4 of 8 (pos_min = 1, pos_max = 1024, size = 24.012 MiB)
slot print_timing: id  3 | task 17209 | 
prompt eval time =     627.87 ms /   558 tokens (    1.13 ms per token,   888.72 tokens per second)
       eval time =    2715.84 ms /   106 tokens (   25.62 ms per token,    39.03 tokens per second)
      total time =    3343.71 ms /   664 tokens
slot      release: id  3 | task 17209 | stop processing: n_tokens = 1194, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.562 (> 0.100 thold), f_keep = 0.912
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17317 | processing task, is_child = 0
slot update_slots: id  3 | task 17317 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1939
slot update_slots: id  3 | task 17317 | n_tokens = 1089, memory_seq_rm [1089, end)
slot update_slots: id  3 | task 17317 | prompt processing progress, n_tokens = 1875, batch.n_tokens = 786, progress = 0.966993
slot update_slots: id  3 | task 17317 | n_tokens = 1875, memory_seq_rm [1875, end)
slot update_slots: id  3 | task 17317 | prompt processing progress, n_tokens = 1939, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17317 | prompt done, n_tokens = 1939, batch.n_tokens = 64
slot init_sampler: id  3 | task 17317 | init sampler, took 0.38 ms, tokens: text = 1939, total = 1939
slot update_slots: id  3 | task 17317 | created context checkpoint 5 of 8 (pos_min = 851, pos_max = 1874, size = 24.012 MiB)
slot print_timing: id  3 | task 17317 | 
prompt eval time =    1041.91 ms /   850 tokens (    1.23 ms per token,   815.81 tokens per second)
       eval time =    1811.83 ms /    70 tokens (   25.88 ms per token,    38.63 tokens per second)
      total time =    2853.75 ms /   920 tokens
slot      release: id  3 | task 17317 | stop processing: n_tokens = 2008, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17389 | processing task, is_child = 0
slot update_slots: id  3 | task 17389 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2018
slot update_slots: id  3 | task 17389 | n_tokens = 1939, memory_seq_rm [1939, end)
slot update_slots: id  3 | task 17389 | prompt processing progress, n_tokens = 1954, batch.n_tokens = 15, progress = 0.968285
slot update_slots: id  3 | task 17389 | n_tokens = 1954, memory_seq_rm [1954, end)
slot update_slots: id  3 | task 17389 | prompt processing progress, n_tokens = 2018, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17389 | prompt done, n_tokens = 2018, batch.n_tokens = 64
slot init_sampler: id  3 | task 17389 | init sampler, took 0.48 ms, tokens: text = 2018, total = 2018
slot update_slots: id  3 | task 17389 | created context checkpoint 6 of 8 (pos_min = 984, pos_max = 1953, size = 22.746 MiB)
slot print_timing: id  3 | task 17389 | 
prompt eval time =     245.47 ms /    79 tokens (    3.11 ms per token,   321.83 tokens per second)
       eval time =    2503.96 ms /    76 tokens (   32.95 ms per token,    30.35 tokens per second)
      total time =    2749.43 ms /   155 tokens
slot      release: id  3 | task 17389 | stop processing: n_tokens = 2093, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17467 | processing task, is_child = 0
slot update_slots: id  3 | task 17467 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2097
slot update_slots: id  3 | task 17467 | n_tokens = 2018, memory_seq_rm [2018, end)
slot update_slots: id  3 | task 17467 | prompt processing progress, n_tokens = 2033, batch.n_tokens = 15, progress = 0.969480
slot update_slots: id  3 | task 17467 | n_tokens = 2033, memory_seq_rm [2033, end)
slot update_slots: id  3 | task 17467 | prompt processing progress, n_tokens = 2097, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17467 | prompt done, n_tokens = 2097, batch.n_tokens = 64
slot init_sampler: id  3 | task 17467 | init sampler, took 0.42 ms, tokens: text = 2097, total = 2097
slot update_slots: id  3 | task 17467 | created context checkpoint 7 of 8 (pos_min = 1069, pos_max = 2032, size = 22.605 MiB)
slot print_timing: id  3 | task 17467 | 
prompt eval time =     337.00 ms /    79 tokens (    4.27 ms per token,   234.42 tokens per second)
       eval time =    1087.91 ms /    33 tokens (   32.97 ms per token,    30.33 tokens per second)
      total time =    1424.91 ms /   112 tokens
slot      release: id  3 | task 17467 | stop processing: n_tokens = 2129, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17502 | processing task, is_child = 0
slot update_slots: id  3 | task 17502 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2182
slot update_slots: id  3 | task 17502 | n_tokens = 2097, memory_seq_rm [2097, end)
slot update_slots: id  3 | task 17502 | prompt processing progress, n_tokens = 2118, batch.n_tokens = 21, progress = 0.970669
slot update_slots: id  3 | task 17502 | n_tokens = 2118, memory_seq_rm [2118, end)
slot update_slots: id  3 | task 17502 | prompt processing progress, n_tokens = 2182, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17502 | prompt done, n_tokens = 2182, batch.n_tokens = 64
slot init_sampler: id  3 | task 17502 | init sampler, took 0.40 ms, tokens: text = 2182, total = 2182
slot update_slots: id  3 | task 17502 | created context checkpoint 8 of 8 (pos_min = 1135, pos_max = 2117, size = 23.051 MiB)
slot print_timing: id  3 | task 17502 | 
prompt eval time =     350.71 ms /    85 tokens (    4.13 ms per token,   242.36 tokens per second)
       eval time =    7220.99 ms /   252 tokens (   28.65 ms per token,    34.90 tokens per second)
      total time =    7571.71 ms /   337 tokens
slot      release: id  3 | task 17502 | stop processing: n_tokens = 2433, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.713 (> 0.100 thold), f_keep = 0.897
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17756 | processing task, is_child = 0
slot update_slots: id  3 | task 17756 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3060
slot update_slots: id  3 | task 17756 | n_tokens = 2182, memory_seq_rm [2182, end)
slot update_slots: id  3 | task 17756 | prompt processing progress, n_tokens = 2996, batch.n_tokens = 814, progress = 0.979085
slot update_slots: id  3 | task 17756 | n_tokens = 2996, memory_seq_rm [2996, end)
slot update_slots: id  3 | task 17756 | prompt processing progress, n_tokens = 3060, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17756 | prompt done, n_tokens = 3060, batch.n_tokens = 64
slot init_sampler: id  3 | task 17756 | init sampler, took 0.57 ms, tokens: text = 3060, total = 3060
slot update_slots: id  3 | task 17756 | erasing old context checkpoint (pos_min = 0, pos_max = 401, size = 9.427 MiB)
slot update_slots: id  3 | task 17756 | created context checkpoint 8 of 8 (pos_min = 2002, pos_max = 2995, size = 23.309 MiB)
slot print_timing: id  3 | task 17756 | 
prompt eval time =    1055.63 ms /   878 tokens (    1.20 ms per token,   831.73 tokens per second)
       eval time =    4922.70 ms /   196 tokens (   25.12 ms per token,    39.82 tokens per second)
      total time =    5978.34 ms /  1074 tokens
slot      release: id  3 | task 17756 | stop processing: n_tokens = 3255, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.940
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17954 | processing task, is_child = 0
slot update_slots: id  3 | task 17954 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3123
slot update_slots: id  3 | task 17954 | n_tokens = 3060, memory_seq_rm [3060, end)
slot update_slots: id  3 | task 17954 | prompt processing progress, n_tokens = 3123, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 17954 | prompt done, n_tokens = 3123, batch.n_tokens = 63
slot init_sampler: id  3 | task 17954 | init sampler, took 0.66 ms, tokens: text = 3123, total = 3123
slot print_timing: id  3 | task 17954 | 
prompt eval time =     259.31 ms /    63 tokens (    4.12 ms per token,   242.95 tokens per second)
       eval time =    2077.47 ms /    80 tokens (   25.97 ms per token,    38.51 tokens per second)
      total time =    2336.78 ms /   143 tokens
slot      release: id  3 | task 17954 | stop processing: n_tokens = 3202, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.857 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18035 | processing task, is_child = 0
slot update_slots: id  3 | task 18035 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3644
slot update_slots: id  3 | task 18035 | n_tokens = 3123, memory_seq_rm [3123, end)
slot update_slots: id  3 | task 18035 | prompt processing progress, n_tokens = 3580, batch.n_tokens = 457, progress = 0.982437
slot update_slots: id  3 | task 18035 | n_tokens = 3580, memory_seq_rm [3580, end)
slot update_slots: id  3 | task 18035 | prompt processing progress, n_tokens = 3644, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18035 | prompt done, n_tokens = 3644, batch.n_tokens = 64
slot init_sampler: id  3 | task 18035 | init sampler, took 0.84 ms, tokens: text = 3644, total = 3644
slot update_slots: id  3 | task 18035 | erasing old context checkpoint (pos_min = 0, pos_max = 466, size = 10.951 MiB)
slot update_slots: id  3 | task 18035 | created context checkpoint 8 of 8 (pos_min = 2556, pos_max = 3579, size = 24.012 MiB)
slot print_timing: id  3 | task 18035 | 
prompt eval time =     621.45 ms /   521 tokens (    1.19 ms per token,   838.36 tokens per second)
       eval time =    2518.61 ms /   104 tokens (   24.22 ms per token,    41.29 tokens per second)
      total time =    3140.06 ms /   625 tokens
slot      release: id  3 | task 18035 | stop processing: n_tokens = 3747, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18141 | processing task, is_child = 0
slot update_slots: id  3 | task 18141 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3701
slot update_slots: id  3 | task 18141 | n_tokens = 3644, memory_seq_rm [3644, end)
slot update_slots: id  3 | task 18141 | prompt processing progress, n_tokens = 3701, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 18141 | prompt done, n_tokens = 3701, batch.n_tokens = 57
slot init_sampler: id  3 | task 18141 | init sampler, took 0.68 ms, tokens: text = 3701, total = 3701
slot print_timing: id  3 | task 18141 | 
prompt eval time =     152.48 ms /    57 tokens (    2.68 ms per token,   373.81 tokens per second)
       eval time =    1436.51 ms /    59 tokens (   24.35 ms per token,    41.07 tokens per second)
      total time =    1588.99 ms /   116 tokens
slot      release: id  3 | task 18141 | stop processing: n_tokens = 3759, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.905 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18201 | processing task, is_child = 0
slot update_slots: id  3 | task 18201 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4091
slot update_slots: id  3 | task 18201 | n_tokens = 3701, memory_seq_rm [3701, end)
slot update_slots: id  3 | task 18201 | prompt processing progress, n_tokens = 4027, batch.n_tokens = 326, progress = 0.984356
slot update_slots: id  3 | task 18201 | n_tokens = 4027, memory_seq_rm [4027, end)
slot update_slots: id  3 | task 18201 | prompt processing progress, n_tokens = 4091, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18201 | prompt done, n_tokens = 4091, batch.n_tokens = 64
slot init_sampler: id  3 | task 18201 | init sampler, took 2.01 ms, tokens: text = 4091, total = 4091
slot update_slots: id  3 | task 18201 | erasing old context checkpoint (pos_min = 0, pos_max = 888, size = 20.846 MiB)
slot update_slots: id  3 | task 18201 | created context checkpoint 8 of 8 (pos_min = 3003, pos_max = 4026, size = 24.012 MiB)
slot print_timing: id  3 | task 18201 | 
prompt eval time =     512.49 ms /   390 tokens (    1.31 ms per token,   761.00 tokens per second)
       eval time =   23973.59 ms /   955 tokens (   25.10 ms per token,    39.84 tokens per second)
      total time =   24486.07 ms /  1345 tokens
slot      release: id  3 | task 18201 | stop processing: n_tokens = 5045, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.885 (> 0.100 thold), f_keep = 0.811
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19158 | processing task, is_child = 0
slot update_slots: id  3 | task 19158 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4620
slot update_slots: id  3 | task 19158 | n_past = 4091, slot.prompt.tokens.size() = 5045, seq_id = 3, pos_min = 4021, n_swa = 128
slot update_slots: id  3 | task 19158 | restored context checkpoint (pos_min = 3003, pos_max = 4026, size = 24.012 MiB)
slot update_slots: id  3 | task 19158 | n_tokens = 4026, memory_seq_rm [4026, end)
slot update_slots: id  3 | task 19158 | prompt processing progress, n_tokens = 4556, batch.n_tokens = 530, progress = 0.986147
slot update_slots: id  3 | task 19158 | n_tokens = 4556, memory_seq_rm [4556, end)
slot update_slots: id  3 | task 19158 | prompt processing progress, n_tokens = 4620, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 19158 | prompt done, n_tokens = 4620, batch.n_tokens = 64
slot init_sampler: id  3 | task 19158 | init sampler, took 1.13 ms, tokens: text = 4620, total = 4620
slot update_slots: id  3 | task 19158 | erasing old context checkpoint (pos_min = 1, pos_max = 1024, size = 24.012 MiB)
slot update_slots: id  3 | task 19158 | created context checkpoint 8 of 8 (pos_min = 3532, pos_max = 4555, size = 24.012 MiB)
slot print_timing: id  3 | task 19158 | 
prompt eval time =     791.98 ms /   594 tokens (    1.33 ms per token,   750.02 tokens per second)
       eval time =    1388.39 ms /    54 tokens (   25.71 ms per token,    38.89 tokens per second)
      total time =    2180.38 ms /   648 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 19158 | stop processing: n_tokens = 4673, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.845 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19214 | processing task, is_child = 0
slot update_slots: id  3 | task 19214 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5470
slot update_slots: id  3 | task 19214 | n_tokens = 4620, memory_seq_rm [4620, end)
slot update_slots: id  3 | task 19214 | prompt processing progress, n_tokens = 5406, batch.n_tokens = 786, progress = 0.988300
slot update_slots: id  3 | task 19214 | n_tokens = 5406, memory_seq_rm [5406, end)
slot update_slots: id  3 | task 19214 | prompt processing progress, n_tokens = 5470, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 19214 | prompt done, n_tokens = 5470, batch.n_tokens = 64
slot init_sampler: id  3 | task 19214 | init sampler, took 1.64 ms, tokens: text = 5470, total = 5470
slot update_slots: id  3 | task 19214 | erasing old context checkpoint (pos_min = 851, pos_max = 1874, size = 24.012 MiB)
slot update_slots: id  3 | task 19214 | created context checkpoint 8 of 8 (pos_min = 4382, pos_max = 5405, size = 24.012 MiB)
slot print_timing: id  3 | task 19214 | 
prompt eval time =    1040.28 ms /   850 tokens (    1.22 ms per token,   817.09 tokens per second)
       eval time =   13635.31 ms /   519 tokens (   26.27 ms per token,    38.06 tokens per second)
      total time =   14675.59 ms /  1369 tokens
slot      release: id  3 | task 19214 | stop processing: n_tokens = 5988, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.914 (> 0.100 thold), f_keep = 0.913
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19735 | processing task, is_child = 0
slot update_slots: id  3 | task 19735 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5986
slot update_slots: id  3 | task 19735 | n_tokens = 5470, memory_seq_rm [5470, end)
slot update_slots: id  3 | task 19735 | prompt processing progress, n_tokens = 5922, batch.n_tokens = 452, progress = 0.989308
slot update_slots: id  3 | task 19735 | n_tokens = 5922, memory_seq_rm [5922, end)
slot update_slots: id  3 | task 19735 | prompt processing progress, n_tokens = 5986, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 19735 | prompt done, n_tokens = 5986, batch.n_tokens = 64
slot init_sampler: id  3 | task 19735 | init sampler, took 1.32 ms, tokens: text = 5986, total = 5986
slot update_slots: id  3 | task 19735 | erasing old context checkpoint (pos_min = 984, pos_max = 1953, size = 22.746 MiB)
slot update_slots: id  3 | task 19735 | created context checkpoint 8 of 8 (pos_min = 4964, pos_max = 5921, size = 22.464 MiB)
slot print_timing: id  3 | task 19735 | 
prompt eval time =     667.29 ms /   516 tokens (    1.29 ms per token,   773.28 tokens per second)
       eval time =    1819.50 ms /    71 tokens (   25.63 ms per token,    39.02 tokens per second)
      total time =    2486.78 ms /   587 tokens
slot      release: id  3 | task 19735 | stop processing: n_tokens = 6056, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.832 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19808 | processing task, is_child = 0
slot update_slots: id  3 | task 19808 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7193
slot update_slots: id  3 | task 19808 | n_tokens = 5986, memory_seq_rm [5986, end)
slot update_slots: id  3 | task 19808 | prompt processing progress, n_tokens = 7129, batch.n_tokens = 1143, progress = 0.991102
slot update_slots: id  3 | task 19808 | n_tokens = 7129, memory_seq_rm [7129, end)
slot update_slots: id  3 | task 19808 | prompt processing progress, n_tokens = 7193, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 19808 | prompt done, n_tokens = 7193, batch.n_tokens = 64
slot init_sampler: id  3 | task 19808 | init sampler, took 1.76 ms, tokens: text = 7193, total = 7193
slot update_slots: id  3 | task 19808 | erasing old context checkpoint (pos_min = 1069, pos_max = 2032, size = 22.605 MiB)
slot update_slots: id  3 | task 19808 | created context checkpoint 8 of 8 (pos_min = 6105, pos_max = 7128, size = 24.012 MiB)
slot print_timing: id  3 | task 19808 | 
prompt eval time =    1515.19 ms /  1207 tokens (    1.26 ms per token,   796.60 tokens per second)
       eval time =   13752.42 ms /   532 tokens (   25.85 ms per token,    38.68 tokens per second)
      total time =   15267.62 ms /  1739 tokens
slot      release: id  3 | task 19808 | stop processing: n_tokens = 7724, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.931
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 20342 | processing task, is_child = 0
slot update_slots: id  3 | task 20342 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7709
slot update_slots: id  3 | task 20342 | n_tokens = 7193, memory_seq_rm [7193, end)
slot update_slots: id  3 | task 20342 | prompt processing progress, n_tokens = 7645, batch.n_tokens = 452, progress = 0.991698
slot update_slots: id  3 | task 20342 | n_tokens = 7645, memory_seq_rm [7645, end)
slot update_slots: id  3 | task 20342 | prompt processing progress, n_tokens = 7709, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 20342 | prompt done, n_tokens = 7709, batch.n_tokens = 64
slot init_sampler: id  3 | task 20342 | init sampler, took 1.72 ms, tokens: text = 7709, total = 7709
slot update_slots: id  3 | task 20342 | erasing old context checkpoint (pos_min = 1135, pos_max = 2117, size = 23.051 MiB)
slot update_slots: id  3 | task 20342 | created context checkpoint 8 of 8 (pos_min = 6700, pos_max = 7644, size = 22.160 MiB)
slot print_timing: id  3 | task 20342 | 
prompt eval time =     673.03 ms /   516 tokens (    1.30 ms per token,   766.68 tokens per second)
       eval time =    1640.17 ms /    64 tokens (   25.63 ms per token,    39.02 tokens per second)
      total time =    2313.20 ms /   580 tokens
slot      release: id  3 | task 20342 | stop processing: n_tokens = 7772, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 20408 | processing task, is_child = 0
slot update_slots: id  3 | task 20408 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7957
slot update_slots: id  3 | task 20408 | n_tokens = 7709, memory_seq_rm [7709, end)
slot update_slots: id  3 | task 20408 | prompt processing progress, n_tokens = 7893, batch.n_tokens = 184, progress = 0.991957
slot update_slots: id  3 | task 20408 | n_tokens = 7893, memory_seq_rm [7893, end)
slot update_slots: id  3 | task 20408 | prompt processing progress, n_tokens = 7957, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 20408 | prompt done, n_tokens = 7957, batch.n_tokens = 64
slot init_sampler: id  3 | task 20408 | init sampler, took 2.02 ms, tokens: text = 7957, total = 7957
slot update_slots: id  3 | task 20408 | erasing old context checkpoint (pos_min = 2002, pos_max = 2995, size = 23.309 MiB)
slot update_slots: id  3 | task 20408 | created context checkpoint 8 of 8 (pos_min = 6869, pos_max = 7892, size = 24.012 MiB)
slot print_timing: id  3 | task 20408 | 
prompt eval time =     448.38 ms /   248 tokens (    1.81 ms per token,   553.10 tokens per second)
       eval time =   13264.49 ms /   518 tokens (   25.61 ms per token,    39.05 tokens per second)
      total time =   13712.87 ms /   766 tokens
slot      release: id  3 | task 20408 | stop processing: n_tokens = 8474, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 20928 | processing task, is_child = 0
slot update_slots: id  3 | task 20928 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8473
slot update_slots: id  3 | task 20928 | n_tokens = 7957, memory_seq_rm [7957, end)
slot update_slots: id  3 | task 20928 | prompt processing progress, n_tokens = 8409, batch.n_tokens = 452, progress = 0.992447
slot update_slots: id  3 | task 20928 | n_tokens = 8409, memory_seq_rm [8409, end)
slot update_slots: id  3 | task 20928 | prompt processing progress, n_tokens = 8473, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 20928 | prompt done, n_tokens = 8473, batch.n_tokens = 64
slot init_sampler: id  3 | task 20928 | init sampler, took 1.52 ms, tokens: text = 8473, total = 8473
slot update_slots: id  3 | task 20928 | erasing old context checkpoint (pos_min = 2556, pos_max = 3579, size = 24.012 MiB)
slot update_slots: id  3 | task 20928 | created context checkpoint 8 of 8 (pos_min = 7590, pos_max = 8408, size = 19.205 MiB)
slot print_timing: id  3 | task 20928 | 
prompt eval time =     679.43 ms /   516 tokens (    1.32 ms per token,   759.46 tokens per second)
       eval time =    1535.34 ms /    60 tokens (   25.59 ms per token,    39.08 tokens per second)
      total time =    2214.77 ms /   576 tokens
slot      release: id  3 | task 20928 | stop processing: n_tokens = 8532, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 20990 | processing task, is_child = 0
slot update_slots: id  3 | task 20990 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8945
slot update_slots: id  3 | task 20990 | n_tokens = 8473, memory_seq_rm [8473, end)
slot update_slots: id  3 | task 20990 | prompt processing progress, n_tokens = 8881, batch.n_tokens = 408, progress = 0.992845
slot update_slots: id  3 | task 20990 | n_tokens = 8881, memory_seq_rm [8881, end)
slot update_slots: id  3 | task 20990 | prompt processing progress, n_tokens = 8945, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 20990 | prompt done, n_tokens = 8945, batch.n_tokens = 64
slot init_sampler: id  3 | task 20990 | init sampler, took 2.10 ms, tokens: text = 8945, total = 8945
slot update_slots: id  3 | task 20990 | erasing old context checkpoint (pos_min = 3003, pos_max = 4026, size = 24.012 MiB)
slot update_slots: id  3 | task 20990 | created context checkpoint 8 of 8 (pos_min = 7957, pos_max = 8880, size = 21.667 MiB)
slot print_timing: id  3 | task 20990 | 
prompt eval time =     615.72 ms /   472 tokens (    1.30 ms per token,   766.58 tokens per second)
       eval time =    2137.31 ms /    76 tokens (   28.12 ms per token,    35.56 tokens per second)
      total time =    2753.04 ms /   548 tokens
slot      release: id  3 | task 20990 | stop processing: n_tokens = 9020, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21068 | processing task, is_child = 0
slot update_slots: id  3 | task 21068 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9195
slot update_slots: id  3 | task 21068 | n_tokens = 8945, memory_seq_rm [8945, end)
slot update_slots: id  3 | task 21068 | prompt processing progress, n_tokens = 9131, batch.n_tokens = 186, progress = 0.993040
slot update_slots: id  3 | task 21068 | n_tokens = 9131, memory_seq_rm [9131, end)
slot update_slots: id  3 | task 21068 | prompt processing progress, n_tokens = 9195, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21068 | prompt done, n_tokens = 9195, batch.n_tokens = 64
slot init_sampler: id  3 | task 21068 | init sampler, took 2.28 ms, tokens: text = 9195, total = 9195
slot update_slots: id  3 | task 21068 | erasing old context checkpoint (pos_min = 3532, pos_max = 4555, size = 24.012 MiB)
slot update_slots: id  3 | task 21068 | created context checkpoint 8 of 8 (pos_min = 8143, pos_max = 9130, size = 23.168 MiB)
slot print_timing: id  3 | task 21068 | 
prompt eval time =     449.69 ms /   250 tokens (    1.80 ms per token,   555.94 tokens per second)
       eval time =    1749.35 ms /    66 tokens (   26.51 ms per token,    37.73 tokens per second)
      total time =    2199.04 ms /   316 tokens
slot      release: id  3 | task 21068 | stop processing: n_tokens = 9260, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21136 | processing task, is_child = 0
slot update_slots: id  3 | task 21136 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9560
slot update_slots: id  3 | task 21136 | n_tokens = 9195, memory_seq_rm [9195, end)
slot update_slots: id  3 | task 21136 | prompt processing progress, n_tokens = 9496, batch.n_tokens = 301, progress = 0.993305
slot update_slots: id  3 | task 21136 | n_tokens = 9496, memory_seq_rm [9496, end)
slot update_slots: id  3 | task 21136 | prompt processing progress, n_tokens = 9560, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21136 | prompt done, n_tokens = 9560, batch.n_tokens = 64
slot init_sampler: id  3 | task 21136 | init sampler, took 2.68 ms, tokens: text = 9560, total = 9560
slot update_slots: id  3 | task 21136 | erasing old context checkpoint (pos_min = 4382, pos_max = 5405, size = 24.012 MiB)
slot update_slots: id  3 | task 21136 | created context checkpoint 8 of 8 (pos_min = 8508, pos_max = 9495, size = 23.168 MiB)
slot print_timing: id  3 | task 21136 | 
prompt eval time =     549.95 ms /   365 tokens (    1.51 ms per token,   663.70 tokens per second)
       eval time =   13576.45 ms /   532 tokens (   25.52 ms per token,    39.19 tokens per second)
      total time =   14126.40 ms /   897 tokens
slot      release: id  3 | task 21136 | stop processing: n_tokens = 10091, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.947
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21670 | processing task, is_child = 0
slot update_slots: id  3 | task 21670 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10076
slot update_slots: id  3 | task 21670 | n_tokens = 9560, memory_seq_rm [9560, end)
slot update_slots: id  3 | task 21670 | prompt processing progress, n_tokens = 10012, batch.n_tokens = 452, progress = 0.993648
slot update_slots: id  3 | task 21670 | n_tokens = 10012, memory_seq_rm [10012, end)
slot update_slots: id  3 | task 21670 | prompt processing progress, n_tokens = 10076, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21670 | prompt done, n_tokens = 10076, batch.n_tokens = 64
slot init_sampler: id  3 | task 21670 | init sampler, took 2.24 ms, tokens: text = 10076, total = 10076
slot update_slots: id  3 | task 21670 | erasing old context checkpoint (pos_min = 4964, pos_max = 5921, size = 22.464 MiB)
slot update_slots: id  3 | task 21670 | created context checkpoint 8 of 8 (pos_min = 9397, pos_max = 10011, size = 14.421 MiB)
slot print_timing: id  3 | task 21670 | 
prompt eval time =     690.51 ms /   516 tokens (    1.34 ms per token,   747.27 tokens per second)
       eval time =    2311.25 ms /    90 tokens (   25.68 ms per token,    38.94 tokens per second)
      total time =    3001.76 ms /   606 tokens
slot      release: id  3 | task 21670 | stop processing: n_tokens = 10165, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21762 | processing task, is_child = 0
slot update_slots: id  3 | task 21762 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10441
slot update_slots: id  3 | task 21762 | n_tokens = 10076, memory_seq_rm [10076, end)
slot update_slots: id  3 | task 21762 | prompt processing progress, n_tokens = 10377, batch.n_tokens = 301, progress = 0.993870
slot update_slots: id  3 | task 21762 | n_tokens = 10377, memory_seq_rm [10377, end)
slot update_slots: id  3 | task 21762 | prompt processing progress, n_tokens = 10441, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21762 | prompt done, n_tokens = 10441, batch.n_tokens = 64
slot init_sampler: id  3 | task 21762 | init sampler, took 2.55 ms, tokens: text = 10441, total = 10441
slot update_slots: id  3 | task 21762 | erasing old context checkpoint (pos_min = 6105, pos_max = 7128, size = 24.012 MiB)
slot update_slots: id  3 | task 21762 | created context checkpoint 8 of 8 (pos_min = 9560, pos_max = 10376, size = 19.158 MiB)
slot print_timing: id  3 | task 21762 | 
prompt eval time =     546.49 ms /   365 tokens (    1.50 ms per token,   667.90 tokens per second)
       eval time =   15210.74 ms /   586 tokens (   25.96 ms per token,    38.53 tokens per second)
      total time =   15757.23 ms /   951 tokens
slot      release: id  3 | task 21762 | stop processing: n_tokens = 11026, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.947
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22350 | processing task, is_child = 0
slot update_slots: id  3 | task 22350 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10957
slot update_slots: id  3 | task 22350 | n_tokens = 10441, memory_seq_rm [10441, end)
slot update_slots: id  3 | task 22350 | prompt processing progress, n_tokens = 10893, batch.n_tokens = 452, progress = 0.994159
slot update_slots: id  3 | task 22350 | n_tokens = 10893, memory_seq_rm [10893, end)
slot update_slots: id  3 | task 22350 | prompt processing progress, n_tokens = 10957, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22350 | prompt done, n_tokens = 10957, batch.n_tokens = 64
slot init_sampler: id  3 | task 22350 | init sampler, took 2.51 ms, tokens: text = 10957, total = 10957
slot update_slots: id  3 | task 22350 | erasing old context checkpoint (pos_min = 6700, pos_max = 7644, size = 22.160 MiB)
slot update_slots: id  3 | task 22350 | created context checkpoint 8 of 8 (pos_min = 10012, pos_max = 10892, size = 20.659 MiB)
slot print_timing: id  3 | task 22350 | 
prompt eval time =     706.65 ms /   516 tokens (    1.37 ms per token,   730.20 tokens per second)
       eval time =    1545.66 ms /    59 tokens (   26.20 ms per token,    38.17 tokens per second)
      total time =    2252.32 ms /   575 tokens
slot      release: id  3 | task 22350 | stop processing: n_tokens = 11015, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22411 | processing task, is_child = 0
slot update_slots: id  3 | task 22411 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11243
slot update_slots: id  3 | task 22411 | n_tokens = 10957, memory_seq_rm [10957, end)
slot update_slots: id  3 | task 22411 | prompt processing progress, n_tokens = 11179, batch.n_tokens = 222, progress = 0.994308
slot update_slots: id  3 | task 22411 | n_tokens = 11179, memory_seq_rm [11179, end)
slot update_slots: id  3 | task 22411 | prompt processing progress, n_tokens = 11243, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22411 | prompt done, n_tokens = 11243, batch.n_tokens = 64
slot init_sampler: id  3 | task 22411 | init sampler, took 2.88 ms, tokens: text = 11243, total = 11243
slot update_slots: id  3 | task 22411 | erasing old context checkpoint (pos_min = 6869, pos_max = 7892, size = 24.012 MiB)
slot update_slots: id  3 | task 22411 | created context checkpoint 8 of 8 (pos_min = 10298, pos_max = 11178, size = 20.659 MiB)
slot print_timing: id  3 | task 22411 | 
prompt eval time =     507.66 ms /   286 tokens (    1.78 ms per token,   563.37 tokens per second)
       eval time =    1482.09 ms /    56 tokens (   26.47 ms per token,    37.78 tokens per second)
      total time =    1989.76 ms /   342 tokens
slot      release: id  3 | task 22411 | stop processing: n_tokens = 11298, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.930 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22469 | processing task, is_child = 0
slot update_slots: id  3 | task 22469 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12093
slot update_slots: id  3 | task 22469 | n_tokens = 11243, memory_seq_rm [11243, end)
slot update_slots: id  3 | task 22469 | prompt processing progress, n_tokens = 12029, batch.n_tokens = 786, progress = 0.994708
slot update_slots: id  3 | task 22469 | n_tokens = 12029, memory_seq_rm [12029, end)
slot update_slots: id  3 | task 22469 | prompt processing progress, n_tokens = 12093, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22469 | prompt done, n_tokens = 12093, batch.n_tokens = 64
slot init_sampler: id  3 | task 22469 | init sampler, took 2.74 ms, tokens: text = 12093, total = 12093
slot update_slots: id  3 | task 22469 | erasing old context checkpoint (pos_min = 7590, pos_max = 8408, size = 19.205 MiB)
slot update_slots: id  3 | task 22469 | created context checkpoint 8 of 8 (pos_min = 11005, pos_max = 12028, size = 24.012 MiB)
slot print_timing: id  3 | task 22469 | 
prompt eval time =    1148.72 ms /   850 tokens (    1.35 ms per token,   739.96 tokens per second)
       eval time =    5681.25 ms /   211 tokens (   26.93 ms per token,    37.14 tokens per second)
      total time =    6829.97 ms /  1061 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 22469 | stop processing: n_tokens = 12303, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22682 | processing task, is_child = 0
slot update_slots: id  3 | task 22682 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12298
slot update_slots: id  3 | task 22682 | n_tokens = 12093, memory_seq_rm [12093, end)
slot update_slots: id  3 | task 22682 | prompt processing progress, n_tokens = 12234, batch.n_tokens = 141, progress = 0.994796
slot update_slots: id  3 | task 22682 | n_tokens = 12234, memory_seq_rm [12234, end)
slot update_slots: id  3 | task 22682 | prompt processing progress, n_tokens = 12298, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22682 | prompt done, n_tokens = 12298, batch.n_tokens = 64
slot init_sampler: id  3 | task 22682 | init sampler, took 2.81 ms, tokens: text = 12298, total = 12298
slot update_slots: id  3 | task 22682 | erasing old context checkpoint (pos_min = 7957, pos_max = 8880, size = 21.667 MiB)
slot update_slots: id  3 | task 22682 | created context checkpoint 8 of 8 (pos_min = 11279, pos_max = 12233, size = 22.394 MiB)
slot print_timing: id  3 | task 22682 | 
prompt eval time =     436.06 ms /   205 tokens (    2.13 ms per token,   470.11 tokens per second)
       eval time =    1481.77 ms /    57 tokens (   26.00 ms per token,    38.47 tokens per second)
      total time =    1917.84 ms /   262 tokens
slot      release: id  3 | task 22682 | stop processing: n_tokens = 12354, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22741 | processing task, is_child = 0
slot update_slots: id  3 | task 22741 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12451
slot update_slots: id  3 | task 22741 | n_tokens = 12298, memory_seq_rm [12298, end)
slot update_slots: id  3 | task 22741 | prompt processing progress, n_tokens = 12387, batch.n_tokens = 89, progress = 0.994860
slot update_slots: id  3 | task 22741 | n_tokens = 12387, memory_seq_rm [12387, end)
slot update_slots: id  3 | task 22741 | prompt processing progress, n_tokens = 12451, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22741 | prompt done, n_tokens = 12451, batch.n_tokens = 64
slot init_sampler: id  3 | task 22741 | init sampler, took 2.27 ms, tokens: text = 12451, total = 12451
slot update_slots: id  3 | task 22741 | erasing old context checkpoint (pos_min = 8143, pos_max = 9130, size = 23.168 MiB)
slot update_slots: id  3 | task 22741 | created context checkpoint 8 of 8 (pos_min = 11363, pos_max = 12386, size = 24.012 MiB)
slot print_timing: id  3 | task 22741 | 
prompt eval time =     405.35 ms /   153 tokens (    2.65 ms per token,   377.45 tokens per second)
       eval time =    1619.99 ms /    62 tokens (   26.13 ms per token,    38.27 tokens per second)
      total time =    2025.34 ms /   215 tokens
slot      release: id  3 | task 22741 | stop processing: n_tokens = 12512, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22805 | processing task, is_child = 0
slot update_slots: id  3 | task 22805 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12619
slot update_slots: id  3 | task 22805 | n_tokens = 12451, memory_seq_rm [12451, end)
slot update_slots: id  3 | task 22805 | prompt processing progress, n_tokens = 12555, batch.n_tokens = 104, progress = 0.994928
slot update_slots: id  3 | task 22805 | n_tokens = 12555, memory_seq_rm [12555, end)
slot update_slots: id  3 | task 22805 | prompt processing progress, n_tokens = 12619, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22805 | prompt done, n_tokens = 12619, batch.n_tokens = 64
slot init_sampler: id  3 | task 22805 | init sampler, took 2.35 ms, tokens: text = 12619, total = 12619
slot update_slots: id  3 | task 22805 | erasing old context checkpoint (pos_min = 8508, pos_max = 9495, size = 23.168 MiB)
slot update_slots: id  3 | task 22805 | created context checkpoint 8 of 8 (pos_min = 11569, pos_max = 12554, size = 23.121 MiB)
slot print_timing: id  3 | task 22805 | 
prompt eval time =     433.50 ms /   168 tokens (    2.58 ms per token,   387.55 tokens per second)
       eval time =    1454.93 ms /    55 tokens (   26.45 ms per token,    37.80 tokens per second)
      total time =    1888.42 ms /   223 tokens
slot      release: id  3 | task 22805 | stop processing: n_tokens = 12673, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22862 | processing task, is_child = 0
slot update_slots: id  3 | task 22862 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12889
slot update_slots: id  3 | task 22862 | n_tokens = 12619, memory_seq_rm [12619, end)
slot update_slots: id  3 | task 22862 | prompt processing progress, n_tokens = 12825, batch.n_tokens = 206, progress = 0.995035
slot update_slots: id  3 | task 22862 | n_tokens = 12825, memory_seq_rm [12825, end)
slot update_slots: id  3 | task 22862 | prompt processing progress, n_tokens = 12889, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22862 | prompt done, n_tokens = 12889, batch.n_tokens = 64
slot init_sampler: id  3 | task 22862 | init sampler, took 3.27 ms, tokens: text = 12889, total = 12889
slot update_slots: id  3 | task 22862 | erasing old context checkpoint (pos_min = 9397, pos_max = 10011, size = 14.421 MiB)
slot update_slots: id  3 | task 22862 | created context checkpoint 8 of 8 (pos_min = 11839, pos_max = 12824, size = 23.121 MiB)
slot print_timing: id  3 | task 22862 | 
prompt eval time =     499.02 ms /   270 tokens (    1.85 ms per token,   541.06 tokens per second)
       eval time =    6024.62 ms /   224 tokens (   26.90 ms per token,    37.18 tokens per second)
      total time =    6523.64 ms /   494 tokens
slot      release: id  3 | task 22862 | stop processing: n_tokens = 13112, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 23088 | processing task, is_child = 0
slot update_slots: id  3 | task 23088 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13094
slot update_slots: id  3 | task 23088 | n_tokens = 12889, memory_seq_rm [12889, end)
slot update_slots: id  3 | task 23088 | prompt processing progress, n_tokens = 13030, batch.n_tokens = 141, progress = 0.995112
slot update_slots: id  3 | task 23088 | n_tokens = 13030, memory_seq_rm [13030, end)
slot update_slots: id  3 | task 23088 | prompt processing progress, n_tokens = 13094, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 23088 | prompt done, n_tokens = 13094, batch.n_tokens = 64
slot init_sampler: id  3 | task 23088 | init sampler, took 2.41 ms, tokens: text = 13094, total = 13094
slot update_slots: id  3 | task 23088 | erasing old context checkpoint (pos_min = 9560, pos_max = 10376, size = 19.158 MiB)
slot update_slots: id  3 | task 23088 | created context checkpoint 8 of 8 (pos_min = 12126, pos_max = 13029, size = 21.198 MiB)
slot print_timing: id  3 | task 23088 | 
prompt eval time =     430.85 ms /   205 tokens (    2.10 ms per token,   475.81 tokens per second)
       eval time =    1761.22 ms /    68 tokens (   25.90 ms per token,    38.61 tokens per second)
      total time =    2192.07 ms /   273 tokens
slot      release: id  3 | task 23088 | stop processing: n_tokens = 13161, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 23158 | processing task, is_child = 0
slot update_slots: id  3 | task 23158 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13387
slot update_slots: id  3 | task 23158 | n_tokens = 13094, memory_seq_rm [13094, end)
slot update_slots: id  3 | task 23158 | prompt processing progress, n_tokens = 13323, batch.n_tokens = 229, progress = 0.995219
slot update_slots: id  3 | task 23158 | n_tokens = 13323, memory_seq_rm [13323, end)
slot update_slots: id  3 | task 23158 | prompt processing progress, n_tokens = 13387, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 23158 | prompt done, n_tokens = 13387, batch.n_tokens = 64
slot init_sampler: id  3 | task 23158 | init sampler, took 2.89 ms, tokens: text = 13387, total = 13387
slot update_slots: id  3 | task 23158 | erasing old context checkpoint (pos_min = 10012, pos_max = 10892, size = 20.659 MiB)
slot update_slots: id  3 | task 23158 | created context checkpoint 8 of 8 (pos_min = 12337, pos_max = 13322, size = 23.121 MiB)
slot print_timing: id  3 | task 23158 | 
prompt eval time =     548.10 ms /   293 tokens (    1.87 ms per token,   534.57 tokens per second)
       eval time =    2142.03 ms /    82 tokens (   26.12 ms per token,    38.28 tokens per second)
      total time =    2690.13 ms /   375 tokens
slot      release: id  3 | task 23158 | stop processing: n_tokens = 13468, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 23242 | processing task, is_child = 0
slot update_slots: id  3 | task 23242 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13650
slot update_slots: id  3 | task 23242 | n_tokens = 13387, memory_seq_rm [13387, end)
slot update_slots: id  3 | task 23242 | prompt processing progress, n_tokens = 13586, batch.n_tokens = 199, progress = 0.995311
slot update_slots: id  3 | task 23242 | n_tokens = 13586, memory_seq_rm [13586, end)
slot update_slots: id  3 | task 23242 | prompt processing progress, n_tokens = 13650, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 23242 | prompt done, n_tokens = 13650, batch.n_tokens = 64
slot init_sampler: id  3 | task 23242 | init sampler, took 2.70 ms, tokens: text = 13650, total = 13650
slot update_slots: id  3 | task 23242 | erasing old context checkpoint (pos_min = 10298, pos_max = 11178, size = 20.659 MiB)
slot update_slots: id  3 | task 23242 | created context checkpoint 8 of 8 (pos_min = 12562, pos_max = 13585, size = 24.012 MiB)
slot print_timing: id  3 | task 23242 | 
prompt eval time =     501.15 ms /   263 tokens (    1.91 ms per token,   524.80 tokens per second)
       eval time =    1820.50 ms /    70 tokens (   26.01 ms per token,    38.45 tokens per second)
      total time =    2321.65 ms /   333 tokens
slot      release: id  3 | task 23242 | stop processing: n_tokens = 13719, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 23314 | processing task, is_child = 0
slot update_slots: id  3 | task 23314 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13892
slot update_slots: id  3 | task 23314 | n_tokens = 13650, memory_seq_rm [13650, end)
slot update_slots: id  3 | task 23314 | prompt processing progress, n_tokens = 13828, batch.n_tokens = 178, progress = 0.995393
slot update_slots: id  3 | task 23314 | n_tokens = 13828, memory_seq_rm [13828, end)
slot update_slots: id  3 | task 23314 | prompt processing progress, n_tokens = 13892, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 23314 | prompt done, n_tokens = 13892, batch.n_tokens = 64
slot init_sampler: id  3 | task 23314 | init sampler, took 3.12 ms, tokens: text = 13892, total = 13892
slot update_slots: id  3 | task 23314 | erasing old context checkpoint (pos_min = 11005, pos_max = 12028, size = 24.012 MiB)
slot update_slots: id  3 | task 23314 | created context checkpoint 8 of 8 (pos_min = 12804, pos_max = 13827, size = 24.012 MiB)
slot print_timing: id  3 | task 23314 | 
prompt eval time =     476.17 ms /   242 tokens (    1.97 ms per token,   508.23 tokens per second)
       eval time =   14165.33 ms /   540 tokens (   26.23 ms per token,    38.12 tokens per second)
      total time =   14641.49 ms /   782 tokens
slot      release: id  3 | task 23314 | stop processing: n_tokens = 14431, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.963
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 23856 | processing task, is_child = 0
slot update_slots: id  3 | task 23856 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14408
slot update_slots: id  3 | task 23856 | n_tokens = 13892, memory_seq_rm [13892, end)
slot update_slots: id  3 | task 23856 | prompt processing progress, n_tokens = 14344, batch.n_tokens = 452, progress = 0.995558
slot update_slots: id  3 | task 23856 | n_tokens = 14344, memory_seq_rm [14344, end)
slot update_slots: id  3 | task 23856 | prompt processing progress, n_tokens = 14408, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 23856 | prompt done, n_tokens = 14408, batch.n_tokens = 64
slot init_sampler: id  3 | task 23856 | init sampler, took 3.67 ms, tokens: text = 14408, total = 14408
slot update_slots: id  3 | task 23856 | erasing old context checkpoint (pos_min = 11279, pos_max = 12233, size = 22.394 MiB)
slot update_slots: id  3 | task 23856 | created context checkpoint 8 of 8 (pos_min = 13407, pos_max = 14343, size = 21.972 MiB)
slot print_timing: id  3 | task 23856 | 
prompt eval time =     747.28 ms /   516 tokens (    1.45 ms per token,   690.50 tokens per second)
       eval time =    1561.42 ms /    54 tokens (   28.92 ms per token,    34.58 tokens per second)
      total time =    2308.70 ms /   570 tokens
slot      release: id  3 | task 23856 | stop processing: n_tokens = 14461, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 23912 | processing task, is_child = 0
slot update_slots: id  3 | task 23912 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15258
slot update_slots: id  3 | task 23912 | n_tokens = 14408, memory_seq_rm [14408, end)
slot update_slots: id  3 | task 23912 | prompt processing progress, n_tokens = 15194, batch.n_tokens = 786, progress = 0.995806
slot update_slots: id  3 | task 23912 | n_tokens = 15194, memory_seq_rm [15194, end)
slot update_slots: id  3 | task 23912 | prompt processing progress, n_tokens = 15258, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 23912 | prompt done, n_tokens = 15258, batch.n_tokens = 64
slot init_sampler: id  3 | task 23912 | init sampler, took 6.65 ms, tokens: text = 15258, total = 15258
slot update_slots: id  3 | task 23912 | erasing old context checkpoint (pos_min = 11363, pos_max = 12386, size = 24.012 MiB)
slot update_slots: id  3 | task 23912 | created context checkpoint 8 of 8 (pos_min = 14170, pos_max = 15193, size = 24.012 MiB)
slot print_timing: id  3 | task 23912 | 
prompt eval time =    1180.73 ms /   850 tokens (    1.39 ms per token,   719.90 tokens per second)
       eval time =    7983.39 ms /   310 tokens (   25.75 ms per token,    38.83 tokens per second)
      total time =    9164.11 ms /  1160 tokens
slot      release: id  3 | task 23912 | stop processing: n_tokens = 15567, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 24224 | processing task, is_child = 0
slot update_slots: id  3 | task 24224 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15463
slot update_slots: id  3 | task 24224 | n_tokens = 15258, memory_seq_rm [15258, end)
slot update_slots: id  3 | task 24224 | prompt processing progress, n_tokens = 15399, batch.n_tokens = 141, progress = 0.995861
slot update_slots: id  3 | task 24224 | n_tokens = 15399, memory_seq_rm [15399, end)
slot update_slots: id  3 | task 24224 | prompt processing progress, n_tokens = 15463, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 24224 | prompt done, n_tokens = 15463, batch.n_tokens = 64
slot init_sampler: id  3 | task 24224 | init sampler, took 2.93 ms, tokens: text = 15463, total = 15463
slot update_slots: id  3 | task 24224 | erasing old context checkpoint (pos_min = 11569, pos_max = 12554, size = 23.121 MiB)
slot update_slots: id  3 | task 24224 | created context checkpoint 8 of 8 (pos_min = 14640, pos_max = 15398, size = 17.798 MiB)
slot print_timing: id  3 | task 24224 | 
prompt eval time =     445.71 ms /   205 tokens (    2.17 ms per token,   459.94 tokens per second)
       eval time =    1685.02 ms /    64 tokens (   26.33 ms per token,    37.98 tokens per second)
      total time =    2130.73 ms /   269 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 24224 | stop processing: n_tokens = 15526, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.968 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 24290 | processing task, is_child = 0
slot update_slots: id  3 | task 24290 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15970
slot update_slots: id  3 | task 24290 | n_tokens = 15463, memory_seq_rm [15463, end)
slot update_slots: id  3 | task 24290 | prompt processing progress, n_tokens = 15906, batch.n_tokens = 443, progress = 0.995992
slot update_slots: id  3 | task 24290 | n_tokens = 15906, memory_seq_rm [15906, end)
slot update_slots: id  3 | task 24290 | prompt processing progress, n_tokens = 15970, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 24290 | prompt done, n_tokens = 15970, batch.n_tokens = 64
slot init_sampler: id  3 | task 24290 | init sampler, took 7.54 ms, tokens: text = 15970, total = 15970
slot update_slots: id  3 | task 24290 | erasing old context checkpoint (pos_min = 11839, pos_max = 12824, size = 23.121 MiB)
slot update_slots: id  3 | task 24290 | created context checkpoint 8 of 8 (pos_min = 15147, pos_max = 15905, size = 17.798 MiB)
slot print_timing: id  3 | task 24290 | 
prompt eval time =     710.95 ms /   507 tokens (    1.40 ms per token,   713.13 tokens per second)
       eval time =    6398.83 ms /   242 tokens (   26.44 ms per token,    37.82 tokens per second)
      total time =    7109.79 ms /   749 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 24290 | stop processing: n_tokens = 16211, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 24534 | processing task, is_child = 0
slot update_slots: id  3 | task 24534 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16175
slot update_slots: id  3 | task 24534 | n_tokens = 15970, memory_seq_rm [15970, end)
slot update_slots: id  3 | task 24534 | prompt processing progress, n_tokens = 16111, batch.n_tokens = 141, progress = 0.996043
slot update_slots: id  3 | task 24534 | n_tokens = 16111, memory_seq_rm [16111, end)
slot update_slots: id  3 | task 24534 | prompt processing progress, n_tokens = 16175, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 24534 | prompt done, n_tokens = 16175, batch.n_tokens = 64
slot init_sampler: id  3 | task 24534 | init sampler, took 3.80 ms, tokens: text = 16175, total = 16175
slot update_slots: id  3 | task 24534 | erasing old context checkpoint (pos_min = 12126, pos_max = 13029, size = 21.198 MiB)
slot update_slots: id  3 | task 24534 | created context checkpoint 8 of 8 (pos_min = 15258, pos_max = 16110, size = 20.002 MiB)
slot print_timing: id  3 | task 24534 | 
prompt eval time =     441.54 ms /   205 tokens (    2.15 ms per token,   464.28 tokens per second)
       eval time =    1636.93 ms /    62 tokens (   26.40 ms per token,    37.88 tokens per second)
      total time =    2078.48 ms /   267 tokens
slot      release: id  3 | task 24534 | stop processing: n_tokens = 16236, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 24598 | processing task, is_child = 0
slot update_slots: id  3 | task 24598 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16383
slot update_slots: id  3 | task 24598 | n_tokens = 16175, memory_seq_rm [16175, end)
slot update_slots: id  3 | task 24598 | prompt processing progress, n_tokens = 16319, batch.n_tokens = 144, progress = 0.996094
slot update_slots: id  3 | task 24598 | n_tokens = 16319, memory_seq_rm [16319, end)
slot update_slots: id  3 | task 24598 | prompt processing progress, n_tokens = 16383, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 24598 | prompt done, n_tokens = 16383, batch.n_tokens = 64
slot init_sampler: id  3 | task 24598 | init sampler, took 3.60 ms, tokens: text = 16383, total = 16383
slot update_slots: id  3 | task 24598 | erasing old context checkpoint (pos_min = 12337, pos_max = 13322, size = 23.121 MiB)
slot update_slots: id  3 | task 24598 | created context checkpoint 8 of 8 (pos_min = 15295, pos_max = 16318, size = 24.012 MiB)
slot print_timing: id  3 | task 24598 | 
prompt eval time =     436.83 ms /   208 tokens (    2.10 ms per token,   476.15 tokens per second)
       eval time =    7487.54 ms /   286 tokens (   26.18 ms per token,    38.20 tokens per second)
      total time =    7924.37 ms /   494 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 24598 | stop processing: n_tokens = 16668, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 24886 | processing task, is_child = 0
slot update_slots: id  3 | task 24886 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16588
slot update_slots: id  3 | task 24886 | n_tokens = 16383, memory_seq_rm [16383, end)
slot update_slots: id  3 | task 24886 | prompt processing progress, n_tokens = 16524, batch.n_tokens = 141, progress = 0.996142
slot update_slots: id  3 | task 24886 | n_tokens = 16524, memory_seq_rm [16524, end)
slot update_slots: id  3 | task 24886 | prompt processing progress, n_tokens = 16588, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 24886 | prompt done, n_tokens = 16588, batch.n_tokens = 64
slot init_sampler: id  3 | task 24886 | init sampler, took 2.85 ms, tokens: text = 16588, total = 16588
slot update_slots: id  3 | task 24886 | erasing old context checkpoint (pos_min = 12562, pos_max = 13585, size = 24.012 MiB)
slot update_slots: id  3 | task 24886 | created context checkpoint 8 of 8 (pos_min = 15644, pos_max = 16523, size = 20.635 MiB)
slot print_timing: id  3 | task 24886 | 
prompt eval time =     437.27 ms /   205 tokens (    2.13 ms per token,   468.82 tokens per second)
       eval time =    1646.58 ms /    63 tokens (   26.14 ms per token,    38.26 tokens per second)
      total time =    2083.85 ms /   268 tokens
slot      release: id  3 | task 24886 | stop processing: n_tokens = 16650, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 24951 | processing task, is_child = 0
slot update_slots: id  3 | task 24951 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16808
slot update_slots: id  3 | task 24951 | n_tokens = 16588, memory_seq_rm [16588, end)
slot update_slots: id  3 | task 24951 | prompt processing progress, n_tokens = 16744, batch.n_tokens = 156, progress = 0.996192
slot update_slots: id  3 | task 24951 | n_tokens = 16744, memory_seq_rm [16744, end)
slot update_slots: id  3 | task 24951 | prompt processing progress, n_tokens = 16808, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 24951 | prompt done, n_tokens = 16808, batch.n_tokens = 64
slot init_sampler: id  3 | task 24951 | init sampler, took 3.31 ms, tokens: text = 16808, total = 16808
slot update_slots: id  3 | task 24951 | erasing old context checkpoint (pos_min = 12804, pos_max = 13827, size = 24.012 MiB)
slot update_slots: id  3 | task 24951 | created context checkpoint 8 of 8 (pos_min = 15720, pos_max = 16743, size = 24.012 MiB)
slot print_timing: id  3 | task 24951 | 
prompt eval time =     443.41 ms /   220 tokens (    2.02 ms per token,   496.15 tokens per second)
       eval time =    7498.84 ms /   286 tokens (   26.22 ms per token,    38.14 tokens per second)
      total time =    7942.25 ms /   506 tokens
slot      release: id  3 | task 24951 | stop processing: n_tokens = 17093, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25239 | processing task, is_child = 0
slot update_slots: id  3 | task 25239 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17013
slot update_slots: id  3 | task 25239 | n_tokens = 16808, memory_seq_rm [16808, end)
slot update_slots: id  3 | task 25239 | prompt processing progress, n_tokens = 16949, batch.n_tokens = 141, progress = 0.996238
slot update_slots: id  3 | task 25239 | n_tokens = 16949, memory_seq_rm [16949, end)
slot update_slots: id  3 | task 25239 | prompt processing progress, n_tokens = 17013, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25239 | prompt done, n_tokens = 17013, batch.n_tokens = 64
slot init_sampler: id  3 | task 25239 | init sampler, took 3.78 ms, tokens: text = 17013, total = 17013
slot update_slots: id  3 | task 25239 | erasing old context checkpoint (pos_min = 13407, pos_max = 14343, size = 21.972 MiB)
slot update_slots: id  3 | task 25239 | created context checkpoint 8 of 8 (pos_min = 16069, pos_max = 16948, size = 20.635 MiB)
slot print_timing: id  3 | task 25239 | 
prompt eval time =     449.39 ms /   205 tokens (    2.19 ms per token,   456.17 tokens per second)
       eval time =    1718.23 ms /    65 tokens (   26.43 ms per token,    37.83 tokens per second)
      total time =    2167.62 ms /   270 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 25239 | stop processing: n_tokens = 17077, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25306 | processing task, is_child = 0
slot update_slots: id  3 | task 25306 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17283
slot update_slots: id  3 | task 25306 | n_tokens = 17013, memory_seq_rm [17013, end)
slot update_slots: id  3 | task 25306 | prompt processing progress, n_tokens = 17219, batch.n_tokens = 206, progress = 0.996297
slot update_slots: id  3 | task 25306 | n_tokens = 17219, memory_seq_rm [17219, end)
slot update_slots: id  3 | task 25306 | prompt processing progress, n_tokens = 17283, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25306 | prompt done, n_tokens = 17283, batch.n_tokens = 64
slot init_sampler: id  3 | task 25306 | init sampler, took 2.73 ms, tokens: text = 17283, total = 17283
slot update_slots: id  3 | task 25306 | erasing old context checkpoint (pos_min = 14170, pos_max = 15193, size = 24.012 MiB)
slot update_slots: id  3 | task 25306 | created context checkpoint 8 of 8 (pos_min = 16195, pos_max = 17218, size = 24.012 MiB)
slot print_timing: id  3 | task 25306 | 
prompt eval time =     517.75 ms /   270 tokens (    1.92 ms per token,   521.49 tokens per second)
       eval time =    2614.57 ms /   100 tokens (   26.15 ms per token,    38.25 tokens per second)
      total time =    3132.32 ms /   370 tokens
slot      release: id  3 | task 25306 | stop processing: n_tokens = 17382, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25408 | processing task, is_child = 0
slot update_slots: id  3 | task 25408 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17576
slot update_slots: id  3 | task 25408 | n_tokens = 17283, memory_seq_rm [17283, end)
slot update_slots: id  3 | task 25408 | prompt processing progress, n_tokens = 17512, batch.n_tokens = 229, progress = 0.996359
slot update_slots: id  3 | task 25408 | n_tokens = 17512, memory_seq_rm [17512, end)
slot update_slots: id  3 | task 25408 | prompt processing progress, n_tokens = 17576, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25408 | prompt done, n_tokens = 17576, batch.n_tokens = 64
slot init_sampler: id  3 | task 25408 | init sampler, took 2.92 ms, tokens: text = 17576, total = 17576
slot update_slots: id  3 | task 25408 | erasing old context checkpoint (pos_min = 14640, pos_max = 15398, size = 17.798 MiB)
slot update_slots: id  3 | task 25408 | created context checkpoint 8 of 8 (pos_min = 16511, pos_max = 17511, size = 23.473 MiB)
slot print_timing: id  3 | task 25408 | 
prompt eval time =     565.19 ms /   293 tokens (    1.93 ms per token,   518.41 tokens per second)
       eval time =    3478.49 ms /   134 tokens (   25.96 ms per token,    38.52 tokens per second)
      total time =    4043.68 ms /   427 tokens
slot      release: id  3 | task 25408 | stop processing: n_tokens = 17709, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25544 | processing task, is_child = 0
slot update_slots: id  3 | task 25544 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18714
slot update_slots: id  3 | task 25544 | n_tokens = 17576, memory_seq_rm [17576, end)
slot update_slots: id  3 | task 25544 | prompt processing progress, n_tokens = 18650, batch.n_tokens = 1074, progress = 0.996580
slot update_slots: id  3 | task 25544 | n_tokens = 18650, memory_seq_rm [18650, end)
slot update_slots: id  3 | task 25544 | prompt processing progress, n_tokens = 18714, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25544 | prompt done, n_tokens = 18714, batch.n_tokens = 64
slot init_sampler: id  3 | task 25544 | init sampler, took 3.03 ms, tokens: text = 18714, total = 18714
slot update_slots: id  3 | task 25544 | erasing old context checkpoint (pos_min = 15147, pos_max = 15905, size = 17.798 MiB)
slot update_slots: id  3 | task 25544 | created context checkpoint 8 of 8 (pos_min = 17626, pos_max = 18649, size = 24.012 MiB)
slot print_timing: id  3 | task 25544 | 
prompt eval time =    1570.28 ms /  1138 tokens (    1.38 ms per token,   724.71 tokens per second)
       eval time =    3364.74 ms /   129 tokens (   26.08 ms per token,    38.34 tokens per second)
      total time =    4935.02 ms /  1267 tokens
slot      release: id  3 | task 25544 | stop processing: n_tokens = 18842, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25675 | processing task, is_child = 0
slot update_slots: id  3 | task 25675 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18791
slot update_slots: id  3 | task 25675 | n_tokens = 18714, memory_seq_rm [18714, end)
slot update_slots: id  3 | task 25675 | prompt processing progress, n_tokens = 18727, batch.n_tokens = 13, progress = 0.996594
slot update_slots: id  3 | task 25675 | n_tokens = 18727, memory_seq_rm [18727, end)
slot update_slots: id  3 | task 25675 | prompt processing progress, n_tokens = 18791, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25675 | prompt done, n_tokens = 18791, batch.n_tokens = 64
slot init_sampler: id  3 | task 25675 | init sampler, took 3.74 ms, tokens: text = 18791, total = 18791
slot update_slots: id  3 | task 25675 | erasing old context checkpoint (pos_min = 15258, pos_max = 16110, size = 20.002 MiB)
slot update_slots: id  3 | task 25675 | created context checkpoint 8 of 8 (pos_min = 17818, pos_max = 18726, size = 21.315 MiB)
slot print_timing: id  3 | task 25675 | 
prompt eval time =     307.82 ms /    77 tokens (    4.00 ms per token,   250.15 tokens per second)
       eval time =    2186.75 ms /    83 tokens (   26.35 ms per token,    37.96 tokens per second)
      total time =    2494.57 ms /   160 tokens
slot      release: id  3 | task 25675 | stop processing: n_tokens = 18873, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25760 | processing task, is_child = 0
slot update_slots: id  3 | task 25760 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18869
slot update_slots: id  3 | task 25760 | n_tokens = 18791, memory_seq_rm [18791, end)
slot update_slots: id  3 | task 25760 | prompt processing progress, n_tokens = 18805, batch.n_tokens = 14, progress = 0.996608
slot update_slots: id  3 | task 25760 | n_tokens = 18805, memory_seq_rm [18805, end)
slot update_slots: id  3 | task 25760 | prompt processing progress, n_tokens = 18869, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25760 | prompt done, n_tokens = 18869, batch.n_tokens = 64
slot init_sampler: id  3 | task 25760 | init sampler, took 3.21 ms, tokens: text = 18869, total = 18869
slot update_slots: id  3 | task 25760 | erasing old context checkpoint (pos_min = 15295, pos_max = 16318, size = 24.012 MiB)
slot update_slots: id  3 | task 25760 | created context checkpoint 8 of 8 (pos_min = 17849, pos_max = 18804, size = 22.417 MiB)
slot print_timing: id  3 | task 25760 | 
prompt eval time =     281.23 ms /    78 tokens (    3.61 ms per token,   277.36 tokens per second)
       eval time =   21971.60 ms /   833 tokens (   26.38 ms per token,    37.91 tokens per second)
      total time =   22252.83 ms /   911 tokens
slot      release: id  3 | task 25760 | stop processing: n_tokens = 19701, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26595 | processing task, is_child = 0
slot update_slots: id  3 | task 26595 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19710
slot update_slots: id  3 | task 26595 | n_tokens = 18869, memory_seq_rm [18869, end)
slot update_slots: id  3 | task 26595 | prompt processing progress, n_tokens = 19646, batch.n_tokens = 777, progress = 0.996753
slot update_slots: id  3 | task 26595 | n_tokens = 19646, memory_seq_rm [19646, end)
slot update_slots: id  3 | task 26595 | prompt processing progress, n_tokens = 19710, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 26595 | prompt done, n_tokens = 19710, batch.n_tokens = 64
slot init_sampler: id  3 | task 26595 | init sampler, took 4.23 ms, tokens: text = 19710, total = 19710
slot update_slots: id  3 | task 26595 | erasing old context checkpoint (pos_min = 15644, pos_max = 16523, size = 20.635 MiB)
slot update_slots: id  3 | task 26595 | created context checkpoint 8 of 8 (pos_min = 18742, pos_max = 19645, size = 21.198 MiB)
slot print_timing: id  3 | task 26595 | 
prompt eval time =    1268.28 ms /   841 tokens (    1.51 ms per token,   663.10 tokens per second)
       eval time =     921.97 ms /    33 tokens (   27.94 ms per token,    35.79 tokens per second)
      total time =    2190.25 ms /   874 tokens
slot      release: id  3 | task 26595 | stop processing: n_tokens = 19742, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26630 | processing task, is_child = 0
slot update_slots: id  3 | task 26630 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19775
slot update_slots: id  3 | task 26630 | n_tokens = 19710, memory_seq_rm [19710, end)
slot update_slots: id  3 | task 26630 | prompt processing progress, n_tokens = 19711, batch.n_tokens = 1, progress = 0.996764
slot update_slots: id  3 | task 26630 | n_tokens = 19711, memory_seq_rm [19711, end)
slot update_slots: id  3 | task 26630 | prompt processing progress, n_tokens = 19775, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 26630 | prompt done, n_tokens = 19775, batch.n_tokens = 64
slot init_sampler: id  3 | task 26630 | init sampler, took 3.18 ms, tokens: text = 19775, total = 19775
slot update_slots: id  3 | task 26630 | erasing old context checkpoint (pos_min = 15720, pos_max = 16743, size = 24.012 MiB)
slot update_slots: id  3 | task 26630 | created context checkpoint 8 of 8 (pos_min = 18742, pos_max = 19710, size = 22.722 MiB)
slot print_timing: id  3 | task 26630 | 
prompt eval time =     301.56 ms /    65 tokens (    4.64 ms per token,   215.55 tokens per second)
       eval time =    1926.34 ms /    75 tokens (   25.68 ms per token,    38.93 tokens per second)
      total time =    2227.89 ms /   140 tokens
slot      release: id  3 | task 26630 | stop processing: n_tokens = 19849, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.023
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 19849, total state size = 486.472 MiB
srv          load:  - looking for better prompt, base f_keep = 0.023, sim = 0.966
srv        update:  - cache state: 7 prompts, 2066.078 MiB (limits: 8192.000 MiB, 64000 tokens, 204031 est)
srv        update:    - prompt 0x56661feaa980:    2670 tokens, checkpoints:  3,   131.011 MiB
srv        update:    - prompt 0x56662004b180:    2104 tokens, checkpoints:  3,   114.573 MiB
srv        update:    - prompt 0x56660efe3f60:    4818 tokens, checkpoints:  4,   202.225 MiB
srv        update:    - prompt 0x5666200df900:    3798 tokens, checkpoints:  4,   178.308 MiB
srv        update:    - prompt 0x56661922e270:    4547 tokens, checkpoints:  6,   243.895 MiB
srv        update:    - prompt 0x56661fe45450:   13672 tokens, checkpoints:  8,   529.808 MiB
srv        update:    - prompt 0x56661fc7be20:   19849 tokens, checkpoints:  8,   666.258 MiB
srv  get_availabl: prompt cache update took 557.89 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26707 | processing task, is_child = 0
slot update_slots: id  3 | task 26707 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 477
slot update_slots: id  3 | task 26707 | n_past = 461, slot.prompt.tokens.size() = 19849, seq_id = 3, pos_min = 18952, n_swa = 128
slot update_slots: id  3 | task 26707 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 26707 | erased invalidated context checkpoint (pos_min = 16069, pos_max = 16948, n_swa = 128, size = 20.635 MiB)
slot update_slots: id  3 | task 26707 | erased invalidated context checkpoint (pos_min = 16195, pos_max = 17218, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 26707 | erased invalidated context checkpoint (pos_min = 16511, pos_max = 17511, n_swa = 128, size = 23.473 MiB)
slot update_slots: id  3 | task 26707 | erased invalidated context checkpoint (pos_min = 17626, pos_max = 18649, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 26707 | erased invalidated context checkpoint (pos_min = 17818, pos_max = 18726, n_swa = 128, size = 21.315 MiB)
slot update_slots: id  3 | task 26707 | erased invalidated context checkpoint (pos_min = 17849, pos_max = 18804, n_swa = 128, size = 22.417 MiB)
slot update_slots: id  3 | task 26707 | erased invalidated context checkpoint (pos_min = 18742, pos_max = 19645, n_swa = 128, size = 21.198 MiB)
slot update_slots: id  3 | task 26707 | erased invalidated context checkpoint (pos_min = 18742, pos_max = 19710, n_swa = 128, size = 22.722 MiB)
slot update_slots: id  3 | task 26707 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 26707 | prompt processing progress, n_tokens = 413, batch.n_tokens = 413, progress = 0.865828
slot update_slots: id  3 | task 26707 | n_tokens = 413, memory_seq_rm [413, end)
slot update_slots: id  3 | task 26707 | prompt processing progress, n_tokens = 477, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 26707 | prompt done, n_tokens = 477, batch.n_tokens = 64
slot init_sampler: id  3 | task 26707 | init sampler, took 0.10 ms, tokens: text = 477, total = 477
slot update_slots: id  3 | task 26707 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 412, size = 9.685 MiB)
slot print_timing: id  3 | task 26707 | 
prompt eval time =     625.52 ms /   477 tokens (    1.31 ms per token,   762.57 tokens per second)
       eval time =     716.37 ms /    30 tokens (   23.88 ms per token,    41.88 tokens per second)
      total time =    1341.89 ms /   507 tokens
slot      release: id  3 | task 26707 | stop processing: n_tokens = 506, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.476 (> 0.100 thold), f_keep = 0.943
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26739 | processing task, is_child = 0
slot update_slots: id  3 | task 26739 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1002
slot update_slots: id  3 | task 26739 | n_tokens = 477, memory_seq_rm [477, end)
slot update_slots: id  3 | task 26739 | prompt processing progress, n_tokens = 938, batch.n_tokens = 461, progress = 0.936128
slot update_slots: id  3 | task 26739 | n_tokens = 938, memory_seq_rm [938, end)
slot update_slots: id  3 | task 26739 | prompt processing progress, n_tokens = 1002, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 26739 | prompt done, n_tokens = 1002, batch.n_tokens = 64
slot init_sampler: id  3 | task 26739 | init sampler, took 0.21 ms, tokens: text = 1002, total = 1002
slot update_slots: id  3 | task 26739 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 937, size = 21.995 MiB)
slot print_timing: id  3 | task 26739 | 
prompt eval time =     692.68 ms /   525 tokens (    1.32 ms per token,   757.93 tokens per second)
       eval time =    1931.35 ms /    81 tokens (   23.84 ms per token,    41.94 tokens per second)
      total time =    2624.03 ms /   606 tokens
slot      release: id  3 | task 26739 | stop processing: n_tokens = 1082, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.630 (> 0.100 thold), f_keep = 0.926
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26822 | processing task, is_child = 0
slot update_slots: id  3 | task 26822 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1591
slot update_slots: id  3 | task 26822 | n_tokens = 1002, memory_seq_rm [1002, end)
slot update_slots: id  3 | task 26822 | prompt processing progress, n_tokens = 1527, batch.n_tokens = 525, progress = 0.959774
slot update_slots: id  3 | task 26822 | n_tokens = 1527, memory_seq_rm [1527, end)
slot update_slots: id  3 | task 26822 | prompt processing progress, n_tokens = 1591, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 26822 | prompt done, n_tokens = 1591, batch.n_tokens = 64
slot init_sampler: id  3 | task 26822 | init sampler, took 0.35 ms, tokens: text = 1591, total = 1591
slot update_slots: id  3 | task 26822 | created context checkpoint 3 of 8 (pos_min = 525, pos_max = 1526, size = 23.496 MiB)
slot print_timing: id  3 | task 26822 | 
prompt eval time =     701.91 ms /   589 tokens (    1.19 ms per token,   839.14 tokens per second)
       eval time =    1812.08 ms /    73 tokens (   24.82 ms per token,    40.29 tokens per second)
      total time =    2513.99 ms /   662 tokens
slot      release: id  3 | task 26822 | stop processing: n_tokens = 1663, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.442 (> 0.100 thold), f_keep = 0.957
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26897 | processing task, is_child = 0
slot update_slots: id  3 | task 26897 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3597
slot update_slots: id  3 | task 26897 | n_tokens = 1591, memory_seq_rm [1591, end)
slot update_slots: id  3 | task 26897 | prompt processing progress, n_tokens = 3533, batch.n_tokens = 1942, progress = 0.982207
slot update_slots: id  3 | task 26897 | n_tokens = 3533, memory_seq_rm [3533, end)
slot update_slots: id  3 | task 26897 | prompt processing progress, n_tokens = 3597, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 26897 | prompt done, n_tokens = 3597, batch.n_tokens = 64
slot init_sampler: id  3 | task 26897 | init sampler, took 0.74 ms, tokens: text = 3597, total = 3597
slot update_slots: id  3 | task 26897 | created context checkpoint 4 of 8 (pos_min = 2509, pos_max = 3532, size = 24.012 MiB)
slot print_timing: id  3 | task 26897 | 
prompt eval time =    2121.78 ms /  2006 tokens (    1.06 ms per token,   945.43 tokens per second)
       eval time =    1529.79 ms /    62 tokens (   24.67 ms per token,    40.53 tokens per second)
      total time =    3651.57 ms /  2068 tokens
slot      release: id  3 | task 26897 | stop processing: n_tokens = 3658, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.638 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26961 | processing task, is_child = 0
slot update_slots: id  3 | task 26961 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5641
slot update_slots: id  3 | task 26961 | n_tokens = 3597, memory_seq_rm [3597, end)
slot update_slots: id  3 | task 26961 | prompt processing progress, n_tokens = 5577, batch.n_tokens = 1980, progress = 0.988654
slot update_slots: id  3 | task 26961 | n_tokens = 5577, memory_seq_rm [5577, end)
slot update_slots: id  3 | task 26961 | prompt processing progress, n_tokens = 5641, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 26961 | prompt done, n_tokens = 5641, batch.n_tokens = 64
slot init_sampler: id  3 | task 26961 | init sampler, took 1.25 ms, tokens: text = 5641, total = 5641
slot update_slots: id  3 | task 26961 | created context checkpoint 5 of 8 (pos_min = 4553, pos_max = 5576, size = 24.012 MiB)
slot print_timing: id  3 | task 26961 | 
prompt eval time =    2210.09 ms /  2044 tokens (    1.08 ms per token,   924.85 tokens per second)
       eval time =    1321.31 ms /    53 tokens (   24.93 ms per token,    40.11 tokens per second)
      total time =    3531.40 ms /  2097 tokens
slot      release: id  3 | task 26961 | stop processing: n_tokens = 5693, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.766 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27016 | processing task, is_child = 0
slot update_slots: id  3 | task 27016 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7363
slot update_slots: id  3 | task 27016 | n_tokens = 5641, memory_seq_rm [5641, end)
slot update_slots: id  3 | task 27016 | prompt processing progress, n_tokens = 7299, batch.n_tokens = 1658, progress = 0.991308
slot update_slots: id  3 | task 27016 | n_tokens = 7299, memory_seq_rm [7299, end)
slot update_slots: id  3 | task 27016 | prompt processing progress, n_tokens = 7363, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 27016 | prompt done, n_tokens = 7363, batch.n_tokens = 64
slot init_sampler: id  3 | task 27016 | init sampler, took 1.40 ms, tokens: text = 7363, total = 7363
slot update_slots: id  3 | task 27016 | created context checkpoint 6 of 8 (pos_min = 6275, pos_max = 7298, size = 24.012 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 27016
slot      release: id  3 | task 27016 | stop processing: n_tokens = 7509, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.062
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7509, total state size = 200.090 MiB
srv          load:  - looking for better prompt, base f_keep = 0.062, sim = 0.963
srv        update:  - cache state: 8 prompts, 2393.380 MiB (limits: 8192.000 MiB, 64000 tokens, 201830 est)
srv        update:    - prompt 0x56661feaa980:    2670 tokens, checkpoints:  3,   131.011 MiB
srv        update:    - prompt 0x56662004b180:    2104 tokens, checkpoints:  3,   114.573 MiB
srv        update:    - prompt 0x56660efe3f60:    4818 tokens, checkpoints:  4,   202.225 MiB
srv        update:    - prompt 0x5666200df900:    3798 tokens, checkpoints:  4,   178.308 MiB
srv        update:    - prompt 0x56661922e270:    4547 tokens, checkpoints:  6,   243.895 MiB
srv        update:    - prompt 0x56661fe45450:   13672 tokens, checkpoints:  8,   529.808 MiB
srv        update:    - prompt 0x56661fc7be20:   19849 tokens, checkpoints:  8,   666.258 MiB
srv        update:    - prompt 0x56661eea4660:    7509 tokens, checkpoints:  6,   327.303 MiB
srv  get_availabl: prompt cache update took 299.84 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27166 | processing task, is_child = 0
slot update_slots: id  3 | task 27166 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 483
slot update_slots: id  3 | task 27166 | n_past = 465, slot.prompt.tokens.size() = 7509, seq_id = 3, pos_min = 6485, n_swa = 128
slot update_slots: id  3 | task 27166 | restored context checkpoint (pos_min = 0, pos_max = 937, size = 21.995 MiB)
slot update_slots: id  3 | task 27166 | erased invalidated context checkpoint (pos_min = 525, pos_max = 1526, n_swa = 128, size = 23.496 MiB)
slot update_slots: id  3 | task 27166 | erased invalidated context checkpoint (pos_min = 2509, pos_max = 3532, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 27166 | erased invalidated context checkpoint (pos_min = 4553, pos_max = 5576, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 27166 | erased invalidated context checkpoint (pos_min = 6275, pos_max = 7298, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 27166 | n_tokens = 465, memory_seq_rm [465, end)
slot update_slots: id  3 | task 27166 | prompt processing progress, n_tokens = 483, batch.n_tokens = 18, progress = 1.000000
slot update_slots: id  3 | task 27166 | prompt done, n_tokens = 483, batch.n_tokens = 18
slot init_sampler: id  3 | task 27166 | init sampler, took 0.09 ms, tokens: text = 483, total = 483
slot print_timing: id  3 | task 27166 | 
prompt eval time =     158.66 ms /    18 tokens (    8.81 ms per token,   113.45 tokens per second)
       eval time =     847.25 ms /    31 tokens (   27.33 ms per token,    36.59 tokens per second)
      total time =    1005.91 ms /    49 tokens
slot      release: id  3 | task 27166 | stop processing: n_tokens = 513, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.342 (> 0.100 thold), f_keep = 0.942
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27198 | processing task, is_child = 0
slot update_slots: id  3 | task 27198 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1412
slot update_slots: id  3 | task 27198 | n_tokens = 483, memory_seq_rm [483, end)
slot update_slots: id  3 | task 27198 | prompt processing progress, n_tokens = 1348, batch.n_tokens = 865, progress = 0.954674
slot update_slots: id  3 | task 27198 | n_tokens = 1348, memory_seq_rm [1348, end)
slot update_slots: id  3 | task 27198 | prompt processing progress, n_tokens = 1412, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 27198 | prompt done, n_tokens = 1412, batch.n_tokens = 64
slot init_sampler: id  3 | task 27198 | init sampler, took 0.28 ms, tokens: text = 1412, total = 1412
slot update_slots: id  3 | task 27198 | created context checkpoint 3 of 8 (pos_min = 324, pos_max = 1347, size = 24.012 MiB)
slot print_timing: id  3 | task 27198 | 
prompt eval time =    1093.65 ms /   929 tokens (    1.18 ms per token,   849.45 tokens per second)
       eval time =    1896.27 ms /    80 tokens (   23.70 ms per token,    42.19 tokens per second)
      total time =    2989.92 ms /  1009 tokens
slot      release: id  3 | task 27198 | stop processing: n_tokens = 1491, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.703 (> 0.100 thold), f_keep = 0.947
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27280 | processing task, is_child = 0
slot update_slots: id  3 | task 27280 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2009
slot update_slots: id  3 | task 27280 | n_tokens = 1412, memory_seq_rm [1412, end)
slot update_slots: id  3 | task 27280 | prompt processing progress, n_tokens = 1945, batch.n_tokens = 533, progress = 0.968143
slot update_slots: id  3 | task 27280 | n_tokens = 1945, memory_seq_rm [1945, end)
slot update_slots: id  3 | task 27280 | prompt processing progress, n_tokens = 2009, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 27280 | prompt done, n_tokens = 2009, batch.n_tokens = 64
slot init_sampler: id  3 | task 27280 | init sampler, took 0.44 ms, tokens: text = 2009, total = 2009
slot update_slots: id  3 | task 27280 | created context checkpoint 4 of 8 (pos_min = 921, pos_max = 1944, size = 24.012 MiB)
slot print_timing: id  3 | task 27280 | 
prompt eval time =     706.77 ms /   597 tokens (    1.18 ms per token,   844.68 tokens per second)
       eval time =    1549.71 ms /    64 tokens (   24.21 ms per token,    41.30 tokens per second)
      total time =    2256.49 ms /   661 tokens
slot      release: id  3 | task 27280 | stop processing: n_tokens = 2072, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.491 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27346 | processing task, is_child = 0
slot update_slots: id  3 | task 27346 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4093
slot update_slots: id  3 | task 27346 | n_tokens = 2009, memory_seq_rm [2009, end)
slot update_slots: id  3 | task 27346 | prompt processing progress, n_tokens = 4029, batch.n_tokens = 2020, progress = 0.984364
slot update_slots: id  3 | task 27346 | n_tokens = 4029, memory_seq_rm [4029, end)
slot update_slots: id  3 | task 27346 | prompt processing progress, n_tokens = 4093, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 27346 | prompt done, n_tokens = 4093, batch.n_tokens = 64
slot init_sampler: id  3 | task 27346 | init sampler, took 0.76 ms, tokens: text = 4093, total = 4093
slot update_slots: id  3 | task 27346 | created context checkpoint 5 of 8 (pos_min = 3005, pos_max = 4028, size = 24.012 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 27346
slot      release: id  3 | task 27346 | stop processing: n_tokens = 4130, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.117
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4130, total state size = 120.856 MiB
srv          load:  - looking for better prompt, base f_keep = 0.117, sim = 1.000
srv        update:  - cache state: 9 prompts, 2617.953 MiB (limits: 8192.000 MiB, 64000 tokens, 197440 est)
srv        update:    - prompt 0x56661feaa980:    2670 tokens, checkpoints:  3,   131.011 MiB
srv        update:    - prompt 0x56662004b180:    2104 tokens, checkpoints:  3,   114.573 MiB
srv        update:    - prompt 0x56660efe3f60:    4818 tokens, checkpoints:  4,   202.225 MiB
srv        update:    - prompt 0x5666200df900:    3798 tokens, checkpoints:  4,   178.308 MiB
srv        update:    - prompt 0x56661922e270:    4547 tokens, checkpoints:  6,   243.895 MiB
srv        update:    - prompt 0x56661fe45450:   13672 tokens, checkpoints:  8,   529.808 MiB
srv        update:    - prompt 0x56661fc7be20:   19849 tokens, checkpoints:  8,   666.258 MiB
srv        update:    - prompt 0x56661eea4660:    7509 tokens, checkpoints:  6,   327.303 MiB
srv        update:    - prompt 0x56662189c840:    4130 tokens, checkpoints:  5,   224.573 MiB
srv  get_availabl: prompt cache update took 174.37 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27387 | processing task, is_child = 0
slot update_slots: id  3 | task 27387 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 483
slot update_slots: id  3 | task 27387 | n_past = 483, slot.prompt.tokens.size() = 4130, seq_id = 3, pos_min = 3106, n_swa = 128
slot update_slots: id  3 | task 27387 | restored context checkpoint (pos_min = 324, pos_max = 1347, size = 24.012 MiB)
slot update_slots: id  3 | task 27387 | erased invalidated context checkpoint (pos_min = 921, pos_max = 1944, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 27387 | erased invalidated context checkpoint (pos_min = 3005, pos_max = 4028, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 27387 | need to evaluate at least 1 token for each active slot (n_past = 483, task.n_tokens() = 483)
slot update_slots: id  3 | task 27387 | n_past was set to 482
slot update_slots: id  3 | task 27387 | n_tokens = 482, memory_seq_rm [482, end)
slot update_slots: id  3 | task 27387 | prompt processing progress, n_tokens = 483, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  3 | task 27387 | prompt done, n_tokens = 483, batch.n_tokens = 1
slot init_sampler: id  3 | task 27387 | init sampler, took 0.08 ms, tokens: text = 483, total = 483
slot print_timing: id  3 | task 27387 | 
prompt eval time =      44.93 ms /     1 tokens (   44.93 ms per token,    22.26 tokens per second)
       eval time =     747.08 ms /    30 tokens (   24.90 ms per token,    40.16 tokens per second)
      total time =     792.01 ms /    31 tokens
slot      release: id  3 | task 27387 | stop processing: n_tokens = 512, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.880 (> 0.100 thold), f_keep = 0.943
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27418 | processing task, is_child = 0
slot update_slots: id  3 | task 27418 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 549
slot update_slots: id  3 | task 27418 | n_tokens = 483, memory_seq_rm [483, end)
slot update_slots: id  3 | task 27418 | prompt processing progress, n_tokens = 485, batch.n_tokens = 2, progress = 0.883424
slot update_slots: id  3 | task 27418 | n_tokens = 485, memory_seq_rm [485, end)
slot update_slots: id  3 | task 27418 | prompt processing progress, n_tokens = 549, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 27418 | prompt done, n_tokens = 549, batch.n_tokens = 64
slot init_sampler: id  3 | task 27418 | init sampler, took 0.11 ms, tokens: text = 549, total = 549
slot print_timing: id  3 | task 27418 | 
prompt eval time =     298.23 ms /    66 tokens (    4.52 ms per token,   221.30 tokens per second)
       eval time =     928.30 ms /    41 tokens (   22.64 ms per token,    44.17 tokens per second)
      total time =    1226.53 ms /   107 tokens
slot      release: id  3 | task 27418 | stop processing: n_tokens = 589, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
