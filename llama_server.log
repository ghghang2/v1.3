ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: using cached file: /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14992 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1578 MiB
llama_params_fit_impl: context size reduced from 131072 to 64000 -> need 1580 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.64 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
............srv  log_server_r: request: GET /health 127.0.0.1 503
......srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
....srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 64000
llama_context: n_ctx_seq     = 64000
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (64000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 64000 cells
llama_kv_cache:      CUDA0 KV buffer size =  1500.00 MiB
llama_kv_cache: size = 1500.00 MiB ( 64000 cells,  12 layers,  4/1 seqs), K (f16):  750.00 MiB, V (f16):  750.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   132.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 106.11 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  1 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  2 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  3 | task -1 | new slot, n_ctx = 64000
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-04

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 501
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 437, batch.n_tokens = 437, progress = 0.872256
slot update_slots: id  3 | task 0 | n_tokens = 437, memory_seq_rm [437, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 501, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 501, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.09 ms, tokens: text = 501, total = 501
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 436, size = 10.247 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =     857.51 ms /   501 tokens (    1.71 ms per token,   584.25 tokens per second)
       eval time =   13248.13 ms /   476 tokens (   27.83 ms per token,    35.93 tokens per second)
      total time =   14105.64 ms /   977 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 976, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.898 (> 0.100 thold), f_keep = 0.494
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 976, total state size = 45.773 MiB
srv          load:  - looking for better prompt, base f_keep = 0.494, sim = 0.898
srv        update:  - cache state: 1 prompts, 56.020 MiB (limits: 8192.000 MiB, 64000 tokens, 142722 est)
srv        update:    - prompt 0x55bd8f1cbf80:     976 tokens, checkpoints:  1,    56.020 MiB
srv  get_availabl: prompt cache update took 53.83 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 478 | processing task, is_child = 0
slot update_slots: id  3 | task 478 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 537
slot update_slots: id  3 | task 478 | n_tokens = 482, memory_seq_rm [482, end)
slot update_slots: id  3 | task 478 | prompt processing progress, n_tokens = 537, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 478 | prompt done, n_tokens = 537, batch.n_tokens = 55
slot init_sampler: id  3 | task 478 | init sampler, took 0.11 ms, tokens: text = 537, total = 537
slot print_timing: id  3 | task 478 | 
prompt eval time =     237.88 ms /    55 tokens (    4.33 ms per token,   231.21 tokens per second)
       eval time =    2285.66 ms /    96 tokens (   23.81 ms per token,    42.00 tokens per second)
      total time =    2523.53 ms /   151 tokens
slot      release: id  3 | task 478 | stop processing: n_tokens = 632, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.539 (> 0.100 thold), f_keep = 0.850
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 575 | processing task, is_child = 0
slot update_slots: id  3 | task 575 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 996
slot update_slots: id  3 | task 575 | n_tokens = 537, memory_seq_rm [537, end)
slot update_slots: id  3 | task 575 | prompt processing progress, n_tokens = 932, batch.n_tokens = 395, progress = 0.935743
slot update_slots: id  3 | task 575 | n_tokens = 932, memory_seq_rm [932, end)
slot update_slots: id  3 | task 575 | prompt processing progress, n_tokens = 996, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 575 | prompt done, n_tokens = 996, batch.n_tokens = 64
slot init_sampler: id  3 | task 575 | init sampler, took 0.17 ms, tokens: text = 996, total = 996
slot update_slots: id  3 | task 575 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 931, size = 21.855 MiB)
slot print_timing: id  3 | task 575 | 
prompt eval time =     534.75 ms /   459 tokens (    1.17 ms per token,   858.35 tokens per second)
       eval time =    1524.52 ms /    63 tokens (   24.20 ms per token,    41.32 tokens per second)
      total time =    2059.27 ms /   522 tokens
slot      release: id  3 | task 575 | stop processing: n_tokens = 1058, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.489 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 640 | processing task, is_child = 0
slot update_slots: id  3 | task 640 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2036
slot update_slots: id  3 | task 640 | n_tokens = 996, memory_seq_rm [996, end)
slot update_slots: id  3 | task 640 | prompt processing progress, n_tokens = 1972, batch.n_tokens = 976, progress = 0.968566
slot update_slots: id  3 | task 640 | n_tokens = 1972, memory_seq_rm [1972, end)
slot update_slots: id  3 | task 640 | prompt processing progress, n_tokens = 2036, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 640 | prompt done, n_tokens = 2036, batch.n_tokens = 64
slot init_sampler: id  3 | task 640 | init sampler, took 0.35 ms, tokens: text = 2036, total = 2036
slot update_slots: id  3 | task 640 | created context checkpoint 3 of 8 (pos_min = 976, pos_max = 1971, size = 23.355 MiB)
slot print_timing: id  3 | task 640 | 
prompt eval time =    1146.33 ms /  1040 tokens (    1.10 ms per token,   907.24 tokens per second)
       eval time =    1610.51 ms /    66 tokens (   24.40 ms per token,    40.98 tokens per second)
      total time =    2756.84 ms /  1106 tokens
slot      release: id  3 | task 640 | stop processing: n_tokens = 2101, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.699 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 708 | processing task, is_child = 0
slot update_slots: id  3 | task 708 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2914
slot update_slots: id  3 | task 708 | n_tokens = 2036, memory_seq_rm [2036, end)
slot update_slots: id  3 | task 708 | prompt processing progress, n_tokens = 2850, batch.n_tokens = 814, progress = 0.978037
slot update_slots: id  3 | task 708 | n_tokens = 2850, memory_seq_rm [2850, end)
slot update_slots: id  3 | task 708 | prompt processing progress, n_tokens = 2914, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 708 | prompt done, n_tokens = 2914, batch.n_tokens = 64
slot init_sampler: id  3 | task 708 | init sampler, took 0.49 ms, tokens: text = 2914, total = 2914
slot update_slots: id  3 | task 708 | created context checkpoint 4 of 8 (pos_min = 1826, pos_max = 2849, size = 24.012 MiB)
slot print_timing: id  3 | task 708 | 
prompt eval time =    1045.57 ms /   878 tokens (    1.19 ms per token,   839.73 tokens per second)
       eval time =    1593.59 ms /    64 tokens (   24.90 ms per token,    40.16 tokens per second)
      total time =    2639.16 ms /   942 tokens
slot      release: id  3 | task 708 | stop processing: n_tokens = 2977, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 774 | processing task, is_child = 0
slot update_slots: id  3 | task 774 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2991
slot update_slots: id  3 | task 774 | n_tokens = 2914, memory_seq_rm [2914, end)
slot update_slots: id  3 | task 774 | prompt processing progress, n_tokens = 2927, batch.n_tokens = 13, progress = 0.978602
slot update_slots: id  3 | task 774 | n_tokens = 2927, memory_seq_rm [2927, end)
slot update_slots: id  3 | task 774 | prompt processing progress, n_tokens = 2991, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 774 | prompt done, n_tokens = 2991, batch.n_tokens = 64
slot init_sampler: id  3 | task 774 | init sampler, took 0.51 ms, tokens: text = 2991, total = 2991
slot update_slots: id  3 | task 774 | created context checkpoint 5 of 8 (pos_min = 1953, pos_max = 2926, size = 22.840 MiB)
slot print_timing: id  3 | task 774 | 
prompt eval time =     269.67 ms /    77 tokens (    3.50 ms per token,   285.54 tokens per second)
       eval time =    1086.86 ms /    43 tokens (   25.28 ms per token,    39.56 tokens per second)
      total time =    1356.53 ms /   120 tokens
slot      release: id  3 | task 774 | stop processing: n_tokens = 3033, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.898 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 819 | processing task, is_child = 0
slot update_slots: id  3 | task 819 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3330
slot update_slots: id  3 | task 819 | n_tokens = 2991, memory_seq_rm [2991, end)
slot update_slots: id  3 | task 819 | prompt processing progress, n_tokens = 3266, batch.n_tokens = 275, progress = 0.980781
slot update_slots: id  3 | task 819 | n_tokens = 3266, memory_seq_rm [3266, end)
slot update_slots: id  3 | task 819 | prompt processing progress, n_tokens = 3330, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 819 | prompt done, n_tokens = 3330, batch.n_tokens = 64
slot init_sampler: id  3 | task 819 | init sampler, took 0.63 ms, tokens: text = 3330, total = 3330
slot update_slots: id  3 | task 819 | created context checkpoint 6 of 8 (pos_min = 2242, pos_max = 3265, size = 24.012 MiB)
slot print_timing: id  3 | task 819 | 
prompt eval time =     513.04 ms /   339 tokens (    1.51 ms per token,   660.76 tokens per second)
       eval time =    6599.44 ms /   251 tokens (   26.29 ms per token,    38.03 tokens per second)
      total time =    7112.48 ms /   590 tokens
slot      release: id  3 | task 819 | stop processing: n_tokens = 3580, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.770 (> 0.100 thold), f_keep = 0.930
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1072 | processing task, is_child = 0
slot update_slots: id  3 | task 1072 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4323
slot update_slots: id  3 | task 1072 | n_tokens = 3330, memory_seq_rm [3330, end)
slot update_slots: id  3 | task 1072 | prompt processing progress, n_tokens = 4259, batch.n_tokens = 929, progress = 0.985195
slot update_slots: id  3 | task 1072 | n_tokens = 4259, memory_seq_rm [4259, end)
slot update_slots: id  3 | task 1072 | prompt processing progress, n_tokens = 4323, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1072 | prompt done, n_tokens = 4323, batch.n_tokens = 64
slot init_sampler: id  3 | task 1072 | init sampler, took 0.65 ms, tokens: text = 4323, total = 4323
slot update_slots: id  3 | task 1072 | created context checkpoint 7 of 8 (pos_min = 3235, pos_max = 4258, size = 24.012 MiB)
slot print_timing: id  3 | task 1072 | 
prompt eval time =    1371.38 ms /   993 tokens (    1.38 ms per token,   724.09 tokens per second)
       eval time =    3409.00 ms /   130 tokens (   26.22 ms per token,    38.13 tokens per second)
      total time =    4780.39 ms /  1123 tokens
slot      release: id  3 | task 1072 | stop processing: n_tokens = 4452, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1204 | processing task, is_child = 0
slot update_slots: id  3 | task 1204 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4388
slot update_slots: id  3 | task 1204 | n_tokens = 4323, memory_seq_rm [4323, end)
slot update_slots: id  3 | task 1204 | prompt processing progress, n_tokens = 4324, batch.n_tokens = 1, progress = 0.985415
slot update_slots: id  3 | task 1204 | n_tokens = 4324, memory_seq_rm [4324, end)
slot update_slots: id  3 | task 1204 | prompt processing progress, n_tokens = 4388, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1204 | prompt done, n_tokens = 4388, batch.n_tokens = 64
slot init_sampler: id  3 | task 1204 | init sampler, took 0.72 ms, tokens: text = 4388, total = 4388
slot update_slots: id  3 | task 1204 | created context checkpoint 8 of 8 (pos_min = 3428, pos_max = 4323, size = 21.011 MiB)
slot print_timing: id  3 | task 1204 | 
prompt eval time =     191.73 ms /    65 tokens (    2.95 ms per token,   339.02 tokens per second)
       eval time =   18552.55 ms /   744 tokens (   24.94 ms per token,    40.10 tokens per second)
      total time =   18744.28 ms /   809 tokens
slot      release: id  3 | task 1204 | stop processing: n_tokens = 5131, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.094
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5131, total state size = 144.329 MiB
srv          load:  - looking for better prompt, base f_keep = 0.094, sim = 0.988
srv        update:  - cache state: 2 prompts, 371.693 MiB (limits: 8192.000 MiB, 64000 tokens, 134596 est)
srv        update:    - prompt 0x55bd8f1cbf80:     976 tokens, checkpoints:  1,    56.020 MiB
srv        update:    - prompt 0x55bd96905720:    5131 tokens, checkpoints:  8,   315.673 MiB
srv  get_availabl: prompt cache update took 224.38 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1950 | processing task, is_child = 0
slot update_slots: id  3 | task 1950 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 488
slot update_slots: id  3 | task 1950 | n_past = 482, slot.prompt.tokens.size() = 5131, seq_id = 3, pos_min = 4107, n_swa = 128
slot update_slots: id  3 | task 1950 | restored context checkpoint (pos_min = 0, pos_max = 931, size = 21.855 MiB)
slot update_slots: id  3 | task 1950 | erased invalidated context checkpoint (pos_min = 976, pos_max = 1971, n_swa = 128, size = 23.355 MiB)
slot update_slots: id  3 | task 1950 | erased invalidated context checkpoint (pos_min = 1826, pos_max = 2849, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1950 | erased invalidated context checkpoint (pos_min = 1953, pos_max = 2926, n_swa = 128, size = 22.840 MiB)
slot update_slots: id  3 | task 1950 | erased invalidated context checkpoint (pos_min = 2242, pos_max = 3265, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1950 | erased invalidated context checkpoint (pos_min = 3235, pos_max = 4258, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1950 | erased invalidated context checkpoint (pos_min = 3428, pos_max = 4323, n_swa = 128, size = 21.011 MiB)
slot update_slots: id  3 | task 1950 | n_tokens = 482, memory_seq_rm [482, end)
slot update_slots: id  3 | task 1950 | prompt processing progress, n_tokens = 488, batch.n_tokens = 6, progress = 1.000000
slot update_slots: id  3 | task 1950 | prompt done, n_tokens = 488, batch.n_tokens = 6
slot init_sampler: id  3 | task 1950 | init sampler, took 0.10 ms, tokens: text = 488, total = 488
slot print_timing: id  3 | task 1950 | 
prompt eval time =     107.41 ms /     6 tokens (   17.90 ms per token,    55.86 tokens per second)
       eval time =    2218.54 ms /    96 tokens (   23.11 ms per token,    43.27 tokens per second)
      total time =    2325.96 ms /   102 tokens
slot      release: id  3 | task 1950 | stop processing: n_tokens = 583, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.827
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2047 | processing task, is_child = 0
slot update_slots: id  3 | task 2047 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 495
slot update_slots: id  3 | task 2047 | n_tokens = 482, memory_seq_rm [482, end)
slot update_slots: id  3 | task 2047 | prompt processing progress, n_tokens = 495, batch.n_tokens = 13, progress = 1.000000
slot update_slots: id  3 | task 2047 | prompt done, n_tokens = 495, batch.n_tokens = 13
slot init_sampler: id  3 | task 2047 | init sampler, took 0.09 ms, tokens: text = 495, total = 495
slot print_timing: id  3 | task 2047 | 
prompt eval time =     120.40 ms /    13 tokens (    9.26 ms per token,   107.97 tokens per second)
       eval time =    1435.53 ms /    62 tokens (   23.15 ms per token,    43.19 tokens per second)
      total time =    1555.93 ms /    75 tokens
slot      release: id  3 | task 2047 | stop processing: n_tokens = 556, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.519 (> 0.100 thold), f_keep = 0.890
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2110 | processing task, is_child = 0
slot update_slots: id  3 | task 2110 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 954
slot update_slots: id  3 | task 2110 | n_tokens = 495, memory_seq_rm [495, end)
slot update_slots: id  3 | task 2110 | prompt processing progress, n_tokens = 890, batch.n_tokens = 395, progress = 0.932914
slot update_slots: id  3 | task 2110 | n_tokens = 890, memory_seq_rm [890, end)
slot update_slots: id  3 | task 2110 | prompt processing progress, n_tokens = 954, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2110 | prompt done, n_tokens = 954, batch.n_tokens = 64
slot init_sampler: id  3 | task 2110 | init sampler, took 0.18 ms, tokens: text = 954, total = 954
slot print_timing: id  3 | task 2110 | 
prompt eval time =     514.01 ms /   459 tokens (    1.12 ms per token,   892.98 tokens per second)
       eval time =    1608.60 ms /    69 tokens (   23.31 ms per token,    42.89 tokens per second)
      total time =    2122.61 ms /   528 tokens
slot      release: id  3 | task 2110 | stop processing: n_tokens = 1022, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.478 (> 0.100 thold), f_keep = 0.933
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2181 | processing task, is_child = 0
slot update_slots: id  3 | task 2181 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1994
slot update_slots: id  3 | task 2181 | n_tokens = 954, memory_seq_rm [954, end)
slot update_slots: id  3 | task 2181 | prompt processing progress, n_tokens = 1930, batch.n_tokens = 976, progress = 0.967904
slot update_slots: id  3 | task 2181 | n_tokens = 1930, memory_seq_rm [1930, end)
slot update_slots: id  3 | task 2181 | prompt processing progress, n_tokens = 1994, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2181 | prompt done, n_tokens = 1994, batch.n_tokens = 64
slot init_sampler: id  3 | task 2181 | init sampler, took 0.34 ms, tokens: text = 1994, total = 1994
slot update_slots: id  3 | task 2181 | created context checkpoint 3 of 8 (pos_min = 906, pos_max = 1929, size = 24.012 MiB)
slot print_timing: id  3 | task 2181 | 
prompt eval time =    1113.52 ms /  1040 tokens (    1.07 ms per token,   933.97 tokens per second)
       eval time =    5862.72 ms /   242 tokens (   24.23 ms per token,    41.28 tokens per second)
      total time =    6976.24 ms /  1282 tokens
slot      release: id  3 | task 2181 | stop processing: n_tokens = 2235, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.892
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2425 | processing task, is_child = 0
slot update_slots: id  3 | task 2425 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2071
slot update_slots: id  3 | task 2425 | n_tokens = 1994, memory_seq_rm [1994, end)
slot update_slots: id  3 | task 2425 | prompt processing progress, n_tokens = 2007, batch.n_tokens = 13, progress = 0.969097
slot update_slots: id  3 | task 2425 | n_tokens = 2007, memory_seq_rm [2007, end)
slot update_slots: id  3 | task 2425 | prompt processing progress, n_tokens = 2071, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2425 | prompt done, n_tokens = 2071, batch.n_tokens = 64
slot init_sampler: id  3 | task 2425 | init sampler, took 0.39 ms, tokens: text = 2071, total = 2071
slot update_slots: id  3 | task 2425 | created context checkpoint 4 of 8 (pos_min = 1211, pos_max = 2006, size = 18.666 MiB)
slot print_timing: id  3 | task 2425 | 
prompt eval time =     234.10 ms /    77 tokens (    3.04 ms per token,   328.92 tokens per second)
       eval time =     897.02 ms /    36 tokens (   24.92 ms per token,    40.13 tokens per second)
      total time =    1131.12 ms /   113 tokens
slot      release: id  3 | task 2425 | stop processing: n_tokens = 2106, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2463 | processing task, is_child = 0
slot update_slots: id  3 | task 2463 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2136
slot update_slots: id  3 | task 2463 | n_tokens = 2071, memory_seq_rm [2071, end)
slot update_slots: id  3 | task 2463 | prompt processing progress, n_tokens = 2072, batch.n_tokens = 1, progress = 0.970037
slot update_slots: id  3 | task 2463 | n_tokens = 2072, memory_seq_rm [2072, end)
slot update_slots: id  3 | task 2463 | prompt processing progress, n_tokens = 2136, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2463 | prompt done, n_tokens = 2136, batch.n_tokens = 64
slot init_sampler: id  3 | task 2463 | init sampler, took 0.40 ms, tokens: text = 2136, total = 2136
slot update_slots: id  3 | task 2463 | created context checkpoint 5 of 8 (pos_min = 1211, pos_max = 2071, size = 20.190 MiB)
slot print_timing: id  3 | task 2463 | 
prompt eval time =     184.93 ms /    65 tokens (    2.85 ms per token,   351.48 tokens per second)
       eval time =   58184.66 ms /  2343 tokens (   24.83 ms per token,    40.27 tokens per second)
      total time =   58369.60 ms /  2408 tokens
slot      release: id  3 | task 2463 | stop processing: n_tokens = 4478, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.113 (> 0.100 thold), f_keep = 0.111
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4478, total state size = 129.017 MiB
srv          load:  - looking for better prompt, base f_keep = 0.111, sim = 0.113
srv        update:  - cache state: 3 prompts, 595.679 MiB (limits: 8192.000 MiB, 64000 tokens, 145568 est)
srv        update:    - prompt 0x55bd8f1cbf80:     976 tokens, checkpoints:  1,    56.020 MiB
srv        update:    - prompt 0x55bd96905720:    5131 tokens, checkpoints:  8,   315.673 MiB
srv        update:    - prompt 0x55bd9697ce40:    4478 tokens, checkpoints:  5,   223.986 MiB
srv  get_availabl: prompt cache update took 179.14 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4808 | processing task, is_child = 0
slot update_slots: id  3 | task 4808 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4370
slot update_slots: id  3 | task 4808 | n_past = 495, slot.prompt.tokens.size() = 4478, seq_id = 3, pos_min = 3454, n_swa = 128
slot update_slots: id  3 | task 4808 | restored context checkpoint (pos_min = 0, pos_max = 931, size = 21.855 MiB)
slot update_slots: id  3 | task 4808 | erased invalidated context checkpoint (pos_min = 906, pos_max = 1929, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 4808 | erased invalidated context checkpoint (pos_min = 1211, pos_max = 2006, n_swa = 128, size = 18.666 MiB)
slot update_slots: id  3 | task 4808 | erased invalidated context checkpoint (pos_min = 1211, pos_max = 2071, n_swa = 128, size = 20.190 MiB)
slot update_slots: id  3 | task 4808 | n_tokens = 495, memory_seq_rm [495, end)
slot update_slots: id  3 | task 4808 | prompt processing progress, n_tokens = 2543, batch.n_tokens = 2048, progress = 0.581922
slot update_slots: id  3 | task 4808 | n_tokens = 2543, memory_seq_rm [2543, end)
slot update_slots: id  3 | task 4808 | prompt processing progress, n_tokens = 4306, batch.n_tokens = 1763, progress = 0.985355
slot update_slots: id  3 | task 4808 | n_tokens = 4306, memory_seq_rm [4306, end)
slot update_slots: id  3 | task 4808 | prompt processing progress, n_tokens = 4370, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4808 | prompt done, n_tokens = 4370, batch.n_tokens = 64
slot init_sampler: id  3 | task 4808 | init sampler, took 0.66 ms, tokens: text = 4370, total = 4370
slot update_slots: id  3 | task 4808 | created context checkpoint 3 of 8 (pos_min = 3282, pos_max = 4305, size = 24.012 MiB)
slot print_timing: id  3 | task 4808 | 
prompt eval time =    3971.34 ms /  3875 tokens (    1.02 ms per token,   975.74 tokens per second)
       eval time =    5899.28 ms /   245 tokens (   24.08 ms per token,    41.53 tokens per second)
      total time =    9870.63 ms /  4120 tokens
slot      release: id  3 | task 4808 | stop processing: n_tokens = 4614, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.947
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5056 | processing task, is_child = 0
slot update_slots: id  3 | task 5056 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4504
slot update_slots: id  3 | task 5056 | n_tokens = 4371, memory_seq_rm [4371, end)
slot update_slots: id  3 | task 5056 | prompt processing progress, n_tokens = 4440, batch.n_tokens = 69, progress = 0.985790
slot update_slots: id  3 | task 5056 | n_tokens = 4440, memory_seq_rm [4440, end)
slot update_slots: id  3 | task 5056 | prompt processing progress, n_tokens = 4504, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5056 | prompt done, n_tokens = 4504, batch.n_tokens = 64
slot init_sampler: id  3 | task 5056 | init sampler, took 0.85 ms, tokens: text = 4504, total = 4504
slot update_slots: id  3 | task 5056 | created context checkpoint 4 of 8 (pos_min = 3590, pos_max = 4439, size = 19.932 MiB)
slot print_timing: id  3 | task 5056 | 
prompt eval time =     455.10 ms /   133 tokens (    3.42 ms per token,   292.24 tokens per second)
       eval time =   35374.38 ms /  1312 tokens (   26.96 ms per token,    37.09 tokens per second)
      total time =   35829.49 ms /  1445 tokens
slot      release: id  3 | task 5056 | stop processing: n_tokens = 5815, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.785 (> 0.100 thold), f_keep = 0.775
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6370 | processing task, is_child = 0
slot update_slots: id  3 | task 6370 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5741
slot update_slots: id  3 | task 6370 | n_past = 4504, slot.prompt.tokens.size() = 5815, seq_id = 3, pos_min = 4791, n_swa = 128
slot update_slots: id  3 | task 6370 | restored context checkpoint (pos_min = 3590, pos_max = 4439, size = 19.932 MiB)
slot update_slots: id  3 | task 6370 | n_tokens = 4439, memory_seq_rm [4439, end)
slot update_slots: id  3 | task 6370 | prompt processing progress, n_tokens = 5677, batch.n_tokens = 1238, progress = 0.988852
slot update_slots: id  3 | task 6370 | n_tokens = 5677, memory_seq_rm [5677, end)
slot update_slots: id  3 | task 6370 | prompt processing progress, n_tokens = 5741, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6370 | prompt done, n_tokens = 5741, batch.n_tokens = 64
slot init_sampler: id  3 | task 6370 | init sampler, took 0.85 ms, tokens: text = 5741, total = 5741
slot update_slots: id  3 | task 6370 | created context checkpoint 5 of 8 (pos_min = 4653, pos_max = 5676, size = 24.012 MiB)
slot print_timing: id  3 | task 6370 | 
prompt eval time =    1547.35 ms /  1302 tokens (    1.19 ms per token,   841.44 tokens per second)
       eval time =    2805.59 ms /   116 tokens (   24.19 ms per token,    41.35 tokens per second)
      total time =    4352.94 ms /  1418 tokens
slot      release: id  3 | task 6370 | stop processing: n_tokens = 5856, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 6488 | processing task, is_child = 0
slot update_slots: id  2 | task 6488 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6042
slot update_slots: id  2 | task 6488 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 6488 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.338961
slot update_slots: id  2 | task 6488 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 6488 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.677921
slot update_slots: id  2 | task 6488 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 6488 | prompt processing progress, n_tokens = 5978, batch.n_tokens = 1882, progress = 0.989407
slot update_slots: id  2 | task 6488 | n_tokens = 5978, memory_seq_rm [5978, end)
slot update_slots: id  2 | task 6488 | prompt processing progress, n_tokens = 6042, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 6488 | prompt done, n_tokens = 6042, batch.n_tokens = 64
slot init_sampler: id  2 | task 6488 | init sampler, took 1.14 ms, tokens: text = 6042, total = 6042
slot update_slots: id  2 | task 6488 | created context checkpoint 1 of 8 (pos_min = 5081, pos_max = 5977, size = 21.034 MiB)
slot print_timing: id  2 | task 6488 | 
prompt eval time =    6633.96 ms /  6042 tokens (    1.10 ms per token,   910.77 tokens per second)
       eval time =    1598.06 ms /    62 tokens (   25.78 ms per token,    38.80 tokens per second)
      total time =    8232.02 ms /  6104 tokens
slot      release: id  2 | task 6488 | stop processing: n_tokens = 6103, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.873 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 6554 | processing task, is_child = 0
slot update_slots: id  2 | task 6554 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6920
slot update_slots: id  2 | task 6554 | n_tokens = 6042, memory_seq_rm [6042, end)
slot update_slots: id  2 | task 6554 | prompt processing progress, n_tokens = 6856, batch.n_tokens = 814, progress = 0.990751
slot update_slots: id  2 | task 6554 | n_tokens = 6856, memory_seq_rm [6856, end)
slot update_slots: id  2 | task 6554 | prompt processing progress, n_tokens = 6920, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 6554 | prompt done, n_tokens = 6920, batch.n_tokens = 64
slot init_sampler: id  2 | task 6554 | init sampler, took 1.40 ms, tokens: text = 6920, total = 6920
slot update_slots: id  2 | task 6554 | created context checkpoint 2 of 8 (pos_min = 5959, pos_max = 6855, size = 21.034 MiB)
slot print_timing: id  2 | task 6554 | 
prompt eval time =    1224.78 ms /   878 tokens (    1.39 ms per token,   716.86 tokens per second)
       eval time =    1579.04 ms /    62 tokens (   25.47 ms per token,    39.26 tokens per second)
      total time =    2803.82 ms /   940 tokens
slot      release: id  2 | task 6554 | stop processing: n_tokens = 6981, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.875 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 6618 | processing task, is_child = 0
slot update_slots: id  2 | task 6618 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7913
slot update_slots: id  2 | task 6618 | n_tokens = 6920, memory_seq_rm [6920, end)
slot update_slots: id  2 | task 6618 | prompt processing progress, n_tokens = 7849, batch.n_tokens = 929, progress = 0.991912
slot update_slots: id  2 | task 6618 | n_tokens = 7849, memory_seq_rm [7849, end)
slot update_slots: id  2 | task 6618 | prompt processing progress, n_tokens = 7913, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 6618 | prompt done, n_tokens = 7913, batch.n_tokens = 64
slot init_sampler: id  2 | task 6618 | init sampler, took 1.48 ms, tokens: text = 7913, total = 7913
slot update_slots: id  2 | task 6618 | created context checkpoint 3 of 8 (pos_min = 6952, pos_max = 7848, size = 21.034 MiB)
slot print_timing: id  2 | task 6618 | 
prompt eval time =    1306.30 ms /   993 tokens (    1.32 ms per token,   760.16 tokens per second)
       eval time =   46378.14 ms /  1780 tokens (   26.06 ms per token,    38.38 tokens per second)
      total time =   47684.44 ms /  2773 tokens
slot      release: id  2 | task 6618 | stop processing: n_tokens = 9692, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.836 (> 0.100 thold), f_keep = 0.816
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 8400 | processing task, is_child = 0
slot update_slots: id  2 | task 8400 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9460
slot update_slots: id  2 | task 8400 | n_past = 7913, slot.prompt.tokens.size() = 9692, seq_id = 2, pos_min = 8795, n_swa = 128
slot update_slots: id  2 | task 8400 | restored context checkpoint (pos_min = 6952, pos_max = 7848, size = 21.034 MiB)
slot update_slots: id  2 | task 8400 | n_tokens = 7848, memory_seq_rm [7848, end)
slot update_slots: id  2 | task 8400 | prompt processing progress, n_tokens = 9396, batch.n_tokens = 1548, progress = 0.993235
slot update_slots: id  2 | task 8400 | n_tokens = 9396, memory_seq_rm [9396, end)
slot update_slots: id  2 | task 8400 | prompt processing progress, n_tokens = 9460, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 8400 | prompt done, n_tokens = 9460, batch.n_tokens = 64
slot init_sampler: id  2 | task 8400 | init sampler, took 1.45 ms, tokens: text = 9460, total = 9460
slot update_slots: id  2 | task 8400 | created context checkpoint 4 of 8 (pos_min = 8499, pos_max = 9395, size = 21.034 MiB)
slot print_timing: id  2 | task 8400 | 
prompt eval time =    2152.59 ms /  1612 tokens (    1.34 ms per token,   748.86 tokens per second)
       eval time =    2659.69 ms /   106 tokens (   25.09 ms per token,    39.85 tokens per second)
      total time =    4812.28 ms /  1718 tokens
slot      release: id  2 | task 8400 | stop processing: n_tokens = 9565, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.356 (> 0.100 thold), f_keep = 0.017
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9565, total state size = 245.323 MiB
srv          load:  - looking for better prompt, base f_keep = 0.017, sim = 0.356
srv        update:  - cache state: 4 prompts, 925.139 MiB (limits: 8192.000 MiB, 64000 tokens, 178425 est)
srv        update:    - prompt 0x55bd8f1cbf80:     976 tokens, checkpoints:  1,    56.020 MiB
srv        update:    - prompt 0x55bd96905720:    5131 tokens, checkpoints:  8,   315.673 MiB
srv        update:    - prompt 0x55bd9697ce40:    4478 tokens, checkpoints:  5,   223.986 MiB
srv        update:    - prompt 0x55bd96c6b950:    9565 tokens, checkpoints:  4,   329.459 MiB
srv  get_availabl: prompt cache update took 314.57 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 8508 | processing task, is_child = 0
slot update_slots: id  2 | task 8508 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 461
slot update_slots: id  2 | task 8508 | n_past = 164, slot.prompt.tokens.size() = 9565, seq_id = 2, pos_min = 8668, n_swa = 128
slot update_slots: id  2 | task 8508 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 8508 | erased invalidated context checkpoint (pos_min = 5081, pos_max = 5977, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 8508 | erased invalidated context checkpoint (pos_min = 5959, pos_max = 6855, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 8508 | erased invalidated context checkpoint (pos_min = 6952, pos_max = 7848, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 8508 | erased invalidated context checkpoint (pos_min = 8499, pos_max = 9395, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 8508 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 8508 | prompt processing progress, n_tokens = 397, batch.n_tokens = 397, progress = 0.861171
slot update_slots: id  2 | task 8508 | n_tokens = 397, memory_seq_rm [397, end)
slot update_slots: id  2 | task 8508 | prompt processing progress, n_tokens = 461, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 8508 | prompt done, n_tokens = 461, batch.n_tokens = 64
slot init_sampler: id  2 | task 8508 | init sampler, took 0.09 ms, tokens: text = 461, total = 461
slot update_slots: id  2 | task 8508 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 396, size = 9.310 MiB)
slot print_timing: id  2 | task 8508 | 
prompt eval time =     706.66 ms /   461 tokens (    1.53 ms per token,   652.37 tokens per second)
       eval time =    6851.18 ms /   282 tokens (   24.29 ms per token,    41.16 tokens per second)
      total time =    7557.83 ms /   743 tokens
slot      release: id  2 | task 8508 | stop processing: n_tokens = 742, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.611
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 8792 | processing task, is_child = 0
slot update_slots: id  2 | task 8792 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 472
slot update_slots: id  2 | task 8792 | n_tokens = 453, memory_seq_rm [453, end)
slot update_slots: id  2 | task 8792 | prompt processing progress, n_tokens = 472, batch.n_tokens = 19, progress = 1.000000
slot update_slots: id  2 | task 8792 | prompt done, n_tokens = 472, batch.n_tokens = 19
slot init_sampler: id  2 | task 8792 | init sampler, took 0.10 ms, tokens: text = 472, total = 472
slot print_timing: id  2 | task 8792 | 
prompt eval time =     162.08 ms /    19 tokens (    8.53 ms per token,   117.22 tokens per second)
       eval time =    1319.84 ms /    54 tokens (   24.44 ms per token,    40.91 tokens per second)
      total time =    1481.92 ms /    73 tokens
slot      release: id  2 | task 8792 | stop processing: n_tokens = 525, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.508 (> 0.100 thold), f_keep = 0.899
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 8847 | processing task, is_child = 0
slot update_slots: id  2 | task 8847 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 930
slot update_slots: id  2 | task 8847 | n_tokens = 472, memory_seq_rm [472, end)
slot update_slots: id  2 | task 8847 | prompt processing progress, n_tokens = 866, batch.n_tokens = 394, progress = 0.931183
slot update_slots: id  2 | task 8847 | n_tokens = 866, memory_seq_rm [866, end)
slot update_slots: id  2 | task 8847 | prompt processing progress, n_tokens = 930, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 8847 | prompt done, n_tokens = 930, batch.n_tokens = 64
slot init_sampler: id  2 | task 8847 | init sampler, took 0.16 ms, tokens: text = 930, total = 930
slot update_slots: id  2 | task 8847 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 865, size = 20.307 MiB)
slot print_timing: id  2 | task 8847 | 
prompt eval time =     595.01 ms /   458 tokens (    1.30 ms per token,   769.74 tokens per second)
       eval time =    2126.82 ms /    85 tokens (   25.02 ms per token,    39.97 tokens per second)
      total time =    2721.82 ms /   543 tokens
slot      release: id  2 | task 8847 | stop processing: n_tokens = 1014, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.586 (> 0.100 thold), f_keep = 0.917
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 8934 | processing task, is_child = 0
slot update_slots: id  2 | task 8934 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1587
slot update_slots: id  2 | task 8934 | n_tokens = 930, memory_seq_rm [930, end)
slot update_slots: id  2 | task 8934 | prompt processing progress, n_tokens = 1523, batch.n_tokens = 593, progress = 0.959672
slot update_slots: id  2 | task 8934 | n_tokens = 1523, memory_seq_rm [1523, end)
slot update_slots: id  2 | task 8934 | prompt processing progress, n_tokens = 1587, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 8934 | prompt done, n_tokens = 1587, batch.n_tokens = 64
slot init_sampler: id  2 | task 8934 | init sampler, took 0.28 ms, tokens: text = 1587, total = 1587
slot update_slots: id  2 | task 8934 | created context checkpoint 3 of 8 (pos_min = 626, pos_max = 1522, size = 21.034 MiB)
slot print_timing: id  2 | task 8934 | 
prompt eval time =     970.36 ms /   657 tokens (    1.48 ms per token,   677.07 tokens per second)
       eval time =   10474.99 ms /   360 tokens (   29.10 ms per token,    34.37 tokens per second)
      total time =   11445.35 ms /  1017 tokens
slot      release: id  2 | task 8934 | stop processing: n_tokens = 1946, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.863 (> 0.100 thold), f_keep = 0.816
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9296 | processing task, is_child = 0
slot update_slots: id  2 | task 9296 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1840
slot update_slots: id  2 | task 9296 | n_tokens = 1587, memory_seq_rm [1587, end)
slot update_slots: id  2 | task 9296 | prompt processing progress, n_tokens = 1776, batch.n_tokens = 189, progress = 0.965217
slot update_slots: id  2 | task 9296 | n_tokens = 1776, memory_seq_rm [1776, end)
slot update_slots: id  2 | task 9296 | prompt processing progress, n_tokens = 1840, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9296 | prompt done, n_tokens = 1840, batch.n_tokens = 64
slot init_sampler: id  2 | task 9296 | init sampler, took 0.36 ms, tokens: text = 1840, total = 1840
slot update_slots: id  2 | task 9296 | created context checkpoint 4 of 8 (pos_min = 1086, pos_max = 1775, size = 16.180 MiB)
slot print_timing: id  2 | task 9296 | 
prompt eval time =     786.26 ms /   253 tokens (    3.11 ms per token,   321.78 tokens per second)
       eval time =    1459.75 ms /    52 tokens (   28.07 ms per token,    35.62 tokens per second)
      total time =    2246.01 ms /   305 tokens
slot      release: id  2 | task 9296 | stop processing: n_tokens = 1891, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.737 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9350 | processing task, is_child = 0
slot update_slots: id  2 | task 9350 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2497
slot update_slots: id  2 | task 9350 | n_tokens = 1840, memory_seq_rm [1840, end)
slot update_slots: id  2 | task 9350 | prompt processing progress, n_tokens = 2433, batch.n_tokens = 593, progress = 0.974369
slot update_slots: id  2 | task 9350 | n_tokens = 2433, memory_seq_rm [2433, end)
slot update_slots: id  2 | task 9350 | prompt processing progress, n_tokens = 2497, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9350 | prompt done, n_tokens = 2497, batch.n_tokens = 64
slot init_sampler: id  2 | task 9350 | init sampler, took 0.41 ms, tokens: text = 2497, total = 2497
slot update_slots: id  2 | task 9350 | created context checkpoint 5 of 8 (pos_min = 1587, pos_max = 2432, size = 19.838 MiB)
slot print_timing: id  2 | task 9350 | 
prompt eval time =    1221.32 ms /   657 tokens (    1.86 ms per token,   537.94 tokens per second)
       eval time =    7255.57 ms /   278 tokens (   26.10 ms per token,    38.32 tokens per second)
      total time =    8476.89 ms /   935 tokens
slot      release: id  2 | task 9350 | stop processing: n_tokens = 2774, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.910 (> 0.100 thold), f_keep = 0.900
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9630 | processing task, is_child = 0
slot update_slots: id  2 | task 9630 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2744
slot update_slots: id  2 | task 9630 | n_tokens = 2497, memory_seq_rm [2497, end)
slot update_slots: id  2 | task 9630 | prompt processing progress, n_tokens = 2680, batch.n_tokens = 183, progress = 0.976676
slot update_slots: id  2 | task 9630 | n_tokens = 2680, memory_seq_rm [2680, end)
slot update_slots: id  2 | task 9630 | prompt processing progress, n_tokens = 2744, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9630 | prompt done, n_tokens = 2744, batch.n_tokens = 64
slot init_sampler: id  2 | task 9630 | init sampler, took 0.52 ms, tokens: text = 2744, total = 2744
slot update_slots: id  2 | task 9630 | created context checkpoint 6 of 8 (pos_min = 1877, pos_max = 2679, size = 18.830 MiB)
slot print_timing: id  2 | task 9630 | 
prompt eval time =     494.95 ms /   247 tokens (    2.00 ms per token,   499.04 tokens per second)
       eval time =    2101.24 ms /    81 tokens (   25.94 ms per token,    38.55 tokens per second)
      total time =    2596.20 ms /   328 tokens
slot      release: id  2 | task 9630 | stop processing: n_tokens = 2824, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.752 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9713 | processing task, is_child = 0
slot update_slots: id  2 | task 9713 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3651
slot update_slots: id  2 | task 9713 | n_tokens = 2744, memory_seq_rm [2744, end)
slot update_slots: id  2 | task 9713 | prompt processing progress, n_tokens = 3587, batch.n_tokens = 843, progress = 0.982471
slot update_slots: id  2 | task 9713 | n_tokens = 3587, memory_seq_rm [3587, end)
slot update_slots: id  2 | task 9713 | prompt processing progress, n_tokens = 3651, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9713 | prompt done, n_tokens = 3651, batch.n_tokens = 64
slot init_sampler: id  2 | task 9713 | init sampler, took 0.59 ms, tokens: text = 3651, total = 3651
slot update_slots: id  2 | task 9713 | created context checkpoint 7 of 8 (pos_min = 2690, pos_max = 3586, size = 21.034 MiB)
slot print_timing: id  2 | task 9713 | 
prompt eval time =    1140.73 ms /   907 tokens (    1.26 ms per token,   795.11 tokens per second)
       eval time =    5680.33 ms /   228 tokens (   24.91 ms per token,    40.14 tokens per second)
      total time =    6821.06 ms /  1135 tokens
slot      release: id  2 | task 9713 | stop processing: n_tokens = 3878, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.941 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9943 | processing task, is_child = 0
slot update_slots: id  2 | task 9943 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3881
slot update_slots: id  2 | task 9943 | n_tokens = 3651, memory_seq_rm [3651, end)
slot update_slots: id  2 | task 9943 | prompt processing progress, n_tokens = 3817, batch.n_tokens = 166, progress = 0.983509
slot update_slots: id  2 | task 9943 | n_tokens = 3817, memory_seq_rm [3817, end)
slot update_slots: id  2 | task 9943 | prompt processing progress, n_tokens = 3881, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9943 | prompt done, n_tokens = 3881, batch.n_tokens = 64
slot init_sampler: id  2 | task 9943 | init sampler, took 0.64 ms, tokens: text = 3881, total = 3881
slot update_slots: id  2 | task 9943 | created context checkpoint 8 of 8 (pos_min = 2981, pos_max = 3816, size = 19.604 MiB)
slot print_timing: id  2 | task 9943 | 
prompt eval time =     452.37 ms /   230 tokens (    1.97 ms per token,   508.43 tokens per second)
       eval time =    1699.26 ms /    68 tokens (   24.99 ms per token,    40.02 tokens per second)
      total time =    2151.63 ms /   298 tokens
slot      release: id  2 | task 9943 | stop processing: n_tokens = 3948, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 10013 | processing task, is_child = 0
slot update_slots: id  2 | task 10013 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4031
slot update_slots: id  2 | task 10013 | n_tokens = 3881, memory_seq_rm [3881, end)
slot update_slots: id  2 | task 10013 | prompt processing progress, n_tokens = 3967, batch.n_tokens = 86, progress = 0.984123
slot update_slots: id  2 | task 10013 | n_tokens = 3967, memory_seq_rm [3967, end)
slot update_slots: id  2 | task 10013 | prompt processing progress, n_tokens = 4031, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 10013 | prompt done, n_tokens = 4031, batch.n_tokens = 64
slot init_sampler: id  2 | task 10013 | init sampler, took 0.63 ms, tokens: text = 4031, total = 4031
slot update_slots: id  2 | task 10013 | erasing old context checkpoint (pos_min = 0, pos_max = 396, size = 9.310 MiB)
slot update_slots: id  2 | task 10013 | created context checkpoint 8 of 8 (pos_min = 3070, pos_max = 3966, size = 21.034 MiB)
slot print_timing: id  2 | task 10013 | 
prompt eval time =     408.12 ms /   150 tokens (    2.72 ms per token,   367.53 tokens per second)
       eval time =   19614.40 ms /   794 tokens (   24.70 ms per token,    40.48 tokens per second)
      total time =   20022.53 ms /   944 tokens
slot      release: id  2 | task 10013 | stop processing: n_tokens = 4824, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.836
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 10809 | processing task, is_child = 0
slot update_slots: id  2 | task 10809 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4248
slot update_slots: id  2 | task 10809 | n_past = 4031, slot.prompt.tokens.size() = 4824, seq_id = 2, pos_min = 3927, n_swa = 128
slot update_slots: id  2 | task 10809 | restored context checkpoint (pos_min = 3070, pos_max = 3966, size = 21.034 MiB)
slot update_slots: id  2 | task 10809 | n_tokens = 3966, memory_seq_rm [3966, end)
slot update_slots: id  2 | task 10809 | prompt processing progress, n_tokens = 4184, batch.n_tokens = 218, progress = 0.984934
slot update_slots: id  2 | task 10809 | n_tokens = 4184, memory_seq_rm [4184, end)
slot update_slots: id  2 | task 10809 | prompt processing progress, n_tokens = 4248, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 10809 | prompt done, n_tokens = 4248, batch.n_tokens = 64
slot init_sampler: id  2 | task 10809 | init sampler, took 0.82 ms, tokens: text = 4248, total = 4248
slot update_slots: id  2 | task 10809 | erasing old context checkpoint (pos_min = 0, pos_max = 865, size = 20.307 MiB)
slot update_slots: id  2 | task 10809 | created context checkpoint 8 of 8 (pos_min = 3287, pos_max = 4183, size = 21.034 MiB)
slot print_timing: id  2 | task 10809 | 
prompt eval time =     633.41 ms /   282 tokens (    2.25 ms per token,   445.21 tokens per second)
       eval time =    5458.80 ms /   223 tokens (   24.48 ms per token,    40.85 tokens per second)
      total time =    6092.20 ms /   505 tokens
slot      release: id  2 | task 10809 | stop processing: n_tokens = 4470, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.950
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11034 | processing task, is_child = 0
slot update_slots: id  2 | task 11034 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4416
slot update_slots: id  2 | task 11034 | n_tokens = 4248, memory_seq_rm [4248, end)
slot update_slots: id  2 | task 11034 | prompt processing progress, n_tokens = 4352, batch.n_tokens = 104, progress = 0.985507
slot update_slots: id  2 | task 11034 | n_tokens = 4352, memory_seq_rm [4352, end)
slot update_slots: id  2 | task 11034 | prompt processing progress, n_tokens = 4416, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11034 | prompt done, n_tokens = 4416, batch.n_tokens = 64
slot init_sampler: id  2 | task 11034 | init sampler, took 0.67 ms, tokens: text = 4416, total = 4416
slot update_slots: id  2 | task 11034 | erasing old context checkpoint (pos_min = 626, pos_max = 1522, size = 21.034 MiB)
slot update_slots: id  2 | task 11034 | created context checkpoint 8 of 8 (pos_min = 3573, pos_max = 4351, size = 18.267 MiB)
slot print_timing: id  2 | task 11034 | 
prompt eval time =     415.94 ms /   168 tokens (    2.48 ms per token,   403.90 tokens per second)
       eval time =   15275.07 ms /   617 tokens (   24.76 ms per token,    40.39 tokens per second)
      total time =   15691.01 ms /   785 tokens
slot      release: id  2 | task 11034 | stop processing: n_tokens = 5032, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.943 (> 0.100 thold), f_keep = 0.878
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11653 | processing task, is_child = 0
slot update_slots: id  2 | task 11653 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4682
slot update_slots: id  2 | task 11653 | n_tokens = 4416, memory_seq_rm [4416, end)
slot update_slots: id  2 | task 11653 | prompt processing progress, n_tokens = 4618, batch.n_tokens = 202, progress = 0.986331
slot update_slots: id  2 | task 11653 | n_tokens = 4618, memory_seq_rm [4618, end)
slot update_slots: id  2 | task 11653 | prompt processing progress, n_tokens = 4682, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11653 | prompt done, n_tokens = 4682, batch.n_tokens = 64
slot init_sampler: id  2 | task 11653 | init sampler, took 0.80 ms, tokens: text = 4682, total = 4682
slot update_slots: id  2 | task 11653 | erasing old context checkpoint (pos_min = 1086, pos_max = 1775, size = 16.180 MiB)
slot update_slots: id  2 | task 11653 | created context checkpoint 8 of 8 (pos_min = 4165, pos_max = 4617, size = 10.623 MiB)
slot print_timing: id  2 | task 11653 | 
prompt eval time =     512.89 ms /   266 tokens (    1.93 ms per token,   518.63 tokens per second)
       eval time =    1686.57 ms /    67 tokens (   25.17 ms per token,    39.73 tokens per second)
      total time =    2199.45 ms /   333 tokens
slot      release: id  2 | task 11653 | stop processing: n_tokens = 4748, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.860 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11722 | processing task, is_child = 0
slot update_slots: id  2 | task 11722 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5444
slot update_slots: id  2 | task 11722 | n_tokens = 4682, memory_seq_rm [4682, end)
slot update_slots: id  2 | task 11722 | prompt processing progress, n_tokens = 5380, batch.n_tokens = 698, progress = 0.988244
slot update_slots: id  2 | task 11722 | n_tokens = 5380, memory_seq_rm [5380, end)
slot update_slots: id  2 | task 11722 | prompt processing progress, n_tokens = 5444, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11722 | prompt done, n_tokens = 5444, batch.n_tokens = 64
slot init_sampler: id  2 | task 11722 | init sampler, took 1.08 ms, tokens: text = 5444, total = 5444
slot update_slots: id  2 | task 11722 | erasing old context checkpoint (pos_min = 1587, pos_max = 2432, size = 19.838 MiB)
slot update_slots: id  2 | task 11722 | created context checkpoint 8 of 8 (pos_min = 4483, pos_max = 5379, size = 21.034 MiB)
slot print_timing: id  2 | task 11722 | 
prompt eval time =    1064.87 ms /   762 tokens (    1.40 ms per token,   715.58 tokens per second)
       eval time =    3239.06 ms /   127 tokens (   25.50 ms per token,    39.21 tokens per second)
      total time =    4303.93 ms /   889 tokens
slot      release: id  2 | task 11722 | stop processing: n_tokens = 5570, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11851 | processing task, is_child = 0
slot update_slots: id  2 | task 11851 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5497
slot update_slots: id  2 | task 11851 | n_tokens = 5444, memory_seq_rm [5444, end)
slot update_slots: id  2 | task 11851 | prompt processing progress, n_tokens = 5497, batch.n_tokens = 53, progress = 1.000000
slot update_slots: id  2 | task 11851 | prompt done, n_tokens = 5497, batch.n_tokens = 53
slot init_sampler: id  2 | task 11851 | init sampler, took 0.87 ms, tokens: text = 5497, total = 5497
slot print_timing: id  2 | task 11851 | 
prompt eval time =     152.75 ms /    53 tokens (    2.88 ms per token,   346.97 tokens per second)
       eval time =   16191.91 ms /   631 tokens (   25.66 ms per token,    38.97 tokens per second)
      total time =   16344.66 ms /   684 tokens
slot      release: id  2 | task 11851 | stop processing: n_tokens = 6127, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.894 (> 0.100 thold), f_keep = 0.897
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12483 | processing task, is_child = 0
slot update_slots: id  2 | task 12483 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6146
slot update_slots: id  2 | task 12483 | n_tokens = 5497, memory_seq_rm [5497, end)
slot update_slots: id  2 | task 12483 | prompt processing progress, n_tokens = 6082, batch.n_tokens = 585, progress = 0.989587
slot update_slots: id  2 | task 12483 | n_tokens = 6082, memory_seq_rm [6082, end)
slot update_slots: id  2 | task 12483 | prompt processing progress, n_tokens = 6146, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12483 | prompt done, n_tokens = 6146, batch.n_tokens = 64
slot init_sampler: id  2 | task 12483 | init sampler, took 0.92 ms, tokens: text = 6146, total = 6146
slot update_slots: id  2 | task 12483 | erasing old context checkpoint (pos_min = 1877, pos_max = 2679, size = 18.830 MiB)
slot update_slots: id  2 | task 12483 | created context checkpoint 8 of 8 (pos_min = 5370, pos_max = 6081, size = 16.696 MiB)
slot print_timing: id  2 | task 12483 | 
prompt eval time =    1019.29 ms /   649 tokens (    1.57 ms per token,   636.72 tokens per second)
       eval time =     870.56 ms /    34 tokens (   25.60 ms per token,    39.06 tokens per second)
      total time =    1889.85 ms /   683 tokens
slot      release: id  2 | task 12483 | stop processing: n_tokens = 6179, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12519 | processing task, is_child = 0
slot update_slots: id  2 | task 12519 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6211
slot update_slots: id  2 | task 12519 | n_tokens = 6146, memory_seq_rm [6146, end)
slot update_slots: id  2 | task 12519 | prompt processing progress, n_tokens = 6147, batch.n_tokens = 1, progress = 0.989696
slot update_slots: id  2 | task 12519 | n_tokens = 6147, memory_seq_rm [6147, end)
slot update_slots: id  2 | task 12519 | prompt processing progress, n_tokens = 6211, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12519 | prompt done, n_tokens = 6211, batch.n_tokens = 64
slot init_sampler: id  2 | task 12519 | init sampler, took 1.29 ms, tokens: text = 6211, total = 6211
slot update_slots: id  2 | task 12519 | erasing old context checkpoint (pos_min = 2690, pos_max = 3586, size = 21.034 MiB)
slot update_slots: id  2 | task 12519 | created context checkpoint 8 of 8 (pos_min = 5370, pos_max = 6146, size = 18.220 MiB)
slot print_timing: id  2 | task 12519 | 
prompt eval time =     202.00 ms /    65 tokens (    3.11 ms per token,   321.79 tokens per second)
       eval time =    2098.57 ms /    82 tokens (   25.59 ms per token,    39.07 tokens per second)
      total time =    2300.57 ms /   147 tokens
slot      release: id  2 | task 12519 | stop processing: n_tokens = 6292, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.948 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12603 | processing task, is_child = 0
slot update_slots: id  2 | task 12603 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6550
slot update_slots: id  2 | task 12603 | n_tokens = 6211, memory_seq_rm [6211, end)
slot update_slots: id  2 | task 12603 | prompt processing progress, n_tokens = 6486, batch.n_tokens = 275, progress = 0.990229
slot update_slots: id  2 | task 12603 | n_tokens = 6486, memory_seq_rm [6486, end)
slot update_slots: id  2 | task 12603 | prompt processing progress, n_tokens = 6550, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12603 | prompt done, n_tokens = 6550, batch.n_tokens = 64
slot init_sampler: id  2 | task 12603 | init sampler, took 0.97 ms, tokens: text = 6550, total = 6550
slot update_slots: id  2 | task 12603 | erasing old context checkpoint (pos_min = 2981, pos_max = 3816, size = 19.604 MiB)
slot update_slots: id  2 | task 12603 | created context checkpoint 8 of 8 (pos_min = 5722, pos_max = 6485, size = 17.915 MiB)
slot print_timing: id  2 | task 12603 | 
prompt eval time =     547.98 ms /   339 tokens (    1.62 ms per token,   618.63 tokens per second)
       eval time =    3232.82 ms /   126 tokens (   25.66 ms per token,    38.98 tokens per second)
      total time =    3780.80 ms /   465 tokens
slot      release: id  2 | task 12603 | stop processing: n_tokens = 6675, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.909 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12731 | processing task, is_child = 0
slot update_slots: id  2 | task 12731 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7206
slot update_slots: id  2 | task 12731 | n_tokens = 6550, memory_seq_rm [6550, end)
slot update_slots: id  2 | task 12731 | prompt processing progress, n_tokens = 7142, batch.n_tokens = 592, progress = 0.991118
slot update_slots: id  2 | task 12731 | n_tokens = 7142, memory_seq_rm [7142, end)
slot update_slots: id  2 | task 12731 | prompt processing progress, n_tokens = 7206, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12731 | prompt done, n_tokens = 7206, batch.n_tokens = 64
slot init_sampler: id  2 | task 12731 | init sampler, took 1.36 ms, tokens: text = 7206, total = 7206
slot update_slots: id  2 | task 12731 | erasing old context checkpoint (pos_min = 3070, pos_max = 3966, size = 21.034 MiB)
slot update_slots: id  2 | task 12731 | created context checkpoint 8 of 8 (pos_min = 6245, pos_max = 7141, size = 21.034 MiB)
slot print_timing: id  2 | task 12731 | 
prompt eval time =     992.78 ms /   656 tokens (    1.51 ms per token,   660.77 tokens per second)
       eval time =    2926.47 ms /   117 tokens (   25.01 ms per token,    39.98 tokens per second)
      total time =    3919.26 ms /   773 tokens
slot      release: id  2 | task 12731 | stop processing: n_tokens = 7322, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12850 | processing task, is_child = 0
slot update_slots: id  1 | task 12850 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7253
slot update_slots: id  1 | task 12850 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 12850 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.282366
slot update_slots: id  1 | task 12850 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 12850 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.564732
slot update_slots: id  1 | task 12850 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 12850 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.847098
slot update_slots: id  1 | task 12850 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 12850 | prompt processing progress, n_tokens = 7189, batch.n_tokens = 1045, progress = 0.991176
slot update_slots: id  1 | task 12850 | n_tokens = 7189, memory_seq_rm [7189, end)
slot update_slots: id  1 | task 12850 | prompt processing progress, n_tokens = 7253, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12850 | prompt done, n_tokens = 7253, batch.n_tokens = 64
slot init_sampler: id  1 | task 12850 | init sampler, took 1.38 ms, tokens: text = 7253, total = 7253
slot update_slots: id  1 | task 12850 | created context checkpoint 1 of 8 (pos_min = 6419, pos_max = 7188, size = 18.056 MiB)
slot print_timing: id  1 | task 12850 | 
prompt eval time =    8646.71 ms /  7253 tokens (    1.19 ms per token,   838.82 tokens per second)
       eval time =    8072.06 ms /   306 tokens (   26.38 ms per token,    37.91 tokens per second)
      total time =   16718.77 ms /  7559 tokens
slot      release: id  1 | task 12850 | stop processing: n_tokens = 7558, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.876 (> 0.100 thold), f_keep = 0.960
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13161 | processing task, is_child = 0
slot update_slots: id  1 | task 13161 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8278
slot update_slots: id  1 | task 13161 | n_tokens = 7254, memory_seq_rm [7254, end)
slot update_slots: id  1 | task 13161 | prompt processing progress, n_tokens = 8214, batch.n_tokens = 960, progress = 0.992269
slot update_slots: id  1 | task 13161 | n_tokens = 8214, memory_seq_rm [8214, end)
slot update_slots: id  1 | task 13161 | prompt processing progress, n_tokens = 8278, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13161 | prompt done, n_tokens = 8278, batch.n_tokens = 64
slot init_sampler: id  1 | task 13161 | init sampler, took 1.23 ms, tokens: text = 8278, total = 8278
slot update_slots: id  1 | task 13161 | created context checkpoint 2 of 8 (pos_min = 7444, pos_max = 8213, size = 18.056 MiB)
slot print_timing: id  1 | task 13161 | 
prompt eval time =    1490.14 ms /  1024 tokens (    1.46 ms per token,   687.18 tokens per second)
       eval time =   17935.24 ms /   602 tokens (   29.79 ms per token,    33.57 tokens per second)
      total time =   19425.38 ms /  1626 tokens
slot      release: id  1 | task 13161 | stop processing: n_tokens = 8879, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.943 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13765 | processing task, is_child = 0
slot update_slots: id  1 | task 13765 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8775
slot update_slots: id  1 | task 13765 | n_tokens = 8279, memory_seq_rm [8279, end)
slot update_slots: id  1 | task 13765 | prompt processing progress, n_tokens = 8711, batch.n_tokens = 432, progress = 0.992707
slot update_slots: id  1 | task 13765 | n_tokens = 8711, memory_seq_rm [8711, end)
slot update_slots: id  1 | task 13765 | prompt processing progress, n_tokens = 8775, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13765 | prompt done, n_tokens = 8775, batch.n_tokens = 64
slot init_sampler: id  1 | task 13765 | init sampler, took 2.01 ms, tokens: text = 8775, total = 8775
slot update_slots: id  1 | task 13765 | created context checkpoint 3 of 8 (pos_min = 8109, pos_max = 8710, size = 14.117 MiB)
slot print_timing: id  1 | task 13765 | 
prompt eval time =     819.17 ms /   496 tokens (    1.65 ms per token,   605.49 tokens per second)
       eval time =    4402.86 ms /   170 tokens (   25.90 ms per token,    38.61 tokens per second)
      total time =    5222.04 ms /   666 tokens
slot      release: id  1 | task 13765 | stop processing: n_tokens = 8944, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13937 | processing task, is_child = 0
slot update_slots: id  1 | task 13937 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8850
slot update_slots: id  1 | task 13937 | n_tokens = 8775, memory_seq_rm [8775, end)
slot update_slots: id  1 | task 13937 | prompt processing progress, n_tokens = 8786, batch.n_tokens = 11, progress = 0.992768
slot update_slots: id  1 | task 13937 | n_tokens = 8786, memory_seq_rm [8786, end)
slot update_slots: id  1 | task 13937 | prompt processing progress, n_tokens = 8850, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13937 | prompt done, n_tokens = 8850, batch.n_tokens = 64
slot init_sampler: id  1 | task 13937 | init sampler, took 1.28 ms, tokens: text = 8850, total = 8850
slot update_slots: id  1 | task 13937 | created context checkpoint 4 of 8 (pos_min = 8278, pos_max = 8785, size = 11.912 MiB)
slot print_timing: id  1 | task 13937 | 
prompt eval time =     268.97 ms /    75 tokens (    3.59 ms per token,   278.84 tokens per second)
       eval time =    4264.46 ms /   162 tokens (   26.32 ms per token,    37.99 tokens per second)
      total time =    4533.43 ms /   237 tokens
slot      release: id  1 | task 13937 | stop processing: n_tokens = 9011, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.741 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 14101 | processing task, is_child = 0
slot update_slots: id  1 | task 14101 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11936
slot update_slots: id  1 | task 14101 | n_tokens = 8850, memory_seq_rm [8850, end)
slot update_slots: id  1 | task 14101 | prompt processing progress, n_tokens = 10898, batch.n_tokens = 2048, progress = 0.913036
slot update_slots: id  1 | task 14101 | n_tokens = 10898, memory_seq_rm [10898, end)
slot update_slots: id  1 | task 14101 | prompt processing progress, n_tokens = 11872, batch.n_tokens = 974, progress = 0.994638
slot update_slots: id  1 | task 14101 | n_tokens = 11872, memory_seq_rm [11872, end)
slot update_slots: id  1 | task 14101 | prompt processing progress, n_tokens = 11936, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 14101 | prompt done, n_tokens = 11936, batch.n_tokens = 64
slot init_sampler: id  1 | task 14101 | init sampler, took 2.62 ms, tokens: text = 11936, total = 11936
slot update_slots: id  1 | task 14101 | created context checkpoint 5 of 8 (pos_min = 11102, pos_max = 11871, size = 18.056 MiB)
slot print_timing: id  1 | task 14101 | 
prompt eval time =    4250.30 ms /  3086 tokens (    1.38 ms per token,   726.07 tokens per second)
       eval time =   29314.73 ms /  1056 tokens (   27.76 ms per token,    36.02 tokens per second)
      total time =   33565.03 ms /  4142 tokens
slot      release: id  1 | task 14101 | stop processing: n_tokens = 12991, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.679 (> 0.100 thold), f_keep = 0.675
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 15160 | processing task, is_child = 0
slot update_slots: id  1 | task 15160 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12931
slot update_slots: id  1 | task 15160 | n_past = 8775, slot.prompt.tokens.size() = 12991, seq_id = 1, pos_min = 12221, n_swa = 128
slot update_slots: id  1 | task 15160 | restored context checkpoint (pos_min = 8278, pos_max = 8785, size = 11.912 MiB)
slot update_slots: id  1 | task 15160 | erased invalidated context checkpoint (pos_min = 11102, pos_max = 11871, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 15160 | n_tokens = 8775, memory_seq_rm [8775, end)
slot update_slots: id  1 | task 15160 | prompt processing progress, n_tokens = 10823, batch.n_tokens = 2048, progress = 0.836981
slot update_slots: id  1 | task 15160 | n_tokens = 10823, memory_seq_rm [10823, end)
slot update_slots: id  1 | task 15160 | prompt processing progress, n_tokens = 12867, batch.n_tokens = 2044, progress = 0.995051
slot update_slots: id  1 | task 15160 | n_tokens = 12867, memory_seq_rm [12867, end)
slot update_slots: id  1 | task 15160 | prompt processing progress, n_tokens = 12931, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 15160 | prompt done, n_tokens = 12931, batch.n_tokens = 64
slot init_sampler: id  1 | task 15160 | init sampler, took 2.99 ms, tokens: text = 12931, total = 12931
slot update_slots: id  1 | task 15160 | created context checkpoint 5 of 8 (pos_min = 12097, pos_max = 12866, size = 18.056 MiB)
slot print_timing: id  1 | task 15160 | 
prompt eval time =    5662.77 ms /  4156 tokens (    1.36 ms per token,   733.92 tokens per second)
       eval time =    4145.31 ms /   155 tokens (   26.74 ms per token,    37.39 tokens per second)
      total time =    9808.07 ms /  4311 tokens
slot      release: id  1 | task 15160 | stop processing: n_tokens = 13085, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.893 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 15318 | processing task, is_child = 0
slot update_slots: id  1 | task 15318 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14486
slot update_slots: id  1 | task 15318 | n_tokens = 12931, memory_seq_rm [12931, end)
slot update_slots: id  1 | task 15318 | prompt processing progress, n_tokens = 14422, batch.n_tokens = 1491, progress = 0.995582
slot update_slots: id  1 | task 15318 | n_tokens = 14422, memory_seq_rm [14422, end)
slot update_slots: id  1 | task 15318 | prompt processing progress, n_tokens = 14486, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 15318 | prompt done, n_tokens = 14486, batch.n_tokens = 64
slot init_sampler: id  1 | task 15318 | init sampler, took 2.06 ms, tokens: text = 14486, total = 14486
slot update_slots: id  1 | task 15318 | created context checkpoint 6 of 8 (pos_min = 13652, pos_max = 14421, size = 18.056 MiB)
slot print_timing: id  1 | task 15318 | 
prompt eval time =    2294.46 ms /  1555 tokens (    1.48 ms per token,   677.72 tokens per second)
       eval time =    5741.09 ms /   208 tokens (   27.60 ms per token,    36.23 tokens per second)
      total time =    8035.55 ms /  1763 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 15318 | stop processing: n_tokens = 14693, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 15528 | processing task, is_child = 0
slot update_slots: id  1 | task 15528 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14578
slot update_slots: id  1 | task 15528 | n_tokens = 14486, memory_seq_rm [14486, end)
slot update_slots: id  1 | task 15528 | prompt processing progress, n_tokens = 14514, batch.n_tokens = 28, progress = 0.995610
slot update_slots: id  1 | task 15528 | n_tokens = 14514, memory_seq_rm [14514, end)
slot update_slots: id  1 | task 15528 | prompt processing progress, n_tokens = 14578, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 15528 | prompt done, n_tokens = 14578, batch.n_tokens = 64
slot init_sampler: id  1 | task 15528 | init sampler, took 2.98 ms, tokens: text = 14578, total = 14578
slot update_slots: id  1 | task 15528 | created context checkpoint 7 of 8 (pos_min = 13923, pos_max = 14513, size = 13.859 MiB)
slot print_timing: id  1 | task 15528 | 
prompt eval time =     356.03 ms /    92 tokens (    3.87 ms per token,   258.41 tokens per second)
       eval time =    2093.24 ms /    73 tokens (   28.67 ms per token,    34.87 tokens per second)
      total time =    2449.27 ms /   165 tokens
slot      release: id  1 | task 15528 | stop processing: n_tokens = 14650, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 15603 | processing task, is_child = 0
slot update_slots: id  1 | task 15603 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14638
slot update_slots: id  1 | task 15603 | n_tokens = 14578, memory_seq_rm [14578, end)
slot update_slots: id  1 | task 15603 | prompt processing progress, n_tokens = 14638, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  1 | task 15603 | prompt done, n_tokens = 14638, batch.n_tokens = 60
slot init_sampler: id  1 | task 15603 | init sampler, took 2.10 ms, tokens: text = 14638, total = 14638
slot print_timing: id  1 | task 15603 | 
prompt eval time =     186.19 ms /    60 tokens (    3.10 ms per token,   322.24 tokens per second)
       eval time =   28454.60 ms /  1020 tokens (   27.90 ms per token,    35.85 tokens per second)
      total time =   28640.79 ms /  1080 tokens
slot      release: id  1 | task 15603 | stop processing: n_tokens = 15657, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.775 (> 0.100 thold), f_keep = 0.935
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 16624 | processing task, is_child = 0
slot update_slots: id  1 | task 16624 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18880
slot update_slots: id  1 | task 16624 | n_past = 14638, slot.prompt.tokens.size() = 15657, seq_id = 1, pos_min = 14887, n_swa = 128
slot update_slots: id  1 | task 16624 | restored context checkpoint (pos_min = 13923, pos_max = 14513, size = 13.859 MiB)
slot update_slots: id  1 | task 16624 | n_tokens = 14513, memory_seq_rm [14513, end)
slot update_slots: id  1 | task 16624 | prompt processing progress, n_tokens = 16561, batch.n_tokens = 2048, progress = 0.877172
slot update_slots: id  1 | task 16624 | n_tokens = 16561, memory_seq_rm [16561, end)
slot update_slots: id  1 | task 16624 | prompt processing progress, n_tokens = 18609, batch.n_tokens = 2048, progress = 0.985646
slot update_slots: id  1 | task 16624 | n_tokens = 18609, memory_seq_rm [18609, end)
slot update_slots: id  1 | task 16624 | prompt processing progress, n_tokens = 18816, batch.n_tokens = 207, progress = 0.996610
slot update_slots: id  1 | task 16624 | n_tokens = 18816, memory_seq_rm [18816, end)
slot update_slots: id  1 | task 16624 | prompt processing progress, n_tokens = 18880, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 16624 | prompt done, n_tokens = 18880, batch.n_tokens = 64
slot init_sampler: id  1 | task 16624 | init sampler, took 2.72 ms, tokens: text = 18880, total = 18880
slot update_slots: id  1 | task 16624 | created context checkpoint 8 of 8 (pos_min = 18046, pos_max = 18815, size = 18.056 MiB)
slot print_timing: id  1 | task 16624 | 
prompt eval time =    6171.04 ms /  4367 tokens (    1.41 ms per token,   707.66 tokens per second)
       eval time =    6532.81 ms /   239 tokens (   27.33 ms per token,    36.58 tokens per second)
      total time =   12703.85 ms /  4606 tokens
slot      release: id  1 | task 16624 | stop processing: n_tokens = 19118, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 16867 | processing task, is_child = 0
slot update_slots: id  1 | task 16867 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18960
slot update_slots: id  1 | task 16867 | n_tokens = 18880, memory_seq_rm [18880, end)
slot update_slots: id  1 | task 16867 | prompt processing progress, n_tokens = 18896, batch.n_tokens = 16, progress = 0.996624
slot update_slots: id  1 | task 16867 | n_tokens = 18896, memory_seq_rm [18896, end)
slot update_slots: id  1 | task 16867 | prompt processing progress, n_tokens = 18960, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 16867 | prompt done, n_tokens = 18960, batch.n_tokens = 64
slot init_sampler: id  1 | task 16867 | init sampler, took 3.90 ms, tokens: text = 18960, total = 18960
slot update_slots: id  1 | task 16867 | erasing old context checkpoint (pos_min = 6419, pos_max = 7188, size = 18.056 MiB)
slot update_slots: id  1 | task 16867 | created context checkpoint 8 of 8 (pos_min = 18348, pos_max = 18895, size = 12.850 MiB)
slot print_timing: id  1 | task 16867 | 
prompt eval time =     307.96 ms /    80 tokens (    3.85 ms per token,   259.77 tokens per second)
       eval time =   15634.68 ms /   576 tokens (   27.14 ms per token,    36.84 tokens per second)
      total time =   15942.65 ms /   656 tokens
slot      release: id  1 | task 16867 | stop processing: n_tokens = 19535, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 17445 | processing task, is_child = 0
slot update_slots: id  1 | task 17445 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19013
slot update_slots: id  1 | task 17445 | n_past = 18960, slot.prompt.tokens.size() = 19535, seq_id = 1, pos_min = 18880, n_swa = 128
slot update_slots: id  1 | task 17445 | restored context checkpoint (pos_min = 18348, pos_max = 18895, size = 12.850 MiB)
slot update_slots: id  1 | task 17445 | n_tokens = 18895, memory_seq_rm [18895, end)
slot update_slots: id  1 | task 17445 | prompt processing progress, n_tokens = 18949, batch.n_tokens = 54, progress = 0.996634
slot update_slots: id  1 | task 17445 | n_tokens = 18949, memory_seq_rm [18949, end)
slot update_slots: id  1 | task 17445 | prompt processing progress, n_tokens = 19013, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 17445 | prompt done, n_tokens = 19013, batch.n_tokens = 64
slot init_sampler: id  1 | task 17445 | init sampler, took 2.86 ms, tokens: text = 19013, total = 19013
slot print_timing: id  1 | task 17445 | 
prompt eval time =     491.27 ms /   118 tokens (    4.16 ms per token,   240.19 tokens per second)
       eval time =    4526.28 ms /   166 tokens (   27.27 ms per token,    36.67 tokens per second)
      total time =    5017.55 ms /   284 tokens
slot      release: id  1 | task 17445 | stop processing: n_tokens = 19178, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 17613 | processing task, is_child = 0
slot update_slots: id  1 | task 17613 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19139
slot update_slots: id  1 | task 17613 | n_tokens = 19013, memory_seq_rm [19013, end)
slot update_slots: id  1 | task 17613 | prompt processing progress, n_tokens = 19075, batch.n_tokens = 62, progress = 0.996656
slot update_slots: id  1 | task 17613 | n_tokens = 19075, memory_seq_rm [19075, end)
slot update_slots: id  1 | task 17613 | prompt processing progress, n_tokens = 19139, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 17613 | prompt done, n_tokens = 19139, batch.n_tokens = 64
slot init_sampler: id  1 | task 17613 | init sampler, took 2.73 ms, tokens: text = 19139, total = 19139
slot update_slots: id  1 | task 17613 | erasing old context checkpoint (pos_min = 7444, pos_max = 8213, size = 18.056 MiB)
slot update_slots: id  1 | task 17613 | created context checkpoint 8 of 8 (pos_min = 18630, pos_max = 19074, size = 10.435 MiB)
slot print_timing: id  1 | task 17613 | 
prompt eval time =     421.62 ms /   126 tokens (    3.35 ms per token,   298.85 tokens per second)
       eval time =   49963.44 ms /  1799 tokens (   27.77 ms per token,    36.01 tokens per second)
      total time =   50385.06 ms /  1925 tokens
slot      release: id  1 | task 17613 | stop processing: n_tokens = 20937, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.938 (> 0.100 thold), f_keep = 0.914
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19414 | processing task, is_child = 0
slot update_slots: id  1 | task 19414 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20402
slot update_slots: id  1 | task 19414 | n_past = 19139, slot.prompt.tokens.size() = 20937, seq_id = 1, pos_min = 20167, n_swa = 128
slot update_slots: id  1 | task 19414 | restored context checkpoint (pos_min = 18630, pos_max = 19074, size = 10.435 MiB)
slot update_slots: id  1 | task 19414 | n_tokens = 19074, memory_seq_rm [19074, end)
slot update_slots: id  1 | task 19414 | prompt processing progress, n_tokens = 20338, batch.n_tokens = 1264, progress = 0.996863
slot update_slots: id  1 | task 19414 | n_tokens = 20338, memory_seq_rm [20338, end)
slot update_slots: id  1 | task 19414 | prompt processing progress, n_tokens = 20402, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 19414 | prompt done, n_tokens = 20402, batch.n_tokens = 64
slot init_sampler: id  1 | task 19414 | init sampler, took 2.90 ms, tokens: text = 20402, total = 20402
slot update_slots: id  1 | task 19414 | erasing old context checkpoint (pos_min = 8109, pos_max = 8710, size = 14.117 MiB)
slot update_slots: id  1 | task 19414 | created context checkpoint 8 of 8 (pos_min = 19568, pos_max = 20337, size = 18.056 MiB)
slot print_timing: id  1 | task 19414 | 
prompt eval time =    2274.53 ms /  1328 tokens (    1.71 ms per token,   583.86 tokens per second)
       eval time =    1179.44 ms /    43 tokens (   27.43 ms per token,    36.46 tokens per second)
      total time =    3453.97 ms /  1371 tokens
slot      release: id  1 | task 19414 | stop processing: n_tokens = 20444, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19459 | processing task, is_child = 0
slot update_slots: id  1 | task 19459 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20467
slot update_slots: id  1 | task 19459 | n_tokens = 20402, memory_seq_rm [20402, end)
slot update_slots: id  1 | task 19459 | prompt processing progress, n_tokens = 20403, batch.n_tokens = 1, progress = 0.996873
slot update_slots: id  1 | task 19459 | n_tokens = 20403, memory_seq_rm [20403, end)
slot update_slots: id  1 | task 19459 | prompt processing progress, n_tokens = 20467, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 19459 | prompt done, n_tokens = 20467, batch.n_tokens = 64
slot init_sampler: id  1 | task 19459 | init sampler, took 2.87 ms, tokens: text = 20467, total = 20467
slot update_slots: id  1 | task 19459 | erasing old context checkpoint (pos_min = 8278, pos_max = 8785, size = 11.912 MiB)
slot update_slots: id  1 | task 19459 | created context checkpoint 8 of 8 (pos_min = 19674, pos_max = 20402, size = 17.095 MiB)
slot print_timing: id  1 | task 19459 | 
prompt eval time =     246.69 ms /    65 tokens (    3.80 ms per token,   263.49 tokens per second)
       eval time =    7185.41 ms /   261 tokens (   27.53 ms per token,    36.32 tokens per second)
      total time =    7432.10 ms /   326 tokens
slot      release: id  1 | task 19459 | stop processing: n_tokens = 20727, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19722 | processing task, is_child = 0
slot update_slots: id  1 | task 19722 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20748
slot update_slots: id  1 | task 19722 | n_tokens = 20467, memory_seq_rm [20467, end)
slot update_slots: id  1 | task 19722 | prompt processing progress, n_tokens = 20684, batch.n_tokens = 217, progress = 0.996915
slot update_slots: id  1 | task 19722 | n_tokens = 20684, memory_seq_rm [20684, end)
slot update_slots: id  1 | task 19722 | prompt processing progress, n_tokens = 20748, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 19722 | prompt done, n_tokens = 20748, batch.n_tokens = 64
slot init_sampler: id  1 | task 19722 | init sampler, took 3.91 ms, tokens: text = 20748, total = 20748
slot update_slots: id  1 | task 19722 | erasing old context checkpoint (pos_min = 12097, pos_max = 12866, size = 18.056 MiB)
slot update_slots: id  1 | task 19722 | created context checkpoint 8 of 8 (pos_min = 19975, pos_max = 20683, size = 16.626 MiB)
slot print_timing: id  1 | task 19722 | 
prompt eval time =     626.75 ms /   281 tokens (    2.23 ms per token,   448.35 tokens per second)
       eval time =   28650.10 ms /  1034 tokens (   27.71 ms per token,    36.09 tokens per second)
      total time =   29276.85 ms /  1315 tokens
slot      release: id  1 | task 19722 | stop processing: n_tokens = 21781, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 20758 | processing task, is_child = 0
slot update_slots: id  1 | task 20758 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21055
slot update_slots: id  1 | task 20758 | n_past = 20748, slot.prompt.tokens.size() = 21781, seq_id = 1, pos_min = 21011, n_swa = 128
slot update_slots: id  1 | task 20758 | restored context checkpoint (pos_min = 19975, pos_max = 20683, size = 16.626 MiB)
slot update_slots: id  1 | task 20758 | n_tokens = 20683, memory_seq_rm [20683, end)
slot update_slots: id  1 | task 20758 | prompt processing progress, n_tokens = 20991, batch.n_tokens = 308, progress = 0.996960
slot update_slots: id  1 | task 20758 | n_tokens = 20991, memory_seq_rm [20991, end)
slot update_slots: id  1 | task 20758 | prompt processing progress, n_tokens = 21055, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 20758 | prompt done, n_tokens = 21055, batch.n_tokens = 64
slot init_sampler: id  1 | task 20758 | init sampler, took 4.28 ms, tokens: text = 21055, total = 21055
slot update_slots: id  1 | task 20758 | erasing old context checkpoint (pos_min = 13652, pos_max = 14421, size = 18.056 MiB)
slot update_slots: id  1 | task 20758 | created context checkpoint 8 of 8 (pos_min = 20282, pos_max = 20990, size = 16.626 MiB)
slot print_timing: id  1 | task 20758 | 
prompt eval time =     910.34 ms /   372 tokens (    2.45 ms per token,   408.64 tokens per second)
       eval time =   29511.74 ms /  1054 tokens (   28.00 ms per token,    35.71 tokens per second)
      total time =   30422.08 ms /  1426 tokens
slot      release: id  1 | task 20758 | stop processing: n_tokens = 22108, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.952
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 21814 | processing task, is_child = 0
slot update_slots: id  1 | task 21814 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21108
slot update_slots: id  1 | task 21814 | n_past = 21055, slot.prompt.tokens.size() = 22108, seq_id = 1, pos_min = 21338, n_swa = 128
slot update_slots: id  1 | task 21814 | restored context checkpoint (pos_min = 20282, pos_max = 20990, size = 16.626 MiB)
slot update_slots: id  1 | task 21814 | n_tokens = 20990, memory_seq_rm [20990, end)
slot update_slots: id  1 | task 21814 | prompt processing progress, n_tokens = 21044, batch.n_tokens = 54, progress = 0.996968
slot update_slots: id  1 | task 21814 | n_tokens = 21044, memory_seq_rm [21044, end)
slot update_slots: id  1 | task 21814 | prompt processing progress, n_tokens = 21108, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 21814 | prompt done, n_tokens = 21108, batch.n_tokens = 64
slot init_sampler: id  1 | task 21814 | init sampler, took 2.99 ms, tokens: text = 21108, total = 21108
slot print_timing: id  1 | task 21814 | 
prompt eval time =     529.25 ms /   118 tokens (    4.49 ms per token,   222.96 tokens per second)
       eval time =   18756.41 ms /   674 tokens (   27.83 ms per token,    35.93 tokens per second)
      total time =   19285.66 ms /   792 tokens
slot      release: id  1 | task 21814 | stop processing: n_tokens = 21781, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 22490 | processing task, is_child = 0
slot update_slots: id  1 | task 22490 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21212
slot update_slots: id  1 | task 22490 | n_past = 21108, slot.prompt.tokens.size() = 21781, seq_id = 1, pos_min = 21011, n_swa = 128
slot update_slots: id  1 | task 22490 | restored context checkpoint (pos_min = 20282, pos_max = 20990, size = 16.626 MiB)
slot update_slots: id  1 | task 22490 | n_tokens = 20990, memory_seq_rm [20990, end)
slot update_slots: id  1 | task 22490 | prompt processing progress, n_tokens = 21148, batch.n_tokens = 158, progress = 0.996983
slot update_slots: id  1 | task 22490 | n_tokens = 21148, memory_seq_rm [21148, end)
slot update_slots: id  1 | task 22490 | prompt processing progress, n_tokens = 21212, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 22490 | prompt done, n_tokens = 21212, batch.n_tokens = 64
slot init_sampler: id  1 | task 22490 | init sampler, took 3.44 ms, tokens: text = 21212, total = 21212
slot update_slots: id  1 | task 22490 | erasing old context checkpoint (pos_min = 13923, pos_max = 14513, size = 13.859 MiB)
slot update_slots: id  1 | task 22490 | created context checkpoint 8 of 8 (pos_min = 20402, pos_max = 21147, size = 17.493 MiB)
slot print_timing: id  1 | task 22490 | 
prompt eval time =     671.38 ms /   222 tokens (    3.02 ms per token,   330.66 tokens per second)
       eval time =    2583.59 ms /    93 tokens (   27.78 ms per token,    36.00 tokens per second)
      total time =    3254.97 ms /   315 tokens
slot      release: id  1 | task 22490 | stop processing: n_tokens = 21304, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 22585 | processing task, is_child = 0
slot update_slots: id  1 | task 22585 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21289
slot update_slots: id  1 | task 22585 | n_tokens = 21212, memory_seq_rm [21212, end)
slot update_slots: id  1 | task 22585 | prompt processing progress, n_tokens = 21225, batch.n_tokens = 13, progress = 0.996994
slot update_slots: id  1 | task 22585 | n_tokens = 21225, memory_seq_rm [21225, end)
slot update_slots: id  1 | task 22585 | prompt processing progress, n_tokens = 21289, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 22585 | prompt done, n_tokens = 21289, batch.n_tokens = 64
slot init_sampler: id  1 | task 22585 | init sampler, took 4.41 ms, tokens: text = 21289, total = 21289
slot update_slots: id  1 | task 22585 | erasing old context checkpoint (pos_min = 18046, pos_max = 18815, size = 18.056 MiB)
slot update_slots: id  1 | task 22585 | created context checkpoint 8 of 8 (pos_min = 20534, pos_max = 21224, size = 16.204 MiB)
slot print_timing: id  1 | task 22585 | 
prompt eval time =     323.19 ms /    77 tokens (    4.20 ms per token,   238.25 tokens per second)
       eval time =    2823.45 ms /   100 tokens (   28.23 ms per token,    35.42 tokens per second)
      total time =    3146.64 ms /   177 tokens
slot      release: id  1 | task 22585 | stop processing: n_tokens = 21388, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 22687 | processing task, is_child = 0
slot update_slots: id  1 | task 22687 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21375
slot update_slots: id  1 | task 22687 | n_tokens = 21289, memory_seq_rm [21289, end)
slot update_slots: id  1 | task 22687 | prompt processing progress, n_tokens = 21311, batch.n_tokens = 22, progress = 0.997006
slot update_slots: id  1 | task 22687 | n_tokens = 21311, memory_seq_rm [21311, end)
slot update_slots: id  1 | task 22687 | prompt processing progress, n_tokens = 21375, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 22687 | prompt done, n_tokens = 21375, batch.n_tokens = 64
slot init_sampler: id  1 | task 22687 | init sampler, took 4.51 ms, tokens: text = 21375, total = 21375
slot update_slots: id  1 | task 22687 | erasing old context checkpoint (pos_min = 18348, pos_max = 18895, size = 12.850 MiB)
slot update_slots: id  1 | task 22687 | created context checkpoint 8 of 8 (pos_min = 20618, pos_max = 21310, size = 16.250 MiB)
slot print_timing: id  1 | task 22687 | 
prompt eval time =     339.33 ms /    86 tokens (    3.95 ms per token,   253.44 tokens per second)
       eval time =   22441.14 ms /   807 tokens (   27.81 ms per token,    35.96 tokens per second)
      total time =   22780.47 ms /   893 tokens
slot      release: id  1 | task 22687 | stop processing: n_tokens = 22181, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23496 | processing task, is_child = 0
slot update_slots: id  1 | task 23496 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21428
slot update_slots: id  1 | task 23496 | n_past = 21375, slot.prompt.tokens.size() = 22181, seq_id = 1, pos_min = 21411, n_swa = 128
slot update_slots: id  1 | task 23496 | restored context checkpoint (pos_min = 20618, pos_max = 21310, size = 16.250 MiB)
slot update_slots: id  1 | task 23496 | n_tokens = 21310, memory_seq_rm [21310, end)
slot update_slots: id  1 | task 23496 | prompt processing progress, n_tokens = 21364, batch.n_tokens = 54, progress = 0.997013
slot update_slots: id  1 | task 23496 | n_tokens = 21364, memory_seq_rm [21364, end)
slot update_slots: id  1 | task 23496 | prompt processing progress, n_tokens = 21428, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23496 | prompt done, n_tokens = 21428, batch.n_tokens = 64
slot init_sampler: id  1 | task 23496 | init sampler, took 2.98 ms, tokens: text = 21428, total = 21428
slot print_timing: id  1 | task 23496 | 
prompt eval time =     525.02 ms /   118 tokens (    4.45 ms per token,   224.75 tokens per second)
       eval time =   17492.14 ms /   626 tokens (   27.94 ms per token,    35.79 tokens per second)
      total time =   18017.16 ms /   744 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 23496 | stop processing: n_tokens = 22053, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24124 | processing task, is_child = 0
slot update_slots: id  1 | task 24124 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21761
slot update_slots: id  1 | task 24124 | n_tokens = 21428, memory_seq_rm [21428, end)
slot update_slots: id  1 | task 24124 | prompt processing progress, n_tokens = 21697, batch.n_tokens = 269, progress = 0.997059
slot update_slots: id  1 | task 24124 | n_tokens = 21697, memory_seq_rm [21697, end)
slot update_slots: id  1 | task 24124 | prompt processing progress, n_tokens = 21761, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24124 | prompt done, n_tokens = 21761, batch.n_tokens = 64
slot init_sampler: id  1 | task 24124 | init sampler, took 4.40 ms, tokens: text = 21761, total = 21761
slot update_slots: id  1 | task 24124 | erasing old context checkpoint (pos_min = 18630, pos_max = 19074, size = 10.435 MiB)
slot update_slots: id  1 | task 24124 | created context checkpoint 8 of 8 (pos_min = 21301, pos_max = 21696, size = 9.286 MiB)
slot print_timing: id  1 | task 24124 | 
prompt eval time =     672.27 ms /   333 tokens (    2.02 ms per token,   495.33 tokens per second)
       eval time =    3642.52 ms /   130 tokens (   28.02 ms per token,    35.69 tokens per second)
      total time =    4314.79 ms /   463 tokens
slot      release: id  1 | task 24124 | stop processing: n_tokens = 21890, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24256 | processing task, is_child = 0
slot update_slots: id  1 | task 24256 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 22639
slot update_slots: id  1 | task 24256 | n_tokens = 21761, memory_seq_rm [21761, end)
slot update_slots: id  1 | task 24256 | prompt processing progress, n_tokens = 22575, batch.n_tokens = 814, progress = 0.997173
slot update_slots: id  1 | task 24256 | n_tokens = 22575, memory_seq_rm [22575, end)
slot update_slots: id  1 | task 24256 | prompt processing progress, n_tokens = 22639, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24256 | prompt done, n_tokens = 22639, batch.n_tokens = 64
slot init_sampler: id  1 | task 24256 | init sampler, took 3.17 ms, tokens: text = 22639, total = 22639
slot update_slots: id  1 | task 24256 | erasing old context checkpoint (pos_min = 19568, pos_max = 20337, size = 18.056 MiB)
slot update_slots: id  1 | task 24256 | created context checkpoint 8 of 8 (pos_min = 21805, pos_max = 22574, size = 18.056 MiB)
slot print_timing: id  1 | task 24256 | 
prompt eval time =    1562.22 ms /   878 tokens (    1.78 ms per token,   562.02 tokens per second)
       eval time =   24616.27 ms /   876 tokens (   28.10 ms per token,    35.59 tokens per second)
      total time =   26178.49 ms /  1754 tokens
slot      release: id  1 | task 24256 | stop processing: n_tokens = 23514, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.963
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25134 | processing task, is_child = 0
slot update_slots: id  1 | task 25134 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 22915
slot update_slots: id  1 | task 25134 | n_past = 22639, slot.prompt.tokens.size() = 23514, seq_id = 1, pos_min = 22744, n_swa = 128
slot update_slots: id  1 | task 25134 | restored context checkpoint (pos_min = 21805, pos_max = 22574, size = 18.056 MiB)
slot update_slots: id  1 | task 25134 | n_tokens = 22574, memory_seq_rm [22574, end)
slot update_slots: id  1 | task 25134 | prompt processing progress, n_tokens = 22851, batch.n_tokens = 277, progress = 0.997207
slot update_slots: id  1 | task 25134 | n_tokens = 22851, memory_seq_rm [22851, end)
slot update_slots: id  1 | task 25134 | prompt processing progress, n_tokens = 22915, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25134 | prompt done, n_tokens = 22915, batch.n_tokens = 64
slot init_sampler: id  1 | task 25134 | init sampler, took 3.46 ms, tokens: text = 22915, total = 22915
slot update_slots: id  1 | task 25134 | erasing old context checkpoint (pos_min = 19674, pos_max = 20402, size = 17.095 MiB)
slot update_slots: id  1 | task 25134 | created context checkpoint 8 of 8 (pos_min = 22081, pos_max = 22850, size = 18.056 MiB)
slot print_timing: id  1 | task 25134 | 
prompt eval time =     834.28 ms /   341 tokens (    2.45 ms per token,   408.73 tokens per second)
       eval time =    9597.88 ms /   341 tokens (   28.15 ms per token,    35.53 tokens per second)
      total time =   10432.16 ms /   682 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 25134 | stop processing: n_tokens = 23255, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.882 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25477 | processing task, is_child = 0
slot update_slots: id  1 | task 25477 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 25989
slot update_slots: id  1 | task 25477 | n_tokens = 22915, memory_seq_rm [22915, end)
slot update_slots: id  1 | task 25477 | prompt processing progress, n_tokens = 24963, batch.n_tokens = 2048, progress = 0.960522
slot update_slots: id  1 | task 25477 | n_tokens = 24963, memory_seq_rm [24963, end)
slot update_slots: id  1 | task 25477 | prompt processing progress, n_tokens = 25925, batch.n_tokens = 962, progress = 0.997537
slot update_slots: id  1 | task 25477 | n_tokens = 25925, memory_seq_rm [25925, end)
slot update_slots: id  1 | task 25477 | prompt processing progress, n_tokens = 25989, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25477 | prompt done, n_tokens = 25989, batch.n_tokens = 64
slot init_sampler: id  1 | task 25477 | init sampler, took 3.91 ms, tokens: text = 25989, total = 25989
slot update_slots: id  1 | task 25477 | erasing old context checkpoint (pos_min = 19975, pos_max = 20683, size = 16.626 MiB)
slot update_slots: id  1 | task 25477 | created context checkpoint 8 of 8 (pos_min = 25155, pos_max = 25924, size = 18.056 MiB)
slot print_timing: id  1 | task 25477 | 
prompt eval time =    4880.91 ms /  3074 tokens (    1.59 ms per token,   629.80 tokens per second)
       eval time =   34157.05 ms /  1199 tokens (   28.49 ms per token,    35.10 tokens per second)
      total time =   39037.96 ms /  4273 tokens
slot      release: id  1 | task 25477 | stop processing: n_tokens = 27187, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.956
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26679 | processing task, is_child = 0
slot update_slots: id  1 | task 26679 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 27027
slot update_slots: id  1 | task 26679 | n_past = 25989, slot.prompt.tokens.size() = 27187, seq_id = 1, pos_min = 26417, n_swa = 128
slot update_slots: id  1 | task 26679 | restored context checkpoint (pos_min = 25155, pos_max = 25924, size = 18.056 MiB)
slot update_slots: id  1 | task 26679 | n_tokens = 25924, memory_seq_rm [25924, end)
slot update_slots: id  1 | task 26679 | prompt processing progress, n_tokens = 26963, batch.n_tokens = 1039, progress = 0.997632
slot update_slots: id  1 | task 26679 | n_tokens = 26963, memory_seq_rm [26963, end)
slot update_slots: id  1 | task 26679 | prompt processing progress, n_tokens = 27027, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26679 | prompt done, n_tokens = 27027, batch.n_tokens = 64
slot init_sampler: id  1 | task 26679 | init sampler, took 5.39 ms, tokens: text = 27027, total = 27027
slot update_slots: id  1 | task 26679 | erasing old context checkpoint (pos_min = 20282, pos_max = 20990, size = 16.626 MiB)
slot update_slots: id  1 | task 26679 | created context checkpoint 8 of 8 (pos_min = 26193, pos_max = 26962, size = 18.056 MiB)
slot print_timing: id  1 | task 26679 | 
prompt eval time =    2119.97 ms /  1103 tokens (    1.92 ms per token,   520.29 tokens per second)
       eval time =     863.57 ms /    28 tokens (   30.84 ms per token,    32.42 tokens per second)
      total time =    2983.54 ms /  1131 tokens
slot      release: id  1 | task 26679 | stop processing: n_tokens = 27054, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26709 | processing task, is_child = 0
slot update_slots: id  1 | task 26709 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 27092
slot update_slots: id  1 | task 26709 | n_tokens = 27027, memory_seq_rm [27027, end)
slot update_slots: id  1 | task 26709 | prompt processing progress, n_tokens = 27028, batch.n_tokens = 1, progress = 0.997638
slot update_slots: id  1 | task 26709 | n_tokens = 27028, memory_seq_rm [27028, end)
slot update_slots: id  1 | task 26709 | prompt processing progress, n_tokens = 27092, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26709 | prompt done, n_tokens = 27092, batch.n_tokens = 64
slot init_sampler: id  1 | task 26709 | init sampler, took 5.38 ms, tokens: text = 27092, total = 27092
slot update_slots: id  1 | task 26709 | erasing old context checkpoint (pos_min = 20402, pos_max = 21147, size = 17.493 MiB)
slot update_slots: id  1 | task 26709 | created context checkpoint 8 of 8 (pos_min = 26284, pos_max = 27027, size = 17.446 MiB)
slot print_timing: id  1 | task 26709 | 
prompt eval time =     353.28 ms /    65 tokens (    5.44 ms per token,   183.99 tokens per second)
       eval time =    2639.82 ms /    93 tokens (   28.39 ms per token,    35.23 tokens per second)
      total time =    2993.10 ms /   158 tokens
slot      release: id  1 | task 26709 | stop processing: n_tokens = 27184, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26804 | processing task, is_child = 0
slot update_slots: id  1 | task 26804 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 27180
slot update_slots: id  1 | task 26804 | n_tokens = 27092, memory_seq_rm [27092, end)
slot update_slots: id  1 | task 26804 | prompt processing progress, n_tokens = 27116, batch.n_tokens = 24, progress = 0.997645
slot update_slots: id  1 | task 26804 | n_tokens = 27116, memory_seq_rm [27116, end)
slot update_slots: id  1 | task 26804 | prompt processing progress, n_tokens = 27180, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26804 | prompt done, n_tokens = 27180, batch.n_tokens = 64
slot init_sampler: id  1 | task 26804 | init sampler, took 3.77 ms, tokens: text = 27180, total = 27180
slot update_slots: id  1 | task 26804 | erasing old context checkpoint (pos_min = 20534, pos_max = 21224, size = 16.204 MiB)
slot update_slots: id  1 | task 26804 | created context checkpoint 8 of 8 (pos_min = 26414, pos_max = 27115, size = 16.461 MiB)
slot print_timing: id  1 | task 26804 | 
prompt eval time =     348.12 ms /    88 tokens (    3.96 ms per token,   252.79 tokens per second)
       eval time =   30589.49 ms /  1064 tokens (   28.75 ms per token,    34.78 tokens per second)
      total time =   30937.61 ms /  1152 tokens
slot      release: id  1 | task 26804 | stop processing: n_tokens = 28243, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27870 | processing task, is_child = 0
slot update_slots: id  1 | task 27870 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 27233
slot update_slots: id  1 | task 27870 | n_past = 27180, slot.prompt.tokens.size() = 28243, seq_id = 1, pos_min = 27473, n_swa = 128
slot update_slots: id  1 | task 27870 | restored context checkpoint (pos_min = 26414, pos_max = 27115, size = 16.461 MiB)
slot update_slots: id  1 | task 27870 | n_tokens = 27115, memory_seq_rm [27115, end)
slot update_slots: id  1 | task 27870 | prompt processing progress, n_tokens = 27169, batch.n_tokens = 54, progress = 0.997650
slot update_slots: id  1 | task 27870 | n_tokens = 27169, memory_seq_rm [27169, end)
slot update_slots: id  1 | task 27870 | prompt processing progress, n_tokens = 27233, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27870 | prompt done, n_tokens = 27233, batch.n_tokens = 64
slot init_sampler: id  1 | task 27870 | init sampler, took 3.95 ms, tokens: text = 27233, total = 27233
slot print_timing: id  1 | task 27870 | 
prompt eval time =     551.89 ms /   118 tokens (    4.68 ms per token,   213.81 tokens per second)
       eval time =   29792.46 ms /  1039 tokens (   28.67 ms per token,    34.87 tokens per second)
      total time =   30344.35 ms /  1157 tokens
slot      release: id  1 | task 27870 | stop processing: n_tokens = 28271, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.963
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28911 | processing task, is_child = 0
slot update_slots: id  1 | task 28911 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 28267
slot update_slots: id  1 | task 28911 | n_past = 27233, slot.prompt.tokens.size() = 28271, seq_id = 1, pos_min = 27501, n_swa = 128
slot update_slots: id  1 | task 28911 | restored context checkpoint (pos_min = 26414, pos_max = 27115, size = 16.461 MiB)
slot update_slots: id  1 | task 28911 | n_tokens = 27115, memory_seq_rm [27115, end)
slot update_slots: id  1 | task 28911 | prompt processing progress, n_tokens = 28203, batch.n_tokens = 1088, progress = 0.997736
slot update_slots: id  1 | task 28911 | n_tokens = 28203, memory_seq_rm [28203, end)
slot update_slots: id  1 | task 28911 | prompt processing progress, n_tokens = 28267, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28911 | prompt done, n_tokens = 28267, batch.n_tokens = 64
slot init_sampler: id  1 | task 28911 | init sampler, took 4.28 ms, tokens: text = 28267, total = 28267
slot update_slots: id  1 | task 28911 | erasing old context checkpoint (pos_min = 20618, pos_max = 21310, size = 16.250 MiB)
slot update_slots: id  1 | task 28911 | created context checkpoint 8 of 8 (pos_min = 27433, pos_max = 28202, size = 18.056 MiB)
slot print_timing: id  1 | task 28911 | 
prompt eval time =    2256.30 ms /  1152 tokens (    1.96 ms per token,   510.57 tokens per second)
       eval time =    1169.65 ms /    40 tokens (   29.24 ms per token,    34.20 tokens per second)
      total time =    3425.95 ms /  1192 tokens
slot      release: id  1 | task 28911 | stop processing: n_tokens = 28306, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28953 | processing task, is_child = 0
slot update_slots: id  1 | task 28953 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 28332
slot update_slots: id  1 | task 28953 | n_tokens = 28267, memory_seq_rm [28267, end)
slot update_slots: id  1 | task 28953 | prompt processing progress, n_tokens = 28268, batch.n_tokens = 1, progress = 0.997741
slot update_slots: id  1 | task 28953 | n_tokens = 28268, memory_seq_rm [28268, end)
slot update_slots: id  1 | task 28953 | prompt processing progress, n_tokens = 28332, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28953 | prompt done, n_tokens = 28332, batch.n_tokens = 64
slot init_sampler: id  1 | task 28953 | init sampler, took 5.48 ms, tokens: text = 28332, total = 28332
slot update_slots: id  1 | task 28953 | erasing old context checkpoint (pos_min = 21301, pos_max = 21696, size = 9.286 MiB)
slot update_slots: id  1 | task 28953 | created context checkpoint 8 of 8 (pos_min = 27536, pos_max = 28267, size = 17.165 MiB)
slot print_timing: id  1 | task 28953 | 
prompt eval time =     320.08 ms /    65 tokens (    4.92 ms per token,   203.07 tokens per second)
       eval time =    7430.49 ms /   261 tokens (   28.47 ms per token,    35.13 tokens per second)
      total time =    7750.57 ms /   326 tokens
slot      release: id  1 | task 28953 | stop processing: n_tokens = 28592, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29216 | processing task, is_child = 0
slot update_slots: id  1 | task 29216 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 28669
slot update_slots: id  1 | task 29216 | n_tokens = 28332, memory_seq_rm [28332, end)
slot update_slots: id  1 | task 29216 | prompt processing progress, n_tokens = 28605, batch.n_tokens = 273, progress = 0.997768
slot update_slots: id  1 | task 29216 | n_tokens = 28605, memory_seq_rm [28605, end)
slot update_slots: id  1 | task 29216 | prompt processing progress, n_tokens = 28669, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29216 | prompt done, n_tokens = 28669, batch.n_tokens = 64
slot init_sampler: id  1 | task 29216 | init sampler, took 4.15 ms, tokens: text = 28669, total = 28669
slot update_slots: id  1 | task 29216 | erasing old context checkpoint (pos_min = 21805, pos_max = 22574, size = 18.056 MiB)
slot update_slots: id  1 | task 29216 | created context checkpoint 8 of 8 (pos_min = 27835, pos_max = 28604, size = 18.056 MiB)
slot print_timing: id  1 | task 29216 | 
prompt eval time =     716.26 ms /   337 tokens (    2.13 ms per token,   470.50 tokens per second)
       eval time =    2016.55 ms /    70 tokens (   28.81 ms per token,    34.71 tokens per second)
      total time =    2732.81 ms /   407 tokens
slot      release: id  1 | task 29216 | stop processing: n_tokens = 28738, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29288 | processing task, is_child = 0
slot update_slots: id  1 | task 29288 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 29710
slot update_slots: id  1 | task 29288 | n_tokens = 28669, memory_seq_rm [28669, end)
slot update_slots: id  1 | task 29288 | prompt processing progress, n_tokens = 29646, batch.n_tokens = 977, progress = 0.997846
slot update_slots: id  1 | task 29288 | n_tokens = 29646, memory_seq_rm [29646, end)
slot update_slots: id  1 | task 29288 | prompt processing progress, n_tokens = 29710, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29288 | prompt done, n_tokens = 29710, batch.n_tokens = 64
slot init_sampler: id  1 | task 29288 | init sampler, took 4.14 ms, tokens: text = 29710, total = 29710
slot update_slots: id  1 | task 29288 | erasing old context checkpoint (pos_min = 22081, pos_max = 22850, size = 18.056 MiB)
slot update_slots: id  1 | task 29288 | created context checkpoint 8 of 8 (pos_min = 28876, pos_max = 29645, size = 18.056 MiB)
slot print_timing: id  1 | task 29288 | 
prompt eval time =    1850.22 ms /  1041 tokens (    1.78 ms per token,   562.64 tokens per second)
       eval time =    5489.73 ms /   190 tokens (   28.89 ms per token,    34.61 tokens per second)
      total time =    7339.95 ms /  1231 tokens
slot      release: id  1 | task 29288 | stop processing: n_tokens = 29899, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29480 | processing task, is_child = 0
slot update_slots: id  1 | task 29480 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 29763
slot update_slots: id  1 | task 29480 | n_tokens = 29710, memory_seq_rm [29710, end)
slot update_slots: id  1 | task 29480 | prompt processing progress, n_tokens = 29763, batch.n_tokens = 53, progress = 1.000000
slot update_slots: id  1 | task 29480 | prompt done, n_tokens = 29763, batch.n_tokens = 53
slot init_sampler: id  1 | task 29480 | init sampler, took 4.37 ms, tokens: text = 29763, total = 29763
slot print_timing: id  1 | task 29480 | 
prompt eval time =     187.40 ms /    53 tokens (    3.54 ms per token,   282.82 tokens per second)
       eval time =    2225.87 ms /    77 tokens (   28.91 ms per token,    34.59 tokens per second)
      total time =    2413.27 ms /   130 tokens
slot      release: id  1 | task 29480 | stop processing: n_tokens = 29839, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29558 | processing task, is_child = 0
slot update_slots: id  1 | task 29558 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 29816
slot update_slots: id  1 | task 29558 | n_tokens = 29763, memory_seq_rm [29763, end)
slot update_slots: id  1 | task 29558 | prompt processing progress, n_tokens = 29816, batch.n_tokens = 53, progress = 1.000000
slot update_slots: id  1 | task 29558 | prompt done, n_tokens = 29816, batch.n_tokens = 53
slot init_sampler: id  1 | task 29558 | init sampler, took 4.27 ms, tokens: text = 29816, total = 29816
slot update_slots: id  1 | task 29558 | erasing old context checkpoint (pos_min = 25155, pos_max = 25924, size = 18.056 MiB)
slot update_slots: id  1 | task 29558 | created context checkpoint 8 of 8 (pos_min = 29130, pos_max = 29762, size = 14.843 MiB)
slot print_timing: id  1 | task 29558 | 
prompt eval time =     194.07 ms /    53 tokens (    3.66 ms per token,   273.10 tokens per second)
       eval time =   32074.62 ms /  1106 tokens (   29.00 ms per token,    34.48 tokens per second)
      total time =   32268.69 ms /  1159 tokens
slot      release: id  1 | task 29558 | stop processing: n_tokens = 30921, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.303 (> 0.100 thold), f_keep = 0.005
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 30921, total state size = 743.121 MiB
srv          load:  - looking for better prompt, base f_keep = 0.005, sim = 0.303
srv        update:  - cache state: 5 prompts, 1806.400 MiB (limits: 8192.000 MiB, 64000 tokens, 231606 est)
srv        update:    - prompt 0x55bd8f1cbf80:     976 tokens, checkpoints:  1,    56.020 MiB
srv        update:    - prompt 0x55bd96905720:    5131 tokens, checkpoints:  8,   315.673 MiB
srv        update:    - prompt 0x55bd9697ce40:    4478 tokens, checkpoints:  5,   223.986 MiB
srv        update:    - prompt 0x55bd96c6b950:    9565 tokens, checkpoints:  4,   329.459 MiB
srv        update:    - prompt 0x55bda9a02fa0:   30921 tokens, checkpoints:  8,   881.261 MiB
srv  get_availabl: prompt cache update took 701.45 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30665 | processing task, is_child = 0
slot update_slots: id  1 | task 30665 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 542
slot update_slots: id  1 | task 30665 | n_past = 164, slot.prompt.tokens.size() = 30921, seq_id = 1, pos_min = 30151, n_swa = 128
slot update_slots: id  1 | task 30665 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 30665 | erased invalidated context checkpoint (pos_min = 26193, pos_max = 26962, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 30665 | erased invalidated context checkpoint (pos_min = 26284, pos_max = 27027, n_swa = 128, size = 17.446 MiB)
slot update_slots: id  1 | task 30665 | erased invalidated context checkpoint (pos_min = 26414, pos_max = 27115, n_swa = 128, size = 16.461 MiB)
slot update_slots: id  1 | task 30665 | erased invalidated context checkpoint (pos_min = 27433, pos_max = 28202, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 30665 | erased invalidated context checkpoint (pos_min = 27536, pos_max = 28267, n_swa = 128, size = 17.165 MiB)
slot update_slots: id  1 | task 30665 | erased invalidated context checkpoint (pos_min = 27835, pos_max = 28604, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 30665 | erased invalidated context checkpoint (pos_min = 28876, pos_max = 29645, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 30665 | erased invalidated context checkpoint (pos_min = 29130, pos_max = 29762, n_swa = 128, size = 14.843 MiB)
slot update_slots: id  1 | task 30665 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 30665 | prompt processing progress, n_tokens = 478, batch.n_tokens = 478, progress = 0.881919
slot update_slots: id  1 | task 30665 | n_tokens = 478, memory_seq_rm [478, end)
slot update_slots: id  1 | task 30665 | prompt processing progress, n_tokens = 542, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30665 | prompt done, n_tokens = 542, batch.n_tokens = 64
slot init_sampler: id  1 | task 30665 | init sampler, took 0.11 ms, tokens: text = 542, total = 542
slot update_slots: id  1 | task 30665 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 477, size = 11.209 MiB)
slot print_timing: id  1 | task 30665 | 
prompt eval time =     805.21 ms /   542 tokens (    1.49 ms per token,   673.11 tokens per second)
       eval time =    7248.26 ms /   283 tokens (   25.61 ms per token,    39.04 tokens per second)
      total time =    8053.47 ms /   825 tokens
slot      release: id  1 | task 30665 | stop processing: n_tokens = 824, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.819 (> 0.100 thold), f_keep = 0.659
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30950 | processing task, is_child = 0
slot update_slots: id  1 | task 30950 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 663
slot update_slots: id  1 | task 30950 | n_tokens = 543, memory_seq_rm [543, end)
slot update_slots: id  1 | task 30950 | prompt processing progress, n_tokens = 599, batch.n_tokens = 56, progress = 0.903469
slot update_slots: id  1 | task 30950 | n_tokens = 599, memory_seq_rm [599, end)
slot update_slots: id  1 | task 30950 | prompt processing progress, n_tokens = 663, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30950 | prompt done, n_tokens = 663, batch.n_tokens = 64
slot init_sampler: id  1 | task 30950 | init sampler, took 0.13 ms, tokens: text = 663, total = 663
slot update_slots: id  1 | task 30950 | created context checkpoint 2 of 8 (pos_min = 56, pos_max = 598, size = 12.733 MiB)
slot print_timing: id  1 | task 30950 | 
prompt eval time =     463.99 ms /   120 tokens (    3.87 ms per token,   258.62 tokens per second)
       eval time =    1715.70 ms /    68 tokens (   25.23 ms per token,    39.63 tokens per second)
      total time =    2179.69 ms /   188 tokens
slot      release: id  1 | task 30950 | stop processing: n_tokens = 730, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.588 (> 0.100 thold), f_keep = 0.908
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31020 | processing task, is_child = 0
slot update_slots: id  1 | task 31020 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1127
slot update_slots: id  1 | task 31020 | n_tokens = 663, memory_seq_rm [663, end)
slot update_slots: id  1 | task 31020 | prompt processing progress, n_tokens = 1063, batch.n_tokens = 400, progress = 0.943212
slot update_slots: id  1 | task 31020 | n_tokens = 1063, memory_seq_rm [1063, end)
slot update_slots: id  1 | task 31020 | prompt processing progress, n_tokens = 1127, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31020 | prompt done, n_tokens = 1127, batch.n_tokens = 64
slot init_sampler: id  1 | task 31020 | init sampler, took 0.20 ms, tokens: text = 1127, total = 1127
slot update_slots: id  1 | task 31020 | created context checkpoint 3 of 8 (pos_min = 520, pos_max = 1062, size = 12.733 MiB)
slot print_timing: id  1 | task 31020 | 
prompt eval time =     665.93 ms /   464 tokens (    1.44 ms per token,   696.77 tokens per second)
       eval time =    1385.68 ms /    53 tokens (   26.14 ms per token,    38.25 tokens per second)
      total time =    2051.61 ms /   517 tokens
slot      release: id  1 | task 31020 | stop processing: n_tokens = 1179, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.632 (> 0.100 thold), f_keep = 0.956
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31075 | processing task, is_child = 0
slot update_slots: id  1 | task 31075 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1783
slot update_slots: id  1 | task 31075 | n_tokens = 1127, memory_seq_rm [1127, end)
slot update_slots: id  1 | task 31075 | prompt processing progress, n_tokens = 1719, batch.n_tokens = 592, progress = 0.964105
slot update_slots: id  1 | task 31075 | n_tokens = 1719, memory_seq_rm [1719, end)
slot update_slots: id  1 | task 31075 | prompt processing progress, n_tokens = 1783, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31075 | prompt done, n_tokens = 1783, batch.n_tokens = 64
slot init_sampler: id  1 | task 31075 | init sampler, took 0.47 ms, tokens: text = 1783, total = 1783
slot update_slots: id  1 | task 31075 | created context checkpoint 4 of 8 (pos_min = 949, pos_max = 1718, size = 18.056 MiB)
slot print_timing: id  1 | task 31075 | 
prompt eval time =    1067.20 ms /   656 tokens (    1.63 ms per token,   614.69 tokens per second)
       eval time =    3738.89 ms /   138 tokens (   27.09 ms per token,    36.91 tokens per second)
      total time =    4806.09 ms /   794 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 31075 | stop processing: n_tokens = 1920, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.929
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31215 | processing task, is_child = 0
slot update_slots: id  1 | task 31215 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1912
slot update_slots: id  1 | task 31215 | n_tokens = 1783, memory_seq_rm [1783, end)
slot update_slots: id  1 | task 31215 | prompt processing progress, n_tokens = 1848, batch.n_tokens = 65, progress = 0.966527
slot update_slots: id  1 | task 31215 | n_tokens = 1848, memory_seq_rm [1848, end)
slot update_slots: id  1 | task 31215 | prompt processing progress, n_tokens = 1912, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31215 | prompt done, n_tokens = 1912, batch.n_tokens = 64
slot init_sampler: id  1 | task 31215 | init sampler, took 0.34 ms, tokens: text = 1912, total = 1912
slot update_slots: id  1 | task 31215 | created context checkpoint 5 of 8 (pos_min = 1150, pos_max = 1847, size = 16.368 MiB)
slot print_timing: id  1 | task 31215 | 
prompt eval time =     441.09 ms /   129 tokens (    3.42 ms per token,   292.46 tokens per second)
       eval time =    1942.91 ms /    71 tokens (   27.36 ms per token,    36.54 tokens per second)
      total time =    2384.01 ms /   200 tokens
slot      release: id  1 | task 31215 | stop processing: n_tokens = 1982, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31288 | processing task, is_child = 0
slot update_slots: id  1 | task 31288 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1979
slot update_slots: id  1 | task 31288 | n_tokens = 1912, memory_seq_rm [1912, end)
slot update_slots: id  1 | task 31288 | prompt processing progress, n_tokens = 1915, batch.n_tokens = 3, progress = 0.967660
slot update_slots: id  1 | task 31288 | n_tokens = 1915, memory_seq_rm [1915, end)
slot update_slots: id  1 | task 31288 | prompt processing progress, n_tokens = 1979, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31288 | prompt done, n_tokens = 1979, batch.n_tokens = 64
slot init_sampler: id  1 | task 31288 | init sampler, took 0.31 ms, tokens: text = 1979, total = 1979
slot update_slots: id  1 | task 31288 | created context checkpoint 6 of 8 (pos_min = 1212, pos_max = 1914, size = 16.485 MiB)
slot print_timing: id  1 | task 31288 | 
prompt eval time =     253.27 ms /    67 tokens (    3.78 ms per token,   264.54 tokens per second)
       eval time =    5644.16 ms /   171 tokens (   33.01 ms per token,    30.30 tokens per second)
      total time =    5897.43 ms /   238 tokens
slot      release: id  1 | task 31288 | stop processing: n_tokens = 2149, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.921
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31461 | processing task, is_child = 0
slot update_slots: id  1 | task 31461 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2052
slot update_slots: id  1 | task 31461 | n_tokens = 1979, memory_seq_rm [1979, end)
slot update_slots: id  1 | task 31461 | prompt processing progress, n_tokens = 1988, batch.n_tokens = 9, progress = 0.968811
slot update_slots: id  1 | task 31461 | n_tokens = 1988, memory_seq_rm [1988, end)
slot update_slots: id  1 | task 31461 | prompt processing progress, n_tokens = 2052, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31461 | prompt done, n_tokens = 2052, batch.n_tokens = 64
slot init_sampler: id  1 | task 31461 | init sampler, took 0.34 ms, tokens: text = 2052, total = 2052
slot update_slots: id  1 | task 31461 | created context checkpoint 7 of 8 (pos_min = 1379, pos_max = 1987, size = 14.281 MiB)
slot print_timing: id  1 | task 31461 | 
prompt eval time =     378.92 ms /    73 tokens (    5.19 ms per token,   192.65 tokens per second)
       eval time =    3872.22 ms /   133 tokens (   29.11 ms per token,    34.35 tokens per second)
      total time =    4251.15 ms /   206 tokens
slot      release: id  1 | task 31461 | stop processing: n_tokens = 2184, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.941 (> 0.100 thold), f_keep = 0.940
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31596 | processing task, is_child = 0
slot update_slots: id  1 | task 31596 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2181
slot update_slots: id  1 | task 31596 | n_tokens = 2052, memory_seq_rm [2052, end)
slot update_slots: id  1 | task 31596 | prompt processing progress, n_tokens = 2117, batch.n_tokens = 65, progress = 0.970656
slot update_slots: id  1 | task 31596 | n_tokens = 2117, memory_seq_rm [2117, end)
slot update_slots: id  1 | task 31596 | prompt processing progress, n_tokens = 2181, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31596 | prompt done, n_tokens = 2181, batch.n_tokens = 64
slot init_sampler: id  1 | task 31596 | init sampler, took 0.42 ms, tokens: text = 2181, total = 2181
slot update_slots: id  1 | task 31596 | created context checkpoint 8 of 8 (pos_min = 1518, pos_max = 2116, size = 14.046 MiB)
slot print_timing: id  1 | task 31596 | 
prompt eval time =     441.40 ms /   129 tokens (    3.42 ms per token,   292.25 tokens per second)
       eval time =    1554.56 ms /    57 tokens (   27.27 ms per token,    36.67 tokens per second)
      total time =    1995.96 ms /   186 tokens
slot      release: id  1 | task 31596 | stop processing: n_tokens = 2237, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.700 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31655 | processing task, is_child = 0
slot update_slots: id  1 | task 31655 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3117
slot update_slots: id  1 | task 31655 | n_tokens = 2181, memory_seq_rm [2181, end)
slot update_slots: id  1 | task 31655 | prompt processing progress, n_tokens = 3053, batch.n_tokens = 872, progress = 0.979467
slot update_slots: id  1 | task 31655 | n_tokens = 3053, memory_seq_rm [3053, end)
slot update_slots: id  1 | task 31655 | prompt processing progress, n_tokens = 3117, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31655 | prompt done, n_tokens = 3117, batch.n_tokens = 64
slot init_sampler: id  1 | task 31655 | init sampler, took 0.58 ms, tokens: text = 3117, total = 3117
slot update_slots: id  1 | task 31655 | erasing old context checkpoint (pos_min = 0, pos_max = 477, size = 11.209 MiB)
slot update_slots: id  1 | task 31655 | created context checkpoint 8 of 8 (pos_min = 2283, pos_max = 3052, size = 18.056 MiB)
slot print_timing: id  1 | task 31655 | 
prompt eval time =    1344.93 ms /   936 tokens (    1.44 ms per token,   695.95 tokens per second)
       eval time =   16524.43 ms /   639 tokens (   25.86 ms per token,    38.67 tokens per second)
      total time =   17869.36 ms /  1575 tokens
slot      release: id  1 | task 31655 | stop processing: n_tokens = 3755, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.830
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 32296 | processing task, is_child = 0
slot update_slots: id  1 | task 32296 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3238
slot update_slots: id  1 | task 32296 | n_tokens = 3117, memory_seq_rm [3117, end)
slot update_slots: id  1 | task 32296 | prompt processing progress, n_tokens = 3174, batch.n_tokens = 57, progress = 0.980235
slot update_slots: id  1 | task 32296 | n_tokens = 3174, memory_seq_rm [3174, end)
slot update_slots: id  1 | task 32296 | prompt processing progress, n_tokens = 3238, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 32296 | prompt done, n_tokens = 3238, batch.n_tokens = 64
slot init_sampler: id  1 | task 32296 | init sampler, took 0.52 ms, tokens: text = 3238, total = 3238
slot update_slots: id  1 | task 32296 | erasing old context checkpoint (pos_min = 56, pos_max = 598, size = 12.733 MiB)
slot update_slots: id  1 | task 32296 | created context checkpoint 8 of 8 (pos_min = 2985, pos_max = 3173, size = 4.432 MiB)
slot print_timing: id  1 | task 32296 | 
prompt eval time =     368.03 ms /   121 tokens (    3.04 ms per token,   328.78 tokens per second)
       eval time =    5995.20 ms /   237 tokens (   25.30 ms per token,    39.53 tokens per second)
      total time =    6363.23 ms /   358 tokens
slot      release: id  1 | task 32296 | stop processing: n_tokens = 3474, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 32535 | processing task, is_child = 0
slot update_slots: id  1 | task 32535 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3367
slot update_slots: id  1 | task 32535 | n_tokens = 3238, memory_seq_rm [3238, end)
slot update_slots: id  1 | task 32535 | prompt processing progress, n_tokens = 3303, batch.n_tokens = 65, progress = 0.980992
slot update_slots: id  1 | task 32535 | n_tokens = 3303, memory_seq_rm [3303, end)
slot update_slots: id  1 | task 32535 | prompt processing progress, n_tokens = 3367, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 32535 | prompt done, n_tokens = 3367, batch.n_tokens = 64
slot init_sampler: id  1 | task 32535 | init sampler, took 0.80 ms, tokens: text = 3367, total = 3367
slot update_slots: id  1 | task 32535 | erasing old context checkpoint (pos_min = 520, pos_max = 1062, size = 12.733 MiB)
slot update_slots: id  1 | task 32535 | created context checkpoint 8 of 8 (pos_min = 3106, pos_max = 3302, size = 4.620 MiB)
slot print_timing: id  1 | task 32535 | 
prompt eval time =     412.17 ms /   129 tokens (    3.20 ms per token,   312.98 tokens per second)
       eval time =    6179.97 ms /   246 tokens (   25.12 ms per token,    39.81 tokens per second)
      total time =    6592.14 ms /   375 tokens
slot      release: id  1 | task 32535 | stop processing: n_tokens = 3612, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 32783 | processing task, is_child = 0
slot update_slots: id  1 | task 32783 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3440
slot update_slots: id  1 | task 32783 | n_tokens = 3367, memory_seq_rm [3367, end)
slot update_slots: id  1 | task 32783 | prompt processing progress, n_tokens = 3376, batch.n_tokens = 9, progress = 0.981395
slot update_slots: id  1 | task 32783 | n_tokens = 3376, memory_seq_rm [3376, end)
slot update_slots: id  1 | task 32783 | prompt processing progress, n_tokens = 3440, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 32783 | prompt done, n_tokens = 3440, batch.n_tokens = 64
slot init_sampler: id  1 | task 32783 | init sampler, took 0.50 ms, tokens: text = 3440, total = 3440
slot update_slots: id  1 | task 32783 | erasing old context checkpoint (pos_min = 949, pos_max = 1718, size = 18.056 MiB)
slot update_slots: id  1 | task 32783 | created context checkpoint 8 of 8 (pos_min = 3117, pos_max = 3375, size = 6.074 MiB)
slot print_timing: id  1 | task 32783 | 
prompt eval time =     252.87 ms /    73 tokens (    3.46 ms per token,   288.68 tokens per second)
       eval time =   14076.07 ms /   554 tokens (   25.41 ms per token,    39.36 tokens per second)
      total time =   14328.94 ms /   627 tokens
slot      release: id  1 | task 32783 | stop processing: n_tokens = 3993, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.841 (> 0.100 thold), f_keep = 0.862
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 33339 | processing task, is_child = 0
slot update_slots: id  1 | task 33339 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4092
slot update_slots: id  1 | task 33339 | n_tokens = 3440, memory_seq_rm [3440, end)
slot update_slots: id  1 | task 33339 | prompt processing progress, n_tokens = 4028, batch.n_tokens = 588, progress = 0.984360
slot update_slots: id  1 | task 33339 | n_tokens = 4028, memory_seq_rm [4028, end)
slot update_slots: id  1 | task 33339 | prompt processing progress, n_tokens = 4092, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 33339 | prompt done, n_tokens = 4092, batch.n_tokens = 64
slot init_sampler: id  1 | task 33339 | init sampler, took 0.68 ms, tokens: text = 4092, total = 4092
slot update_slots: id  1 | task 33339 | erasing old context checkpoint (pos_min = 1150, pos_max = 1847, size = 16.368 MiB)
slot update_slots: id  1 | task 33339 | created context checkpoint 8 of 8 (pos_min = 3313, pos_max = 4027, size = 16.766 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 33339
slot      release: id  1 | task 33339 | stop processing: n_tokens = 4162, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.592 (> 0.100 thold), f_keep = 0.078
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4162, total state size = 112.673 MiB
srv          load:  - looking for better prompt, base f_keep = 0.078, sim = 0.592
srv        update:  - cache state: 6 prompts, 2013.832 MiB (limits: 8192.000 MiB, 64000 tokens, 224680 est)
srv        update:    - prompt 0x55bd8f1cbf80:     976 tokens, checkpoints:  1,    56.020 MiB
srv        update:    - prompt 0x55bd96905720:    5131 tokens, checkpoints:  8,   315.673 MiB
srv        update:    - prompt 0x55bd9697ce40:    4478 tokens, checkpoints:  5,   223.986 MiB
srv        update:    - prompt 0x55bd96c6b950:    9565 tokens, checkpoints:  4,   329.459 MiB
srv        update:    - prompt 0x55bda9a02fa0:   30921 tokens, checkpoints:  8,   881.261 MiB
srv        update:    - prompt 0x55bd9cfa5230:    4162 tokens, checkpoints:  8,   207.432 MiB
srv  get_availabl: prompt cache update took 160.56 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 33413 | processing task, is_child = 0
slot update_slots: id  1 | task 33413 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 547
slot update_slots: id  1 | task 33413 | n_past = 324, slot.prompt.tokens.size() = 4162, seq_id = 1, pos_min = 3519, n_swa = 128
slot update_slots: id  1 | task 33413 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 33413 | erased invalidated context checkpoint (pos_min = 1212, pos_max = 1914, n_swa = 128, size = 16.485 MiB)
slot update_slots: id  1 | task 33413 | erased invalidated context checkpoint (pos_min = 1379, pos_max = 1987, n_swa = 128, size = 14.281 MiB)
slot update_slots: id  1 | task 33413 | erased invalidated context checkpoint (pos_min = 1518, pos_max = 2116, n_swa = 128, size = 14.046 MiB)
slot update_slots: id  1 | task 33413 | erased invalidated context checkpoint (pos_min = 2283, pos_max = 3052, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 33413 | erased invalidated context checkpoint (pos_min = 2985, pos_max = 3173, n_swa = 128, size = 4.432 MiB)
slot update_slots: id  1 | task 33413 | erased invalidated context checkpoint (pos_min = 3106, pos_max = 3302, n_swa = 128, size = 4.620 MiB)
slot update_slots: id  1 | task 33413 | erased invalidated context checkpoint (pos_min = 3117, pos_max = 3375, n_swa = 128, size = 6.074 MiB)
slot update_slots: id  1 | task 33413 | erased invalidated context checkpoint (pos_min = 3313, pos_max = 4027, n_swa = 128, size = 16.766 MiB)
slot update_slots: id  1 | task 33413 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 33413 | prompt processing progress, n_tokens = 483, batch.n_tokens = 483, progress = 0.882998
slot update_slots: id  1 | task 33413 | n_tokens = 483, memory_seq_rm [483, end)
slot update_slots: id  1 | task 33413 | prompt processing progress, n_tokens = 547, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 33413 | prompt done, n_tokens = 547, batch.n_tokens = 64
slot init_sampler: id  1 | task 33413 | init sampler, took 0.12 ms, tokens: text = 547, total = 547
slot update_slots: id  1 | task 33413 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 482, size = 11.326 MiB)
slot print_timing: id  1 | task 33413 | 
prompt eval time =     865.88 ms /   547 tokens (    1.58 ms per token,   631.73 tokens per second)
       eval time =    5898.29 ms /   225 tokens (   26.21 ms per token,    38.15 tokens per second)
      total time =    6764.17 ms /   772 tokens
slot      release: id  1 | task 33413 | stop processing: n_tokens = 771, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.801 (> 0.100 thold), f_keep = 0.711
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 33640 | processing task, is_child = 0
slot update_slots: id  1 | task 33640 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 684
slot update_slots: id  1 | task 33640 | n_tokens = 548, memory_seq_rm [548, end)
slot update_slots: id  1 | task 33640 | prompt processing progress, n_tokens = 620, batch.n_tokens = 72, progress = 0.906433
slot update_slots: id  1 | task 33640 | n_tokens = 620, memory_seq_rm [620, end)
slot update_slots: id  1 | task 33640 | prompt processing progress, n_tokens = 684, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 33640 | prompt done, n_tokens = 684, batch.n_tokens = 64
slot init_sampler: id  1 | task 33640 | init sampler, took 0.12 ms, tokens: text = 684, total = 684
slot update_slots: id  1 | task 33640 | created context checkpoint 2 of 8 (pos_min = 72, pos_max = 619, size = 12.850 MiB)
slot print_timing: id  1 | task 33640 | 
prompt eval time =     489.35 ms /   136 tokens (    3.60 ms per token,   277.92 tokens per second)
       eval time =   14366.24 ms /   553 tokens (   25.98 ms per token,    38.49 tokens per second)
      total time =   14855.59 ms /   689 tokens
slot      release: id  1 | task 33640 | stop processing: n_tokens = 1236, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.400
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1236, total state size = 45.116 MiB
srv          load:  - looking for better prompt, base f_keep = 0.400, sim = 0.974
srv        update:  - cache state: 7 prompts, 2083.125 MiB (limits: 8192.000 MiB, 64000 tokens, 222067 est)
srv        update:    - prompt 0x55bd8f1cbf80:     976 tokens, checkpoints:  1,    56.020 MiB
srv        update:    - prompt 0x55bd96905720:    5131 tokens, checkpoints:  8,   315.673 MiB
srv        update:    - prompt 0x55bd9697ce40:    4478 tokens, checkpoints:  5,   223.986 MiB
srv        update:    - prompt 0x55bd96c6b950:    9565 tokens, checkpoints:  4,   329.459 MiB
srv        update:    - prompt 0x55bda9a02fa0:   30921 tokens, checkpoints:  8,   881.261 MiB
srv        update:    - prompt 0x55bd9cfa5230:    4162 tokens, checkpoints:  8,   207.432 MiB
srv        update:    - prompt 0x55bd9cd6b9a0:    1236 tokens, checkpoints:  2,    69.293 MiB
srv  get_availabl: prompt cache update took 40.59 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 34195 | processing task, is_child = 0
slot update_slots: id  1 | task 34195 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 507
slot update_slots: id  1 | task 34195 | n_past = 494, slot.prompt.tokens.size() = 1236, seq_id = 1, pos_min = 548, n_swa = 128
slot update_slots: id  1 | task 34195 | restored context checkpoint (pos_min = 72, pos_max = 619, size = 12.850 MiB)
slot update_slots: id  1 | task 34195 | n_tokens = 494, memory_seq_rm [494, end)
slot update_slots: id  1 | task 34195 | prompt processing progress, n_tokens = 507, batch.n_tokens = 13, progress = 1.000000
slot update_slots: id  1 | task 34195 | prompt done, n_tokens = 507, batch.n_tokens = 13
slot init_sampler: id  1 | task 34195 | init sampler, took 0.08 ms, tokens: text = 507, total = 507
slot print_timing: id  1 | task 34195 | 
prompt eval time =     260.05 ms /    13 tokens (   20.00 ms per token,    49.99 tokens per second)
       eval time =    4975.37 ms /   193 tokens (   25.78 ms per token,    38.79 tokens per second)
      total time =    5235.42 ms /   206 tokens
slot      release: id  1 | task 34195 | stop processing: n_tokens = 699, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.707
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 34389 | processing task, is_child = 0
slot update_slots: id  1 | task 34389 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 500
slot update_slots: id  1 | task 34389 | n_tokens = 494, memory_seq_rm [494, end)
slot update_slots: id  1 | task 34389 | prompt processing progress, n_tokens = 500, batch.n_tokens = 6, progress = 1.000000
slot update_slots: id  1 | task 34389 | prompt done, n_tokens = 500, batch.n_tokens = 6
slot init_sampler: id  1 | task 34389 | init sampler, took 0.09 ms, tokens: text = 500, total = 500
slot print_timing: id  1 | task 34389 | 
prompt eval time =      97.06 ms /     6 tokens (   16.18 ms per token,    61.82 tokens per second)
       eval time =    2245.07 ms /    86 tokens (   26.11 ms per token,    38.31 tokens per second)
      total time =    2342.13 ms /    92 tokens
slot      release: id  1 | task 34389 | stop processing: n_tokens = 585, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.856
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 34476 | processing task, is_child = 0
slot update_slots: id  1 | task 34476 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 519
slot update_slots: id  1 | task 34476 | n_tokens = 501, memory_seq_rm [501, end)
slot update_slots: id  1 | task 34476 | prompt processing progress, n_tokens = 519, batch.n_tokens = 18, progress = 1.000000
slot update_slots: id  1 | task 34476 | prompt done, n_tokens = 519, batch.n_tokens = 18
slot init_sampler: id  1 | task 34476 | init sampler, took 0.11 ms, tokens: text = 519, total = 519
slot print_timing: id  1 | task 34476 | 
prompt eval time =     161.90 ms /    18 tokens (    8.99 ms per token,   111.18 tokens per second)
       eval time =    1595.26 ms /    62 tokens (   25.73 ms per token,    38.87 tokens per second)
      total time =    1757.16 ms /    80 tokens
slot      release: id  1 | task 34476 | stop processing: n_tokens = 580, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.901 (> 0.100 thold), f_keep = 0.895
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 34539 | processing task, is_child = 0
slot update_slots: id  1 | task 34539 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 576
slot update_slots: id  1 | task 34539 | n_tokens = 519, memory_seq_rm [519, end)
slot update_slots: id  1 | task 34539 | prompt processing progress, n_tokens = 576, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  1 | task 34539 | prompt done, n_tokens = 576, batch.n_tokens = 57
slot init_sampler: id  1 | task 34539 | init sampler, took 0.10 ms, tokens: text = 576, total = 576
slot print_timing: id  1 | task 34539 | 
prompt eval time =     292.34 ms /    57 tokens (    5.13 ms per token,   194.98 tokens per second)
       eval time =    1602.94 ms /    61 tokens (   26.28 ms per token,    38.06 tokens per second)
      total time =    1895.27 ms /   118 tokens
slot      release: id  1 | task 34539 | stop processing: n_tokens = 636, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.917 (> 0.100 thold), f_keep = 0.906
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 34601 | processing task, is_child = 0
slot update_slots: id  1 | task 34601 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 628
slot update_slots: id  1 | task 34601 | n_tokens = 576, memory_seq_rm [576, end)
slot update_slots: id  1 | task 34601 | prompt processing progress, n_tokens = 628, batch.n_tokens = 52, progress = 1.000000
slot update_slots: id  1 | task 34601 | prompt done, n_tokens = 628, batch.n_tokens = 52
slot init_sampler: id  1 | task 34601 | init sampler, took 0.09 ms, tokens: text = 628, total = 628
slot print_timing: id  1 | task 34601 | 
prompt eval time =     254.92 ms /    52 tokens (    4.90 ms per token,   203.99 tokens per second)
       eval time =    7182.90 ms /   270 tokens (   26.60 ms per token,    37.59 tokens per second)
      total time =    7437.82 ms /   322 tokens
slot      release: id  1 | task 34601 | stop processing: n_tokens = 897, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.909 (> 0.100 thold), f_keep = 0.700
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 34872 | processing task, is_child = 0
slot update_slots: id  1 | task 34872 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 691
slot update_slots: id  1 | task 34872 | n_tokens = 628, memory_seq_rm [628, end)
slot update_slots: id  1 | task 34872 | prompt processing progress, n_tokens = 691, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  1 | task 34872 | prompt done, n_tokens = 691, batch.n_tokens = 63
slot init_sampler: id  1 | task 34872 | init sampler, took 0.13 ms, tokens: text = 691, total = 691
slot print_timing: id  1 | task 34872 | 
prompt eval time =     268.11 ms /    63 tokens (    4.26 ms per token,   234.98 tokens per second)
       eval time =    6562.76 ms /   257 tokens (   25.54 ms per token,    39.16 tokens per second)
      total time =    6830.87 ms /   320 tokens
slot      release: id  1 | task 34872 | stop processing: n_tokens = 947, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.667 (> 0.100 thold), f_keep = 0.548
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35130 | processing task, is_child = 0
slot update_slots: id  1 | task 35130 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 778
slot update_slots: id  1 | task 35130 | n_past = 519, slot.prompt.tokens.size() = 947, seq_id = 1, pos_min = 453, n_swa = 128
slot update_slots: id  1 | task 35130 | restored context checkpoint (pos_min = 72, pos_max = 619, size = 12.850 MiB)
slot update_slots: id  1 | task 35130 | n_tokens = 519, memory_seq_rm [519, end)
slot update_slots: id  1 | task 35130 | prompt processing progress, n_tokens = 714, batch.n_tokens = 195, progress = 0.917738
slot update_slots: id  1 | task 35130 | n_tokens = 714, memory_seq_rm [714, end)
slot update_slots: id  1 | task 35130 | prompt processing progress, n_tokens = 778, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35130 | prompt done, n_tokens = 778, batch.n_tokens = 64
slot init_sampler: id  1 | task 35130 | init sampler, took 0.14 ms, tokens: text = 778, total = 778
slot update_slots: id  1 | task 35130 | created context checkpoint 3 of 8 (pos_min = 195, pos_max = 713, size = 12.170 MiB)
slot print_timing: id  1 | task 35130 | 
prompt eval time =     686.50 ms /   259 tokens (    2.65 ms per token,   377.28 tokens per second)
       eval time =    6665.01 ms /   266 tokens (   25.06 ms per token,    39.91 tokens per second)
      total time =    7351.51 ms /   525 tokens
slot      release: id  1 | task 35130 | stop processing: n_tokens = 1043, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.864 (> 0.100 thold), f_keep = 0.747
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35398 | processing task, is_child = 0
slot update_slots: id  1 | task 35398 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 902
slot update_slots: id  1 | task 35398 | n_tokens = 779, memory_seq_rm [779, end)
slot update_slots: id  1 | task 35398 | prompt processing progress, n_tokens = 838, batch.n_tokens = 59, progress = 0.929047
slot update_slots: id  1 | task 35398 | n_tokens = 838, memory_seq_rm [838, end)
slot update_slots: id  1 | task 35398 | prompt processing progress, n_tokens = 902, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35398 | prompt done, n_tokens = 902, batch.n_tokens = 64
slot init_sampler: id  1 | task 35398 | init sampler, took 0.17 ms, tokens: text = 902, total = 902
slot update_slots: id  1 | task 35398 | created context checkpoint 4 of 8 (pos_min = 519, pos_max = 837, size = 7.481 MiB)
slot print_timing: id  1 | task 35398 | 
prompt eval time =     478.18 ms /   123 tokens (    3.89 ms per token,   257.22 tokens per second)
       eval time =    1509.47 ms /    61 tokens (   24.75 ms per token,    40.41 tokens per second)
      total time =    1987.65 ms /   184 tokens
slot      release: id  1 | task 35398 | stop processing: n_tokens = 962, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.941 (> 0.100 thold), f_keep = 0.938
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35461 | processing task, is_child = 0
slot update_slots: id  1 | task 35461 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 959
slot update_slots: id  1 | task 35461 | n_tokens = 902, memory_seq_rm [902, end)
slot update_slots: id  1 | task 35461 | prompt processing progress, n_tokens = 959, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  1 | task 35461 | prompt done, n_tokens = 959, batch.n_tokens = 57
slot init_sampler: id  1 | task 35461 | init sampler, took 0.22 ms, tokens: text = 959, total = 959
slot print_timing: id  1 | task 35461 | 
prompt eval time =     277.28 ms /    57 tokens (    4.86 ms per token,   205.57 tokens per second)
       eval time =    1510.28 ms /    58 tokens (   26.04 ms per token,    38.40 tokens per second)
      total time =    1787.56 ms /   115 tokens
slot      release: id  1 | task 35461 | stop processing: n_tokens = 1016, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.944
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35520 | processing task, is_child = 0
slot update_slots: id  1 | task 35520 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1038
slot update_slots: id  1 | task 35520 | n_tokens = 959, memory_seq_rm [959, end)
slot update_slots: id  1 | task 35520 | prompt processing progress, n_tokens = 974, batch.n_tokens = 15, progress = 0.938343
slot update_slots: id  1 | task 35520 | n_tokens = 974, memory_seq_rm [974, end)
slot update_slots: id  1 | task 35520 | prompt processing progress, n_tokens = 1038, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35520 | prompt done, n_tokens = 1038, batch.n_tokens = 64
slot init_sampler: id  1 | task 35520 | init sampler, took 0.19 ms, tokens: text = 1038, total = 1038
slot update_slots: id  1 | task 35520 | created context checkpoint 5 of 8 (pos_min = 519, pos_max = 973, size = 10.670 MiB)
slot print_timing: id  1 | task 35520 | 
prompt eval time =     359.33 ms /    79 tokens (    4.55 ms per token,   219.86 tokens per second)
       eval time =    3993.43 ms /   152 tokens (   26.27 ms per token,    38.06 tokens per second)
      total time =    4352.75 ms /   231 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 35520 | stop processing: n_tokens = 1189, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.750
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35674 | processing task, is_child = 0
slot update_slots: id  1 | task 35674 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 901
slot update_slots: id  1 | task 35674 | n_tokens = 892, memory_seq_rm [892, end)
slot update_slots: id  1 | task 35674 | prompt processing progress, n_tokens = 901, batch.n_tokens = 9, progress = 1.000000
slot update_slots: id  1 | task 35674 | prompt done, n_tokens = 901, batch.n_tokens = 9
slot init_sampler: id  1 | task 35674 | init sampler, took 0.16 ms, tokens: text = 901, total = 901
slot print_timing: id  1 | task 35674 | 
prompt eval time =     119.02 ms /     9 tokens (   13.22 ms per token,    75.62 tokens per second)
       eval time =    8558.20 ms /   323 tokens (   26.50 ms per token,    37.74 tokens per second)
      total time =    8677.22 ms /   332 tokens
slot      release: id  1 | task 35674 | stop processing: n_tokens = 1223, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.927 (> 0.100 thold), f_keep = 0.404
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1223, total state size = 45.187 MiB
srv          load:  - looking for better prompt, base f_keep = 0.404, sim = 0.927
srv        update:  - cache state: 8 prompts, 2182.808 MiB (limits: 8192.000 MiB, 64000 tokens, 216515 est)
srv        update:    - prompt 0x55bd8f1cbf80:     976 tokens, checkpoints:  1,    56.020 MiB
srv        update:    - prompt 0x55bd96905720:    5131 tokens, checkpoints:  8,   315.673 MiB
srv        update:    - prompt 0x55bd9697ce40:    4478 tokens, checkpoints:  5,   223.986 MiB
srv        update:    - prompt 0x55bd96c6b950:    9565 tokens, checkpoints:  4,   329.459 MiB
srv        update:    - prompt 0x55bda9a02fa0:   30921 tokens, checkpoints:  8,   881.261 MiB
srv        update:    - prompt 0x55bd9cfa5230:    4162 tokens, checkpoints:  8,   207.432 MiB
srv        update:    - prompt 0x55bd9cd6b9a0:    1236 tokens, checkpoints:  2,    69.293 MiB
srv        update:    - prompt 0x55bd9bdea660:    1223 tokens, checkpoints:  5,    99.683 MiB
srv  get_availabl: prompt cache update took 73.31 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35998 | processing task, is_child = 0
slot update_slots: id  1 | task 35998 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 533
slot update_slots: id  1 | task 35998 | n_past = 494, slot.prompt.tokens.size() = 1223, seq_id = 1, pos_min = 519, n_swa = 128
slot update_slots: id  1 | task 35998 | restored context checkpoint (pos_min = 195, pos_max = 713, size = 12.170 MiB)
slot update_slots: id  1 | task 35998 | erased invalidated context checkpoint (pos_min = 519, pos_max = 837, n_swa = 128, size = 7.481 MiB)
slot update_slots: id  1 | task 35998 | erased invalidated context checkpoint (pos_min = 519, pos_max = 973, n_swa = 128, size = 10.670 MiB)
slot update_slots: id  1 | task 35998 | n_tokens = 494, memory_seq_rm [494, end)
slot update_slots: id  1 | task 35998 | prompt processing progress, n_tokens = 533, batch.n_tokens = 39, progress = 1.000000
slot update_slots: id  1 | task 35998 | prompt done, n_tokens = 533, batch.n_tokens = 39
slot init_sampler: id  1 | task 35998 | init sampler, took 0.09 ms, tokens: text = 533, total = 533
slot print_timing: id  1 | task 35998 | 
prompt eval time =     280.96 ms /    39 tokens (    7.20 ms per token,   138.81 tokens per second)
       eval time =    3669.46 ms /   146 tokens (   25.13 ms per token,    39.79 tokens per second)
      total time =    3950.42 ms /   185 tokens
slot      release: id  1 | task 35998 | stop processing: n_tokens = 678, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.896 (> 0.100 thold), f_keep = 0.788
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 36145 | processing task, is_child = 0
slot update_slots: id  1 | task 36145 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 596
slot update_slots: id  1 | task 36145 | n_tokens = 534, memory_seq_rm [534, end)
slot update_slots: id  1 | task 36145 | prompt processing progress, n_tokens = 596, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  1 | task 36145 | prompt done, n_tokens = 596, batch.n_tokens = 62
slot init_sampler: id  1 | task 36145 | init sampler, took 0.10 ms, tokens: text = 596, total = 596
slot print_timing: id  1 | task 36145 | 
prompt eval time =     256.35 ms /    62 tokens (    4.13 ms per token,   241.85 tokens per second)
       eval time =    3321.93 ms /   129 tokens (   25.75 ms per token,    38.83 tokens per second)
      total time =    3578.28 ms /   191 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 36145 | stop processing: n_tokens = 724, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.266 (> 0.100 thold), f_keep = 0.823
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 36275 | processing task, is_child = 0
slot update_slots: id  1 | task 36275 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2244
slot update_slots: id  1 | task 36275 | n_tokens = 596, memory_seq_rm [596, end)
slot update_slots: id  1 | task 36275 | prompt processing progress, n_tokens = 2180, batch.n_tokens = 1584, progress = 0.971479
slot update_slots: id  1 | task 36275 | n_tokens = 2180, memory_seq_rm [2180, end)
slot update_slots: id  1 | task 36275 | prompt processing progress, n_tokens = 2244, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 36275 | prompt done, n_tokens = 2244, batch.n_tokens = 64
slot init_sampler: id  1 | task 36275 | init sampler, took 0.36 ms, tokens: text = 2244, total = 2244
slot update_slots: id  1 | task 36275 | created context checkpoint 4 of 8 (pos_min = 1410, pos_max = 2179, size = 18.056 MiB)
slot print_timing: id  1 | task 36275 | 
prompt eval time =    2162.86 ms /  1648 tokens (    1.31 ms per token,   761.96 tokens per second)
       eval time =    1164.68 ms /    44 tokens (   26.47 ms per token,    37.78 tokens per second)
      total time =    3327.54 ms /  1692 tokens
slot      release: id  1 | task 36275 | stop processing: n_tokens = 2287, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.778 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 36321 | processing task, is_child = 0
slot update_slots: id  1 | task 36321 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2884
slot update_slots: id  1 | task 36321 | n_tokens = 2244, memory_seq_rm [2244, end)
slot update_slots: id  1 | task 36321 | prompt processing progress, n_tokens = 2820, batch.n_tokens = 576, progress = 0.977809
slot update_slots: id  1 | task 36321 | n_tokens = 2820, memory_seq_rm [2820, end)
slot update_slots: id  1 | task 36321 | prompt processing progress, n_tokens = 2884, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 36321 | prompt done, n_tokens = 2884, batch.n_tokens = 64
slot init_sampler: id  1 | task 36321 | init sampler, took 0.50 ms, tokens: text = 2884, total = 2884
slot update_slots: id  1 | task 36321 | created context checkpoint 5 of 8 (pos_min = 2050, pos_max = 2819, size = 18.056 MiB)
slot print_timing: id  1 | task 36321 | 
prompt eval time =    1034.96 ms /   640 tokens (    1.62 ms per token,   618.38 tokens per second)
       eval time =    1225.56 ms /    46 tokens (   26.64 ms per token,    37.53 tokens per second)
      total time =    2260.52 ms /   686 tokens
slot      release: id  1 | task 36321 | stop processing: n_tokens = 2929, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 36369 | processing task, is_child = 0
slot update_slots: id  1 | task 36369 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2955
slot update_slots: id  1 | task 36369 | n_tokens = 2884, memory_seq_rm [2884, end)
slot update_slots: id  1 | task 36369 | prompt processing progress, n_tokens = 2891, batch.n_tokens = 7, progress = 0.978342
slot update_slots: id  1 | task 36369 | n_tokens = 2891, memory_seq_rm [2891, end)
slot update_slots: id  1 | task 36369 | prompt processing progress, n_tokens = 2955, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 36369 | prompt done, n_tokens = 2955, batch.n_tokens = 64
slot init_sampler: id  1 | task 36369 | init sampler, took 0.45 ms, tokens: text = 2955, total = 2955
slot update_slots: id  1 | task 36369 | created context checkpoint 6 of 8 (pos_min = 2159, pos_max = 2890, size = 17.165 MiB)
slot print_timing: id  1 | task 36369 | 
prompt eval time =     264.53 ms /    71 tokens (    3.73 ms per token,   268.40 tokens per second)
       eval time =     955.58 ms /    35 tokens (   27.30 ms per token,    36.63 tokens per second)
      total time =    1220.11 ms /   106 tokens
slot      release: id  1 | task 36369 | stop processing: n_tokens = 2989, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 36406 | processing task, is_child = 0
slot update_slots: id  1 | task 36406 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3197
slot update_slots: id  1 | task 36406 | n_tokens = 2955, memory_seq_rm [2955, end)
slot update_slots: id  1 | task 36406 | prompt processing progress, n_tokens = 3133, batch.n_tokens = 178, progress = 0.979981
slot update_slots: id  1 | task 36406 | n_tokens = 3133, memory_seq_rm [3133, end)
slot update_slots: id  1 | task 36406 | prompt processing progress, n_tokens = 3197, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 36406 | prompt done, n_tokens = 3197, batch.n_tokens = 64
slot init_sampler: id  1 | task 36406 | init sampler, took 0.47 ms, tokens: text = 3197, total = 3197
slot update_slots: id  1 | task 36406 | created context checkpoint 7 of 8 (pos_min = 2363, pos_max = 3132, size = 18.056 MiB)
slot print_timing: id  1 | task 36406 | 
prompt eval time =     544.29 ms /   242 tokens (    2.25 ms per token,   444.61 tokens per second)
       eval time =    2062.16 ms /    62 tokens (   33.26 ms per token,    30.07 tokens per second)
      total time =    2606.46 ms /   304 tokens
slot      release: id  1 | task 36406 | stop processing: n_tokens = 3258, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.667 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 36470 | processing task, is_child = 0
slot update_slots: id  1 | task 36470 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4790
slot update_slots: id  1 | task 36470 | n_tokens = 3197, memory_seq_rm [3197, end)
slot update_slots: id  1 | task 36470 | prompt processing progress, n_tokens = 4726, batch.n_tokens = 1529, progress = 0.986639
slot update_slots: id  1 | task 36470 | n_tokens = 4726, memory_seq_rm [4726, end)
slot update_slots: id  1 | task 36470 | prompt processing progress, n_tokens = 4790, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 36470 | prompt done, n_tokens = 4790, batch.n_tokens = 64
slot init_sampler: id  1 | task 36470 | init sampler, took 0.88 ms, tokens: text = 4790, total = 4790
slot update_slots: id  1 | task 36470 | created context checkpoint 8 of 8 (pos_min = 3956, pos_max = 4725, size = 18.056 MiB)
slot print_timing: id  1 | task 36470 | 
prompt eval time =    3538.29 ms /  1593 tokens (    2.22 ms per token,   450.22 tokens per second)
       eval time =    8990.44 ms /   324 tokens (   27.75 ms per token,    36.04 tokens per second)
      total time =   12528.73 ms /  1917 tokens
slot      release: id  1 | task 36470 | stop processing: n_tokens = 5113, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.836 (> 0.100 thold), f_keep = 0.937
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 36796 | processing task, is_child = 0
slot update_slots: id  1 | task 36796 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5730
slot update_slots: id  1 | task 36796 | n_tokens = 4790, memory_seq_rm [4790, end)
slot update_slots: id  1 | task 36796 | prompt processing progress, n_tokens = 5666, batch.n_tokens = 876, progress = 0.988831
slot update_slots: id  1 | task 36796 | n_tokens = 5666, memory_seq_rm [5666, end)
slot update_slots: id  1 | task 36796 | prompt processing progress, n_tokens = 5730, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 36796 | prompt done, n_tokens = 5730, batch.n_tokens = 64
slot init_sampler: id  1 | task 36796 | init sampler, took 0.86 ms, tokens: text = 5730, total = 5730
slot update_slots: id  1 | task 36796 | erasing old context checkpoint (pos_min = 0, pos_max = 482, size = 11.326 MiB)
slot update_slots: id  1 | task 36796 | created context checkpoint 8 of 8 (pos_min = 5023, pos_max = 5665, size = 15.078 MiB)
slot print_timing: id  1 | task 36796 | 
prompt eval time =    1399.30 ms /   940 tokens (    1.49 ms per token,   671.76 tokens per second)
       eval time =   75329.99 ms /  2863 tokens (   26.31 ms per token,    38.01 tokens per second)
      total time =   76729.29 ms /  3803 tokens
slot      release: id  1 | task 36796 | stop processing: n_tokens = 8592, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.901 (> 0.100 thold), f_keep = 0.667
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 39661 | processing task, is_child = 0
slot update_slots: id  1 | task 39661 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6361
slot update_slots: id  1 | task 39661 | n_past = 5730, slot.prompt.tokens.size() = 8592, seq_id = 1, pos_min = 7822, n_swa = 128
slot update_slots: id  1 | task 39661 | restored context checkpoint (pos_min = 5023, pos_max = 5665, size = 15.078 MiB)
slot update_slots: id  1 | task 39661 | n_tokens = 5665, memory_seq_rm [5665, end)
slot update_slots: id  1 | task 39661 | prompt processing progress, n_tokens = 6297, batch.n_tokens = 632, progress = 0.989939
slot update_slots: id  1 | task 39661 | n_tokens = 6297, memory_seq_rm [6297, end)
slot update_slots: id  1 | task 39661 | prompt processing progress, n_tokens = 6361, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 39661 | prompt done, n_tokens = 6361, batch.n_tokens = 64
slot init_sampler: id  1 | task 39661 | init sampler, took 1.19 ms, tokens: text = 6361, total = 6361
slot update_slots: id  1 | task 39661 | erasing old context checkpoint (pos_min = 72, pos_max = 619, size = 12.850 MiB)
slot update_slots: id  1 | task 39661 | created context checkpoint 8 of 8 (pos_min = 5527, pos_max = 6296, size = 18.056 MiB)
slot print_timing: id  1 | task 39661 | 
prompt eval time =    1324.95 ms /   696 tokens (    1.90 ms per token,   525.30 tokens per second)
       eval time =   16161.92 ms /   614 tokens (   26.32 ms per token,    37.99 tokens per second)
      total time =   17486.87 ms /  1310 tokens
slot      release: id  1 | task 39661 | stop processing: n_tokens = 6974, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.910 (> 0.100 thold), f_keep = 0.912
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 40277 | processing task, is_child = 0
slot update_slots: id  1 | task 40277 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6992
slot update_slots: id  1 | task 40277 | n_tokens = 6361, memory_seq_rm [6361, end)
slot update_slots: id  1 | task 40277 | prompt processing progress, n_tokens = 6928, batch.n_tokens = 567, progress = 0.990847
slot update_slots: id  1 | task 40277 | n_tokens = 6928, memory_seq_rm [6928, end)
slot update_slots: id  1 | task 40277 | prompt processing progress, n_tokens = 6992, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 40277 | prompt done, n_tokens = 6992, batch.n_tokens = 64
slot init_sampler: id  1 | task 40277 | init sampler, took 1.02 ms, tokens: text = 6992, total = 6992
slot update_slots: id  1 | task 40277 | erasing old context checkpoint (pos_min = 195, pos_max = 713, size = 12.170 MiB)
slot update_slots: id  1 | task 40277 | created context checkpoint 8 of 8 (pos_min = 6234, pos_max = 6927, size = 16.274 MiB)
slot print_timing: id  1 | task 40277 | 
prompt eval time =    1055.87 ms /   631 tokens (    1.67 ms per token,   597.61 tokens per second)
       eval time =    1551.68 ms /    60 tokens (   25.86 ms per token,    38.67 tokens per second)
      total time =    2607.55 ms /   691 tokens
slot      release: id  1 | task 40277 | stop processing: n_tokens = 7051, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 40339 | processing task, is_child = 0
slot update_slots: id  1 | task 40339 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7140
slot update_slots: id  1 | task 40339 | n_tokens = 6992, memory_seq_rm [6992, end)
slot update_slots: id  1 | task 40339 | prompt processing progress, n_tokens = 7076, batch.n_tokens = 84, progress = 0.991036
slot update_slots: id  1 | task 40339 | n_tokens = 7076, memory_seq_rm [7076, end)
slot update_slots: id  1 | task 40339 | prompt processing progress, n_tokens = 7140, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 40339 | prompt done, n_tokens = 7140, batch.n_tokens = 64
slot init_sampler: id  1 | task 40339 | init sampler, took 1.03 ms, tokens: text = 7140, total = 7140
slot update_slots: id  1 | task 40339 | erasing old context checkpoint (pos_min = 1410, pos_max = 2179, size = 18.056 MiB)
slot update_slots: id  1 | task 40339 | created context checkpoint 8 of 8 (pos_min = 6392, pos_max = 7075, size = 16.039 MiB)
slot print_timing: id  1 | task 40339 | 
prompt eval time =     445.10 ms /   148 tokens (    3.01 ms per token,   332.51 tokens per second)
       eval time =    2892.40 ms /   111 tokens (   26.06 ms per token,    38.38 tokens per second)
      total time =    3337.50 ms /   259 tokens
slot      release: id  1 | task 40339 | stop processing: n_tokens = 7250, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 40452 | processing task, is_child = 0
slot update_slots: id  1 | task 40452 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7211
slot update_slots: id  1 | task 40452 | n_tokens = 7140, memory_seq_rm [7140, end)
slot update_slots: id  1 | task 40452 | prompt processing progress, n_tokens = 7147, batch.n_tokens = 7, progress = 0.991125
slot update_slots: id  1 | task 40452 | n_tokens = 7147, memory_seq_rm [7147, end)
slot update_slots: id  1 | task 40452 | prompt processing progress, n_tokens = 7211, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 40452 | prompt done, n_tokens = 7211, batch.n_tokens = 64
slot init_sampler: id  1 | task 40452 | init sampler, took 1.26 ms, tokens: text = 7211, total = 7211
slot update_slots: id  1 | task 40452 | erasing old context checkpoint (pos_min = 2050, pos_max = 2819, size = 18.056 MiB)
slot update_slots: id  1 | task 40452 | created context checkpoint 8 of 8 (pos_min = 6492, pos_max = 7146, size = 15.359 MiB)
slot print_timing: id  1 | task 40452 | 
prompt eval time =     255.46 ms /    71 tokens (    3.60 ms per token,   277.93 tokens per second)
       eval time =    1909.45 ms /    73 tokens (   26.16 ms per token,    38.23 tokens per second)
      total time =    2164.91 ms /   144 tokens
slot      release: id  1 | task 40452 | stop processing: n_tokens = 7283, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 40527 | processing task, is_child = 0
slot update_slots: id  1 | task 40527 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7282
slot update_slots: id  1 | task 40527 | n_tokens = 7211, memory_seq_rm [7211, end)
slot update_slots: id  1 | task 40527 | prompt processing progress, n_tokens = 7218, batch.n_tokens = 7, progress = 0.991211
slot update_slots: id  1 | task 40527 | n_tokens = 7218, memory_seq_rm [7218, end)
slot update_slots: id  1 | task 40527 | prompt processing progress, n_tokens = 7282, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 40527 | prompt done, n_tokens = 7282, batch.n_tokens = 64
slot init_sampler: id  1 | task 40527 | init sampler, took 1.16 ms, tokens: text = 7282, total = 7282
slot update_slots: id  1 | task 40527 | erasing old context checkpoint (pos_min = 2159, pos_max = 2890, size = 17.165 MiB)
slot update_slots: id  1 | task 40527 | created context checkpoint 8 of 8 (pos_min = 6525, pos_max = 7217, size = 16.250 MiB)
slot print_timing: id  1 | task 40527 | 
prompt eval time =     261.98 ms /    71 tokens (    3.69 ms per token,   271.02 tokens per second)
       eval time =   16189.37 ms /   620 tokens (   26.11 ms per token,    38.30 tokens per second)
      total time =   16451.35 ms /   691 tokens
slot      release: id  1 | task 40527 | stop processing: n_tokens = 7901, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.922 (> 0.100 thold), f_keep = 0.922
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 41149 | processing task, is_child = 0
slot update_slots: id  1 | task 41149 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7901
slot update_slots: id  1 | task 41149 | n_tokens = 7282, memory_seq_rm [7282, end)
slot update_slots: id  1 | task 41149 | prompt processing progress, n_tokens = 7837, batch.n_tokens = 555, progress = 0.991900
slot update_slots: id  1 | task 41149 | n_tokens = 7837, memory_seq_rm [7837, end)
slot update_slots: id  1 | task 41149 | prompt processing progress, n_tokens = 7901, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 41149 | prompt done, n_tokens = 7901, batch.n_tokens = 64
slot init_sampler: id  1 | task 41149 | init sampler, took 1.49 ms, tokens: text = 7901, total = 7901
slot update_slots: id  1 | task 41149 | erasing old context checkpoint (pos_min = 2363, pos_max = 3132, size = 18.056 MiB)
slot update_slots: id  1 | task 41149 | created context checkpoint 8 of 8 (pos_min = 7155, pos_max = 7836, size = 15.992 MiB)
slot print_timing: id  1 | task 41149 | 
prompt eval time =    1036.21 ms /   619 tokens (    1.67 ms per token,   597.37 tokens per second)
       eval time =    2000.83 ms /    77 tokens (   25.98 ms per token,    38.48 tokens per second)
      total time =    3037.04 ms /   696 tokens
slot      release: id  1 | task 41149 | stop processing: n_tokens = 7977, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 41228 | processing task, is_child = 0
slot update_slots: id  1 | task 41228 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7972
slot update_slots: id  1 | task 41228 | n_tokens = 7901, memory_seq_rm [7901, end)
slot update_slots: id  1 | task 41228 | prompt processing progress, n_tokens = 7908, batch.n_tokens = 7, progress = 0.991972
slot update_slots: id  1 | task 41228 | n_tokens = 7908, memory_seq_rm [7908, end)
slot update_slots: id  1 | task 41228 | prompt processing progress, n_tokens = 7972, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 41228 | prompt done, n_tokens = 7972, batch.n_tokens = 64
slot init_sampler: id  1 | task 41228 | init sampler, took 1.15 ms, tokens: text = 7972, total = 7972
slot update_slots: id  1 | task 41228 | erasing old context checkpoint (pos_min = 3956, pos_max = 4725, size = 18.056 MiB)
slot update_slots: id  1 | task 41228 | created context checkpoint 8 of 8 (pos_min = 7334, pos_max = 7907, size = 13.460 MiB)
slot print_timing: id  1 | task 41228 | 
prompt eval time =     256.86 ms /    71 tokens (    3.62 ms per token,   276.42 tokens per second)
       eval time =    2763.74 ms /   106 tokens (   26.07 ms per token,    38.35 tokens per second)
      total time =    3020.60 ms /   177 tokens
slot      release: id  1 | task 41228 | stop processing: n_tokens = 8077, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.833 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 41336 | processing task, is_child = 0
slot update_slots: id  1 | task 41336 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9565
slot update_slots: id  1 | task 41336 | n_tokens = 7972, memory_seq_rm [7972, end)
slot update_slots: id  1 | task 41336 | prompt processing progress, n_tokens = 9501, batch.n_tokens = 1529, progress = 0.993309
slot update_slots: id  1 | task 41336 | n_tokens = 9501, memory_seq_rm [9501, end)
slot update_slots: id  1 | task 41336 | prompt processing progress, n_tokens = 9565, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 41336 | prompt done, n_tokens = 9565, batch.n_tokens = 64
slot init_sampler: id  1 | task 41336 | init sampler, took 1.38 ms, tokens: text = 9565, total = 9565
slot update_slots: id  1 | task 41336 | erasing old context checkpoint (pos_min = 5023, pos_max = 5665, size = 15.078 MiB)
slot update_slots: id  1 | task 41336 | created context checkpoint 8 of 8 (pos_min = 8731, pos_max = 9500, size = 18.056 MiB)
slot print_timing: id  1 | task 41336 | 
prompt eval time =    2121.22 ms /  1593 tokens (    1.33 ms per token,   750.98 tokens per second)
       eval time =   17623.87 ms /   666 tokens (   26.46 ms per token,    37.79 tokens per second)
      total time =   19745.09 ms /  2259 tokens
slot      release: id  1 | task 41336 | stop processing: n_tokens = 10230, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.938 (> 0.100 thold), f_keep = 0.935
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42004 | processing task, is_child = 0
slot update_slots: id  1 | task 42004 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10196
slot update_slots: id  1 | task 42004 | n_past = 9565, slot.prompt.tokens.size() = 10230, seq_id = 1, pos_min = 9460, n_swa = 128
slot update_slots: id  1 | task 42004 | restored context checkpoint (pos_min = 8731, pos_max = 9500, size = 18.056 MiB)
slot update_slots: id  1 | task 42004 | n_tokens = 9500, memory_seq_rm [9500, end)
slot update_slots: id  1 | task 42004 | prompt processing progress, n_tokens = 10132, batch.n_tokens = 632, progress = 0.993723
slot update_slots: id  1 | task 42004 | n_tokens = 10132, memory_seq_rm [10132, end)
slot update_slots: id  1 | task 42004 | prompt processing progress, n_tokens = 10196, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42004 | prompt done, n_tokens = 10196, batch.n_tokens = 64
slot init_sampler: id  1 | task 42004 | init sampler, took 1.89 ms, tokens: text = 10196, total = 10196
slot update_slots: id  1 | task 42004 | erasing old context checkpoint (pos_min = 5527, pos_max = 6296, size = 18.056 MiB)
slot update_slots: id  1 | task 42004 | created context checkpoint 8 of 8 (pos_min = 9362, pos_max = 10131, size = 18.056 MiB)
slot print_timing: id  1 | task 42004 | 
prompt eval time =    1392.84 ms /   696 tokens (    2.00 ms per token,   499.70 tokens per second)
       eval time =   17525.22 ms /   653 tokens (   26.84 ms per token,    37.26 tokens per second)
      total time =   18918.06 ms /  1349 tokens
slot      release: id  1 | task 42004 | stop processing: n_tokens = 10848, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.942 (> 0.100 thold), f_keep = 0.940
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42659 | processing task, is_child = 0
slot update_slots: id  1 | task 42659 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10827
slot update_slots: id  1 | task 42659 | n_past = 10196, slot.prompt.tokens.size() = 10848, seq_id = 1, pos_min = 10078, n_swa = 128
slot update_slots: id  1 | task 42659 | restored context checkpoint (pos_min = 9362, pos_max = 10131, size = 18.056 MiB)
slot update_slots: id  1 | task 42659 | n_tokens = 10131, memory_seq_rm [10131, end)
slot update_slots: id  1 | task 42659 | prompt processing progress, n_tokens = 10763, batch.n_tokens = 632, progress = 0.994089
slot update_slots: id  1 | task 42659 | n_tokens = 10763, memory_seq_rm [10763, end)
slot update_slots: id  1 | task 42659 | prompt processing progress, n_tokens = 10827, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42659 | prompt done, n_tokens = 10827, batch.n_tokens = 64
slot init_sampler: id  1 | task 42659 | init sampler, took 1.57 ms, tokens: text = 10827, total = 10827
slot update_slots: id  1 | task 42659 | erasing old context checkpoint (pos_min = 6234, pos_max = 6927, size = 16.274 MiB)
slot update_slots: id  1 | task 42659 | created context checkpoint 8 of 8 (pos_min = 9993, pos_max = 10762, size = 18.056 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 42659
slot      release: id  1 | task 42659 | stop processing: n_tokens = 10831, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.385 (> 0.100 thold), f_keep = 0.015
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 10831, total state size = 272.032 MiB
srv          load:  - looking for better prompt, base f_keep = 0.015, sim = 0.385
srv        update:  - cache state: 9 prompts, 2586.110 MiB (limits: 8192.000 MiB, 64000 tokens, 217059 est)
srv        update:    - prompt 0x55bd8f1cbf80:     976 tokens, checkpoints:  1,    56.020 MiB
srv        update:    - prompt 0x55bd96905720:    5131 tokens, checkpoints:  8,   315.673 MiB
srv        update:    - prompt 0x55bd9697ce40:    4478 tokens, checkpoints:  5,   223.986 MiB
srv        update:    - prompt 0x55bd96c6b950:    9565 tokens, checkpoints:  4,   329.459 MiB
srv        update:    - prompt 0x55bda9a02fa0:   30921 tokens, checkpoints:  8,   881.261 MiB
srv        update:    - prompt 0x55bd9cfa5230:    4162 tokens, checkpoints:  8,   207.432 MiB
srv        update:    - prompt 0x55bd9cd6b9a0:    1236 tokens, checkpoints:  2,    69.293 MiB
srv        update:    - prompt 0x55bd9bdea660:    1223 tokens, checkpoints:  5,    99.683 MiB
srv        update:    - prompt 0x55bd9cec1880:   10831 tokens, checkpoints:  8,   403.301 MiB
srv  get_availabl: prompt cache update took 306.33 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42667 | processing task, is_child = 0
slot update_slots: id  1 | task 42667 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 426
slot update_slots: id  1 | task 42667 | n_past = 164, slot.prompt.tokens.size() = 10831, seq_id = 1, pos_min = 10061, n_swa = 128
slot update_slots: id  1 | task 42667 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 42667 | erased invalidated context checkpoint (pos_min = 6392, pos_max = 7075, n_swa = 128, size = 16.039 MiB)
slot update_slots: id  1 | task 42667 | erased invalidated context checkpoint (pos_min = 6492, pos_max = 7146, n_swa = 128, size = 15.359 MiB)
slot update_slots: id  1 | task 42667 | erased invalidated context checkpoint (pos_min = 6525, pos_max = 7217, n_swa = 128, size = 16.250 MiB)
slot update_slots: id  1 | task 42667 | erased invalidated context checkpoint (pos_min = 7155, pos_max = 7836, n_swa = 128, size = 15.992 MiB)
slot update_slots: id  1 | task 42667 | erased invalidated context checkpoint (pos_min = 7334, pos_max = 7907, n_swa = 128, size = 13.460 MiB)
slot update_slots: id  1 | task 42667 | erased invalidated context checkpoint (pos_min = 8731, pos_max = 9500, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 42667 | erased invalidated context checkpoint (pos_min = 9362, pos_max = 10131, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 42667 | erased invalidated context checkpoint (pos_min = 9993, pos_max = 10762, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 42667 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 42667 | prompt processing progress, n_tokens = 362, batch.n_tokens = 362, progress = 0.849765
slot update_slots: id  1 | task 42667 | n_tokens = 362, memory_seq_rm [362, end)
slot update_slots: id  1 | task 42667 | prompt processing progress, n_tokens = 426, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42667 | prompt done, n_tokens = 426, batch.n_tokens = 64
slot init_sampler: id  1 | task 42667 | init sampler, took 0.08 ms, tokens: text = 426, total = 426
slot update_slots: id  1 | task 42667 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 361, size = 8.489 MiB)
slot print_timing: id  1 | task 42667 | 
prompt eval time =     706.22 ms /   426 tokens (    1.66 ms per token,   603.21 tokens per second)
       eval time =    2854.60 ms /   115 tokens (   24.82 ms per token,    40.29 tokens per second)
      total time =    3560.82 ms /   541 tokens
slot      release: id  1 | task 42667 | stop processing: n_tokens = 540, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.205 (> 0.100 thold), f_keep = 0.789
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42784 | processing task, is_child = 0
slot update_slots: id  1 | task 42784 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2074
slot update_slots: id  1 | task 42784 | n_tokens = 426, memory_seq_rm [426, end)
slot update_slots: id  1 | task 42784 | prompt processing progress, n_tokens = 2010, batch.n_tokens = 1584, progress = 0.969142
slot update_slots: id  1 | task 42784 | n_tokens = 2010, memory_seq_rm [2010, end)
slot update_slots: id  1 | task 42784 | prompt processing progress, n_tokens = 2074, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42784 | prompt done, n_tokens = 2074, batch.n_tokens = 64
slot init_sampler: id  1 | task 42784 | init sampler, took 0.36 ms, tokens: text = 2074, total = 2074
slot update_slots: id  1 | task 42784 | created context checkpoint 2 of 8 (pos_min = 1240, pos_max = 2009, size = 18.056 MiB)
slot print_timing: id  1 | task 42784 | 
prompt eval time =    2041.83 ms /  1648 tokens (    1.24 ms per token,   807.12 tokens per second)
       eval time =    1057.63 ms /    43 tokens (   24.60 ms per token,    40.66 tokens per second)
      total time =    3099.46 ms /  1691 tokens
slot      release: id  1 | task 42784 | stop processing: n_tokens = 2116, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.764 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42829 | processing task, is_child = 0
slot update_slots: id  1 | task 42829 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2714
slot update_slots: id  1 | task 42829 | n_tokens = 2074, memory_seq_rm [2074, end)
slot update_slots: id  1 | task 42829 | prompt processing progress, n_tokens = 2650, batch.n_tokens = 576, progress = 0.976419
slot update_slots: id  1 | task 42829 | n_tokens = 2650, memory_seq_rm [2650, end)
slot update_slots: id  1 | task 42829 | prompt processing progress, n_tokens = 2714, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42829 | prompt done, n_tokens = 2714, batch.n_tokens = 64
slot init_sampler: id  1 | task 42829 | init sampler, took 0.44 ms, tokens: text = 2714, total = 2714
slot update_slots: id  1 | task 42829 | created context checkpoint 3 of 8 (pos_min = 1880, pos_max = 2649, size = 18.056 MiB)
slot print_timing: id  1 | task 42829 | 
prompt eval time =     972.14 ms /   640 tokens (    1.52 ms per token,   658.34 tokens per second)
       eval time =     700.89 ms /    28 tokens (   25.03 ms per token,    39.95 tokens per second)
      total time =    1673.03 ms /   668 tokens
slot      release: id  1 | task 42829 | stop processing: n_tokens = 2741, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.630 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42859 | processing task, is_child = 0
slot update_slots: id  1 | task 42859 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4307
slot update_slots: id  1 | task 42859 | n_tokens = 2714, memory_seq_rm [2714, end)
slot update_slots: id  1 | task 42859 | prompt processing progress, n_tokens = 4243, batch.n_tokens = 1529, progress = 0.985140
slot update_slots: id  1 | task 42859 | n_tokens = 4243, memory_seq_rm [4243, end)
slot update_slots: id  1 | task 42859 | prompt processing progress, n_tokens = 4307, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42859 | prompt done, n_tokens = 4307, batch.n_tokens = 64
slot init_sampler: id  1 | task 42859 | init sampler, took 0.73 ms, tokens: text = 4307, total = 4307
slot update_slots: id  1 | task 42859 | created context checkpoint 4 of 8 (pos_min = 3473, pos_max = 4242, size = 18.056 MiB)
slot print_timing: id  1 | task 42859 | 
prompt eval time =    1999.95 ms /  1593 tokens (    1.26 ms per token,   796.52 tokens per second)
       eval time =   12266.33 ms /   465 tokens (   26.38 ms per token,    37.91 tokens per second)
      total time =   14266.28 ms /  2058 tokens
slot      release: id  1 | task 42859 | stop processing: n_tokens = 4771, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.821 (> 0.100 thold), f_keep = 0.903
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 43326 | processing task, is_child = 0
slot update_slots: id  1 | task 43326 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5247
slot update_slots: id  1 | task 43326 | n_tokens = 4307, memory_seq_rm [4307, end)
slot update_slots: id  1 | task 43326 | prompt processing progress, n_tokens = 5183, batch.n_tokens = 876, progress = 0.987803
slot update_slots: id  1 | task 43326 | n_tokens = 5183, memory_seq_rm [5183, end)
slot update_slots: id  1 | task 43326 | prompt processing progress, n_tokens = 5247, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 43326 | prompt done, n_tokens = 5247, batch.n_tokens = 64
slot init_sampler: id  1 | task 43326 | init sampler, took 0.88 ms, tokens: text = 5247, total = 5247
slot update_slots: id  1 | task 43326 | created context checkpoint 5 of 8 (pos_min = 4540, pos_max = 5182, size = 15.078 MiB)
slot print_timing: id  1 | task 43326 | 
prompt eval time =    1403.49 ms /   940 tokens (    1.49 ms per token,   669.76 tokens per second)
       eval time =   73062.31 ms /  2779 tokens (   26.29 ms per token,    38.04 tokens per second)
      total time =   74465.80 ms /  3719 tokens
slot      release: id  1 | task 43326 | stop processing: n_tokens = 8025, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.780 (> 0.100 thold), f_keep = 0.654
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 46107 | processing task, is_child = 0
slot update_slots: id  1 | task 46107 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6731
slot update_slots: id  1 | task 46107 | n_past = 5247, slot.prompt.tokens.size() = 8025, seq_id = 1, pos_min = 7255, n_swa = 128
slot update_slots: id  1 | task 46107 | restored context checkpoint (pos_min = 4540, pos_max = 5182, size = 15.078 MiB)
slot update_slots: id  1 | task 46107 | n_tokens = 5182, memory_seq_rm [5182, end)
slot update_slots: id  1 | task 46107 | prompt processing progress, n_tokens = 6667, batch.n_tokens = 1485, progress = 0.990492
slot update_slots: id  1 | task 46107 | n_tokens = 6667, memory_seq_rm [6667, end)
slot update_slots: id  1 | task 46107 | prompt processing progress, n_tokens = 6731, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 46107 | prompt done, n_tokens = 6731, batch.n_tokens = 64
slot init_sampler: id  1 | task 46107 | init sampler, took 1.00 ms, tokens: text = 6731, total = 6731
slot update_slots: id  1 | task 46107 | created context checkpoint 6 of 8 (pos_min = 5897, pos_max = 6666, size = 18.056 MiB)
slot print_timing: id  1 | task 46107 | 
prompt eval time =    2153.39 ms /  1549 tokens (    1.39 ms per token,   719.33 tokens per second)
       eval time =    5249.98 ms /   200 tokens (   26.25 ms per token,    38.10 tokens per second)
      total time =    7403.37 ms /  1749 tokens
slot      release: id  1 | task 46107 | stop processing: n_tokens = 6930, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.298 (> 0.100 thold), f_keep = 0.018
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6930, total state size = 180.557 MiB
srv          load:  - looking for better prompt, base f_keep = 0.018, sim = 0.298
srv        update:  - cache state: 10 prompts, 2862.458 MiB (limits: 8192.000 MiB, 64000 tokens, 215937 est)
srv        update:    - prompt 0x55bd8f1cbf80:     976 tokens, checkpoints:  1,    56.020 MiB
srv        update:    - prompt 0x55bd96905720:    5131 tokens, checkpoints:  8,   315.673 MiB
srv        update:    - prompt 0x55bd9697ce40:    4478 tokens, checkpoints:  5,   223.986 MiB
srv        update:    - prompt 0x55bd96c6b950:    9565 tokens, checkpoints:  4,   329.459 MiB
srv        update:    - prompt 0x55bda9a02fa0:   30921 tokens, checkpoints:  8,   881.261 MiB
srv        update:    - prompt 0x55bd9cfa5230:    4162 tokens, checkpoints:  8,   207.432 MiB
srv        update:    - prompt 0x55bd9cd6b9a0:    1236 tokens, checkpoints:  2,    69.293 MiB
srv        update:    - prompt 0x55bd9bdea660:    1223 tokens, checkpoints:  5,    99.683 MiB
srv        update:    - prompt 0x55bd9cec1880:   10831 tokens, checkpoints:  8,   403.301 MiB
srv        update:    - prompt 0x55bdb26a1eb0:    6930 tokens, checkpoints:  6,   276.348 MiB
srv  get_availabl: prompt cache update took 201.06 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 46309 | processing task, is_child = 0
slot update_slots: id  1 | task 46309 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 410
slot update_slots: id  1 | task 46309 | n_past = 122, slot.prompt.tokens.size() = 6930, seq_id = 1, pos_min = 6160, n_swa = 128
slot update_slots: id  1 | task 46309 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 46309 | erased invalidated context checkpoint (pos_min = 1240, pos_max = 2009, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 46309 | erased invalidated context checkpoint (pos_min = 1880, pos_max = 2649, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 46309 | erased invalidated context checkpoint (pos_min = 3473, pos_max = 4242, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 46309 | erased invalidated context checkpoint (pos_min = 4540, pos_max = 5182, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 46309 | erased invalidated context checkpoint (pos_min = 5897, pos_max = 6666, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 46309 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 46309 | prompt processing progress, n_tokens = 346, batch.n_tokens = 346, progress = 0.843902
slot update_slots: id  1 | task 46309 | n_tokens = 346, memory_seq_rm [346, end)
slot update_slots: id  1 | task 46309 | prompt processing progress, n_tokens = 410, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 46309 | prompt done, n_tokens = 410, batch.n_tokens = 64
slot init_sampler: id  1 | task 46309 | init sampler, took 0.07 ms, tokens: text = 410, total = 410
slot print_timing: id  1 | task 46309 | 
prompt eval time =     775.35 ms /   410 tokens (    1.89 ms per token,   528.80 tokens per second)
       eval time =    2111.80 ms /    81 tokens (   26.07 ms per token,    38.36 tokens per second)
      total time =    2887.15 ms /   491 tokens
slot      release: id  1 | task 46309 | stop processing: n_tokens = 490, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.824
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 46392 | processing task, is_child = 0
slot update_slots: id  1 | task 46392 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 409
slot update_slots: id  1 | task 46392 | n_tokens = 404, memory_seq_rm [404, end)
slot update_slots: id  1 | task 46392 | prompt processing progress, n_tokens = 409, batch.n_tokens = 5, progress = 1.000000
slot update_slots: id  1 | task 46392 | prompt done, n_tokens = 409, batch.n_tokens = 5
slot init_sampler: id  1 | task 46392 | init sampler, took 0.08 ms, tokens: text = 409, total = 409
slot print_timing: id  1 | task 46392 | 
prompt eval time =      92.89 ms /     5 tokens (   18.58 ms per token,    53.82 tokens per second)
       eval time =    3493.48 ms /   129 tokens (   27.08 ms per token,    36.93 tokens per second)
      total time =    3586.38 ms /   134 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 46392 | stop processing: n_tokens = 537, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 46522 | processing task, is_child = 0
slot update_slots: id  0 | task 46522 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7008
slot update_slots: id  0 | task 46522 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 46522 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.292237
slot update_slots: id  0 | task 46522 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 46522 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.584475
slot update_slots: id  0 | task 46522 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 46522 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.876712
slot update_slots: id  0 | task 46522 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 46522 | prompt processing progress, n_tokens = 6944, batch.n_tokens = 800, progress = 0.990868
slot update_slots: id  0 | task 46522 | n_tokens = 6944, memory_seq_rm [6944, end)
slot update_slots: id  0 | task 46522 | prompt processing progress, n_tokens = 7008, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 46522 | prompt done, n_tokens = 7008, batch.n_tokens = 64
slot init_sampler: id  0 | task 46522 | init sampler, took 1.23 ms, tokens: text = 7008, total = 7008
slot update_slots: id  0 | task 46522 | created context checkpoint 1 of 8 (pos_min = 6301, pos_max = 6943, size = 15.078 MiB)
slot print_timing: id  0 | task 46522 | 
prompt eval time =    9692.48 ms /  7008 tokens (    1.38 ms per token,   723.03 tokens per second)
       eval time =  115926.52 ms /  4237 tokens (   27.36 ms per token,    36.55 tokens per second)
      total time =  125619.00 ms / 11245 tokens
slot      release: id  0 | task 46522 | stop processing: n_tokens = 11244, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.825 (> 0.100 thold), f_keep = 0.623
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 50764 | processing task, is_child = 0
slot update_slots: id  0 | task 50764 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8492
slot update_slots: id  0 | task 50764 | n_past = 7008, slot.prompt.tokens.size() = 11244, seq_id = 0, pos_min = 10601, n_swa = 128
slot update_slots: id  0 | task 50764 | restored context checkpoint (pos_min = 6301, pos_max = 6943, size = 15.078 MiB)
slot update_slots: id  0 | task 50764 | n_tokens = 6943, memory_seq_rm [6943, end)
slot update_slots: id  0 | task 50764 | prompt processing progress, n_tokens = 8428, batch.n_tokens = 1485, progress = 0.992463
slot update_slots: id  0 | task 50764 | n_tokens = 8428, memory_seq_rm [8428, end)
slot update_slots: id  0 | task 50764 | prompt processing progress, n_tokens = 8492, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 50764 | prompt done, n_tokens = 8492, batch.n_tokens = 64
slot init_sampler: id  0 | task 50764 | init sampler, took 1.25 ms, tokens: text = 8492, total = 8492
slot update_slots: id  0 | task 50764 | created context checkpoint 2 of 8 (pos_min = 7785, pos_max = 8427, size = 15.078 MiB)
slot print_timing: id  0 | task 50764 | 
prompt eval time =    2189.36 ms /  1549 tokens (    1.41 ms per token,   707.51 tokens per second)
       eval time =   13741.75 ms /   523 tokens (   26.27 ms per token,    38.06 tokens per second)
      total time =   15931.11 ms /  2072 tokens
slot      release: id  0 | task 50764 | stop processing: n_tokens = 9014, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.942
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 51289 | processing task, is_child = 0
slot update_slots: id  0 | task 51289 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8863
slot update_slots: id  0 | task 51289 | n_past = 8492, slot.prompt.tokens.size() = 9014, seq_id = 0, pos_min = 8371, n_swa = 128
slot update_slots: id  0 | task 51289 | restored context checkpoint (pos_min = 7785, pos_max = 8427, size = 15.078 MiB)
slot update_slots: id  0 | task 51289 | n_tokens = 8427, memory_seq_rm [8427, end)
slot update_slots: id  0 | task 51289 | prompt processing progress, n_tokens = 8799, batch.n_tokens = 372, progress = 0.992779
slot update_slots: id  0 | task 51289 | n_tokens = 8799, memory_seq_rm [8799, end)
slot update_slots: id  0 | task 51289 | prompt processing progress, n_tokens = 8863, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 51289 | prompt done, n_tokens = 8863, batch.n_tokens = 64
slot init_sampler: id  0 | task 51289 | init sampler, took 1.39 ms, tokens: text = 8863, total = 8863
slot update_slots: id  0 | task 51289 | created context checkpoint 3 of 8 (pos_min = 8156, pos_max = 8798, size = 15.078 MiB)
slot print_timing: id  0 | task 51289 | 
prompt eval time =     827.58 ms /   436 tokens (    1.90 ms per token,   526.84 tokens per second)
       eval time =   25870.88 ms /   978 tokens (   26.45 ms per token,    37.80 tokens per second)
      total time =   26698.46 ms /  1414 tokens
slot      release: id  0 | task 51289 | stop processing: n_tokens = 9840, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.752
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 52269 | processing task, is_child = 0
slot update_slots: id  1 | task 52269 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 410
slot update_slots: id  1 | task 52269 | n_past = 404, slot.prompt.tokens.size() = 537, seq_id = 1, pos_min = 410, n_swa = 128
slot update_slots: id  1 | task 52269 | restored context checkpoint (pos_min = 0, pos_max = 361, size = 8.489 MiB)
slot update_slots: id  1 | task 52269 | n_tokens = 361, memory_seq_rm [361, end)
slot update_slots: id  1 | task 52269 | prompt processing progress, n_tokens = 410, batch.n_tokens = 49, progress = 1.000000
slot update_slots: id  1 | task 52269 | prompt done, n_tokens = 410, batch.n_tokens = 49
slot init_sampler: id  1 | task 52269 | init sampler, took 0.07 ms, tokens: text = 410, total = 410
slot print_timing: id  1 | task 52269 | 
prompt eval time =     334.59 ms /    49 tokens (    6.83 ms per token,   146.45 tokens per second)
       eval time =    1857.43 ms /    72 tokens (   25.80 ms per token,    38.76 tokens per second)
      total time =    2192.02 ms /   121 tokens
slot      release: id  1 | task 52269 | stop processing: n_tokens = 481, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.854
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 52342 | processing task, is_child = 0
slot update_slots: id  1 | task 52342 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 423
slot update_slots: id  1 | task 52342 | n_tokens = 411, memory_seq_rm [411, end)
slot update_slots: id  1 | task 52342 | prompt processing progress, n_tokens = 423, batch.n_tokens = 12, progress = 1.000000
slot update_slots: id  1 | task 52342 | prompt done, n_tokens = 423, batch.n_tokens = 12
slot init_sampler: id  1 | task 52342 | init sampler, took 0.07 ms, tokens: text = 423, total = 423
slot print_timing: id  1 | task 52342 | 
prompt eval time =     117.72 ms /    12 tokens (    9.81 ms per token,   101.93 tokens per second)
       eval time =    4880.26 ms /   182 tokens (   26.81 ms per token,    37.29 tokens per second)
      total time =    4997.98 ms /   194 tokens
slot      release: id  1 | task 52342 | stop processing: n_tokens = 604, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.679
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 52525 | processing task, is_child = 0
slot update_slots: id  1 | task 52525 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 410
slot update_slots: id  1 | task 52525 | need to evaluate at least 1 token for each active slot (n_past = 410, task.n_tokens() = 410)
slot update_slots: id  1 | task 52525 | n_past was set to 409
slot update_slots: id  1 | task 52525 | n_tokens = 409, memory_seq_rm [409, end)
slot update_slots: id  1 | task 52525 | prompt processing progress, n_tokens = 410, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  1 | task 52525 | prompt done, n_tokens = 410, batch.n_tokens = 1
slot init_sampler: id  1 | task 52525 | init sampler, took 0.07 ms, tokens: text = 410, total = 410
slot print_timing: id  1 | task 52525 | 
prompt eval time =      36.77 ms /     1 tokens (   36.77 ms per token,    27.19 tokens per second)
       eval time =    1004.21 ms /    37 tokens (   27.14 ms per token,    36.84 tokens per second)
      total time =    1040.98 ms /    38 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 52525 | stop processing: n_tokens = 446, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.922
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 52563 | processing task, is_child = 0
slot update_slots: id  1 | task 52563 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 423
slot update_slots: id  1 | task 52563 | n_tokens = 411, memory_seq_rm [411, end)
slot update_slots: id  1 | task 52563 | prompt processing progress, n_tokens = 423, batch.n_tokens = 12, progress = 1.000000
slot update_slots: id  1 | task 52563 | prompt done, n_tokens = 423, batch.n_tokens = 12
slot init_sampler: id  1 | task 52563 | init sampler, took 0.07 ms, tokens: text = 423, total = 423
slot print_timing: id  1 | task 52563 | 
prompt eval time =     117.95 ms /    12 tokens (    9.83 ms per token,   101.73 tokens per second)
       eval time =    4321.33 ms /   159 tokens (   27.18 ms per token,    36.79 tokens per second)
      total time =    4439.28 ms /   171 tokens
slot      release: id  1 | task 52563 | stop processing: n_tokens = 581, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
