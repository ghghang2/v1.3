ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14807 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1763 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 56064 -> need 1767 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.85 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14807 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
.srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 56064
llama_context: n_ctx_seq     = 56064
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (56064) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 56064 cells
llama_kv_cache:      CUDA0 KV buffer size =  1314.00 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_kv_cache: size = 1314.00 MiB ( 56064 cells,  12 layers,  4/1 seqs), K (f16):  657.00 MiB, V (f16):  657.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   117.15 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 77.94 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  1 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  2 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  3 | task -1 | new slot, n_ctx = 56064
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-18

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 960
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 896, batch.n_tokens = 896, progress = 0.933333
slot update_slots: id  3 | task 0 | n_tokens = 896, memory_seq_rm [896, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 960, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 960, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.14 ms, tokens: text = 960, total = 960
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 895, size = 21.011 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =    1330.41 ms /   960 tokens (    1.39 ms per token,   721.58 tokens per second)
       eval time =    1011.58 ms /    44 tokens (   22.99 ms per token,    43.50 tokens per second)
      total time =    2341.99 ms /  1004 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 1003, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.654 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46 | processing task, is_child = 0
slot update_slots: id  3 | task 46 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1496
slot update_slots: id  3 | task 46 | n_tokens = 978, memory_seq_rm [978, end)
slot update_slots: id  3 | task 46 | prompt processing progress, n_tokens = 1432, batch.n_tokens = 454, progress = 0.957219
slot update_slots: id  3 | task 46 | n_tokens = 1432, memory_seq_rm [1432, end)
slot update_slots: id  3 | task 46 | prompt processing progress, n_tokens = 1496, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 46 | prompt done, n_tokens = 1496, batch.n_tokens = 64
slot init_sampler: id  3 | task 46 | init sampler, took 0.30 ms, tokens: text = 1496, total = 1496
slot update_slots: id  3 | task 46 | created context checkpoint 2 of 8 (pos_min = 408, pos_max = 1431, size = 24.012 MiB)
slot print_timing: id  3 | task 46 | 
prompt eval time =     598.06 ms /   518 tokens (    1.15 ms per token,   866.14 tokens per second)
       eval time =   15119.81 ms /   616 tokens (   24.55 ms per token,    40.74 tokens per second)
      total time =   15717.87 ms /  1134 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 46 | stop processing: n_tokens = 2111, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.701 (> 0.100 thold), f_keep = 0.714
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 664 | processing task, is_child = 0
slot update_slots: id  3 | task 664 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2150
slot update_slots: id  3 | task 664 | n_tokens = 1507, memory_seq_rm [1507, end)
slot update_slots: id  3 | task 664 | prompt processing progress, n_tokens = 2086, batch.n_tokens = 579, progress = 0.970233
slot update_slots: id  3 | task 664 | n_tokens = 2086, memory_seq_rm [2086, end)
slot update_slots: id  3 | task 664 | prompt processing progress, n_tokens = 2150, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 664 | prompt done, n_tokens = 2150, batch.n_tokens = 64
slot init_sampler: id  3 | task 664 | init sampler, took 0.30 ms, tokens: text = 2150, total = 2150
slot update_slots: id  3 | task 664 | created context checkpoint 3 of 8 (pos_min = 1380, pos_max = 2085, size = 16.555 MiB)
slot print_timing: id  3 | task 664 | 
prompt eval time =     882.92 ms /   643 tokens (    1.37 ms per token,   728.27 tokens per second)
       eval time =    1404.38 ms /    56 tokens (   25.08 ms per token,    39.88 tokens per second)
      total time =    2287.30 ms /   699 tokens
slot      release: id  3 | task 664 | stop processing: n_tokens = 2205, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.530 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 722 | processing task, is_child = 0
slot update_slots: id  3 | task 722 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4093
slot update_slots: id  3 | task 722 | n_tokens = 2169, memory_seq_rm [2169, end)
slot update_slots: id  3 | task 722 | prompt processing progress, n_tokens = 4029, batch.n_tokens = 1860, progress = 0.984364
slot update_slots: id  3 | task 722 | n_tokens = 4029, memory_seq_rm [4029, end)
slot update_slots: id  3 | task 722 | prompt processing progress, n_tokens = 4093, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 722 | prompt done, n_tokens = 4093, batch.n_tokens = 64
slot init_sampler: id  3 | task 722 | init sampler, took 0.58 ms, tokens: text = 4093, total = 4093
slot update_slots: id  3 | task 722 | created context checkpoint 4 of 8 (pos_min = 3005, pos_max = 4028, size = 24.012 MiB)
slot print_timing: id  3 | task 722 | 
prompt eval time =    2170.39 ms /  1924 tokens (    1.13 ms per token,   886.48 tokens per second)
       eval time =    1092.16 ms /    42 tokens (   26.00 ms per token,    38.46 tokens per second)
      total time =    3262.55 ms /  1966 tokens
slot      release: id  3 | task 722 | stop processing: n_tokens = 4134, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.675 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 766 | processing task, is_child = 0
slot update_slots: id  3 | task 766 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6079
slot update_slots: id  3 | task 766 | n_tokens = 4102, memory_seq_rm [4102, end)
slot update_slots: id  3 | task 766 | prompt processing progress, n_tokens = 6015, batch.n_tokens = 1913, progress = 0.989472
slot update_slots: id  3 | task 766 | n_tokens = 6015, memory_seq_rm [6015, end)
slot update_slots: id  3 | task 766 | prompt processing progress, n_tokens = 6079, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 766 | prompt done, n_tokens = 6079, batch.n_tokens = 64
slot init_sampler: id  3 | task 766 | init sampler, took 0.86 ms, tokens: text = 6079, total = 6079
slot update_slots: id  3 | task 766 | created context checkpoint 5 of 8 (pos_min = 4991, pos_max = 6014, size = 24.012 MiB)
slot print_timing: id  3 | task 766 | 
prompt eval time =    2299.62 ms /  1977 tokens (    1.16 ms per token,   859.71 tokens per second)
       eval time =   33208.14 ms /  1243 tokens (   26.72 ms per token,    37.43 tokens per second)
      total time =   35507.76 ms /  3220 tokens
slot      release: id  3 | task 766 | stop processing: n_tokens = 7321, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.941 (> 0.100 thold), f_keep = 0.947
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2011 | processing task, is_child = 0
slot update_slots: id  3 | task 2011 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7363
slot update_slots: id  3 | task 2011 | n_tokens = 6931, memory_seq_rm [6931, end)
slot update_slots: id  3 | task 2011 | prompt processing progress, n_tokens = 7299, batch.n_tokens = 368, progress = 0.991308
slot update_slots: id  3 | task 2011 | n_tokens = 7299, memory_seq_rm [7299, end)
slot update_slots: id  3 | task 2011 | prompt processing progress, n_tokens = 7363, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2011 | prompt done, n_tokens = 7363, batch.n_tokens = 64
slot init_sampler: id  3 | task 2011 | init sampler, took 1.14 ms, tokens: text = 7363, total = 7363
slot update_slots: id  3 | task 2011 | created context checkpoint 6 of 8 (pos_min = 6297, pos_max = 7298, size = 23.496 MiB)
slot print_timing: id  3 | task 2011 | 
prompt eval time =     651.86 ms /   432 tokens (    1.51 ms per token,   662.72 tokens per second)
       eval time =    5438.33 ms /   214 tokens (   25.41 ms per token,    39.35 tokens per second)
      total time =    6090.20 ms /   646 tokens
slot      release: id  3 | task 2011 | stop processing: n_tokens = 7576, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2227 | processing task, is_child = 0
slot update_slots: id  3 | task 2227 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7622
slot update_slots: id  3 | task 2227 | n_tokens = 7392, memory_seq_rm [7392, end)
slot update_slots: id  3 | task 2227 | prompt processing progress, n_tokens = 7558, batch.n_tokens = 166, progress = 0.991603
slot update_slots: id  3 | task 2227 | n_tokens = 7558, memory_seq_rm [7558, end)
slot update_slots: id  3 | task 2227 | prompt processing progress, n_tokens = 7622, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2227 | prompt done, n_tokens = 7622, batch.n_tokens = 64
slot init_sampler: id  3 | task 2227 | init sampler, took 1.05 ms, tokens: text = 7622, total = 7622
slot update_slots: id  3 | task 2227 | created context checkpoint 7 of 8 (pos_min = 6666, pos_max = 7557, size = 20.917 MiB)
slot print_timing: id  3 | task 2227 | 
prompt eval time =     465.09 ms /   230 tokens (    2.02 ms per token,   494.53 tokens per second)
       eval time =    4327.96 ms /   171 tokens (   25.31 ms per token,    39.51 tokens per second)
      total time =    4793.05 ms /   401 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 2227 | stop processing: n_tokens = 7792, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2400 | processing task, is_child = 0
slot update_slots: id  3 | task 2400 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7843
slot update_slots: id  3 | task 2400 | n_tokens = 7634, memory_seq_rm [7634, end)
slot update_slots: id  3 | task 2400 | prompt processing progress, n_tokens = 7779, batch.n_tokens = 145, progress = 0.991840
slot update_slots: id  3 | task 2400 | n_tokens = 7779, memory_seq_rm [7779, end)
slot update_slots: id  3 | task 2400 | prompt processing progress, n_tokens = 7843, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2400 | prompt done, n_tokens = 7843, batch.n_tokens = 64
slot init_sampler: id  3 | task 2400 | init sampler, took 1.55 ms, tokens: text = 7843, total = 7843
slot update_slots: id  3 | task 2400 | created context checkpoint 8 of 8 (pos_min = 6900, pos_max = 7778, size = 20.612 MiB)
slot print_timing: id  3 | task 2400 | 
prompt eval time =     449.81 ms /   209 tokens (    2.15 ms per token,   464.64 tokens per second)
       eval time =    1557.29 ms /    59 tokens (   26.39 ms per token,    37.89 tokens per second)
      total time =    2007.11 ms /   268 tokens
slot      release: id  3 | task 2400 | stop processing: n_tokens = 7901, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2461 | processing task, is_child = 0
slot update_slots: id  3 | task 2461 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7945
slot update_slots: id  3 | task 2461 | n_tokens = 7864, memory_seq_rm [7864, end)
slot update_slots: id  3 | task 2461 | prompt processing progress, n_tokens = 7881, batch.n_tokens = 17, progress = 0.991945
slot update_slots: id  3 | task 2461 | n_tokens = 7881, memory_seq_rm [7881, end)
slot update_slots: id  3 | task 2461 | prompt processing progress, n_tokens = 7945, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2461 | prompt done, n_tokens = 7945, batch.n_tokens = 64
slot init_sampler: id  3 | task 2461 | init sampler, took 1.23 ms, tokens: text = 7945, total = 7945
slot update_slots: id  3 | task 2461 | erasing old context checkpoint (pos_min = 0, pos_max = 895, size = 21.011 MiB)
slot update_slots: id  3 | task 2461 | created context checkpoint 8 of 8 (pos_min = 7009, pos_max = 7880, size = 20.448 MiB)
slot print_timing: id  3 | task 2461 | 
prompt eval time =     266.50 ms /    81 tokens (    3.29 ms per token,   303.93 tokens per second)
       eval time =    2426.99 ms /    97 tokens (   25.02 ms per token,    39.97 tokens per second)
      total time =    2693.49 ms /   178 tokens
slot      release: id  3 | task 2461 | stop processing: n_tokens = 8041, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2560 | processing task, is_child = 0
slot update_slots: id  3 | task 2560 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8083
slot update_slots: id  3 | task 2560 | n_tokens = 7966, memory_seq_rm [7966, end)
slot update_slots: id  3 | task 2560 | prompt processing progress, n_tokens = 8019, batch.n_tokens = 53, progress = 0.992082
slot update_slots: id  3 | task 2560 | n_tokens = 8019, memory_seq_rm [8019, end)
slot update_slots: id  3 | task 2560 | prompt processing progress, n_tokens = 8083, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2560 | prompt done, n_tokens = 8083, batch.n_tokens = 64
slot init_sampler: id  3 | task 2560 | init sampler, took 1.20 ms, tokens: text = 8083, total = 8083
slot update_slots: id  3 | task 2560 | erasing old context checkpoint (pos_min = 408, pos_max = 1431, size = 24.012 MiB)
slot update_slots: id  3 | task 2560 | created context checkpoint 8 of 8 (pos_min = 7149, pos_max = 8018, size = 20.401 MiB)
slot print_timing: id  3 | task 2560 | 
prompt eval time =     340.35 ms /   117 tokens (    2.91 ms per token,   343.76 tokens per second)
       eval time =    1719.14 ms /    68 tokens (   25.28 ms per token,    39.55 tokens per second)
      total time =    2059.49 ms /   185 tokens
slot      release: id  3 | task 2560 | stop processing: n_tokens = 8150, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2630 | processing task, is_child = 0
slot update_slots: id  3 | task 2630 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8190
slot update_slots: id  3 | task 2630 | n_tokens = 8100, memory_seq_rm [8100, end)
slot update_slots: id  3 | task 2630 | prompt processing progress, n_tokens = 8126, batch.n_tokens = 26, progress = 0.992186
slot update_slots: id  3 | task 2630 | n_tokens = 8126, memory_seq_rm [8126, end)
slot update_slots: id  3 | task 2630 | prompt processing progress, n_tokens = 8190, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2630 | prompt done, n_tokens = 8190, batch.n_tokens = 64
slot init_sampler: id  3 | task 2630 | init sampler, took 1.29 ms, tokens: text = 8190, total = 8190
slot update_slots: id  3 | task 2630 | erasing old context checkpoint (pos_min = 1380, pos_max = 2085, size = 16.555 MiB)
slot update_slots: id  3 | task 2630 | created context checkpoint 8 of 8 (pos_min = 7258, pos_max = 8125, size = 20.354 MiB)
slot print_timing: id  3 | task 2630 | 
prompt eval time =     302.11 ms /    90 tokens (    3.36 ms per token,   297.91 tokens per second)
       eval time =    2920.71 ms /   115 tokens (   25.40 ms per token,    39.37 tokens per second)
      total time =    3222.81 ms /   205 tokens
slot      release: id  3 | task 2630 | stop processing: n_tokens = 8304, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2747 | processing task, is_child = 0
slot update_slots: id  3 | task 2747 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8346
slot update_slots: id  3 | task 2747 | n_tokens = 8204, memory_seq_rm [8204, end)
slot update_slots: id  3 | task 2747 | prompt processing progress, n_tokens = 8282, batch.n_tokens = 78, progress = 0.992332
slot update_slots: id  3 | task 2747 | n_tokens = 8282, memory_seq_rm [8282, end)
slot update_slots: id  3 | task 2747 | prompt processing progress, n_tokens = 8346, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2747 | prompt done, n_tokens = 8346, batch.n_tokens = 64
slot init_sampler: id  3 | task 2747 | init sampler, took 1.16 ms, tokens: text = 8346, total = 8346
slot update_slots: id  3 | task 2747 | erasing old context checkpoint (pos_min = 3005, pos_max = 4028, size = 24.012 MiB)
slot update_slots: id  3 | task 2747 | created context checkpoint 8 of 8 (pos_min = 7392, pos_max = 8281, size = 20.870 MiB)
slot print_timing: id  3 | task 2747 | 
prompt eval time =     402.45 ms /   142 tokens (    2.83 ms per token,   352.84 tokens per second)
       eval time =    9719.76 ms /   380 tokens (   25.58 ms per token,    39.10 tokens per second)
      total time =   10122.21 ms /   522 tokens
slot      release: id  3 | task 2747 | stop processing: n_tokens = 8725, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3129 | processing task, is_child = 0
slot update_slots: id  3 | task 3129 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8771
slot update_slots: id  3 | task 3129 | n_tokens = 8421, memory_seq_rm [8421, end)
slot update_slots: id  3 | task 3129 | prompt processing progress, n_tokens = 8707, batch.n_tokens = 286, progress = 0.992703
slot update_slots: id  3 | task 3129 | n_tokens = 8707, memory_seq_rm [8707, end)
slot update_slots: id  3 | task 3129 | prompt processing progress, n_tokens = 8771, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3129 | prompt done, n_tokens = 8771, batch.n_tokens = 64
slot init_sampler: id  3 | task 3129 | init sampler, took 1.31 ms, tokens: text = 8771, total = 8771
slot update_slots: id  3 | task 3129 | erasing old context checkpoint (pos_min = 4991, pos_max = 6014, size = 24.012 MiB)
slot update_slots: id  3 | task 3129 | created context checkpoint 8 of 8 (pos_min = 7701, pos_max = 8706, size = 23.590 MiB)
slot print_timing: id  3 | task 3129 | 
prompt eval time =     569.98 ms /   350 tokens (    1.63 ms per token,   614.05 tokens per second)
       eval time =    1777.94 ms /    69 tokens (   25.77 ms per token,    38.81 tokens per second)
      total time =    2347.93 ms /   419 tokens
slot      release: id  3 | task 3129 | stop processing: n_tokens = 8839, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3200 | processing task, is_child = 0
slot update_slots: id  3 | task 3200 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8884
slot update_slots: id  3 | task 3200 | n_tokens = 8784, memory_seq_rm [8784, end)
slot update_slots: id  3 | task 3200 | prompt processing progress, n_tokens = 8820, batch.n_tokens = 36, progress = 0.992796
slot update_slots: id  3 | task 3200 | n_tokens = 8820, memory_seq_rm [8820, end)
slot update_slots: id  3 | task 3200 | prompt processing progress, n_tokens = 8884, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3200 | prompt done, n_tokens = 8884, batch.n_tokens = 64
slot init_sampler: id  3 | task 3200 | init sampler, took 1.44 ms, tokens: text = 8884, total = 8884
slot update_slots: id  3 | task 3200 | erasing old context checkpoint (pos_min = 6297, pos_max = 7298, size = 23.496 MiB)
slot update_slots: id  3 | task 3200 | created context checkpoint 8 of 8 (pos_min = 7815, pos_max = 8819, size = 23.566 MiB)
slot print_timing: id  3 | task 3200 | 
prompt eval time =     325.65 ms /   100 tokens (    3.26 ms per token,   307.07 tokens per second)
       eval time =    4026.61 ms /   157 tokens (   25.65 ms per token,    38.99 tokens per second)
      total time =    4352.26 ms /   257 tokens
slot      release: id  3 | task 3200 | stop processing: n_tokens = 9040, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3359 | processing task, is_child = 0
slot update_slots: id  3 | task 3359 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9082
slot update_slots: id  3 | task 3359 | n_tokens = 8896, memory_seq_rm [8896, end)
slot update_slots: id  3 | task 3359 | prompt processing progress, n_tokens = 9018, batch.n_tokens = 122, progress = 0.992953
slot update_slots: id  3 | task 3359 | n_tokens = 9018, memory_seq_rm [9018, end)
slot update_slots: id  3 | task 3359 | prompt processing progress, n_tokens = 9082, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3359 | prompt done, n_tokens = 9082, batch.n_tokens = 64
slot init_sampler: id  3 | task 3359 | init sampler, took 1.25 ms, tokens: text = 9082, total = 9082
slot update_slots: id  3 | task 3359 | erasing old context checkpoint (pos_min = 6666, pos_max = 7557, size = 20.917 MiB)
slot update_slots: id  3 | task 3359 | created context checkpoint 8 of 8 (pos_min = 8016, pos_max = 9017, size = 23.496 MiB)
slot print_timing: id  3 | task 3359 | 
prompt eval time =     527.07 ms /   186 tokens (    2.83 ms per token,   352.89 tokens per second)
       eval time =    7165.34 ms /   274 tokens (   26.15 ms per token,    38.24 tokens per second)
      total time =    7692.41 ms /   460 tokens
slot      release: id  3 | task 3359 | stop processing: n_tokens = 9355, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3635 | processing task, is_child = 0
slot update_slots: id  3 | task 3635 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9403
slot update_slots: id  3 | task 3635 | n_tokens = 9343, memory_seq_rm [9343, end)
slot update_slots: id  3 | task 3635 | prompt processing progress, n_tokens = 9403, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  3 | task 3635 | prompt done, n_tokens = 9403, batch.n_tokens = 60
slot init_sampler: id  3 | task 3635 | init sampler, took 1.30 ms, tokens: text = 9403, total = 9403
slot update_slots: id  3 | task 3635 | erasing old context checkpoint (pos_min = 6900, pos_max = 7778, size = 20.612 MiB)
slot update_slots: id  3 | task 3635 | created context checkpoint 8 of 8 (pos_min = 8331, pos_max = 9342, size = 23.731 MiB)
slot print_timing: id  3 | task 3635 | 
prompt eval time =     198.83 ms /    60 tokens (    3.31 ms per token,   301.77 tokens per second)
       eval time =    9452.27 ms /   358 tokens (   26.40 ms per token,    37.87 tokens per second)
      total time =    9651.10 ms /   418 tokens
slot      release: id  3 | task 3635 | stop processing: n_tokens = 9760, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.092
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9760, total state size = 252.874 MiB
srv          load:  - looking for better prompt, base f_keep = 0.092, sim = 0.991
srv        update:  - cache state: 1 prompts, 429.330 MiB (limits: 8192.000 MiB, 56064 tokens, 186229 est)
srv        update:    - prompt 0x59014c2315e0:    9760 tokens, checkpoints:  8,   429.330 MiB
srv  get_availabl: prompt cache update took 322.43 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3994 | processing task, is_child = 0
slot update_slots: id  3 | task 3994 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 902
slot update_slots: id  3 | task 3994 | n_past = 894, slot.prompt.tokens.size() = 9760, seq_id = 3, pos_min = 8736, n_swa = 128
slot update_slots: id  3 | task 3994 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 3994 | erased invalidated context checkpoint (pos_min = 7009, pos_max = 7880, n_swa = 128, size = 20.448 MiB)
slot update_slots: id  3 | task 3994 | erased invalidated context checkpoint (pos_min = 7149, pos_max = 8018, n_swa = 128, size = 20.401 MiB)
slot update_slots: id  3 | task 3994 | erased invalidated context checkpoint (pos_min = 7258, pos_max = 8125, n_swa = 128, size = 20.354 MiB)
slot update_slots: id  3 | task 3994 | erased invalidated context checkpoint (pos_min = 7392, pos_max = 8281, n_swa = 128, size = 20.870 MiB)
slot update_slots: id  3 | task 3994 | erased invalidated context checkpoint (pos_min = 7701, pos_max = 8706, n_swa = 128, size = 23.590 MiB)
slot update_slots: id  3 | task 3994 | erased invalidated context checkpoint (pos_min = 7815, pos_max = 8819, n_swa = 128, size = 23.566 MiB)
slot update_slots: id  3 | task 3994 | erased invalidated context checkpoint (pos_min = 8016, pos_max = 9017, n_swa = 128, size = 23.496 MiB)
slot update_slots: id  3 | task 3994 | erased invalidated context checkpoint (pos_min = 8331, pos_max = 9342, n_swa = 128, size = 23.731 MiB)
slot update_slots: id  3 | task 3994 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 3994 | prompt processing progress, n_tokens = 838, batch.n_tokens = 838, progress = 0.929047
slot update_slots: id  3 | task 3994 | n_tokens = 838, memory_seq_rm [838, end)
slot update_slots: id  3 | task 3994 | prompt processing progress, n_tokens = 902, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3994 | prompt done, n_tokens = 902, batch.n_tokens = 64
slot init_sampler: id  3 | task 3994 | init sampler, took 0.13 ms, tokens: text = 902, total = 902
slot update_slots: id  3 | task 3994 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 837, size = 19.651 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4539 | processing task, is_child = 0
slot update_slots: id  2 | task 4539 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 908
slot update_slots: id  2 | task 4539 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 4539 | prompt processing progress, n_tokens = 844, batch.n_tokens = 845, progress = 0.929515
slot update_slots: id  2 | task 4539 | n_tokens = 844, memory_seq_rm [844, end)
slot update_slots: id  2 | task 4539 | prompt processing progress, n_tokens = 908, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  2 | task 4539 | prompt done, n_tokens = 908, batch.n_tokens = 65
slot init_sampler: id  2 | task 4539 | init sampler, took 0.14 ms, tokens: text = 908, total = 908
slot update_slots: id  2 | task 4539 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 843, size = 19.791 MiB)
slot print_timing: id  3 | task 3994 | 
prompt eval time =    1083.60 ms /   902 tokens (    1.20 ms per token,   832.41 tokens per second)
       eval time =   19799.26 ms /   648 tokens (   30.55 ms per token,    32.73 tokens per second)
      total time =   20882.86 ms /  1550 tokens
slot      release: id  3 | task 3994 | stop processing: n_tokens = 1549, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  2 | task 4539 | 
prompt eval time =    1150.61 ms /   908 tokens (    1.27 ms per token,   789.15 tokens per second)
       eval time =   11105.66 ms /   337 tokens (   32.95 ms per token,    30.34 tokens per second)
      total time =   12256.26 ms /  1245 tokens
slot      release: id  2 | task 4539 | stop processing: n_tokens = 1244, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.729
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4878 | processing task, is_child = 0
slot update_slots: id  2 | task 4878 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 920
slot update_slots: id  2 | task 4878 | n_tokens = 907, memory_seq_rm [907, end)
slot update_slots: id  2 | task 4878 | prompt processing progress, n_tokens = 920, batch.n_tokens = 13, progress = 1.000000
slot update_slots: id  2 | task 4878 | prompt done, n_tokens = 920, batch.n_tokens = 13
slot init_sampler: id  2 | task 4878 | init sampler, took 0.14 ms, tokens: text = 920, total = 920
slot print_timing: id  2 | task 4878 | 
prompt eval time =     149.07 ms /    13 tokens (   11.47 ms per token,    87.20 tokens per second)
       eval time =   18187.58 ms /   731 tokens (   24.88 ms per token,    40.19 tokens per second)
      total time =   18336.65 ms /   744 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 4878 | stop processing: n_tokens = 1650, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.821 (> 0.100 thold), f_keep = 0.558
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 5610 | processing task, is_child = 0
slot update_slots: id  2 | task 5610 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1122
slot update_slots: id  2 | task 5610 | n_past = 921, slot.prompt.tokens.size() = 1650, seq_id = 2, pos_min = 857, n_swa = 128
slot update_slots: id  2 | task 5610 | restored context checkpoint (pos_min = 0, pos_max = 843, size = 19.791 MiB)
slot update_slots: id  2 | task 5610 | n_tokens = 843, memory_seq_rm [843, end)
slot update_slots: id  2 | task 5610 | prompt processing progress, n_tokens = 1058, batch.n_tokens = 215, progress = 0.942959
slot update_slots: id  2 | task 5610 | n_tokens = 1058, memory_seq_rm [1058, end)
slot update_slots: id  2 | task 5610 | prompt processing progress, n_tokens = 1122, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 5610 | prompt done, n_tokens = 1122, batch.n_tokens = 64
slot init_sampler: id  2 | task 5610 | init sampler, took 0.17 ms, tokens: text = 1122, total = 1122
slot update_slots: id  2 | task 5610 | created context checkpoint 2 of 8 (pos_min = 214, pos_max = 1057, size = 19.791 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 5610
slot      release: id  2 | task 5610 | stop processing: n_tokens = 1858, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.793 (> 0.100 thold), f_keep = 0.604
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 6350 | processing task, is_child = 0
slot update_slots: id  2 | task 6350 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1417
slot update_slots: id  2 | task 6350 | n_tokens = 1123, memory_seq_rm [1123, end)
slot update_slots: id  2 | task 6350 | prompt processing progress, n_tokens = 1353, batch.n_tokens = 230, progress = 0.954834
slot update_slots: id  2 | task 6350 | n_tokens = 1353, memory_seq_rm [1353, end)
slot update_slots: id  2 | task 6350 | prompt processing progress, n_tokens = 1417, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 6350 | prompt done, n_tokens = 1417, batch.n_tokens = 64
slot init_sampler: id  2 | task 6350 | init sampler, took 0.27 ms, tokens: text = 1417, total = 1417
slot update_slots: id  2 | task 6350 | created context checkpoint 3 of 8 (pos_min = 961, pos_max = 1352, size = 9.192 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 6350
slot      release: id  2 | task 6350 | stop processing: n_tokens = 1631, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.548
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 6568 | processing task, is_child = 0
slot update_slots: id  2 | task 6568 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 911
slot update_slots: id  2 | task 6568 | n_past = 894, slot.prompt.tokens.size() = 1631, seq_id = 2, pos_min = 1111, n_swa = 128
slot update_slots: id  2 | task 6568 | restored context checkpoint (pos_min = 214, pos_max = 1057, size = 19.791 MiB)
slot update_slots: id  2 | task 6568 | erased invalidated context checkpoint (pos_min = 961, pos_max = 1352, n_swa = 128, size = 9.192 MiB)
slot update_slots: id  2 | task 6568 | n_tokens = 894, memory_seq_rm [894, end)
slot update_slots: id  2 | task 6568 | prompt processing progress, n_tokens = 911, batch.n_tokens = 17, progress = 1.000000
slot update_slots: id  2 | task 6568 | prompt done, n_tokens = 911, batch.n_tokens = 17
slot init_sampler: id  2 | task 6568 | init sampler, took 0.13 ms, tokens: text = 911, total = 911
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 6568
slot      release: id  2 | task 6568 | stop processing: n_tokens = 1114, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.937 (> 0.100 thold), f_keep = 0.803
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 6774 | processing task, is_child = 0
slot update_slots: id  2 | task 6774 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 954
slot update_slots: id  2 | task 6774 | n_tokens = 894, memory_seq_rm [894, end)
slot update_slots: id  2 | task 6774 | prompt processing progress, n_tokens = 954, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  2 | task 6774 | prompt done, n_tokens = 954, batch.n_tokens = 60
slot init_sampler: id  2 | task 6774 | init sampler, took 0.14 ms, tokens: text = 954, total = 954
slot print_timing: id  2 | task 6774 | 
prompt eval time =     219.44 ms /    60 tokens (    3.66 ms per token,   273.42 tokens per second)
       eval time =    1023.63 ms /    41 tokens (   24.97 ms per token,    40.05 tokens per second)
      total time =    1243.08 ms /   101 tokens
slot      release: id  2 | task 6774 | stop processing: n_tokens = 994, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.655 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 6816 | processing task, is_child = 0
slot update_slots: id  2 | task 6816 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1489
slot update_slots: id  2 | task 6816 | n_tokens = 976, memory_seq_rm [976, end)
slot update_slots: id  2 | task 6816 | prompt processing progress, n_tokens = 1425, batch.n_tokens = 449, progress = 0.957018
slot update_slots: id  2 | task 6816 | n_tokens = 1425, memory_seq_rm [1425, end)
slot update_slots: id  2 | task 6816 | prompt processing progress, n_tokens = 1489, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 6816 | prompt done, n_tokens = 1489, batch.n_tokens = 64
slot init_sampler: id  2 | task 6816 | init sampler, took 0.22 ms, tokens: text = 1489, total = 1489
slot update_slots: id  2 | task 6816 | created context checkpoint 3 of 8 (pos_min = 581, pos_max = 1424, size = 19.791 MiB)
slot print_timing: id  2 | task 6816 | 
prompt eval time =     652.87 ms /   513 tokens (    1.27 ms per token,   785.76 tokens per second)
       eval time =    1103.95 ms /    43 tokens (   25.67 ms per token,    38.95 tokens per second)
      total time =    1756.82 ms /   556 tokens
slot      release: id  2 | task 6816 | stop processing: n_tokens = 1531, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.511 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 6861 | processing task, is_child = 0
slot update_slots: id  2 | task 6861 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2937
slot update_slots: id  2 | task 6861 | n_tokens = 1500, memory_seq_rm [1500, end)
slot update_slots: id  2 | task 6861 | prompt processing progress, n_tokens = 2873, batch.n_tokens = 1373, progress = 0.978209
slot update_slots: id  2 | task 6861 | n_tokens = 2873, memory_seq_rm [2873, end)
slot update_slots: id  2 | task 6861 | prompt processing progress, n_tokens = 2937, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 6861 | prompt done, n_tokens = 2937, batch.n_tokens = 64
slot init_sampler: id  2 | task 6861 | init sampler, took 0.41 ms, tokens: text = 2937, total = 2937
slot update_slots: id  2 | task 6861 | created context checkpoint 4 of 8 (pos_min = 1976, pos_max = 2872, size = 21.034 MiB)
slot print_timing: id  2 | task 6861 | 
prompt eval time =    1697.27 ms /  1437 tokens (    1.18 ms per token,   846.65 tokens per second)
       eval time =    1537.69 ms /    59 tokens (   26.06 ms per token,    38.37 tokens per second)
      total time =    3234.95 ms /  1496 tokens
slot      release: id  2 | task 6861 | stop processing: n_tokens = 2995, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.599 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 6922 | processing task, is_child = 0
slot update_slots: id  2 | task 6922 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4955
slot update_slots: id  2 | task 6922 | n_tokens = 2969, memory_seq_rm [2969, end)
slot update_slots: id  2 | task 6922 | prompt processing progress, n_tokens = 4891, batch.n_tokens = 1922, progress = 0.987084
slot update_slots: id  2 | task 6922 | n_tokens = 4891, memory_seq_rm [4891, end)
slot update_slots: id  2 | task 6922 | prompt processing progress, n_tokens = 4955, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 6922 | prompt done, n_tokens = 4955, batch.n_tokens = 64
slot init_sampler: id  2 | task 6922 | init sampler, took 0.93 ms, tokens: text = 4955, total = 4955
slot update_slots: id  2 | task 6922 | created context checkpoint 5 of 8 (pos_min = 3994, pos_max = 4890, size = 21.034 MiB)
slot print_timing: id  2 | task 6922 | 
prompt eval time =    2394.20 ms /  1986 tokens (    1.21 ms per token,   829.50 tokens per second)
       eval time =     971.24 ms /    36 tokens (   26.98 ms per token,    37.07 tokens per second)
      total time =    3365.44 ms /  2022 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 6922 | stop processing: n_tokens = 4990, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.700 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 6960 | processing task, is_child = 0
slot update_slots: id  2 | task 6960 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7092
slot update_slots: id  2 | task 6960 | n_tokens = 4964, memory_seq_rm [4964, end)
slot update_slots: id  2 | task 6960 | prompt processing progress, n_tokens = 7012, batch.n_tokens = 2048, progress = 0.988720
slot update_slots: id  2 | task 6960 | n_tokens = 7012, memory_seq_rm [7012, end)
slot update_slots: id  2 | task 6960 | prompt processing progress, n_tokens = 7028, batch.n_tokens = 16, progress = 0.990976
slot update_slots: id  2 | task 6960 | n_tokens = 7028, memory_seq_rm [7028, end)
slot update_slots: id  2 | task 6960 | prompt processing progress, n_tokens = 7092, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 6960 | prompt done, n_tokens = 7092, batch.n_tokens = 64
slot init_sampler: id  2 | task 6960 | init sampler, took 1.34 ms, tokens: text = 7092, total = 7092
slot update_slots: id  2 | task 6960 | created context checkpoint 6 of 8 (pos_min = 6131, pos_max = 7027, size = 21.034 MiB)
slot print_timing: id  2 | task 6960 | 
prompt eval time =    2741.34 ms /  2128 tokens (    1.29 ms per token,   776.26 tokens per second)
       eval time =    4967.92 ms /   178 tokens (   27.91 ms per token,    35.83 tokens per second)
      total time =    7709.26 ms /  2306 tokens
slot      release: id  2 | task 6960 | stop processing: n_tokens = 7269, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 7141 | processing task, is_child = 0
slot update_slots: id  2 | task 7141 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7298
slot update_slots: id  2 | task 7141 | n_tokens = 7242, memory_seq_rm [7242, end)
slot update_slots: id  2 | task 7141 | prompt processing progress, n_tokens = 7298, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  2 | task 7141 | prompt done, n_tokens = 7298, batch.n_tokens = 56
slot init_sampler: id  2 | task 7141 | init sampler, took 1.01 ms, tokens: text = 7298, total = 7298
slot update_slots: id  2 | task 7141 | created context checkpoint 7 of 8 (pos_min = 6372, pos_max = 7241, size = 20.401 MiB)
slot print_timing: id  2 | task 7141 | 
prompt eval time =     271.93 ms /    56 tokens (    4.86 ms per token,   205.93 tokens per second)
       eval time =    1034.58 ms /    41 tokens (   25.23 ms per token,    39.63 tokens per second)
      total time =    1306.51 ms /    97 tokens
slot      release: id  2 | task 7141 | stop processing: n_tokens = 7338, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 7183 | processing task, is_child = 0
slot update_slots: id  2 | task 7183 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7459
slot update_slots: id  2 | task 7183 | n_tokens = 7311, memory_seq_rm [7311, end)
slot update_slots: id  2 | task 7183 | prompt processing progress, n_tokens = 7395, batch.n_tokens = 84, progress = 0.991420
slot update_slots: id  2 | task 7183 | n_tokens = 7395, memory_seq_rm [7395, end)
slot update_slots: id  2 | task 7183 | prompt processing progress, n_tokens = 7459, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 7183 | prompt done, n_tokens = 7459, batch.n_tokens = 64
slot init_sampler: id  2 | task 7183 | init sampler, took 1.05 ms, tokens: text = 7459, total = 7459
slot update_slots: id  2 | task 7183 | created context checkpoint 8 of 8 (pos_min = 6498, pos_max = 7394, size = 21.034 MiB)
slot print_timing: id  2 | task 7183 | 
prompt eval time =     424.06 ms /   148 tokens (    2.87 ms per token,   349.01 tokens per second)
       eval time =    1050.11 ms /    42 tokens (   25.00 ms per token,    40.00 tokens per second)
      total time =    1474.16 ms /   190 tokens
slot      release: id  2 | task 7183 | stop processing: n_tokens = 7500, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.917 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 7227 | processing task, is_child = 0
slot update_slots: id  2 | task 7227 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8149
slot update_slots: id  2 | task 7227 | n_tokens = 7473, memory_seq_rm [7473, end)
slot update_slots: id  2 | task 7227 | prompt processing progress, n_tokens = 8085, batch.n_tokens = 612, progress = 0.992146
slot update_slots: id  2 | task 7227 | n_tokens = 8085, memory_seq_rm [8085, end)
slot update_slots: id  2 | task 7227 | prompt processing progress, n_tokens = 8149, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 7227 | prompt done, n_tokens = 8149, batch.n_tokens = 64
slot init_sampler: id  2 | task 7227 | init sampler, took 1.14 ms, tokens: text = 8149, total = 8149
slot update_slots: id  2 | task 7227 | erasing old context checkpoint (pos_min = 0, pos_max = 843, size = 19.791 MiB)
slot update_slots: id  2 | task 7227 | created context checkpoint 8 of 8 (pos_min = 7188, pos_max = 8084, size = 21.034 MiB)
slot print_timing: id  2 | task 7227 | 
prompt eval time =    1014.03 ms /   676 tokens (    1.50 ms per token,   666.64 tokens per second)
       eval time =   12509.74 ms /   474 tokens (   26.39 ms per token,    37.89 tokens per second)
      total time =   13523.77 ms /  1150 tokens
slot      release: id  2 | task 7227 | stop processing: n_tokens = 8622, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 7703 | processing task, is_child = 0
slot update_slots: id  2 | task 7703 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8757
slot update_slots: id  2 | task 7703 | n_tokens = 8597, memory_seq_rm [8597, end)
slot update_slots: id  2 | task 7703 | prompt processing progress, n_tokens = 8693, batch.n_tokens = 96, progress = 0.992692
slot update_slots: id  2 | task 7703 | n_tokens = 8693, memory_seq_rm [8693, end)
slot update_slots: id  2 | task 7703 | prompt processing progress, n_tokens = 8757, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 7703 | prompt done, n_tokens = 8757, batch.n_tokens = 64
slot init_sampler: id  2 | task 7703 | init sampler, took 1.36 ms, tokens: text = 8757, total = 8757
slot update_slots: id  2 | task 7703 | erasing old context checkpoint (pos_min = 214, pos_max = 1057, size = 19.791 MiB)
slot update_slots: id  2 | task 7703 | created context checkpoint 8 of 8 (pos_min = 7796, pos_max = 8692, size = 21.034 MiB)
slot print_timing: id  2 | task 7703 | 
prompt eval time =     453.92 ms /   160 tokens (    2.84 ms per token,   352.49 tokens per second)
       eval time =    3514.63 ms /   129 tokens (   27.25 ms per token,    36.70 tokens per second)
      total time =    3968.55 ms /   289 tokens
slot      release: id  2 | task 7703 | stop processing: n_tokens = 8885, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 7834 | processing task, is_child = 0
slot update_slots: id  2 | task 7834 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8917
slot update_slots: id  2 | task 7834 | n_tokens = 8860, memory_seq_rm [8860, end)
slot update_slots: id  2 | task 7834 | prompt processing progress, n_tokens = 8917, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  2 | task 7834 | prompt done, n_tokens = 8917, batch.n_tokens = 57
slot init_sampler: id  2 | task 7834 | init sampler, took 1.24 ms, tokens: text = 8917, total = 8917
slot update_slots: id  2 | task 7834 | erasing old context checkpoint (pos_min = 581, pos_max = 1424, size = 19.791 MiB)
slot update_slots: id  2 | task 7834 | created context checkpoint 8 of 8 (pos_min = 7988, pos_max = 8859, size = 20.448 MiB)
slot print_timing: id  2 | task 7834 | 
prompt eval time =     279.03 ms /    57 tokens (    4.90 ms per token,   204.28 tokens per second)
       eval time =     838.91 ms /    33 tokens (   25.42 ms per token,    39.34 tokens per second)
      total time =    1117.95 ms /    90 tokens
slot      release: id  2 | task 7834 | stop processing: n_tokens = 8949, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 7868 | processing task, is_child = 0
slot update_slots: id  2 | task 7868 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9025
slot update_slots: id  2 | task 7868 | n_tokens = 8926, memory_seq_rm [8926, end)
slot update_slots: id  2 | task 7868 | prompt processing progress, n_tokens = 8961, batch.n_tokens = 35, progress = 0.992909
slot update_slots: id  2 | task 7868 | n_tokens = 8961, memory_seq_rm [8961, end)
slot update_slots: id  2 | task 7868 | prompt processing progress, n_tokens = 9025, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 7868 | prompt done, n_tokens = 9025, batch.n_tokens = 64
slot init_sampler: id  2 | task 7868 | init sampler, took 1.27 ms, tokens: text = 9025, total = 9025
slot update_slots: id  2 | task 7868 | erasing old context checkpoint (pos_min = 1976, pos_max = 2872, size = 21.034 MiB)
slot update_slots: id  2 | task 7868 | created context checkpoint 8 of 8 (pos_min = 8064, pos_max = 8960, size = 21.034 MiB)
slot print_timing: id  2 | task 7868 | 
prompt eval time =     344.59 ms /    99 tokens (    3.48 ms per token,   287.30 tokens per second)
       eval time =   22251.16 ms /   817 tokens (   27.24 ms per token,    36.72 tokens per second)
      total time =   22595.75 ms /   916 tokens
slot      release: id  2 | task 7868 | stop processing: n_tokens = 9841, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.913 (> 0.100 thold), f_keep = 0.933
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 8687 | processing task, is_child = 0
slot update_slots: id  2 | task 8687 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10059
slot update_slots: id  2 | task 8687 | n_tokens = 9179, memory_seq_rm [9179, end)
slot update_slots: id  2 | task 8687 | prompt processing progress, n_tokens = 9995, batch.n_tokens = 816, progress = 0.993638
slot update_slots: id  2 | task 8687 | n_tokens = 9995, memory_seq_rm [9995, end)
slot update_slots: id  2 | task 8687 | prompt processing progress, n_tokens = 10059, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 8687 | prompt done, n_tokens = 10059, batch.n_tokens = 64
slot init_sampler: id  2 | task 8687 | init sampler, took 1.40 ms, tokens: text = 10059, total = 10059
slot update_slots: id  2 | task 8687 | erasing old context checkpoint (pos_min = 3994, pos_max = 4890, size = 21.034 MiB)
slot update_slots: id  2 | task 8687 | created context checkpoint 8 of 8 (pos_min = 9225, pos_max = 9994, size = 18.056 MiB)
slot print_timing: id  2 | task 8687 | 
prompt eval time =    1322.84 ms /   880 tokens (    1.50 ms per token,   665.23 tokens per second)
       eval time =    1397.86 ms /    49 tokens (   28.53 ms per token,    35.05 tokens per second)
      total time =    2720.70 ms /   929 tokens
slot      release: id  2 | task 8687 | stop processing: n_tokens = 10107, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 8738 | processing task, is_child = 0
slot update_slots: id  2 | task 8738 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10619
slot update_slots: id  2 | task 8738 | n_tokens = 10081, memory_seq_rm [10081, end)
slot update_slots: id  2 | task 8738 | prompt processing progress, n_tokens = 10555, batch.n_tokens = 474, progress = 0.993973
slot update_slots: id  2 | task 8738 | n_tokens = 10555, memory_seq_rm [10555, end)
slot update_slots: id  2 | task 8738 | prompt processing progress, n_tokens = 10619, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 8738 | prompt done, n_tokens = 10619, batch.n_tokens = 64
slot init_sampler: id  2 | task 8738 | init sampler, took 1.50 ms, tokens: text = 10619, total = 10619
slot update_slots: id  2 | task 8738 | erasing old context checkpoint (pos_min = 6131, pos_max = 7027, size = 21.034 MiB)
slot update_slots: id  2 | task 8738 | created context checkpoint 8 of 8 (pos_min = 9658, pos_max = 10554, size = 21.034 MiB)
slot print_timing: id  2 | task 8738 | 
prompt eval time =     842.35 ms /   538 tokens (    1.57 ms per token,   638.69 tokens per second)
       eval time =   18272.13 ms /   669 tokens (   27.31 ms per token,    36.61 tokens per second)
      total time =   19114.48 ms /  1207 tokens
slot      release: id  2 | task 8738 | stop processing: n_tokens = 11287, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.942
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9409 | processing task, is_child = 0
slot update_slots: id  2 | task 9409 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11505
slot update_slots: id  2 | task 9409 | n_tokens = 10636, memory_seq_rm [10636, end)
slot update_slots: id  2 | task 9409 | prompt processing progress, n_tokens = 11441, batch.n_tokens = 805, progress = 0.994437
slot update_slots: id  2 | task 9409 | n_tokens = 11441, memory_seq_rm [11441, end)
slot update_slots: id  2 | task 9409 | prompt processing progress, n_tokens = 11505, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9409 | prompt done, n_tokens = 11505, batch.n_tokens = 64
slot init_sampler: id  2 | task 9409 | init sampler, took 2.16 ms, tokens: text = 11505, total = 11505
slot update_slots: id  2 | task 9409 | erasing old context checkpoint (pos_min = 6372, pos_max = 7241, size = 20.401 MiB)
slot update_slots: id  2 | task 9409 | created context checkpoint 8 of 8 (pos_min = 10636, pos_max = 11440, size = 18.877 MiB)
slot print_timing: id  2 | task 9409 | 
prompt eval time =    1226.29 ms /   869 tokens (    1.41 ms per token,   708.64 tokens per second)
       eval time =    1216.88 ms /    46 tokens (   26.45 ms per token,    37.80 tokens per second)
      total time =    2443.18 ms /   915 tokens
slot      release: id  2 | task 9409 | stop processing: n_tokens = 11550, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9457 | processing task, is_child = 0
slot update_slots: id  2 | task 9457 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11754
slot update_slots: id  2 | task 9457 | n_tokens = 11524, memory_seq_rm [11524, end)
slot update_slots: id  2 | task 9457 | prompt processing progress, n_tokens = 11690, batch.n_tokens = 166, progress = 0.994555
slot update_slots: id  2 | task 9457 | n_tokens = 11690, memory_seq_rm [11690, end)
slot update_slots: id  2 | task 9457 | prompt processing progress, n_tokens = 11754, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9457 | prompt done, n_tokens = 11754, batch.n_tokens = 64
slot init_sampler: id  2 | task 9457 | init sampler, took 1.63 ms, tokens: text = 11754, total = 11754
slot update_slots: id  2 | task 9457 | erasing old context checkpoint (pos_min = 6498, pos_max = 7394, size = 21.034 MiB)
slot update_slots: id  2 | task 9457 | created context checkpoint 8 of 8 (pos_min = 10802, pos_max = 11689, size = 20.823 MiB)
slot print_timing: id  2 | task 9457 | 
prompt eval time =     510.03 ms /   230 tokens (    2.22 ms per token,   450.95 tokens per second)
       eval time =    1036.05 ms /    40 tokens (   25.90 ms per token,    38.61 tokens per second)
      total time =    1546.08 ms /   270 tokens
slot      release: id  2 | task 9457 | stop processing: n_tokens = 11793, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9499 | processing task, is_child = 0
slot update_slots: id  2 | task 9499 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11869
slot update_slots: id  2 | task 9499 | n_tokens = 11763, memory_seq_rm [11763, end)
slot update_slots: id  2 | task 9499 | prompt processing progress, n_tokens = 11805, batch.n_tokens = 42, progress = 0.994608
slot update_slots: id  2 | task 9499 | n_tokens = 11805, memory_seq_rm [11805, end)
slot update_slots: id  2 | task 9499 | prompt processing progress, n_tokens = 11869, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9499 | prompt done, n_tokens = 11869, batch.n_tokens = 64
slot init_sampler: id  2 | task 9499 | init sampler, took 1.85 ms, tokens: text = 11869, total = 11869
slot update_slots: id  2 | task 9499 | erasing old context checkpoint (pos_min = 7188, pos_max = 8084, size = 21.034 MiB)
slot update_slots: id  2 | task 9499 | created context checkpoint 8 of 8 (pos_min = 10917, pos_max = 11804, size = 20.823 MiB)
slot print_timing: id  2 | task 9499 | 
prompt eval time =     359.94 ms /   106 tokens (    3.40 ms per token,   294.50 tokens per second)
       eval time =    1148.56 ms /    44 tokens (   26.10 ms per token,    38.31 tokens per second)
      total time =    1508.49 ms /   150 tokens
slot      release: id  2 | task 9499 | stop processing: n_tokens = 11912, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9545 | processing task, is_child = 0
slot update_slots: id  2 | task 9545 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12254
slot update_slots: id  2 | task 9545 | n_tokens = 11886, memory_seq_rm [11886, end)
slot update_slots: id  2 | task 9545 | prompt processing progress, n_tokens = 12190, batch.n_tokens = 304, progress = 0.994777
slot update_slots: id  2 | task 9545 | n_tokens = 12190, memory_seq_rm [12190, end)
slot update_slots: id  2 | task 9545 | prompt processing progress, n_tokens = 12254, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9545 | prompt done, n_tokens = 12254, batch.n_tokens = 64
slot init_sampler: id  2 | task 9545 | init sampler, took 1.73 ms, tokens: text = 12254, total = 12254
slot update_slots: id  2 | task 9545 | erasing old context checkpoint (pos_min = 7796, pos_max = 8692, size = 21.034 MiB)
slot update_slots: id  2 | task 9545 | created context checkpoint 8 of 8 (pos_min = 11302, pos_max = 12189, size = 20.823 MiB)
slot print_timing: id  2 | task 9545 | 
prompt eval time =     623.99 ms /   368 tokens (    1.70 ms per token,   589.75 tokens per second)
       eval time =   17437.72 ms /   675 tokens (   25.83 ms per token,    38.71 tokens per second)
      total time =   18061.71 ms /  1043 tokens
slot      release: id  2 | task 9545 | stop processing: n_tokens = 12928, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.950
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 10222 | processing task, is_child = 0
slot update_slots: id  2 | task 10222 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12972
slot update_slots: id  2 | task 10222 | n_tokens = 12281, memory_seq_rm [12281, end)
slot update_slots: id  2 | task 10222 | prompt processing progress, n_tokens = 12908, batch.n_tokens = 627, progress = 0.995066
slot update_slots: id  2 | task 10222 | n_tokens = 12908, memory_seq_rm [12908, end)
slot update_slots: id  2 | task 10222 | prompt processing progress, n_tokens = 12972, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 10222 | prompt done, n_tokens = 12972, batch.n_tokens = 64
slot init_sampler: id  2 | task 10222 | init sampler, took 2.13 ms, tokens: text = 12972, total = 12972
slot update_slots: id  2 | task 10222 | erasing old context checkpoint (pos_min = 7988, pos_max = 8859, size = 20.448 MiB)
slot update_slots: id  2 | task 10222 | created context checkpoint 8 of 8 (pos_min = 12151, pos_max = 12907, size = 17.751 MiB)
slot print_timing: id  2 | task 10222 | 
prompt eval time =    1114.73 ms /   691 tokens (    1.61 ms per token,   619.88 tokens per second)
       eval time =    9399.40 ms /   361 tokens (   26.04 ms per token,    38.41 tokens per second)
      total time =   10514.13 ms /  1052 tokens
slot      release: id  2 | task 10222 | stop processing: n_tokens = 13332, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 10585 | processing task, is_child = 0
slot update_slots: id  2 | task 10585 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13378
slot update_slots: id  2 | task 10585 | n_tokens = 12989, memory_seq_rm [12989, end)
slot update_slots: id  2 | task 10585 | prompt processing progress, n_tokens = 13314, batch.n_tokens = 325, progress = 0.995216
slot update_slots: id  2 | task 10585 | n_tokens = 13314, memory_seq_rm [13314, end)
slot update_slots: id  2 | task 10585 | prompt processing progress, n_tokens = 13378, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 10585 | prompt done, n_tokens = 13378, batch.n_tokens = 64
slot init_sampler: id  2 | task 10585 | init sampler, took 1.84 ms, tokens: text = 13378, total = 13378
slot update_slots: id  2 | task 10585 | erasing old context checkpoint (pos_min = 8064, pos_max = 8960, size = 21.034 MiB)
slot update_slots: id  2 | task 10585 | created context checkpoint 8 of 8 (pos_min = 12606, pos_max = 13313, size = 16.602 MiB)
slot print_timing: id  2 | task 10585 | 
prompt eval time =     674.33 ms /   389 tokens (    1.73 ms per token,   576.87 tokens per second)
       eval time =    3819.78 ms /   145 tokens (   26.34 ms per token,    37.96 tokens per second)
      total time =    4494.11 ms /   534 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 10585 | stop processing: n_tokens = 13522, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 10732 | processing task, is_child = 0
slot update_slots: id  2 | task 10732 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13568
slot update_slots: id  2 | task 10732 | n_tokens = 13397, memory_seq_rm [13397, end)
slot update_slots: id  2 | task 10732 | prompt processing progress, n_tokens = 13504, batch.n_tokens = 107, progress = 0.995283
slot update_slots: id  2 | task 10732 | n_tokens = 13504, memory_seq_rm [13504, end)
slot update_slots: id  2 | task 10732 | prompt processing progress, n_tokens = 13568, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 10732 | prompt done, n_tokens = 13568, batch.n_tokens = 64
slot init_sampler: id  2 | task 10732 | init sampler, took 2.53 ms, tokens: text = 13568, total = 13568
slot update_slots: id  2 | task 10732 | erasing old context checkpoint (pos_min = 9225, pos_max = 9994, size = 18.056 MiB)
slot update_slots: id  2 | task 10732 | created context checkpoint 8 of 8 (pos_min = 12814, pos_max = 13503, size = 16.180 MiB)
slot print_timing: id  2 | task 10732 | 
prompt eval time =     473.76 ms /   171 tokens (    2.77 ms per token,   360.94 tokens per second)
       eval time =    4842.33 ms /   182 tokens (   26.61 ms per token,    37.59 tokens per second)
      total time =    5316.10 ms /   353 tokens
slot      release: id  2 | task 10732 | stop processing: n_tokens = 13749, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 10916 | processing task, is_child = 0
slot update_slots: id  2 | task 10916 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13793
slot update_slots: id  2 | task 10916 | n_tokens = 13594, memory_seq_rm [13594, end)
slot update_slots: id  2 | task 10916 | prompt processing progress, n_tokens = 13729, batch.n_tokens = 135, progress = 0.995360
slot update_slots: id  2 | task 10916 | n_tokens = 13729, memory_seq_rm [13729, end)
slot update_slots: id  2 | task 10916 | prompt processing progress, n_tokens = 13793, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 10916 | prompt done, n_tokens = 13793, batch.n_tokens = 64
slot init_sampler: id  2 | task 10916 | init sampler, took 2.07 ms, tokens: text = 13793, total = 13793
slot update_slots: id  2 | task 10916 | erasing old context checkpoint (pos_min = 9658, pos_max = 10554, size = 21.034 MiB)
slot update_slots: id  2 | task 10916 | created context checkpoint 8 of 8 (pos_min = 12989, pos_max = 13728, size = 17.353 MiB)
slot print_timing: id  2 | task 10916 | 
prompt eval time =     480.75 ms /   199 tokens (    2.42 ms per token,   413.93 tokens per second)
       eval time =    2364.20 ms /    88 tokens (   26.87 ms per token,    37.22 tokens per second)
      total time =    2844.96 ms /   287 tokens
slot      release: id  2 | task 10916 | stop processing: n_tokens = 13880, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11006 | processing task, is_child = 0
slot update_slots: id  2 | task 11006 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13971
slot update_slots: id  2 | task 11006 | n_tokens = 13850, memory_seq_rm [13850, end)
slot update_slots: id  2 | task 11006 | prompt processing progress, n_tokens = 13907, batch.n_tokens = 57, progress = 0.995419
slot update_slots: id  2 | task 11006 | n_tokens = 13907, memory_seq_rm [13907, end)
slot update_slots: id  2 | task 11006 | prompt processing progress, n_tokens = 13971, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11006 | prompt done, n_tokens = 13971, batch.n_tokens = 64
slot init_sampler: id  2 | task 11006 | init sampler, took 1.92 ms, tokens: text = 13971, total = 13971
slot update_slots: id  2 | task 11006 | erasing old context checkpoint (pos_min = 10636, pos_max = 11440, size = 18.877 MiB)
slot update_slots: id  2 | task 11006 | created context checkpoint 8 of 8 (pos_min = 13010, pos_max = 13906, size = 21.034 MiB)
slot print_timing: id  2 | task 11006 | 
prompt eval time =     401.29 ms /   121 tokens (    3.32 ms per token,   301.53 tokens per second)
       eval time =    1283.43 ms /    48 tokens (   26.74 ms per token,    37.40 tokens per second)
      total time =    1684.72 ms /   169 tokens
slot      release: id  2 | task 11006 | stop processing: n_tokens = 14018, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11056 | processing task, is_child = 0
slot update_slots: id  2 | task 11056 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14292
slot update_slots: id  2 | task 11056 | n_tokens = 13991, memory_seq_rm [13991, end)
slot update_slots: id  2 | task 11056 | prompt processing progress, n_tokens = 14228, batch.n_tokens = 237, progress = 0.995522
slot update_slots: id  2 | task 11056 | n_tokens = 14228, memory_seq_rm [14228, end)
slot update_slots: id  2 | task 11056 | prompt processing progress, n_tokens = 14292, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11056 | prompt done, n_tokens = 14292, batch.n_tokens = 64
slot init_sampler: id  2 | task 11056 | init sampler, took 1.96 ms, tokens: text = 14292, total = 14292
slot update_slots: id  2 | task 11056 | erasing old context checkpoint (pos_min = 10802, pos_max = 11689, size = 20.823 MiB)
slot update_slots: id  2 | task 11056 | created context checkpoint 8 of 8 (pos_min = 13331, pos_max = 14227, size = 21.034 MiB)
slot print_timing: id  2 | task 11056 | 
prompt eval time =     609.92 ms /   301 tokens (    2.03 ms per token,   493.51 tokens per second)
       eval time =    3242.50 ms /   121 tokens (   26.80 ms per token,    37.32 tokens per second)
      total time =    3852.42 ms /   422 tokens
slot      release: id  2 | task 11056 | stop processing: n_tokens = 14412, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11179 | processing task, is_child = 0
slot update_slots: id  2 | task 11179 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14458
slot update_slots: id  2 | task 11179 | n_tokens = 14329, memory_seq_rm [14329, end)
slot update_slots: id  2 | task 11179 | prompt processing progress, n_tokens = 14394, batch.n_tokens = 65, progress = 0.995573
slot update_slots: id  2 | task 11179 | n_tokens = 14394, memory_seq_rm [14394, end)
slot update_slots: id  2 | task 11179 | prompt processing progress, n_tokens = 14458, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11179 | prompt done, n_tokens = 14458, batch.n_tokens = 64
slot init_sampler: id  2 | task 11179 | init sampler, took 1.99 ms, tokens: text = 14458, total = 14458
slot update_slots: id  2 | task 11179 | erasing old context checkpoint (pos_min = 10917, pos_max = 11804, size = 20.823 MiB)
slot update_slots: id  2 | task 11179 | created context checkpoint 8 of 8 (pos_min = 13515, pos_max = 14393, size = 20.612 MiB)
slot print_timing: id  2 | task 11179 | 
prompt eval time =     445.13 ms /   129 tokens (    3.45 ms per token,   289.80 tokens per second)
       eval time =    2210.97 ms /    82 tokens (   26.96 ms per token,    37.09 tokens per second)
      total time =    2656.11 ms /   211 tokens
slot      release: id  2 | task 11179 | stop processing: n_tokens = 14539, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11263 | processing task, is_child = 0
slot update_slots: id  2 | task 11263 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14581
slot update_slots: id  2 | task 11263 | n_tokens = 14499, memory_seq_rm [14499, end)
slot update_slots: id  2 | task 11263 | prompt processing progress, n_tokens = 14517, batch.n_tokens = 18, progress = 0.995611
slot update_slots: id  2 | task 11263 | n_tokens = 14517, memory_seq_rm [14517, end)
slot update_slots: id  2 | task 11263 | prompt processing progress, n_tokens = 14581, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11263 | prompt done, n_tokens = 14581, batch.n_tokens = 64
slot init_sampler: id  2 | task 11263 | init sampler, took 2.04 ms, tokens: text = 14581, total = 14581
slot update_slots: id  2 | task 11263 | erasing old context checkpoint (pos_min = 11302, pos_max = 12189, size = 20.823 MiB)
slot update_slots: id  2 | task 11263 | created context checkpoint 8 of 8 (pos_min = 13642, pos_max = 14516, size = 20.518 MiB)
slot print_timing: id  2 | task 11263 | 
prompt eval time =     319.31 ms /    82 tokens (    3.89 ms per token,   256.80 tokens per second)
       eval time =    1449.50 ms /    54 tokens (   26.84 ms per token,    37.25 tokens per second)
      total time =    1768.81 ms /   136 tokens
slot      release: id  2 | task 11263 | stop processing: n_tokens = 14634, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11319 | processing task, is_child = 0
slot update_slots: id  2 | task 11319 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14685
slot update_slots: id  2 | task 11319 | n_tokens = 14625, memory_seq_rm [14625, end)
slot update_slots: id  2 | task 11319 | prompt processing progress, n_tokens = 14685, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  2 | task 11319 | prompt done, n_tokens = 14685, batch.n_tokens = 60
slot init_sampler: id  2 | task 11319 | init sampler, took 2.05 ms, tokens: text = 14685, total = 14685
slot update_slots: id  2 | task 11319 | erasing old context checkpoint (pos_min = 12151, pos_max = 12907, size = 17.751 MiB)
slot update_slots: id  2 | task 11319 | created context checkpoint 8 of 8 (pos_min = 13737, pos_max = 14624, size = 20.823 MiB)
slot print_timing: id  2 | task 11319 | 
prompt eval time =     216.46 ms /    60 tokens (    3.61 ms per token,   277.19 tokens per second)
       eval time =    2672.23 ms /    99 tokens (   26.99 ms per token,    37.05 tokens per second)
      total time =    2888.69 ms /   159 tokens
slot      release: id  2 | task 11319 | stop processing: n_tokens = 14783, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11419 | processing task, is_child = 0
slot update_slots: id  2 | task 11419 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14815
slot update_slots: id  2 | task 11419 | n_tokens = 14754, memory_seq_rm [14754, end)
slot update_slots: id  2 | task 11419 | prompt processing progress, n_tokens = 14815, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  2 | task 11419 | prompt done, n_tokens = 14815, batch.n_tokens = 61
slot init_sampler: id  2 | task 11419 | init sampler, took 2.05 ms, tokens: text = 14815, total = 14815
slot update_slots: id  2 | task 11419 | erasing old context checkpoint (pos_min = 12606, pos_max = 13313, size = 16.602 MiB)
slot update_slots: id  2 | task 11419 | created context checkpoint 8 of 8 (pos_min = 13886, pos_max = 14753, size = 20.354 MiB)
slot print_timing: id  2 | task 11419 | 
prompt eval time =     300.06 ms /    61 tokens (    4.92 ms per token,   203.29 tokens per second)
       eval time =    1064.10 ms /    41 tokens (   25.95 ms per token,    38.53 tokens per second)
      total time =    1364.16 ms /   102 tokens
slot      release: id  2 | task 11419 | stop processing: n_tokens = 14855, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11461 | processing task, is_child = 0
slot update_slots: id  2 | task 11461 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14979
slot update_slots: id  2 | task 11461 | n_tokens = 14826, memory_seq_rm [14826, end)
slot update_slots: id  2 | task 11461 | prompt processing progress, n_tokens = 14915, batch.n_tokens = 89, progress = 0.995727
slot update_slots: id  2 | task 11461 | n_tokens = 14915, memory_seq_rm [14915, end)
slot update_slots: id  2 | task 11461 | prompt processing progress, n_tokens = 14979, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11461 | prompt done, n_tokens = 14979, batch.n_tokens = 64
slot init_sampler: id  2 | task 11461 | init sampler, took 2.05 ms, tokens: text = 14979, total = 14979
slot update_slots: id  2 | task 11461 | erasing old context checkpoint (pos_min = 12814, pos_max = 13503, size = 16.180 MiB)
slot update_slots: id  2 | task 11461 | created context checkpoint 8 of 8 (pos_min = 14018, pos_max = 14914, size = 21.034 MiB)
slot print_timing: id  2 | task 11461 | 
prompt eval time =     456.29 ms /   153 tokens (    2.98 ms per token,   335.31 tokens per second)
       eval time =    1499.26 ms /    57 tokens (   26.30 ms per token,    38.02 tokens per second)
      total time =    1955.55 ms /   210 tokens
slot      release: id  2 | task 11461 | stop processing: n_tokens = 15035, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11520 | processing task, is_child = 0
slot update_slots: id  2 | task 11520 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15141
slot update_slots: id  2 | task 11520 | n_tokens = 15010, memory_seq_rm [15010, end)
slot update_slots: id  2 | task 11520 | prompt processing progress, n_tokens = 15077, batch.n_tokens = 67, progress = 0.995773
slot update_slots: id  2 | task 11520 | n_tokens = 15077, memory_seq_rm [15077, end)
slot update_slots: id  2 | task 11520 | prompt processing progress, n_tokens = 15141, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11520 | prompt done, n_tokens = 15141, batch.n_tokens = 64
slot init_sampler: id  2 | task 11520 | init sampler, took 2.16 ms, tokens: text = 15141, total = 15141
slot update_slots: id  2 | task 11520 | erasing old context checkpoint (pos_min = 12989, pos_max = 13728, size = 17.353 MiB)
slot update_slots: id  2 | task 11520 | created context checkpoint 8 of 8 (pos_min = 14180, pos_max = 15076, size = 21.034 MiB)
slot print_timing: id  2 | task 11520 | 
prompt eval time =     449.90 ms /   131 tokens (    3.43 ms per token,   291.17 tokens per second)
       eval time =    5910.23 ms /   220 tokens (   26.86 ms per token,    37.22 tokens per second)
      total time =    6360.14 ms /   351 tokens
slot      release: id  2 | task 11520 | stop processing: n_tokens = 15360, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11742 | processing task, is_child = 0
slot update_slots: id  2 | task 11742 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15395
slot update_slots: id  2 | task 11742 | n_tokens = 15325, memory_seq_rm [15325, end)
slot update_slots: id  2 | task 11742 | prompt processing progress, n_tokens = 15331, batch.n_tokens = 6, progress = 0.995843
slot update_slots: id  2 | task 11742 | n_tokens = 15331, memory_seq_rm [15331, end)
slot update_slots: id  2 | task 11742 | prompt processing progress, n_tokens = 15395, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11742 | prompt done, n_tokens = 15395, batch.n_tokens = 64
slot init_sampler: id  2 | task 11742 | init sampler, took 2.10 ms, tokens: text = 15395, total = 15395
slot update_slots: id  2 | task 11742 | erasing old context checkpoint (pos_min = 13010, pos_max = 13906, size = 21.034 MiB)
slot update_slots: id  2 | task 11742 | created context checkpoint 8 of 8 (pos_min = 14463, pos_max = 15330, size = 20.354 MiB)
slot print_timing: id  2 | task 11742 | 
prompt eval time =     269.28 ms /    70 tokens (    3.85 ms per token,   259.95 tokens per second)
       eval time =    2342.63 ms /    87 tokens (   26.93 ms per token,    37.14 tokens per second)
      total time =    2611.91 ms /   157 tokens
slot      release: id  2 | task 11742 | stop processing: n_tokens = 15481, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.058
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 15481, total state size = 384.047 MiB
srv          load:  - looking for better prompt, base f_keep = 0.058, sim = 0.991
srv        update:  - cache state: 2 prompts, 979.140 MiB (limits: 8192.000 MiB, 56064 tokens, 211179 est)
srv        update:    - prompt 0x59014c2315e0:    9760 tokens, checkpoints:  8,   429.330 MiB
srv        update:    - prompt 0x59015104bc40:   15481 tokens, checkpoints:  8,   549.810 MiB
srv  get_availabl: prompt cache update took 428.54 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11831 | processing task, is_child = 0
slot update_slots: id  2 | task 11831 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 902
slot update_slots: id  2 | task 11831 | n_past = 894, slot.prompt.tokens.size() = 15481, seq_id = 2, pos_min = 14584, n_swa = 128
slot update_slots: id  2 | task 11831 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 11831 | erased invalidated context checkpoint (pos_min = 13331, pos_max = 14227, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 11831 | erased invalidated context checkpoint (pos_min = 13515, pos_max = 14393, n_swa = 128, size = 20.612 MiB)
slot update_slots: id  2 | task 11831 | erased invalidated context checkpoint (pos_min = 13642, pos_max = 14516, n_swa = 128, size = 20.518 MiB)
slot update_slots: id  2 | task 11831 | erased invalidated context checkpoint (pos_min = 13737, pos_max = 14624, n_swa = 128, size = 20.823 MiB)
slot update_slots: id  2 | task 11831 | erased invalidated context checkpoint (pos_min = 13886, pos_max = 14753, n_swa = 128, size = 20.354 MiB)
slot update_slots: id  2 | task 11831 | erased invalidated context checkpoint (pos_min = 14018, pos_max = 14914, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 11831 | erased invalidated context checkpoint (pos_min = 14180, pos_max = 15076, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 11831 | erased invalidated context checkpoint (pos_min = 14463, pos_max = 15330, n_swa = 128, size = 20.354 MiB)
slot update_slots: id  2 | task 11831 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 11831 | prompt processing progress, n_tokens = 838, batch.n_tokens = 838, progress = 0.929047
slot update_slots: id  2 | task 11831 | n_tokens = 838, memory_seq_rm [838, end)
slot update_slots: id  2 | task 11831 | prompt processing progress, n_tokens = 902, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11831 | prompt done, n_tokens = 902, batch.n_tokens = 64
slot init_sampler: id  2 | task 11831 | init sampler, took 0.18 ms, tokens: text = 902, total = 902
slot update_slots: id  2 | task 11831 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 837, size = 19.651 MiB)
slot print_timing: id  2 | task 11831 | 
prompt eval time =    1146.81 ms /   902 tokens (    1.27 ms per token,   786.53 tokens per second)
       eval time =    1073.72 ms /    43 tokens (   24.97 ms per token,    40.05 tokens per second)
      total time =    2220.53 ms /   945 tokens
slot      release: id  2 | task 11831 | stop processing: n_tokens = 944, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.849 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11876 | processing task, is_child = 0
slot update_slots: id  2 | task 11876 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1091
slot update_slots: id  2 | task 11876 | n_tokens = 926, memory_seq_rm [926, end)
slot update_slots: id  2 | task 11876 | prompt processing progress, n_tokens = 1027, batch.n_tokens = 101, progress = 0.941338
slot update_slots: id  2 | task 11876 | n_tokens = 1027, memory_seq_rm [1027, end)
slot update_slots: id  2 | task 11876 | prompt processing progress, n_tokens = 1091, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11876 | prompt done, n_tokens = 1091, batch.n_tokens = 64
slot init_sampler: id  2 | task 11876 | init sampler, took 0.16 ms, tokens: text = 1091, total = 1091
slot update_slots: id  2 | task 11876 | created context checkpoint 2 of 8 (pos_min = 130, pos_max = 1026, size = 21.034 MiB)
slot print_timing: id  2 | task 11876 | 
prompt eval time =     434.65 ms /   165 tokens (    2.63 ms per token,   379.62 tokens per second)
       eval time =    3977.23 ms /   153 tokens (   25.99 ms per token,    38.47 tokens per second)
      total time =    4411.87 ms /   318 tokens
slot      release: id  2 | task 11876 | stop processing: n_tokens = 1243, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.726 (> 0.100 thold), f_keep = 0.726
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12031 | processing task, is_child = 0
slot update_slots: id  2 | task 12031 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1242
slot update_slots: id  2 | task 12031 | n_tokens = 902, memory_seq_rm [902, end)
slot update_slots: id  2 | task 12031 | prompt processing progress, n_tokens = 1178, batch.n_tokens = 276, progress = 0.948470
slot update_slots: id  2 | task 12031 | n_tokens = 1178, memory_seq_rm [1178, end)
slot update_slots: id  2 | task 12031 | prompt processing progress, n_tokens = 1242, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12031 | prompt done, n_tokens = 1242, batch.n_tokens = 64
slot init_sampler: id  2 | task 12031 | init sampler, took 0.23 ms, tokens: text = 1242, total = 1242
slot update_slots: id  2 | task 12031 | created context checkpoint 3 of 8 (pos_min = 346, pos_max = 1177, size = 19.510 MiB)
slot print_timing: id  2 | task 12031 | 
prompt eval time =     617.94 ms /   340 tokens (    1.82 ms per token,   550.22 tokens per second)
       eval time =    1150.32 ms /    44 tokens (   26.14 ms per token,    38.25 tokens per second)
      total time =    1768.26 ms /   384 tokens
slot      release: id  2 | task 12031 | stop processing: n_tokens = 1285, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.885 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12077 | processing task, is_child = 0
slot update_slots: id  2 | task 12077 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1430
slot update_slots: id  2 | task 12077 | n_tokens = 1265, memory_seq_rm [1265, end)
slot update_slots: id  2 | task 12077 | prompt processing progress, n_tokens = 1366, batch.n_tokens = 101, progress = 0.955245
slot update_slots: id  2 | task 12077 | n_tokens = 1366, memory_seq_rm [1366, end)
slot update_slots: id  2 | task 12077 | prompt processing progress, n_tokens = 1430, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12077 | prompt done, n_tokens = 1430, batch.n_tokens = 64
slot init_sampler: id  2 | task 12077 | init sampler, took 0.27 ms, tokens: text = 1430, total = 1430
slot update_slots: id  2 | task 12077 | created context checkpoint 4 of 8 (pos_min = 469, pos_max = 1365, size = 21.034 MiB)
slot print_timing: id  2 | task 12077 | 
prompt eval time =     447.71 ms /   165 tokens (    2.71 ms per token,   368.54 tokens per second)
       eval time =    1180.59 ms /    47 tokens (   25.12 ms per token,    39.81 tokens per second)
      total time =    1628.30 ms /   212 tokens
slot      release: id  2 | task 12077 | stop processing: n_tokens = 1476, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.606
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12126 | processing task, is_child = 0
slot update_slots: id  2 | task 12126 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 913
slot update_slots: id  2 | task 12126 | n_tokens = 894, memory_seq_rm [894, end)
slot update_slots: id  2 | task 12126 | prompt processing progress, n_tokens = 913, batch.n_tokens = 19, progress = 1.000000
slot update_slots: id  2 | task 12126 | prompt done, n_tokens = 913, batch.n_tokens = 19
slot init_sampler: id  2 | task 12126 | init sampler, took 0.13 ms, tokens: text = 913, total = 913
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.594 (> 0.100 thold), f_keep = 0.577
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12132 | processing task, is_child = 0
slot update_slots: id  3 | task 12132 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1504
slot update_slots: id  3 | task 12132 | n_past = 894, slot.prompt.tokens.size() = 1549, seq_id = 3, pos_min = 1422, n_swa = 128
slot update_slots: id  3 | task 12132 | restored context checkpoint (pos_min = 0, pos_max = 837, size = 19.651 MiB)
slot update_slots: id  3 | task 12132 | n_tokens = 837, memory_seq_rm [837, end)
slot update_slots: id  3 | task 12132 | prompt processing progress, n_tokens = 1440, batch.n_tokens = 604, progress = 0.957447
slot update_slots: id  3 | task 12132 | n_tokens = 1440, memory_seq_rm [1440, end)
slot update_slots: id  3 | task 12132 | prompt processing progress, n_tokens = 1504, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 12132 | prompt done, n_tokens = 1504, batch.n_tokens = 65
slot init_sampler: id  3 | task 12132 | init sampler, took 0.25 ms, tokens: text = 1504, total = 1504
slot update_slots: id  3 | task 12132 | created context checkpoint 2 of 8 (pos_min = 544, pos_max = 1439, size = 21.011 MiB)
slot print_timing: id  2 | task 12126 | 
prompt eval time =     156.10 ms /    19 tokens (    8.22 ms per token,   121.71 tokens per second)
       eval time =    3718.33 ms /    61 tokens (   60.96 ms per token,    16.41 tokens per second)
      total time =    3874.43 ms /    80 tokens
slot      release: id  2 | task 12126 | stop processing: n_tokens = 973, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 12132 | 
prompt eval time =     992.89 ms /   667 tokens (    1.49 ms per token,   671.78 tokens per second)
       eval time =    2911.31 ms /    67 tokens (   43.45 ms per token,    23.01 tokens per second)
      total time =    3904.20 ms /   734 tokens
slot      release: id  3 | task 12132 | stop processing: n_tokens = 1570, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.911 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12201 | processing task, is_child = 0
slot update_slots: id  3 | task 12201 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1692
slot update_slots: id  3 | task 12201 | n_tokens = 1541, memory_seq_rm [1541, end)
slot update_slots: id  3 | task 12201 | prompt processing progress, n_tokens = 1628, batch.n_tokens = 87, progress = 0.962175
slot update_slots: id  3 | task 12201 | n_tokens = 1628, memory_seq_rm [1628, end)
slot update_slots: id  3 | task 12201 | prompt processing progress, n_tokens = 1692, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12201 | prompt done, n_tokens = 1692, batch.n_tokens = 64
slot init_sampler: id  3 | task 12201 | init sampler, took 0.24 ms, tokens: text = 1692, total = 1692
slot update_slots: id  3 | task 12201 | created context checkpoint 3 of 8 (pos_min = 787, pos_max = 1627, size = 19.721 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12205 | processing task, is_child = 0
slot update_slots: id  2 | task 12205 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 995
slot update_slots: id  2 | task 12205 | n_tokens = 949, memory_seq_rm [949, end)
slot update_slots: id  2 | task 12205 | prompt processing progress, n_tokens = 995, batch.n_tokens = 47, progress = 1.000000
slot update_slots: id  2 | task 12205 | prompt done, n_tokens = 995, batch.n_tokens = 47
slot init_sampler: id  2 | task 12205 | init sampler, took 0.15 ms, tokens: text = 995, total = 995
slot print_timing: id  2 | task 12205 | 
prompt eval time =     151.23 ms /    46 tokens (    3.29 ms per token,   304.18 tokens per second)
       eval time =    2029.94 ms /    44 tokens (   46.14 ms per token,    21.68 tokens per second)
      total time =    2181.17 ms /    90 tokens
slot      release: id  2 | task 12205 | stop processing: n_tokens = 1038, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 12201 | 
prompt eval time =     405.61 ms /   151 tokens (    2.69 ms per token,   372.28 tokens per second)
       eval time =    2309.50 ms /    50 tokens (   46.19 ms per token,    21.65 tokens per second)
      total time =    2715.11 ms /   201 tokens
slot      release: id  3 | task 12201 | stop processing: n_tokens = 1741, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.674 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12254 | processing task, is_child = 0
slot update_slots: id  3 | task 12254 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2544
slot update_slots: id  3 | task 12254 | n_tokens = 1714, memory_seq_rm [1714, end)
slot update_slots: id  3 | task 12254 | prompt processing progress, n_tokens = 2480, batch.n_tokens = 766, progress = 0.974843
slot update_slots: id  3 | task 12254 | n_tokens = 2480, memory_seq_rm [2480, end)
slot update_slots: id  3 | task 12254 | prompt processing progress, n_tokens = 2544, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12254 | prompt done, n_tokens = 2544, batch.n_tokens = 64
slot init_sampler: id  3 | task 12254 | init sampler, took 0.36 ms, tokens: text = 2544, total = 2544
slot update_slots: id  3 | task 12254 | created context checkpoint 4 of 8 (pos_min = 1694, pos_max = 2479, size = 18.431 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12260 | processing task, is_child = 0
slot update_slots: id  2 | task 12260 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1062
slot update_slots: id  2 | task 12260 | n_tokens = 1016, memory_seq_rm [1016, end)
slot update_slots: id  2 | task 12260 | prompt processing progress, n_tokens = 1062, batch.n_tokens = 47, progress = 1.000000
slot update_slots: id  2 | task 12260 | prompt done, n_tokens = 1062, batch.n_tokens = 47
slot init_sampler: id  2 | task 12260 | init sampler, took 0.22 ms, tokens: text = 1062, total = 1062
slot print_timing: id  2 | task 12260 | 
prompt eval time =     160.74 ms /    46 tokens (    3.49 ms per token,   286.18 tokens per second)
       eval time =    1913.36 ms /    38 tokens (   50.35 ms per token,    19.86 tokens per second)
      total time =    2074.10 ms /    84 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 12260 | stop processing: n_tokens = 1099, truncated = 0
slot print_timing: id  3 | task 12254 | 
prompt eval time =    1131.58 ms /   830 tokens (    1.36 ms per token,   733.49 tokens per second)
       eval time =    2314.24 ms /    48 tokens (   48.21 ms per token,    20.74 tokens per second)
      total time =    3445.82 ms /   878 tokens
slot      release: id  3 | task 12254 | stop processing: n_tokens = 2591, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.749 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12305 | processing task, is_child = 0
slot update_slots: id  3 | task 12305 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3417
slot update_slots: id  3 | task 12305 | n_tokens = 2560, memory_seq_rm [2560, end)
slot update_slots: id  3 | task 12305 | prompt processing progress, n_tokens = 3353, batch.n_tokens = 793, progress = 0.981270
slot update_slots: id  3 | task 12305 | n_tokens = 3353, memory_seq_rm [3353, end)
slot update_slots: id  3 | task 12305 | prompt processing progress, n_tokens = 3417, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12305 | prompt done, n_tokens = 3417, batch.n_tokens = 64
slot init_sampler: id  3 | task 12305 | init sampler, took 0.67 ms, tokens: text = 3417, total = 3417
slot update_slots: id  3 | task 12305 | created context checkpoint 5 of 8 (pos_min = 2527, pos_max = 3352, size = 19.369 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.959 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12308 | processing task, is_child = 0
slot update_slots: id  2 | task 12308 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1126
slot update_slots: id  2 | task 12308 | n_past = 1080, slot.prompt.tokens.size() = 1099, seq_id = 2, pos_min = 972, n_swa = 128
slot update_slots: id  2 | task 12308 | restored context checkpoint (pos_min = 469, pos_max = 1365, size = 21.034 MiB)
slot update_slots: id  2 | task 12308 | n_tokens = 1080, memory_seq_rm [1080, end)
slot update_slots: id  2 | task 12308 | prompt processing progress, n_tokens = 1126, batch.n_tokens = 47, progress = 1.000000
slot update_slots: id  2 | task 12308 | prompt done, n_tokens = 1126, batch.n_tokens = 47
slot init_sampler: id  2 | task 12308 | init sampler, took 0.17 ms, tokens: text = 1126, total = 1126
slot print_timing: id  2 | task 12308 | 
prompt eval time =     160.26 ms /    46 tokens (    3.48 ms per token,   287.03 tokens per second)
       eval time =    1527.53 ms /    32 tokens (   47.74 ms per token,    20.95 tokens per second)
      total time =    1687.80 ms /    78 tokens
slot      release: id  2 | task 12308 | stop processing: n_tokens = 1157, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 12305 | 
prompt eval time =    1164.54 ms /   857 tokens (    1.36 ms per token,   735.91 tokens per second)
       eval time =    1877.58 ms /    40 tokens (   46.94 ms per token,    21.30 tokens per second)
      total time =    3042.12 ms /   897 tokens
slot      release: id  3 | task 12305 | stop processing: n_tokens = 3456, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12348 | processing task, is_child = 0
slot update_slots: id  3 | task 12348 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3527
slot update_slots: id  3 | task 12348 | n_tokens = 3425, memory_seq_rm [3425, end)
slot update_slots: id  3 | task 12348 | prompt processing progress, n_tokens = 3463, batch.n_tokens = 38, progress = 0.981854
slot update_slots: id  3 | task 12348 | n_tokens = 3463, memory_seq_rm [3463, end)
slot update_slots: id  3 | task 12348 | prompt processing progress, n_tokens = 3527, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12348 | prompt done, n_tokens = 3527, batch.n_tokens = 64
slot init_sampler: id  3 | task 12348 | init sampler, took 0.49 ms, tokens: text = 3527, total = 3527
slot update_slots: id  3 | task 12348 | created context checkpoint 6 of 8 (pos_min = 3290, pos_max = 3462, size = 4.057 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12377 | processing task, is_child = 0
slot update_slots: id  2 | task 12377 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1183
slot update_slots: id  2 | task 12377 | n_tokens = 1137, memory_seq_rm [1137, end)
slot update_slots: id  2 | task 12377 | prompt processing progress, n_tokens = 1183, batch.n_tokens = 47, progress = 1.000000
slot update_slots: id  2 | task 12377 | prompt done, n_tokens = 1183, batch.n_tokens = 47
slot init_sampler: id  2 | task 12377 | init sampler, took 0.19 ms, tokens: text = 1183, total = 1183
slot print_timing: id  3 | task 12348 | 
prompt eval time =     334.05 ms /   102 tokens (    3.27 ms per token,   305.35 tokens per second)
       eval time =    2498.78 ms /    60 tokens (   41.65 ms per token,    24.01 tokens per second)
      total time =    2832.83 ms /   162 tokens
slot      release: id  3 | task 12348 | stop processing: n_tokens = 3586, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12446 | processing task, is_child = 0
slot update_slots: id  3 | task 12446 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3632
slot update_slots: id  3 | task 12446 | n_tokens = 3552, memory_seq_rm [3552, end)
slot update_slots: id  3 | task 12446 | prompt processing progress, n_tokens = 3568, batch.n_tokens = 17, progress = 0.982379
slot update_slots: id  3 | task 12446 | n_tokens = 3568, memory_seq_rm [3568, end)
slot update_slots: id  3 | task 12446 | prompt processing progress, n_tokens = 3632, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 12446 | prompt done, n_tokens = 3632, batch.n_tokens = 65
slot init_sampler: id  3 | task 12446 | init sampler, took 0.62 ms, tokens: text = 3632, total = 3632
slot update_slots: id  3 | task 12446 | created context checkpoint 7 of 8 (pos_min = 3290, pos_max = 3567, size = 6.519 MiB)
slot print_timing: id  2 | task 12377 | 
prompt eval time =     165.96 ms /    46 tokens (    3.61 ms per token,   277.18 tokens per second)
       eval time =    3108.10 ms /    73 tokens (   42.58 ms per token,    23.49 tokens per second)
      total time =    3274.05 ms /   119 tokens
slot      release: id  2 | task 12377 | stop processing: n_tokens = 1255, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 12446 | 
prompt eval time =     342.31 ms /    80 tokens (    4.28 ms per token,   233.70 tokens per second)
       eval time =    1826.15 ms /    64 tokens (   28.53 ms per token,    35.05 tokens per second)
      total time =    2168.47 ms /   144 tokens
slot      release: id  3 | task 12446 | stop processing: n_tokens = 3695, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12512 | processing task, is_child = 0
slot update_slots: id  2 | task 12512 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1277
slot update_slots: id  2 | task 12512 | n_tokens = 1199, memory_seq_rm [1199, end)
slot update_slots: id  2 | task 12512 | prompt processing progress, n_tokens = 1213, batch.n_tokens = 14, progress = 0.949883
slot update_slots: id  2 | task 12512 | n_tokens = 1213, memory_seq_rm [1213, end)
slot update_slots: id  2 | task 12512 | prompt processing progress, n_tokens = 1277, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12512 | prompt done, n_tokens = 1277, batch.n_tokens = 64
slot init_sampler: id  2 | task 12512 | init sampler, took 0.18 ms, tokens: text = 1277, total = 1277
slot print_timing: id  2 | task 12512 | 
prompt eval time =     333.62 ms /    78 tokens (    4.28 ms per token,   233.80 tokens per second)
       eval time =    2647.53 ms /   110 tokens (   24.07 ms per token,    41.55 tokens per second)
      total time =    2981.15 ms /   188 tokens
slot      release: id  2 | task 12512 | stop processing: n_tokens = 1386, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.897 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12624 | processing task, is_child = 0
slot update_slots: id  2 | task 12624 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1512
slot update_slots: id  2 | task 12624 | n_tokens = 1356, memory_seq_rm [1356, end)
slot update_slots: id  2 | task 12624 | prompt processing progress, n_tokens = 1448, batch.n_tokens = 92, progress = 0.957672
slot update_slots: id  2 | task 12624 | n_tokens = 1448, memory_seq_rm [1448, end)
slot update_slots: id  2 | task 12624 | prompt processing progress, n_tokens = 1512, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12624 | prompt done, n_tokens = 1512, batch.n_tokens = 64
slot init_sampler: id  2 | task 12624 | init sampler, took 0.21 ms, tokens: text = 1512, total = 1512
slot update_slots: id  2 | task 12624 | created context checkpoint 5 of 8 (pos_min = 1157, pos_max = 1447, size = 6.824 MiB)
slot print_timing: id  2 | task 12624 | 
prompt eval time =     393.63 ms /   156 tokens (    2.52 ms per token,   396.31 tokens per second)
       eval time =    5168.32 ms /   209 tokens (   24.73 ms per token,    40.44 tokens per second)
      total time =    5561.96 ms /   365 tokens
slot      release: id  2 | task 12624 | stop processing: n_tokens = 1720, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.407 (> 0.100 thold), f_keep = 0.208
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1720, total state size = 50.041 MiB
srv          load:  - looking for better prompt, base f_keep = 0.208, sim = 0.407
srv        update:  - cache state: 3 prompts, 1117.233 MiB (limits: 8192.000 MiB, 56064 tokens, 197688 est)
srv        update:    - prompt 0x59014c2315e0:    9760 tokens, checkpoints:  8,   429.330 MiB
srv        update:    - prompt 0x59015104bc40:   15481 tokens, checkpoints:  8,   549.810 MiB
srv        update:    - prompt 0x590159548210:    1720 tokens, checkpoints:  5,   138.093 MiB
srv  get_availabl: prompt cache update took 148.41 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12835 | processing task, is_child = 0
slot update_slots: id  2 | task 12835 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 878
slot update_slots: id  2 | task 12835 | n_past = 357, slot.prompt.tokens.size() = 1720, seq_id = 2, pos_min = 1306, n_swa = 128
slot update_slots: id  2 | task 12835 | restored context checkpoint (pos_min = 130, pos_max = 1026, size = 21.034 MiB)
slot update_slots: id  2 | task 12835 | erased invalidated context checkpoint (pos_min = 346, pos_max = 1177, n_swa = 128, size = 19.510 MiB)
slot update_slots: id  2 | task 12835 | erased invalidated context checkpoint (pos_min = 469, pos_max = 1365, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 12835 | erased invalidated context checkpoint (pos_min = 1157, pos_max = 1447, n_swa = 128, size = 6.824 MiB)
slot update_slots: id  2 | task 12835 | n_tokens = 357, memory_seq_rm [357, end)
slot update_slots: id  2 | task 12835 | prompt processing progress, n_tokens = 814, batch.n_tokens = 457, progress = 0.927107
slot update_slots: id  2 | task 12835 | n_tokens = 814, memory_seq_rm [814, end)
slot update_slots: id  2 | task 12835 | prompt processing progress, n_tokens = 878, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12835 | prompt done, n_tokens = 878, batch.n_tokens = 64
slot init_sampler: id  2 | task 12835 | init sampler, took 0.15 ms, tokens: text = 878, total = 878
slot print_timing: id  2 | task 12835 | 
prompt eval time =     918.32 ms /   521 tokens (    1.76 ms per token,   567.34 tokens per second)
       eval time =     825.21 ms /    33 tokens (   25.01 ms per token,    39.99 tokens per second)
      total time =    1743.53 ms /   554 tokens
slot      release: id  2 | task 12835 | stop processing: n_tokens = 910, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.844 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12870 | processing task, is_child = 0
slot update_slots: id  2 | task 12870 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1057
slot update_slots: id  2 | task 12870 | n_tokens = 892, memory_seq_rm [892, end)
slot update_slots: id  2 | task 12870 | prompt processing progress, n_tokens = 993, batch.n_tokens = 101, progress = 0.939451
slot update_slots: id  2 | task 12870 | n_tokens = 993, memory_seq_rm [993, end)
slot update_slots: id  2 | task 12870 | prompt processing progress, n_tokens = 1057, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12870 | prompt done, n_tokens = 1057, batch.n_tokens = 64
slot init_sampler: id  2 | task 12870 | init sampler, took 0.15 ms, tokens: text = 1057, total = 1057
slot print_timing: id  2 | task 12870 | 
prompt eval time =     415.87 ms /   165 tokens (    2.52 ms per token,   396.76 tokens per second)
       eval time =    2828.93 ms /   109 tokens (   25.95 ms per token,    38.53 tokens per second)
      total time =    3244.79 ms /   274 tokens
slot      release: id  2 | task 12870 | stop processing: n_tokens = 1165, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.748 (> 0.100 thold), f_keep = 0.754
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12981 | processing task, is_child = 0
slot update_slots: id  2 | task 12981 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1174
slot update_slots: id  2 | task 12981 | n_tokens = 878, memory_seq_rm [878, end)
slot update_slots: id  2 | task 12981 | prompt processing progress, n_tokens = 1110, batch.n_tokens = 232, progress = 0.945486
slot update_slots: id  2 | task 12981 | n_tokens = 1110, memory_seq_rm [1110, end)
slot update_slots: id  2 | task 12981 | prompt processing progress, n_tokens = 1174, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12981 | prompt done, n_tokens = 1174, batch.n_tokens = 64
slot init_sampler: id  2 | task 12981 | init sampler, took 0.17 ms, tokens: text = 1174, total = 1174
slot update_slots: id  2 | task 12981 | created context checkpoint 3 of 8 (pos_min = 587, pos_max = 1109, size = 12.264 MiB)
slot print_timing: id  2 | task 12981 | 
prompt eval time =     680.81 ms /   296 tokens (    2.30 ms per token,   434.77 tokens per second)
       eval time =    1254.79 ms /    45 tokens (   27.88 ms per token,    35.86 tokens per second)
      total time =    1935.61 ms /   341 tokens
slot      release: id  2 | task 12981 | stop processing: n_tokens = 1218, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.879 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13028 | processing task, is_child = 0
slot update_slots: id  2 | task 13028 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1363
slot update_slots: id  2 | task 13028 | n_tokens = 1198, memory_seq_rm [1198, end)
slot update_slots: id  2 | task 13028 | prompt processing progress, n_tokens = 1299, batch.n_tokens = 101, progress = 0.953045
slot update_slots: id  2 | task 13028 | n_tokens = 1299, memory_seq_rm [1299, end)
slot update_slots: id  2 | task 13028 | prompt processing progress, n_tokens = 1363, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13028 | prompt done, n_tokens = 1363, batch.n_tokens = 64
slot init_sampler: id  2 | task 13028 | init sampler, took 0.20 ms, tokens: text = 1363, total = 1363
slot update_slots: id  2 | task 13028 | created context checkpoint 4 of 8 (pos_min = 651, pos_max = 1298, size = 15.195 MiB)
slot print_timing: id  2 | task 13028 | 
prompt eval time =     452.23 ms /   165 tokens (    2.74 ms per token,   364.86 tokens per second)
       eval time =    1118.71 ms /    42 tokens (   26.64 ms per token,    37.54 tokens per second)
      total time =    1570.94 ms /   207 tokens
slot      release: id  2 | task 13028 | stop processing: n_tokens = 1404, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.901 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13072 | processing task, is_child = 0
slot update_slots: id  2 | task 13072 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1526
slot update_slots: id  2 | task 13072 | n_tokens = 1375, memory_seq_rm [1375, end)
slot update_slots: id  2 | task 13072 | prompt processing progress, n_tokens = 1462, batch.n_tokens = 87, progress = 0.958060
slot update_slots: id  2 | task 13072 | n_tokens = 1462, memory_seq_rm [1462, end)
slot update_slots: id  2 | task 13072 | prompt processing progress, n_tokens = 1526, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13072 | prompt done, n_tokens = 1526, batch.n_tokens = 64
slot init_sampler: id  2 | task 13072 | init sampler, took 0.26 ms, tokens: text = 1526, total = 1526
slot update_slots: id  2 | task 13072 | created context checkpoint 5 of 8 (pos_min = 814, pos_max = 1461, size = 15.195 MiB)
slot print_timing: id  2 | task 13072 | 
prompt eval time =     417.77 ms /   151 tokens (    2.77 ms per token,   361.44 tokens per second)
       eval time =    2600.02 ms /    97 tokens (   26.80 ms per token,    37.31 tokens per second)
      total time =    3017.80 ms /   248 tokens
slot      release: id  2 | task 13072 | stop processing: n_tokens = 1622, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.951
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13171 | processing task, is_child = 0
slot update_slots: id  2 | task 13171 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1668
slot update_slots: id  2 | task 13171 | n_tokens = 1542, memory_seq_rm [1542, end)
slot update_slots: id  2 | task 13171 | prompt processing progress, n_tokens = 1604, batch.n_tokens = 62, progress = 0.961631
slot update_slots: id  2 | task 13171 | n_tokens = 1604, memory_seq_rm [1604, end)
slot update_slots: id  2 | task 13171 | prompt processing progress, n_tokens = 1668, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13171 | prompt done, n_tokens = 1668, batch.n_tokens = 64
slot init_sampler: id  2 | task 13171 | init sampler, took 0.26 ms, tokens: text = 1668, total = 1668
slot update_slots: id  2 | task 13171 | created context checkpoint 6 of 8 (pos_min = 878, pos_max = 1603, size = 17.024 MiB)
slot print_timing: id  2 | task 13171 | 
prompt eval time =     389.48 ms /   126 tokens (    3.09 ms per token,   323.51 tokens per second)
       eval time =    2290.66 ms /    84 tokens (   27.27 ms per token,    36.67 tokens per second)
      total time =    2680.14 ms /   210 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 13171 | stop processing: n_tokens = 1751, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.925 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13257 | processing task, is_child = 0
slot update_slots: id  2 | task 13257 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1851
slot update_slots: id  2 | task 13257 | n_tokens = 1713, memory_seq_rm [1713, end)
slot update_slots: id  2 | task 13257 | prompt processing progress, n_tokens = 1787, batch.n_tokens = 74, progress = 0.965424
slot update_slots: id  2 | task 13257 | n_tokens = 1787, memory_seq_rm [1787, end)
slot update_slots: id  2 | task 13257 | prompt processing progress, n_tokens = 1851, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13257 | prompt done, n_tokens = 1851, batch.n_tokens = 64
slot init_sampler: id  2 | task 13257 | init sampler, took 0.36 ms, tokens: text = 1851, total = 1851
slot update_slots: id  2 | task 13257 | created context checkpoint 7 of 8 (pos_min = 890, pos_max = 1786, size = 21.034 MiB)
slot print_timing: id  2 | task 13257 | 
prompt eval time =     395.34 ms /   138 tokens (    2.86 ms per token,   349.07 tokens per second)
       eval time =    3526.14 ms /   128 tokens (   27.55 ms per token,    36.30 tokens per second)
      total time =    3921.48 ms /   266 tokens
slot      release: id  2 | task 13257 | stop processing: n_tokens = 1978, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.609 (> 0.100 thold), f_keep = 0.594
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13387 | processing task, is_child = 0
slot update_slots: id  2 | task 13387 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1929
slot update_slots: id  2 | task 13387 | n_past = 1174, slot.prompt.tokens.size() = 1978, seq_id = 2, pos_min = 1081, n_swa = 128
slot update_slots: id  2 | task 13387 | restored context checkpoint (pos_min = 890, pos_max = 1786, size = 21.034 MiB)
slot update_slots: id  2 | task 13387 | n_tokens = 1174, memory_seq_rm [1174, end)
slot update_slots: id  2 | task 13387 | prompt processing progress, n_tokens = 1865, batch.n_tokens = 691, progress = 0.966822
slot update_slots: id  2 | task 13387 | n_tokens = 1865, memory_seq_rm [1865, end)
slot update_slots: id  2 | task 13387 | prompt processing progress, n_tokens = 1929, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13387 | prompt done, n_tokens = 1929, batch.n_tokens = 64
slot init_sampler: id  2 | task 13387 | init sampler, took 0.27 ms, tokens: text = 1929, total = 1929
slot update_slots: id  2 | task 13387 | created context checkpoint 8 of 8 (pos_min = 1047, pos_max = 1864, size = 19.182 MiB)
slot print_timing: id  2 | task 13387 | 
prompt eval time =    1231.05 ms /   755 tokens (    1.63 ms per token,   613.30 tokens per second)
       eval time =    1610.26 ms /    63 tokens (   25.56 ms per token,    39.12 tokens per second)
      total time =    2841.31 ms /   818 tokens
slot      release: id  2 | task 13387 | stop processing: n_tokens = 1991, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.704 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13452 | processing task, is_child = 0
slot update_slots: id  2 | task 13452 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2783
slot update_slots: id  2 | task 13452 | n_tokens = 1960, memory_seq_rm [1960, end)
slot update_slots: id  2 | task 13452 | prompt processing progress, n_tokens = 2719, batch.n_tokens = 759, progress = 0.977003
slot update_slots: id  2 | task 13452 | n_tokens = 2719, memory_seq_rm [2719, end)
slot update_slots: id  2 | task 13452 | prompt processing progress, n_tokens = 2783, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13452 | prompt done, n_tokens = 2783, batch.n_tokens = 64
slot init_sampler: id  2 | task 13452 | init sampler, took 0.40 ms, tokens: text = 2783, total = 2783
slot update_slots: id  2 | task 13452 | erasing old context checkpoint (pos_min = 0, pos_max = 837, size = 19.651 MiB)
slot update_slots: id  2 | task 13452 | created context checkpoint 8 of 8 (pos_min = 1822, pos_max = 2718, size = 21.034 MiB)
slot print_timing: id  2 | task 13452 | 
prompt eval time =    1108.94 ms /   823 tokens (    1.35 ms per token,   742.15 tokens per second)
       eval time =    1264.93 ms /    48 tokens (   26.35 ms per token,    37.95 tokens per second)
      total time =    2373.87 ms /   871 tokens
slot      release: id  2 | task 13452 | stop processing: n_tokens = 2830, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.929 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13502 | processing task, is_child = 0
slot update_slots: id  2 | task 13502 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3018
slot update_slots: id  2 | task 13502 | n_tokens = 2803, memory_seq_rm [2803, end)
slot update_slots: id  2 | task 13502 | prompt processing progress, n_tokens = 2954, batch.n_tokens = 151, progress = 0.978794
slot update_slots: id  2 | task 13502 | n_tokens = 2954, memory_seq_rm [2954, end)
slot update_slots: id  2 | task 13502 | prompt processing progress, n_tokens = 3018, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13502 | prompt done, n_tokens = 3018, batch.n_tokens = 64
slot init_sampler: id  2 | task 13502 | init sampler, took 0.61 ms, tokens: text = 3018, total = 3018
slot update_slots: id  2 | task 13502 | erasing old context checkpoint (pos_min = 130, pos_max = 1026, size = 21.034 MiB)
slot update_slots: id  2 | task 13502 | created context checkpoint 8 of 8 (pos_min = 2057, pos_max = 2953, size = 21.034 MiB)
slot print_timing: id  2 | task 13502 | 
prompt eval time =     447.02 ms /   215 tokens (    2.08 ms per token,   480.96 tokens per second)
       eval time =     755.37 ms /    28 tokens (   26.98 ms per token,    37.07 tokens per second)
      total time =    1202.39 ms /   243 tokens
slot      release: id  2 | task 13502 | stop processing: n_tokens = 3045, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.816 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13532 | processing task, is_child = 0
slot update_slots: id  2 | task 13532 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3700
slot update_slots: id  2 | task 13532 | n_tokens = 3018, memory_seq_rm [3018, end)
slot update_slots: id  2 | task 13532 | prompt processing progress, n_tokens = 3636, batch.n_tokens = 618, progress = 0.982703
slot update_slots: id  2 | task 13532 | n_tokens = 3636, memory_seq_rm [3636, end)
slot update_slots: id  2 | task 13532 | prompt processing progress, n_tokens = 3700, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13532 | prompt done, n_tokens = 3700, batch.n_tokens = 64
slot init_sampler: id  2 | task 13532 | init sampler, took 0.52 ms, tokens: text = 3700, total = 3700
slot update_slots: id  2 | task 13532 | erasing old context checkpoint (pos_min = 587, pos_max = 1109, size = 12.264 MiB)
slot update_slots: id  2 | task 13532 | created context checkpoint 8 of 8 (pos_min = 2739, pos_max = 3635, size = 21.034 MiB)
slot print_timing: id  2 | task 13532 | 
prompt eval time =    1008.21 ms /   682 tokens (    1.48 ms per token,   676.45 tokens per second)
       eval time =    1309.89 ms /    49 tokens (   26.73 ms per token,    37.41 tokens per second)
      total time =    2318.11 ms /   731 tokens
slot      release: id  2 | task 13532 | stop processing: n_tokens = 3748, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13583 | processing task, is_child = 0
slot update_slots: id  2 | task 13583 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3795
slot update_slots: id  2 | task 13583 | n_tokens = 3718, memory_seq_rm [3718, end)
slot update_slots: id  2 | task 13583 | prompt processing progress, n_tokens = 3731, batch.n_tokens = 13, progress = 0.983136
slot update_slots: id  2 | task 13583 | n_tokens = 3731, memory_seq_rm [3731, end)
slot update_slots: id  2 | task 13583 | prompt processing progress, n_tokens = 3795, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13583 | prompt done, n_tokens = 3795, batch.n_tokens = 64
slot init_sampler: id  2 | task 13583 | init sampler, took 0.53 ms, tokens: text = 3795, total = 3795
slot update_slots: id  2 | task 13583 | erasing old context checkpoint (pos_min = 651, pos_max = 1298, size = 15.195 MiB)
slot update_slots: id  2 | task 13583 | created context checkpoint 8 of 8 (pos_min = 2854, pos_max = 3730, size = 20.565 MiB)
slot print_timing: id  2 | task 13583 | 
prompt eval time =     275.98 ms /    77 tokens (    3.58 ms per token,   279.00 tokens per second)
       eval time =     909.48 ms /    34 tokens (   26.75 ms per token,    37.38 tokens per second)
      total time =    1185.47 ms /   111 tokens
slot      release: id  2 | task 13583 | stop processing: n_tokens = 3828, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13619 | processing task, is_child = 0
slot update_slots: id  2 | task 13619 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3860
slot update_slots: id  2 | task 13619 | n_tokens = 3805, memory_seq_rm [3805, end)
slot update_slots: id  2 | task 13619 | prompt processing progress, n_tokens = 3860, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  2 | task 13619 | prompt done, n_tokens = 3860, batch.n_tokens = 55
slot init_sampler: id  2 | task 13619 | init sampler, took 0.54 ms, tokens: text = 3860, total = 3860
slot update_slots: id  2 | task 13619 | erasing old context checkpoint (pos_min = 814, pos_max = 1461, size = 15.195 MiB)
slot update_slots: id  2 | task 13619 | created context checkpoint 8 of 8 (pos_min = 2951, pos_max = 3804, size = 20.026 MiB)
slot print_timing: id  2 | task 13619 | 
prompt eval time =     253.76 ms /    55 tokens (    4.61 ms per token,   216.74 tokens per second)
       eval time =     898.87 ms /    35 tokens (   25.68 ms per token,    38.94 tokens per second)
      total time =    1152.63 ms /    90 tokens
slot      release: id  2 | task 13619 | stop processing: n_tokens = 3894, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13655 | processing task, is_child = 0
slot update_slots: id  2 | task 13655 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3968
slot update_slots: id  2 | task 13655 | n_tokens = 3870, memory_seq_rm [3870, end)
slot update_slots: id  2 | task 13655 | prompt processing progress, n_tokens = 3904, batch.n_tokens = 34, progress = 0.983871
slot update_slots: id  2 | task 13655 | n_tokens = 3904, memory_seq_rm [3904, end)
slot update_slots: id  2 | task 13655 | prompt processing progress, n_tokens = 3968, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13655 | prompt done, n_tokens = 3968, batch.n_tokens = 64
slot init_sampler: id  2 | task 13655 | init sampler, took 0.79 ms, tokens: text = 3968, total = 3968
slot update_slots: id  2 | task 13655 | erasing old context checkpoint (pos_min = 878, pos_max = 1603, size = 17.024 MiB)
slot update_slots: id  2 | task 13655 | created context checkpoint 8 of 8 (pos_min = 3027, pos_max = 3903, size = 20.565 MiB)
slot print_timing: id  2 | task 13655 | 
prompt eval time =     316.30 ms /    98 tokens (    3.23 ms per token,   309.83 tokens per second)
       eval time =     992.95 ms /    38 tokens (   26.13 ms per token,    38.27 tokens per second)
      total time =    1309.25 ms /   136 tokens
slot      release: id  2 | task 13655 | stop processing: n_tokens = 4005, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.724 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13695 | processing task, is_child = 0
slot update_slots: id  2 | task 13695 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5494
slot update_slots: id  2 | task 13695 | n_tokens = 3978, memory_seq_rm [3978, end)
slot update_slots: id  2 | task 13695 | prompt processing progress, n_tokens = 5430, batch.n_tokens = 1452, progress = 0.988351
slot update_slots: id  2 | task 13695 | n_tokens = 5430, memory_seq_rm [5430, end)
slot update_slots: id  2 | task 13695 | prompt processing progress, n_tokens = 5494, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13695 | prompt done, n_tokens = 5494, batch.n_tokens = 64
slot init_sampler: id  2 | task 13695 | init sampler, took 0.78 ms, tokens: text = 5494, total = 5494
slot update_slots: id  2 | task 13695 | erasing old context checkpoint (pos_min = 890, pos_max = 1786, size = 21.034 MiB)
slot update_slots: id  2 | task 13695 | created context checkpoint 8 of 8 (pos_min = 4533, pos_max = 5429, size = 21.034 MiB)
slot print_timing: id  2 | task 13695 | 
prompt eval time =    1862.03 ms /  1516 tokens (    1.23 ms per token,   814.17 tokens per second)
       eval time =    1088.50 ms /    41 tokens (   26.55 ms per token,    37.67 tokens per second)
      total time =    2950.53 ms /  1557 tokens
slot      release: id  2 | task 13695 | stop processing: n_tokens = 5534, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.735 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13738 | processing task, is_child = 0
slot update_slots: id  2 | task 13738 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7494
slot update_slots: id  2 | task 13738 | n_tokens = 5508, memory_seq_rm [5508, end)
slot update_slots: id  2 | task 13738 | prompt processing progress, n_tokens = 7430, batch.n_tokens = 1922, progress = 0.991460
slot update_slots: id  2 | task 13738 | n_tokens = 7430, memory_seq_rm [7430, end)
slot update_slots: id  2 | task 13738 | prompt processing progress, n_tokens = 7494, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13738 | prompt done, n_tokens = 7494, batch.n_tokens = 64
slot init_sampler: id  2 | task 13738 | init sampler, took 1.05 ms, tokens: text = 7494, total = 7494
slot update_slots: id  2 | task 13738 | erasing old context checkpoint (pos_min = 1047, pos_max = 1864, size = 19.182 MiB)
slot update_slots: id  2 | task 13738 | created context checkpoint 8 of 8 (pos_min = 6533, pos_max = 7429, size = 21.034 MiB)
slot print_timing: id  2 | task 13738 | 
prompt eval time =    2561.07 ms /  1986 tokens (    1.29 ms per token,   775.46 tokens per second)
       eval time =     938.74 ms /    35 tokens (   26.82 ms per token,    37.28 tokens per second)
      total time =    3499.81 ms /  2021 tokens
slot      release: id  2 | task 13738 | stop processing: n_tokens = 7528, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.763 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13775 | processing task, is_child = 0
slot update_slots: id  2 | task 13775 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9833
slot update_slots: id  2 | task 13775 | n_tokens = 7502, memory_seq_rm [7502, end)
slot update_slots: id  2 | task 13775 | prompt processing progress, n_tokens = 9550, batch.n_tokens = 2048, progress = 0.971219
slot update_slots: id  2 | task 13775 | n_tokens = 9550, memory_seq_rm [9550, end)
slot update_slots: id  2 | task 13775 | prompt processing progress, n_tokens = 9769, batch.n_tokens = 219, progress = 0.993491
slot update_slots: id  2 | task 13775 | n_tokens = 9769, memory_seq_rm [9769, end)
slot update_slots: id  2 | task 13775 | prompt processing progress, n_tokens = 9833, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13775 | prompt done, n_tokens = 9833, batch.n_tokens = 64
slot init_sampler: id  2 | task 13775 | init sampler, took 1.37 ms, tokens: text = 9833, total = 9833
slot update_slots: id  2 | task 13775 | erasing old context checkpoint (pos_min = 1822, pos_max = 2718, size = 21.034 MiB)
slot update_slots: id  2 | task 13775 | created context checkpoint 8 of 8 (pos_min = 8872, pos_max = 9768, size = 21.034 MiB)
slot print_timing: id  2 | task 13775 | 
prompt eval time =    3228.32 ms /  2331 tokens (    1.38 ms per token,   722.05 tokens per second)
       eval time =   32762.48 ms /  1174 tokens (   27.91 ms per token,    35.83 tokens per second)
      total time =   35990.80 ms /  3505 tokens
slot      release: id  2 | task 13775 | stop processing: n_tokens = 11006, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 14952 | processing task, is_child = 0
slot update_slots: id  2 | task 14952 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11108
slot update_slots: id  2 | task 14952 | n_tokens = 10978, memory_seq_rm [10978, end)
slot update_slots: id  2 | task 14952 | prompt processing progress, n_tokens = 11044, batch.n_tokens = 66, progress = 0.994238
slot update_slots: id  2 | task 14952 | n_tokens = 11044, memory_seq_rm [11044, end)
slot update_slots: id  2 | task 14952 | prompt processing progress, n_tokens = 11108, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 14952 | prompt done, n_tokens = 11108, batch.n_tokens = 64
slot init_sampler: id  2 | task 14952 | init sampler, took 1.53 ms, tokens: text = 11108, total = 11108
slot update_slots: id  2 | task 14952 | erasing old context checkpoint (pos_min = 2057, pos_max = 2953, size = 21.034 MiB)
slot update_slots: id  2 | task 14952 | created context checkpoint 8 of 8 (pos_min = 10147, pos_max = 11043, size = 21.034 MiB)
slot print_timing: id  2 | task 14952 | 
prompt eval time =     419.38 ms /   130 tokens (    3.23 ms per token,   309.98 tokens per second)
       eval time =    4431.81 ms /   170 tokens (   26.07 ms per token,    38.36 tokens per second)
      total time =    4851.18 ms /   300 tokens
slot      release: id  2 | task 14952 | stop processing: n_tokens = 11277, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 15124 | processing task, is_child = 0
slot update_slots: id  2 | task 15124 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11322
slot update_slots: id  2 | task 15124 | n_tokens = 11250, memory_seq_rm [11250, end)
slot update_slots: id  2 | task 15124 | prompt processing progress, n_tokens = 11258, batch.n_tokens = 8, progress = 0.994347
slot update_slots: id  2 | task 15124 | n_tokens = 11258, memory_seq_rm [11258, end)
slot update_slots: id  2 | task 15124 | prompt processing progress, n_tokens = 11322, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 15124 | prompt done, n_tokens = 11322, batch.n_tokens = 64
slot init_sampler: id  2 | task 15124 | init sampler, took 1.59 ms, tokens: text = 11322, total = 11322
slot update_slots: id  2 | task 15124 | erasing old context checkpoint (pos_min = 2739, pos_max = 3635, size = 21.034 MiB)
slot update_slots: id  2 | task 15124 | created context checkpoint 8 of 8 (pos_min = 10380, pos_max = 11257, size = 20.588 MiB)
slot print_timing: id  2 | task 15124 | 
prompt eval time =     263.44 ms /    72 tokens (    3.66 ms per token,   273.31 tokens per second)
       eval time =   15328.30 ms /   592 tokens (   25.89 ms per token,    38.62 tokens per second)
      total time =   15591.74 ms /   664 tokens
slot      release: id  2 | task 15124 | stop processing: n_tokens = 11913, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.955 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 15718 | processing task, is_child = 0
slot update_slots: id  2 | task 15718 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11957
slot update_slots: id  2 | task 15718 | n_tokens = 11413, memory_seq_rm [11413, end)
slot update_slots: id  2 | task 15718 | prompt processing progress, n_tokens = 11893, batch.n_tokens = 480, progress = 0.994648
slot update_slots: id  2 | task 15718 | n_tokens = 11893, memory_seq_rm [11893, end)
slot update_slots: id  2 | task 15718 | prompt processing progress, n_tokens = 11957, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 15718 | prompt done, n_tokens = 11957, batch.n_tokens = 64
slot init_sampler: id  2 | task 15718 | init sampler, took 1.94 ms, tokens: text = 11957, total = 11957
slot update_slots: id  2 | task 15718 | erasing old context checkpoint (pos_min = 2854, pos_max = 3730, size = 20.565 MiB)
slot update_slots: id  2 | task 15718 | created context checkpoint 8 of 8 (pos_min = 11286, pos_max = 11892, size = 14.234 MiB)
slot print_timing: id  2 | task 15718 | 
prompt eval time =     804.03 ms /   544 tokens (    1.48 ms per token,   676.60 tokens per second)
       eval time =    2267.79 ms /    88 tokens (   25.77 ms per token,    38.80 tokens per second)
      total time =    3071.82 ms /   632 tokens
slot      release: id  2 | task 15718 | stop processing: n_tokens = 12044, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 15808 | processing task, is_child = 0
slot update_slots: id  2 | task 15808 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12088
slot update_slots: id  2 | task 15808 | n_tokens = 11974, memory_seq_rm [11974, end)
slot update_slots: id  2 | task 15808 | prompt processing progress, n_tokens = 12024, batch.n_tokens = 50, progress = 0.994705
slot update_slots: id  2 | task 15808 | n_tokens = 12024, memory_seq_rm [12024, end)
slot update_slots: id  2 | task 15808 | prompt processing progress, n_tokens = 12088, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 15808 | prompt done, n_tokens = 12088, batch.n_tokens = 64
slot init_sampler: id  2 | task 15808 | init sampler, took 1.69 ms, tokens: text = 12088, total = 12088
slot update_slots: id  2 | task 15808 | erasing old context checkpoint (pos_min = 2951, pos_max = 3804, size = 20.026 MiB)
slot update_slots: id  2 | task 15808 | created context checkpoint 8 of 8 (pos_min = 11286, pos_max = 12023, size = 17.306 MiB)
slot print_timing: id  2 | task 15808 | 
prompt eval time =     373.95 ms /   114 tokens (    3.28 ms per token,   304.85 tokens per second)
       eval time =    3591.26 ms /   138 tokens (   26.02 ms per token,    38.43 tokens per second)
      total time =    3965.22 ms /   252 tokens
slot      release: id  2 | task 15808 | stop processing: n_tokens = 12225, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 15948 | processing task, is_child = 0
slot update_slots: id  2 | task 15948 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12376
slot update_slots: id  2 | task 15948 | n_tokens = 12201, memory_seq_rm [12201, end)
slot update_slots: id  2 | task 15948 | prompt processing progress, n_tokens = 12312, batch.n_tokens = 111, progress = 0.994829
slot update_slots: id  2 | task 15948 | n_tokens = 12312, memory_seq_rm [12312, end)
slot update_slots: id  2 | task 15948 | prompt processing progress, n_tokens = 12376, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 15948 | prompt done, n_tokens = 12376, batch.n_tokens = 64
slot init_sampler: id  2 | task 15948 | init sampler, took 1.71 ms, tokens: text = 12376, total = 12376
slot update_slots: id  2 | task 15948 | erasing old context checkpoint (pos_min = 3027, pos_max = 3903, size = 20.565 MiB)
slot update_slots: id  2 | task 15948 | created context checkpoint 8 of 8 (pos_min = 11542, pos_max = 12311, size = 18.056 MiB)
slot print_timing: id  2 | task 15948 | 
prompt eval time =     477.84 ms /   175 tokens (    2.73 ms per token,   366.23 tokens per second)
       eval time =    1702.44 ms /    65 tokens (   26.19 ms per token,    38.18 tokens per second)
      total time =    2180.28 ms /   240 tokens
slot      release: id  2 | task 15948 | stop processing: n_tokens = 12440, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16015 | processing task, is_child = 0
slot update_slots: id  2 | task 16015 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12524
slot update_slots: id  2 | task 16015 | n_tokens = 12417, memory_seq_rm [12417, end)
slot update_slots: id  2 | task 16015 | prompt processing progress, n_tokens = 12460, batch.n_tokens = 43, progress = 0.994890
slot update_slots: id  2 | task 16015 | n_tokens = 12460, memory_seq_rm [12460, end)
slot update_slots: id  2 | task 16015 | prompt processing progress, n_tokens = 12524, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16015 | prompt done, n_tokens = 12524, batch.n_tokens = 64
slot init_sampler: id  2 | task 16015 | init sampler, took 1.74 ms, tokens: text = 12524, total = 12524
slot update_slots: id  2 | task 16015 | erasing old context checkpoint (pos_min = 4533, pos_max = 5429, size = 21.034 MiB)
slot update_slots: id  2 | task 16015 | created context checkpoint 8 of 8 (pos_min = 11690, pos_max = 12459, size = 18.056 MiB)
slot print_timing: id  2 | task 16015 | 
prompt eval time =     361.77 ms /   107 tokens (    3.38 ms per token,   295.77 tokens per second)
       eval time =    2575.65 ms /    98 tokens (   26.28 ms per token,    38.05 tokens per second)
      total time =    2937.42 ms /   205 tokens
slot      release: id  2 | task 16015 | stop processing: n_tokens = 12621, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16115 | processing task, is_child = 0
slot update_slots: id  2 | task 16115 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13045
slot update_slots: id  2 | task 16115 | n_tokens = 12594, memory_seq_rm [12594, end)
slot update_slots: id  2 | task 16115 | prompt processing progress, n_tokens = 12981, batch.n_tokens = 387, progress = 0.995094
slot update_slots: id  2 | task 16115 | n_tokens = 12981, memory_seq_rm [12981, end)
slot update_slots: id  2 | task 16115 | prompt processing progress, n_tokens = 13045, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16115 | prompt done, n_tokens = 13045, batch.n_tokens = 64
slot init_sampler: id  2 | task 16115 | init sampler, took 2.49 ms, tokens: text = 13045, total = 13045
slot update_slots: id  2 | task 16115 | erasing old context checkpoint (pos_min = 6533, pos_max = 7429, size = 21.034 MiB)
slot update_slots: id  2 | task 16115 | created context checkpoint 8 of 8 (pos_min = 12084, pos_max = 12980, size = 21.034 MiB)
slot print_timing: id  2 | task 16115 | 
prompt eval time =     733.82 ms /   451 tokens (    1.63 ms per token,   614.59 tokens per second)
       eval time =    8512.42 ms /   317 tokens (   26.85 ms per token,    37.24 tokens per second)
      total time =    9246.24 ms /   768 tokens
slot      release: id  2 | task 16115 | stop processing: n_tokens = 13361, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.166 (> 0.100 thold), f_keep = 0.011
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 13361, total state size = 334.336 MiB
srv          load:  - looking for better prompt, base f_keep = 0.011, sim = 0.166
srv        update:  - cache state: 4 prompts, 1602.910 MiB (limits: 8192.000 MiB, 56064 tokens, 206073 est)
srv        update:    - prompt 0x59014c2315e0:    9760 tokens, checkpoints:  8,   429.330 MiB
srv        update:    - prompt 0x59015104bc40:   15481 tokens, checkpoints:  8,   549.810 MiB
srv        update:    - prompt 0x590159548210:    1720 tokens, checkpoints:  5,   138.093 MiB
srv        update:    - prompt 0x59014d293850:   13361 tokens, checkpoints:  8,   485.677 MiB
srv  get_availabl: prompt cache update took 401.20 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16434 | processing task, is_child = 0
slot update_slots: id  2 | task 16434 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 889
slot update_slots: id  2 | task 16434 | n_past = 148, slot.prompt.tokens.size() = 13361, seq_id = 2, pos_min = 12464, n_swa = 128
slot update_slots: id  2 | task 16434 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 16434 | erased invalidated context checkpoint (pos_min = 8872, pos_max = 9768, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 16434 | erased invalidated context checkpoint (pos_min = 10147, pos_max = 11043, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 16434 | erased invalidated context checkpoint (pos_min = 10380, pos_max = 11257, n_swa = 128, size = 20.588 MiB)
slot update_slots: id  2 | task 16434 | erased invalidated context checkpoint (pos_min = 11286, pos_max = 11892, n_swa = 128, size = 14.234 MiB)
slot update_slots: id  2 | task 16434 | erased invalidated context checkpoint (pos_min = 11286, pos_max = 12023, n_swa = 128, size = 17.306 MiB)
slot update_slots: id  2 | task 16434 | erased invalidated context checkpoint (pos_min = 11542, pos_max = 12311, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  2 | task 16434 | erased invalidated context checkpoint (pos_min = 11690, pos_max = 12459, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  2 | task 16434 | erased invalidated context checkpoint (pos_min = 12084, pos_max = 12980, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 16434 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 16434 | prompt processing progress, n_tokens = 825, batch.n_tokens = 825, progress = 0.928009
slot update_slots: id  2 | task 16434 | n_tokens = 825, memory_seq_rm [825, end)
slot update_slots: id  2 | task 16434 | prompt processing progress, n_tokens = 889, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16434 | prompt done, n_tokens = 889, batch.n_tokens = 64
slot init_sampler: id  2 | task 16434 | init sampler, took 0.13 ms, tokens: text = 889, total = 889
slot update_slots: id  2 | task 16434 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 824, size = 19.346 MiB)
slot print_timing: id  2 | task 16434 | 
prompt eval time =    1116.04 ms /   889 tokens (    1.26 ms per token,   796.57 tokens per second)
       eval time =    1266.42 ms /    51 tokens (   24.83 ms per token,    40.27 tokens per second)
      total time =    2382.46 ms /   940 tokens
slot      release: id  2 | task 16434 | stop processing: n_tokens = 939, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.848 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16487 | processing task, is_child = 0
slot update_slots: id  2 | task 16487 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1086
slot update_slots: id  2 | task 16487 | n_tokens = 921, memory_seq_rm [921, end)
slot update_slots: id  2 | task 16487 | prompt processing progress, n_tokens = 1022, batch.n_tokens = 101, progress = 0.941068
slot update_slots: id  2 | task 16487 | n_tokens = 1022, memory_seq_rm [1022, end)
slot update_slots: id  2 | task 16487 | prompt processing progress, n_tokens = 1086, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16487 | prompt done, n_tokens = 1086, batch.n_tokens = 64
slot init_sampler: id  2 | task 16487 | init sampler, took 0.16 ms, tokens: text = 1086, total = 1086
slot update_slots: id  2 | task 16487 | created context checkpoint 2 of 8 (pos_min = 125, pos_max = 1021, size = 21.034 MiB)
slot print_timing: id  2 | task 16487 | 
prompt eval time =     421.10 ms /   165 tokens (    2.55 ms per token,   391.83 tokens per second)
       eval time =    1836.61 ms /    73 tokens (   25.16 ms per token,    39.75 tokens per second)
      total time =    2257.70 ms /   238 tokens
slot      release: id  2 | task 16487 | stop processing: n_tokens = 1158, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.768
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16562 | processing task, is_child = 0
slot update_slots: id  2 | task 16562 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 889
slot update_slots: id  2 | task 16562 | need to evaluate at least 1 token for each active slot (n_past = 889, task.n_tokens() = 889)
slot update_slots: id  2 | task 16562 | n_past was set to 888
slot update_slots: id  2 | task 16562 | n_tokens = 888, memory_seq_rm [888, end)
slot update_slots: id  2 | task 16562 | prompt processing progress, n_tokens = 889, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  2 | task 16562 | prompt done, n_tokens = 889, batch.n_tokens = 1
slot init_sampler: id  2 | task 16562 | init sampler, took 0.13 ms, tokens: text = 889, total = 889
slot print_timing: id  2 | task 16562 | 
prompt eval time =      34.79 ms /     1 tokens (   34.79 ms per token,    28.74 tokens per second)
       eval time =    1681.74 ms /    65 tokens (   25.87 ms per token,    38.65 tokens per second)
      total time =    1716.54 ms /    66 tokens
slot      release: id  2 | task 16562 | stop processing: n_tokens = 953, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.850 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16628 | processing task, is_child = 0
slot update_slots: id  2 | task 16628 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1100
slot update_slots: id  2 | task 16628 | n_tokens = 935, memory_seq_rm [935, end)
slot update_slots: id  2 | task 16628 | prompt processing progress, n_tokens = 1036, batch.n_tokens = 101, progress = 0.941818
slot update_slots: id  2 | task 16628 | n_tokens = 1036, memory_seq_rm [1036, end)
slot update_slots: id  2 | task 16628 | prompt processing progress, n_tokens = 1100, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16628 | prompt done, n_tokens = 1100, batch.n_tokens = 64
slot init_sampler: id  2 | task 16628 | init sampler, took 0.20 ms, tokens: text = 1100, total = 1100
slot print_timing: id  2 | task 16628 | 
prompt eval time =     425.96 ms /   165 tokens (    2.58 ms per token,   387.36 tokens per second)
       eval time =    3312.44 ms /   125 tokens (   26.50 ms per token,    37.74 tokens per second)
      total time =    3738.40 ms /   290 tokens
slot      release: id  2 | task 16628 | stop processing: n_tokens = 1224, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
