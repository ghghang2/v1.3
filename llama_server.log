ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: using cached file: /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14992 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1578 MiB
llama_params_fit_impl: context size reduced from 131072 to 64000 -> need 1580 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.78 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 64000
llama_context: n_ctx_seq     = 64000
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (64000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 64000 cells
llama_kv_cache:      CUDA0 KV buffer size =  1500.00 MiB
llama_kv_cache: size = 1500.00 MiB ( 64000 cells,  12 layers,  4/1 seqs), K (f16):  750.00 MiB, V (f16):  750.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   132.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 88.57 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  1 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  2 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  3 | task -1 | new slot, n_ctx = 64000
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-10

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 678
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 614, batch.n_tokens = 614, progress = 0.905605
slot update_slots: id  3 | task 0 | n_tokens = 614, memory_seq_rm [614, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 678, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 678, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.13 ms, tokens: text = 678, total = 678
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 613, size = 14.398 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =    1158.77 ms /   678 tokens (    1.71 ms per token,   585.10 tokens per second)
       eval time =    1045.20 ms /    48 tokens (   21.78 ms per token,    45.92 tokens per second)
      total time =    2203.97 ms /   726 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 725, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.477 (> 0.100 thold), f_keep = 0.935
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 50 | processing task, is_child = 0
slot update_slots: id  3 | task 50 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1421
slot update_slots: id  3 | task 50 | n_tokens = 678, memory_seq_rm [678, end)
slot update_slots: id  3 | task 50 | prompt processing progress, n_tokens = 1357, batch.n_tokens = 679, progress = 0.954961
slot update_slots: id  3 | task 50 | n_tokens = 1357, memory_seq_rm [1357, end)
slot update_slots: id  3 | task 50 | prompt processing progress, n_tokens = 1421, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 50 | prompt done, n_tokens = 1421, batch.n_tokens = 64
slot init_sampler: id  3 | task 50 | init sampler, took 0.58 ms, tokens: text = 1421, total = 1421
slot update_slots: id  3 | task 50 | created context checkpoint 2 of 8 (pos_min = 333, pos_max = 1356, size = 24.012 MiB)
slot print_timing: id  3 | task 50 | 
prompt eval time =     806.80 ms /   743 tokens (    1.09 ms per token,   920.93 tokens per second)
       eval time =    1975.61 ms /    84 tokens (   23.52 ms per token,    42.52 tokens per second)
      total time =    2782.41 ms /   827 tokens
slot      release: id  3 | task 50 | stop processing: n_tokens = 1504, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.694 (> 0.100 thold), f_keep = 0.945
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 136 | processing task, is_child = 0
slot update_slots: id  3 | task 136 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2048
slot update_slots: id  3 | task 136 | n_tokens = 1421, memory_seq_rm [1421, end)
slot update_slots: id  3 | task 136 | prompt processing progress, n_tokens = 1984, batch.n_tokens = 563, progress = 0.968750
slot update_slots: id  3 | task 136 | n_tokens = 1984, memory_seq_rm [1984, end)
slot update_slots: id  3 | task 136 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 136 | prompt done, n_tokens = 2048, batch.n_tokens = 64
slot init_sampler: id  3 | task 136 | init sampler, took 0.35 ms, tokens: text = 2048, total = 2048
slot update_slots: id  3 | task 136 | created context checkpoint 3 of 8 (pos_min = 960, pos_max = 1983, size = 24.012 MiB)
slot print_timing: id  3 | task 136 | 
prompt eval time =     764.60 ms /   627 tokens (    1.22 ms per token,   820.04 tokens per second)
       eval time =    2305.68 ms /   100 tokens (   23.06 ms per token,    43.37 tokens per second)
      total time =    3070.28 ms /   727 tokens
slot      release: id  3 | task 136 | stop processing: n_tokens = 2147, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.967 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 238 | processing task, is_child = 0
slot update_slots: id  3 | task 238 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2118
slot update_slots: id  3 | task 238 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 238 | prompt processing progress, n_tokens = 2054, batch.n_tokens = 6, progress = 0.969783
slot update_slots: id  3 | task 238 | n_tokens = 2054, memory_seq_rm [2054, end)
slot update_slots: id  3 | task 238 | prompt processing progress, n_tokens = 2118, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 238 | prompt done, n_tokens = 2118, batch.n_tokens = 64
slot init_sampler: id  3 | task 238 | init sampler, took 0.35 ms, tokens: text = 2118, total = 2118
slot update_slots: id  3 | task 238 | created context checkpoint 4 of 8 (pos_min = 1123, pos_max = 2053, size = 21.831 MiB)
slot print_timing: id  3 | task 238 | 
prompt eval time =     228.37 ms /    70 tokens (    3.26 ms per token,   306.51 tokens per second)
       eval time =    2908.67 ms /   127 tokens (   22.90 ms per token,    43.66 tokens per second)
      total time =    3137.05 ms /   197 tokens
slot      release: id  3 | task 238 | stop processing: n_tokens = 2244, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.944
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 367 | processing task, is_child = 0
slot update_slots: id  3 | task 367 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2179
slot update_slots: id  3 | task 367 | n_tokens = 2118, memory_seq_rm [2118, end)
slot update_slots: id  3 | task 367 | prompt processing progress, n_tokens = 2179, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  3 | task 367 | prompt done, n_tokens = 2179, batch.n_tokens = 61
slot init_sampler: id  3 | task 367 | init sampler, took 2.30 ms, tokens: text = 2179, total = 2179
slot print_timing: id  3 | task 367 | 
prompt eval time =     220.73 ms /    61 tokens (    3.62 ms per token,   276.35 tokens per second)
       eval time =    1404.81 ms /    60 tokens (   23.41 ms per token,    42.71 tokens per second)
      total time =    1625.54 ms /   121 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 367 | stop processing: n_tokens = 2238, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 428 | processing task, is_child = 0
slot update_slots: id  3 | task 428 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2246
slot update_slots: id  3 | task 428 | n_tokens = 2179, memory_seq_rm [2179, end)
slot update_slots: id  3 | task 428 | prompt processing progress, n_tokens = 2182, batch.n_tokens = 3, progress = 0.971505
slot update_slots: id  3 | task 428 | n_tokens = 2182, memory_seq_rm [2182, end)
slot update_slots: id  3 | task 428 | prompt processing progress, n_tokens = 2246, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 428 | prompt done, n_tokens = 2246, batch.n_tokens = 64
slot init_sampler: id  3 | task 428 | init sampler, took 0.37 ms, tokens: text = 2246, total = 2246
slot update_slots: id  3 | task 428 | created context checkpoint 5 of 8 (pos_min = 1220, pos_max = 2181, size = 22.558 MiB)
slot print_timing: id  3 | task 428 | 
prompt eval time =     201.68 ms /    67 tokens (    3.01 ms per token,   332.21 tokens per second)
       eval time =    4002.17 ms /   167 tokens (   23.97 ms per token,    41.73 tokens per second)
      total time =    4203.85 ms /   234 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 428 | stop processing: n_tokens = 2412, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.931
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 597 | processing task, is_child = 0
slot update_slots: id  3 | task 597 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2315
slot update_slots: id  3 | task 597 | n_tokens = 2246, memory_seq_rm [2246, end)
slot update_slots: id  3 | task 597 | prompt processing progress, n_tokens = 2251, batch.n_tokens = 5, progress = 0.972354
slot update_slots: id  3 | task 597 | n_tokens = 2251, memory_seq_rm [2251, end)
slot update_slots: id  3 | task 597 | prompt processing progress, n_tokens = 2315, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 597 | prompt done, n_tokens = 2315, batch.n_tokens = 64
slot init_sampler: id  3 | task 597 | init sampler, took 0.44 ms, tokens: text = 2315, total = 2315
slot update_slots: id  3 | task 597 | created context checkpoint 6 of 8 (pos_min = 1388, pos_max = 2250, size = 20.237 MiB)
slot print_timing: id  3 | task 597 | 
prompt eval time =     213.29 ms /    69 tokens (    3.09 ms per token,   323.50 tokens per second)
       eval time =    3724.64 ms /   161 tokens (   23.13 ms per token,    43.23 tokens per second)
      total time =    3937.93 ms /   230 tokens
slot      release: id  3 | task 597 | stop processing: n_tokens = 2475, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.935
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 760 | processing task, is_child = 0
slot update_slots: id  3 | task 760 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2377
slot update_slots: id  3 | task 760 | n_tokens = 2315, memory_seq_rm [2315, end)
slot update_slots: id  3 | task 760 | prompt processing progress, n_tokens = 2377, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 760 | prompt done, n_tokens = 2377, batch.n_tokens = 62
slot init_sampler: id  3 | task 760 | init sampler, took 0.37 ms, tokens: text = 2377, total = 2377
slot print_timing: id  3 | task 760 | 
prompt eval time =     148.55 ms /    62 tokens (    2.40 ms per token,   417.38 tokens per second)
       eval time =    1883.50 ms /    82 tokens (   22.97 ms per token,    43.54 tokens per second)
      total time =    2032.04 ms /   144 tokens
slot      release: id  3 | task 760 | stop processing: n_tokens = 2458, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.718 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 843 | processing task, is_child = 0
slot update_slots: id  3 | task 843 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3312
slot update_slots: id  3 | task 843 | n_tokens = 2377, memory_seq_rm [2377, end)
slot update_slots: id  3 | task 843 | prompt processing progress, n_tokens = 3248, batch.n_tokens = 871, progress = 0.980676
slot update_slots: id  3 | task 843 | n_tokens = 3248, memory_seq_rm [3248, end)
slot update_slots: id  3 | task 843 | prompt processing progress, n_tokens = 3312, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 843 | prompt done, n_tokens = 3312, batch.n_tokens = 64
slot init_sampler: id  3 | task 843 | init sampler, took 0.54 ms, tokens: text = 3312, total = 3312
slot update_slots: id  3 | task 843 | created context checkpoint 7 of 8 (pos_min = 2224, pos_max = 3247, size = 24.012 MiB)
slot print_timing: id  3 | task 843 | 
prompt eval time =    1026.06 ms /   935 tokens (    1.10 ms per token,   911.26 tokens per second)
       eval time =    2165.88 ms /    92 tokens (   23.54 ms per token,    42.48 tokens per second)
      total time =    3191.93 ms /  1027 tokens
slot      release: id  3 | task 843 | stop processing: n_tokens = 3403, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.631 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 937 | processing task, is_child = 0
slot update_slots: id  3 | task 937 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5247
slot update_slots: id  3 | task 937 | n_tokens = 3312, memory_seq_rm [3312, end)
slot update_slots: id  3 | task 937 | prompt processing progress, n_tokens = 5183, batch.n_tokens = 1871, progress = 0.987803
slot update_slots: id  3 | task 937 | n_tokens = 5183, memory_seq_rm [5183, end)
slot update_slots: id  3 | task 937 | prompt processing progress, n_tokens = 5247, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 937 | prompt done, n_tokens = 5247, batch.n_tokens = 64
slot init_sampler: id  3 | task 937 | init sampler, took 0.99 ms, tokens: text = 5247, total = 5247
slot update_slots: id  3 | task 937 | created context checkpoint 8 of 8 (pos_min = 4159, pos_max = 5182, size = 24.012 MiB)
slot print_timing: id  3 | task 937 | 
prompt eval time =    1977.42 ms /  1935 tokens (    1.02 ms per token,   978.55 tokens per second)
       eval time =    4334.32 ms /   181 tokens (   23.95 ms per token,    41.76 tokens per second)
      total time =    6311.74 ms /  2116 tokens
slot      release: id  3 | task 937 | stop processing: n_tokens = 5427, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1120 | processing task, is_child = 0
slot update_slots: id  3 | task 1120 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5308
slot update_slots: id  3 | task 1120 | n_tokens = 5247, memory_seq_rm [5247, end)
slot update_slots: id  3 | task 1120 | prompt processing progress, n_tokens = 5308, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  3 | task 1120 | prompt done, n_tokens = 5308, batch.n_tokens = 61
slot init_sampler: id  3 | task 1120 | init sampler, took 0.78 ms, tokens: text = 5308, total = 5308
slot print_timing: id  3 | task 1120 | 
prompt eval time =     148.64 ms /    61 tokens (    2.44 ms per token,   410.38 tokens per second)
       eval time =    1543.99 ms /    66 tokens (   23.39 ms per token,    42.75 tokens per second)
      total time =    1692.63 ms /   127 tokens
slot      release: id  3 | task 1120 | stop processing: n_tokens = 5373, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1187 | processing task, is_child = 0
slot update_slots: id  3 | task 1187 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5370
slot update_slots: id  3 | task 1187 | n_tokens = 5308, memory_seq_rm [5308, end)
slot update_slots: id  3 | task 1187 | prompt processing progress, n_tokens = 5370, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 1187 | prompt done, n_tokens = 5370, batch.n_tokens = 62
slot init_sampler: id  3 | task 1187 | init sampler, took 0.83 ms, tokens: text = 5370, total = 5370
slot update_slots: id  3 | task 1187 | erasing old context checkpoint (pos_min = 0, pos_max = 613, size = 14.398 MiB)
slot update_slots: id  3 | task 1187 | created context checkpoint 8 of 8 (pos_min = 4403, pos_max = 5307, size = 21.222 MiB)
slot print_timing: id  3 | task 1187 | 
prompt eval time =     180.56 ms /    62 tokens (    2.91 ms per token,   343.37 tokens per second)
       eval time =    4415.64 ms /   188 tokens (   23.49 ms per token,    42.58 tokens per second)
      total time =    4596.20 ms /   250 tokens
slot      release: id  3 | task 1187 | stop processing: n_tokens = 5557, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1376 | processing task, is_child = 0
slot update_slots: id  3 | task 1376 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5432
slot update_slots: id  3 | task 1376 | n_tokens = 5370, memory_seq_rm [5370, end)
slot update_slots: id  3 | task 1376 | prompt processing progress, n_tokens = 5432, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 1376 | prompt done, n_tokens = 5432, batch.n_tokens = 62
slot init_sampler: id  3 | task 1376 | init sampler, took 0.83 ms, tokens: text = 5432, total = 5432
slot print_timing: id  3 | task 1376 | 
prompt eval time =     158.34 ms /    62 tokens (    2.55 ms per token,   391.55 tokens per second)
       eval time =   12330.87 ms /   516 tokens (   23.90 ms per token,    41.85 tokens per second)
      total time =   12489.22 ms /   578 tokens
slot      release: id  3 | task 1376 | stop processing: n_tokens = 5947, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.913
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1893 | processing task, is_child = 0
slot update_slots: id  3 | task 1893 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5501
slot update_slots: id  3 | task 1893 | n_tokens = 5432, memory_seq_rm [5432, end)
slot update_slots: id  3 | task 1893 | prompt processing progress, n_tokens = 5437, batch.n_tokens = 5, progress = 0.988366
slot update_slots: id  3 | task 1893 | n_tokens = 5437, memory_seq_rm [5437, end)
slot update_slots: id  3 | task 1893 | prompt processing progress, n_tokens = 5501, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1893 | prompt done, n_tokens = 5501, batch.n_tokens = 64
slot init_sampler: id  3 | task 1893 | init sampler, took 0.83 ms, tokens: text = 5501, total = 5501
slot update_slots: id  3 | task 1893 | erasing old context checkpoint (pos_min = 333, pos_max = 1356, size = 24.012 MiB)
slot update_slots: id  3 | task 1893 | created context checkpoint 8 of 8 (pos_min = 4923, pos_max = 5436, size = 12.053 MiB)
slot print_timing: id  3 | task 1893 | 
prompt eval time =     217.23 ms /    69 tokens (    3.15 ms per token,   317.64 tokens per second)
       eval time =    1307.21 ms /    55 tokens (   23.77 ms per token,    42.07 tokens per second)
      total time =    1524.44 ms /   124 tokens
slot      release: id  3 | task 1893 | stop processing: n_tokens = 5555, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.851 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1950 | processing task, is_child = 0
slot update_slots: id  3 | task 1950 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6461
slot update_slots: id  3 | task 1950 | n_tokens = 5501, memory_seq_rm [5501, end)
slot update_slots: id  3 | task 1950 | prompt processing progress, n_tokens = 6397, batch.n_tokens = 896, progress = 0.990094
slot update_slots: id  3 | task 1950 | n_tokens = 6397, memory_seq_rm [6397, end)
slot update_slots: id  3 | task 1950 | prompt processing progress, n_tokens = 6461, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1950 | prompt done, n_tokens = 6461, batch.n_tokens = 64
slot init_sampler: id  3 | task 1950 | init sampler, took 1.81 ms, tokens: text = 6461, total = 6461
slot update_slots: id  3 | task 1950 | erasing old context checkpoint (pos_min = 960, pos_max = 1983, size = 24.012 MiB)
slot update_slots: id  3 | task 1950 | created context checkpoint 8 of 8 (pos_min = 5373, pos_max = 6396, size = 24.012 MiB)
slot print_timing: id  3 | task 1950 | 
prompt eval time =    1080.00 ms /   960 tokens (    1.12 ms per token,   888.89 tokens per second)
       eval time =   18326.32 ms /   750 tokens (   24.44 ms per token,    40.92 tokens per second)
      total time =   19406.32 ms /  1710 tokens
slot      release: id  3 | task 1950 | stop processing: n_tokens = 7210, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.896
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2702 | processing task, is_child = 0
slot update_slots: id  3 | task 2702 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6523
slot update_slots: id  3 | task 2702 | n_tokens = 6461, memory_seq_rm [6461, end)
slot update_slots: id  3 | task 2702 | prompt processing progress, n_tokens = 6523, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 2702 | prompt done, n_tokens = 6523, batch.n_tokens = 62
slot init_sampler: id  3 | task 2702 | init sampler, took 1.11 ms, tokens: text = 6523, total = 6523
slot print_timing: id  3 | task 2702 | 
prompt eval time =     169.26 ms /    62 tokens (    2.73 ms per token,   366.31 tokens per second)
       eval time =    2447.93 ms /    99 tokens (   24.73 ms per token,    40.44 tokens per second)
      total time =    2617.19 ms /   161 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 2702 | stop processing: n_tokens = 6621, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2802 | processing task, is_child = 0
slot update_slots: id  3 | task 2802 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6585
slot update_slots: id  3 | task 2802 | n_tokens = 6523, memory_seq_rm [6523, end)
slot update_slots: id  3 | task 2802 | prompt processing progress, n_tokens = 6585, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 2802 | prompt done, n_tokens = 6585, batch.n_tokens = 62
slot init_sampler: id  3 | task 2802 | init sampler, took 1.42 ms, tokens: text = 6585, total = 6585
slot update_slots: id  3 | task 2802 | erasing old context checkpoint (pos_min = 1123, pos_max = 2053, size = 21.831 MiB)
slot update_slots: id  3 | task 2802 | created context checkpoint 8 of 8 (pos_min = 6304, pos_max = 6522, size = 5.136 MiB)
slot print_timing: id  3 | task 2802 | 
prompt eval time =     171.20 ms /    62 tokens (    2.76 ms per token,   362.14 tokens per second)
       eval time =   25845.51 ms /  1047 tokens (   24.69 ms per token,    40.51 tokens per second)
      total time =   26016.71 ms /  1109 tokens
slot      release: id  3 | task 2802 | stop processing: n_tokens = 7631, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.863
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3850 | processing task, is_child = 0
slot update_slots: id  3 | task 3850 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6647
slot update_slots: id  3 | task 3850 | n_past = 6585, slot.prompt.tokens.size() = 7631, seq_id = 3, pos_min = 6607, n_swa = 128
slot update_slots: id  3 | task 3850 | restored context checkpoint (pos_min = 6304, pos_max = 6522, size = 5.136 MiB)
slot update_slots: id  3 | task 3850 | n_tokens = 6522, memory_seq_rm [6522, end)
slot update_slots: id  3 | task 3850 | prompt processing progress, n_tokens = 6583, batch.n_tokens = 61, progress = 0.990372
slot update_slots: id  3 | task 3850 | n_tokens = 6583, memory_seq_rm [6583, end)
slot update_slots: id  3 | task 3850 | prompt processing progress, n_tokens = 6647, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3850 | prompt done, n_tokens = 6647, batch.n_tokens = 64
slot init_sampler: id  3 | task 3850 | init sampler, took 1.50 ms, tokens: text = 6647, total = 6647
slot print_timing: id  3 | task 3850 | 
prompt eval time =     338.33 ms /   125 tokens (    2.71 ms per token,   369.46 tokens per second)
       eval time =   31186.97 ms /  1270 tokens (   24.56 ms per token,    40.72 tokens per second)
      total time =   31525.30 ms /  1395 tokens
slot      release: id  3 | task 3850 | stop processing: n_tokens = 7916, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.395 (> 0.100 thold), f_keep = 0.078
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7916, total state size = 209.634 MiB
srv          load:  - looking for better prompt, base f_keep = 0.078, sim = 0.395
srv        update:  - cache state: 1 prompts, 362.875 MiB (limits: 8192.000 MiB, 64000 tokens, 178705 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv  get_availabl: prompt cache update took 278.00 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5122 | processing task, is_child = 0
slot update_slots: id  3 | task 5122 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1573
slot update_slots: id  3 | task 5122 | n_past = 621, slot.prompt.tokens.size() = 7916, seq_id = 3, pos_min = 6892, n_swa = 128
slot update_slots: id  3 | task 5122 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 1220, pos_max = 2181, n_swa = 128, size = 22.558 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 1388, pos_max = 2250, n_swa = 128, size = 20.237 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 2224, pos_max = 3247, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 4159, pos_max = 5182, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 4403, pos_max = 5307, n_swa = 128, size = 21.222 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 4923, pos_max = 5436, n_swa = 128, size = 12.053 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 5373, pos_max = 6396, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 6304, pos_max = 6522, n_swa = 128, size = 5.136 MiB)
slot update_slots: id  3 | task 5122 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 5122 | prompt processing progress, n_tokens = 1509, batch.n_tokens = 1509, progress = 0.959313
slot update_slots: id  3 | task 5122 | n_tokens = 1509, memory_seq_rm [1509, end)
slot update_slots: id  3 | task 5122 | prompt processing progress, n_tokens = 1573, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5122 | prompt done, n_tokens = 1573, batch.n_tokens = 64
slot init_sampler: id  3 | task 5122 | init sampler, took 0.25 ms, tokens: text = 1573, total = 1573
slot update_slots: id  3 | task 5122 | created context checkpoint 1 of 8 (pos_min = 485, pos_max = 1508, size = 24.012 MiB)
slot print_timing: id  3 | task 5122 | 
prompt eval time =    1546.52 ms /  1573 tokens (    0.98 ms per token,  1017.13 tokens per second)
       eval time =    3732.10 ms /   160 tokens (   23.33 ms per token,    42.87 tokens per second)
      total time =    5278.62 ms /  1733 tokens
slot      release: id  3 | task 5122 | stop processing: n_tokens = 1732, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.919 (> 0.100 thold), f_keep = 0.902
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5284 | processing task, is_child = 0
slot update_slots: id  3 | task 5284 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1701
slot update_slots: id  3 | task 5284 | n_tokens = 1563, memory_seq_rm [1563, end)
slot update_slots: id  3 | task 5284 | prompt processing progress, n_tokens = 1637, batch.n_tokens = 74, progress = 0.962375
slot update_slots: id  3 | task 5284 | n_tokens = 1637, memory_seq_rm [1637, end)
slot update_slots: id  3 | task 5284 | prompt processing progress, n_tokens = 1701, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5284 | prompt done, n_tokens = 1701, batch.n_tokens = 64
slot init_sampler: id  3 | task 5284 | init sampler, took 0.32 ms, tokens: text = 1701, total = 1701
slot update_slots: id  3 | task 5284 | created context checkpoint 2 of 8 (pos_min = 708, pos_max = 1636, size = 21.784 MiB)
slot print_timing: id  3 | task 5284 | 
prompt eval time =     451.32 ms /   138 tokens (    3.27 ms per token,   305.77 tokens per second)
       eval time =    2603.65 ms /   112 tokens (   23.25 ms per token,    43.02 tokens per second)
      total time =    3054.97 ms /   250 tokens
slot      release: id  3 | task 5284 | stop processing: n_tokens = 1812, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.684 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5398 | processing task, is_child = 0
slot update_slots: id  3 | task 5398 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2488
slot update_slots: id  3 | task 5398 | n_tokens = 1701, memory_seq_rm [1701, end)
slot update_slots: id  3 | task 5398 | prompt processing progress, n_tokens = 2424, batch.n_tokens = 723, progress = 0.974277
slot update_slots: id  3 | task 5398 | n_tokens = 2424, memory_seq_rm [2424, end)
slot update_slots: id  3 | task 5398 | prompt processing progress, n_tokens = 2488, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5398 | prompt done, n_tokens = 2488, batch.n_tokens = 64
slot init_sampler: id  3 | task 5398 | init sampler, took 0.41 ms, tokens: text = 2488, total = 2488
slot update_slots: id  3 | task 5398 | created context checkpoint 3 of 8 (pos_min = 1400, pos_max = 2423, size = 24.012 MiB)
slot print_timing: id  3 | task 5398 | 
prompt eval time =     877.19 ms /   787 tokens (    1.11 ms per token,   897.18 tokens per second)
       eval time =    4107.97 ms /   176 tokens (   23.34 ms per token,    42.84 tokens per second)
      total time =    4985.16 ms /   963 tokens
slot      release: id  3 | task 5398 | stop processing: n_tokens = 2663, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.662 (> 0.100 thold), f_keep = 0.934
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5576 | processing task, is_child = 0
slot update_slots: id  3 | task 5576 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3757
slot update_slots: id  3 | task 5576 | n_tokens = 2488, memory_seq_rm [2488, end)
slot update_slots: id  3 | task 5576 | prompt processing progress, n_tokens = 3693, batch.n_tokens = 1205, progress = 0.982965
slot update_slots: id  3 | task 5576 | n_tokens = 3693, memory_seq_rm [3693, end)
slot update_slots: id  3 | task 5576 | prompt processing progress, n_tokens = 3757, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5576 | prompt done, n_tokens = 3757, batch.n_tokens = 64
slot init_sampler: id  3 | task 5576 | init sampler, took 0.70 ms, tokens: text = 3757, total = 3757
slot update_slots: id  3 | task 5576 | created context checkpoint 4 of 8 (pos_min = 2669, pos_max = 3692, size = 24.012 MiB)
slot print_timing: id  3 | task 5576 | 
prompt eval time =    1348.92 ms /  1269 tokens (    1.06 ms per token,   940.75 tokens per second)
       eval time =   10171.63 ms /   424 tokens (   23.99 ms per token,    41.68 tokens per second)
      total time =   11520.56 ms /  1693 tokens
slot      release: id  3 | task 5576 | stop processing: n_tokens = 4180, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.899
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6002 | processing task, is_child = 0
slot update_slots: id  3 | task 6002 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3861
slot update_slots: id  3 | task 6002 | n_tokens = 3757, memory_seq_rm [3757, end)
slot update_slots: id  3 | task 6002 | prompt processing progress, n_tokens = 3797, batch.n_tokens = 40, progress = 0.983424
slot update_slots: id  3 | task 6002 | n_tokens = 3797, memory_seq_rm [3797, end)
slot update_slots: id  3 | task 6002 | prompt processing progress, n_tokens = 3861, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6002 | prompt done, n_tokens = 3861, batch.n_tokens = 64
slot init_sampler: id  3 | task 6002 | init sampler, took 0.90 ms, tokens: text = 3861, total = 3861
slot update_slots: id  3 | task 6002 | created context checkpoint 5 of 8 (pos_min = 3156, pos_max = 3796, size = 15.031 MiB)
slot print_timing: id  3 | task 6002 | 
prompt eval time =     295.87 ms /   104 tokens (    2.84 ms per token,   351.51 tokens per second)
       eval time =    4392.87 ms /   183 tokens (   24.00 ms per token,    41.66 tokens per second)
      total time =    4688.73 ms /   287 tokens
slot      release: id  3 | task 6002 | stop processing: n_tokens = 4043, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6187 | processing task, is_child = 0
slot update_slots: id  3 | task 6187 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4001
slot update_slots: id  3 | task 6187 | n_tokens = 3861, memory_seq_rm [3861, end)
slot update_slots: id  3 | task 6187 | prompt processing progress, n_tokens = 3937, batch.n_tokens = 76, progress = 0.984004
slot update_slots: id  3 | task 6187 | n_tokens = 3937, memory_seq_rm [3937, end)
slot update_slots: id  3 | task 6187 | prompt processing progress, n_tokens = 4001, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6187 | prompt done, n_tokens = 4001, batch.n_tokens = 64
slot init_sampler: id  3 | task 6187 | init sampler, took 0.79 ms, tokens: text = 4001, total = 4001
slot update_slots: id  3 | task 6187 | created context checkpoint 6 of 8 (pos_min = 3358, pos_max = 3936, size = 13.577 MiB)
slot print_timing: id  3 | task 6187 | 
prompt eval time =     364.42 ms /   140 tokens (    2.60 ms per token,   384.18 tokens per second)
       eval time =    6220.32 ms /   259 tokens (   24.02 ms per token,    41.64 tokens per second)
      total time =    6584.74 ms /   399 tokens
slot      release: id  3 | task 6187 | stop processing: n_tokens = 4259, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6448 | processing task, is_child = 0
slot update_slots: id  3 | task 6448 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4105
slot update_slots: id  3 | task 6448 | n_tokens = 4001, memory_seq_rm [4001, end)
slot update_slots: id  3 | task 6448 | prompt processing progress, n_tokens = 4041, batch.n_tokens = 40, progress = 0.984409
slot update_slots: id  3 | task 6448 | n_tokens = 4041, memory_seq_rm [4041, end)
slot update_slots: id  3 | task 6448 | prompt processing progress, n_tokens = 4105, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6448 | prompt done, n_tokens = 4105, batch.n_tokens = 64
slot init_sampler: id  3 | task 6448 | init sampler, took 0.62 ms, tokens: text = 4105, total = 4105
slot update_slots: id  3 | task 6448 | created context checkpoint 7 of 8 (pos_min = 3574, pos_max = 4040, size = 10.951 MiB)
slot print_timing: id  3 | task 6448 | 
prompt eval time =     298.05 ms /   104 tokens (    2.87 ms per token,   348.94 tokens per second)
       eval time =    2496.56 ms /   103 tokens (   24.24 ms per token,    41.26 tokens per second)
      total time =    2794.61 ms /   207 tokens
slot      release: id  3 | task 6448 | stop processing: n_tokens = 4207, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6553 | processing task, is_child = 0
slot update_slots: id  3 | task 6553 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4209
slot update_slots: id  3 | task 6553 | n_tokens = 4105, memory_seq_rm [4105, end)
slot update_slots: id  3 | task 6553 | prompt processing progress, n_tokens = 4145, batch.n_tokens = 40, progress = 0.984794
slot update_slots: id  3 | task 6553 | n_tokens = 4145, memory_seq_rm [4145, end)
slot update_slots: id  3 | task 6553 | prompt processing progress, n_tokens = 4209, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6553 | prompt done, n_tokens = 4209, batch.n_tokens = 64
slot init_sampler: id  3 | task 6553 | init sampler, took 0.64 ms, tokens: text = 4209, total = 4209
slot update_slots: id  3 | task 6553 | created context checkpoint 8 of 8 (pos_min = 3574, pos_max = 4144, size = 13.390 MiB)
slot print_timing: id  3 | task 6553 | 
prompt eval time =     307.10 ms /   104 tokens (    2.95 ms per token,   338.65 tokens per second)
       eval time =    2486.99 ms /   101 tokens (   24.62 ms per token,    40.61 tokens per second)
      total time =    2794.09 ms /   205 tokens
slot      release: id  3 | task 6553 | stop processing: n_tokens = 4309, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6656 | processing task, is_child = 0
slot update_slots: id  3 | task 6656 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4281
slot update_slots: id  3 | task 6656 | n_tokens = 4209, memory_seq_rm [4209, end)
slot update_slots: id  3 | task 6656 | prompt processing progress, n_tokens = 4217, batch.n_tokens = 8, progress = 0.985050
slot update_slots: id  3 | task 6656 | n_tokens = 4217, memory_seq_rm [4217, end)
slot update_slots: id  3 | task 6656 | prompt processing progress, n_tokens = 4281, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6656 | prompt done, n_tokens = 4281, batch.n_tokens = 64
slot init_sampler: id  3 | task 6656 | init sampler, took 0.93 ms, tokens: text = 4281, total = 4281
slot update_slots: id  3 | task 6656 | erasing old context checkpoint (pos_min = 485, pos_max = 1508, size = 24.012 MiB)
slot update_slots: id  3 | task 6656 | created context checkpoint 8 of 8 (pos_min = 3624, pos_max = 4216, size = 13.906 MiB)
slot print_timing: id  3 | task 6656 | 
prompt eval time =     232.29 ms /    72 tokens (    3.23 ms per token,   309.96 tokens per second)
       eval time =    4089.77 ms /   166 tokens (   24.64 ms per token,    40.59 tokens per second)
      total time =    4322.06 ms /   238 tokens
slot      release: id  3 | task 6656 | stop processing: n_tokens = 4446, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.963
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6824 | processing task, is_child = 0
slot update_slots: id  3 | task 6824 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4353
slot update_slots: id  3 | task 6824 | n_tokens = 4281, memory_seq_rm [4281, end)
slot update_slots: id  3 | task 6824 | prompt processing progress, n_tokens = 4289, batch.n_tokens = 8, progress = 0.985298
slot update_slots: id  3 | task 6824 | n_tokens = 4289, memory_seq_rm [4289, end)
slot update_slots: id  3 | task 6824 | prompt processing progress, n_tokens = 4353, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6824 | prompt done, n_tokens = 4353, batch.n_tokens = 64
slot init_sampler: id  3 | task 6824 | init sampler, took 0.76 ms, tokens: text = 4353, total = 4353
slot update_slots: id  3 | task 6824 | erasing old context checkpoint (pos_min = 708, pos_max = 1636, size = 21.784 MiB)
slot update_slots: id  3 | task 6824 | created context checkpoint 8 of 8 (pos_min = 3757, pos_max = 4288, size = 12.475 MiB)
slot print_timing: id  3 | task 6824 | 
prompt eval time =     226.72 ms /    72 tokens (    3.15 ms per token,   317.57 tokens per second)
       eval time =    2258.67 ms /    93 tokens (   24.29 ms per token,    41.17 tokens per second)
      total time =    2485.39 ms /   165 tokens
slot      release: id  3 | task 6824 | stop processing: n_tokens = 4445, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6919 | processing task, is_child = 0
slot update_slots: id  3 | task 6919 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4425
slot update_slots: id  3 | task 6919 | n_tokens = 4353, memory_seq_rm [4353, end)
slot update_slots: id  3 | task 6919 | prompt processing progress, n_tokens = 4361, batch.n_tokens = 8, progress = 0.985537
slot update_slots: id  3 | task 6919 | n_tokens = 4361, memory_seq_rm [4361, end)
slot update_slots: id  3 | task 6919 | prompt processing progress, n_tokens = 4425, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6919 | prompt done, n_tokens = 4425, batch.n_tokens = 64
slot init_sampler: id  3 | task 6919 | init sampler, took 0.72 ms, tokens: text = 4425, total = 4425
slot update_slots: id  3 | task 6919 | erasing old context checkpoint (pos_min = 1400, pos_max = 2423, size = 24.012 MiB)
slot update_slots: id  3 | task 6919 | created context checkpoint 8 of 8 (pos_min = 3757, pos_max = 4360, size = 14.163 MiB)
slot print_timing: id  3 | task 6919 | 
prompt eval time =     222.63 ms /    72 tokens (    3.09 ms per token,   323.41 tokens per second)
       eval time =    2547.45 ms /   105 tokens (   24.26 ms per token,    41.22 tokens per second)
      total time =    2770.08 ms /   177 tokens
slot      release: id  3 | task 6919 | stop processing: n_tokens = 4529, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7026 | processing task, is_child = 0
slot update_slots: id  3 | task 7026 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4497
slot update_slots: id  3 | task 7026 | n_tokens = 4425, memory_seq_rm [4425, end)
slot update_slots: id  3 | task 7026 | prompt processing progress, n_tokens = 4433, batch.n_tokens = 8, progress = 0.985768
slot update_slots: id  3 | task 7026 | n_tokens = 4433, memory_seq_rm [4433, end)
slot update_slots: id  3 | task 7026 | prompt processing progress, n_tokens = 4497, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7026 | prompt done, n_tokens = 4497, batch.n_tokens = 64
slot init_sampler: id  3 | task 7026 | init sampler, took 0.69 ms, tokens: text = 4497, total = 4497
slot update_slots: id  3 | task 7026 | erasing old context checkpoint (pos_min = 2669, pos_max = 3692, size = 24.012 MiB)
slot update_slots: id  3 | task 7026 | created context checkpoint 8 of 8 (pos_min = 3757, pos_max = 4432, size = 15.852 MiB)
slot print_timing: id  3 | task 7026 | 
prompt eval time =     226.03 ms /    72 tokens (    3.14 ms per token,   318.54 tokens per second)
       eval time =    3170.81 ms /   129 tokens (   24.58 ms per token,    40.68 tokens per second)
      total time =    3396.84 ms /   201 tokens
slot      release: id  3 | task 7026 | stop processing: n_tokens = 4625, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7157 | processing task, is_child = 0
slot update_slots: id  3 | task 7157 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4601
slot update_slots: id  3 | task 7157 | n_tokens = 4497, memory_seq_rm [4497, end)
slot update_slots: id  3 | task 7157 | prompt processing progress, n_tokens = 4537, batch.n_tokens = 40, progress = 0.986090
slot update_slots: id  3 | task 7157 | n_tokens = 4537, memory_seq_rm [4537, end)
slot update_slots: id  3 | task 7157 | prompt processing progress, n_tokens = 4601, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7157 | prompt done, n_tokens = 4601, batch.n_tokens = 64
slot init_sampler: id  3 | task 7157 | init sampler, took 0.87 ms, tokens: text = 4601, total = 4601
slot update_slots: id  3 | task 7157 | erasing old context checkpoint (pos_min = 3156, pos_max = 3796, size = 15.031 MiB)
slot update_slots: id  3 | task 7157 | created context checkpoint 8 of 8 (pos_min = 3757, pos_max = 4536, size = 18.290 MiB)
slot print_timing: id  3 | task 7157 | 
prompt eval time =     303.39 ms /   104 tokens (    2.92 ms per token,   342.79 tokens per second)
       eval time =    3699.23 ms /   150 tokens (   24.66 ms per token,    40.55 tokens per second)
      total time =    4002.63 ms /   254 tokens
slot      release: id  3 | task 7157 | stop processing: n_tokens = 4750, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7309 | processing task, is_child = 0
slot update_slots: id  3 | task 7309 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4673
slot update_slots: id  3 | task 7309 | n_tokens = 4601, memory_seq_rm [4601, end)
slot update_slots: id  3 | task 7309 | prompt processing progress, n_tokens = 4609, batch.n_tokens = 8, progress = 0.986304
slot update_slots: id  3 | task 7309 | n_tokens = 4609, memory_seq_rm [4609, end)
slot update_slots: id  3 | task 7309 | prompt processing progress, n_tokens = 4673, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7309 | prompt done, n_tokens = 4673, batch.n_tokens = 64
slot init_sampler: id  3 | task 7309 | init sampler, took 0.80 ms, tokens: text = 4673, total = 4673
slot update_slots: id  3 | task 7309 | erasing old context checkpoint (pos_min = 3358, pos_max = 3936, size = 13.577 MiB)
slot update_slots: id  3 | task 7309 | created context checkpoint 8 of 8 (pos_min = 3757, pos_max = 4608, size = 19.979 MiB)
slot print_timing: id  3 | task 7309 | 
prompt eval time =     232.56 ms /    72 tokens (    3.23 ms per token,   309.59 tokens per second)
       eval time =    4228.87 ms /   176 tokens (   24.03 ms per token,    41.62 tokens per second)
      total time =    4461.43 ms /   248 tokens
slot      release: id  3 | task 7309 | stop processing: n_tokens = 4848, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7487 | processing task, is_child = 0
slot update_slots: id  3 | task 7487 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4745
slot update_slots: id  3 | task 7487 | n_tokens = 4673, memory_seq_rm [4673, end)
slot update_slots: id  3 | task 7487 | prompt processing progress, n_tokens = 4681, batch.n_tokens = 8, progress = 0.986512
slot update_slots: id  3 | task 7487 | n_tokens = 4681, memory_seq_rm [4681, end)
slot update_slots: id  3 | task 7487 | prompt processing progress, n_tokens = 4745, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7487 | prompt done, n_tokens = 4745, batch.n_tokens = 64
slot init_sampler: id  3 | task 7487 | init sampler, took 0.77 ms, tokens: text = 4745, total = 4745
slot update_slots: id  3 | task 7487 | erasing old context checkpoint (pos_min = 3574, pos_max = 4040, size = 10.951 MiB)
slot update_slots: id  3 | task 7487 | created context checkpoint 8 of 8 (pos_min = 3824, pos_max = 4680, size = 20.096 MiB)
slot print_timing: id  3 | task 7487 | 
prompt eval time =     225.84 ms /    72 tokens (    3.14 ms per token,   318.82 tokens per second)
       eval time =    3118.97 ms /   130 tokens (   23.99 ms per token,    41.68 tokens per second)
      total time =    3344.81 ms /   202 tokens
slot      release: id  3 | task 7487 | stop processing: n_tokens = 4874, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7619 | processing task, is_child = 0
slot update_slots: id  3 | task 7619 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4819
slot update_slots: id  3 | task 7619 | n_tokens = 4745, memory_seq_rm [4745, end)
slot update_slots: id  3 | task 7619 | prompt processing progress, n_tokens = 4755, batch.n_tokens = 10, progress = 0.986719
slot update_slots: id  3 | task 7619 | n_tokens = 4755, memory_seq_rm [4755, end)
slot update_slots: id  3 | task 7619 | prompt processing progress, n_tokens = 4819, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7619 | prompt done, n_tokens = 4819, batch.n_tokens = 64
slot init_sampler: id  3 | task 7619 | init sampler, took 0.71 ms, tokens: text = 4819, total = 4819
slot update_slots: id  3 | task 7619 | erasing old context checkpoint (pos_min = 3574, pos_max = 4144, size = 13.390 MiB)
slot update_slots: id  3 | task 7619 | created context checkpoint 8 of 8 (pos_min = 3958, pos_max = 4754, size = 18.689 MiB)
slot print_timing: id  3 | task 7619 | 
prompt eval time =     234.09 ms /    74 tokens (    3.16 ms per token,   316.12 tokens per second)
       eval time =    3246.66 ms /   130 tokens (   24.97 ms per token,    40.04 tokens per second)
      total time =    3480.75 ms /   204 tokens
slot      release: id  3 | task 7619 | stop processing: n_tokens = 4948, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.791 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7751 | processing task, is_child = 0
slot update_slots: id  3 | task 7751 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6093
slot update_slots: id  3 | task 7751 | n_tokens = 4819, memory_seq_rm [4819, end)
slot update_slots: id  3 | task 7751 | prompt processing progress, n_tokens = 6029, batch.n_tokens = 1210, progress = 0.989496
slot update_slots: id  3 | task 7751 | n_tokens = 6029, memory_seq_rm [6029, end)
slot update_slots: id  3 | task 7751 | prompt processing progress, n_tokens = 6093, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7751 | prompt done, n_tokens = 6093, batch.n_tokens = 64
slot init_sampler: id  3 | task 7751 | init sampler, took 1.78 ms, tokens: text = 6093, total = 6093
slot update_slots: id  3 | task 7751 | erasing old context checkpoint (pos_min = 3624, pos_max = 4216, size = 13.906 MiB)
slot update_slots: id  3 | task 7751 | created context checkpoint 8 of 8 (pos_min = 5005, pos_max = 6028, size = 24.012 MiB)
slot print_timing: id  3 | task 7751 | 
prompt eval time =    1397.16 ms /  1274 tokens (    1.10 ms per token,   911.85 tokens per second)
       eval time =    1921.92 ms /    80 tokens (   24.02 ms per token,    41.62 tokens per second)
      total time =    3319.08 ms /  1354 tokens
slot      release: id  3 | task 7751 | stop processing: n_tokens = 6172, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7833 | processing task, is_child = 0
slot update_slots: id  3 | task 7833 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6165
slot update_slots: id  3 | task 7833 | n_tokens = 6093, memory_seq_rm [6093, end)
slot update_slots: id  3 | task 7833 | prompt processing progress, n_tokens = 6101, batch.n_tokens = 8, progress = 0.989619
slot update_slots: id  3 | task 7833 | n_tokens = 6101, memory_seq_rm [6101, end)
slot update_slots: id  3 | task 7833 | prompt processing progress, n_tokens = 6165, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7833 | prompt done, n_tokens = 6165, batch.n_tokens = 64
slot init_sampler: id  3 | task 7833 | init sampler, took 1.85 ms, tokens: text = 6165, total = 6165
slot update_slots: id  3 | task 7833 | erasing old context checkpoint (pos_min = 3757, pos_max = 4288, size = 12.475 MiB)
slot update_slots: id  3 | task 7833 | created context checkpoint 8 of 8 (pos_min = 5148, pos_max = 6100, size = 22.347 MiB)
slot print_timing: id  3 | task 7833 | 
prompt eval time =     229.85 ms /    72 tokens (    3.19 ms per token,   313.25 tokens per second)
       eval time =    7668.56 ms /   315 tokens (   24.34 ms per token,    41.08 tokens per second)
      total time =    7898.41 ms /   387 tokens
slot      release: id  3 | task 7833 | stop processing: n_tokens = 6479, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.923 (> 0.100 thold), f_keep = 0.952
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8150 | processing task, is_child = 0
slot update_slots: id  3 | task 8150 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6679
slot update_slots: id  3 | task 8150 | n_tokens = 6165, memory_seq_rm [6165, end)
slot update_slots: id  3 | task 8150 | prompt processing progress, n_tokens = 6615, batch.n_tokens = 450, progress = 0.990418
slot update_slots: id  3 | task 8150 | n_tokens = 6615, memory_seq_rm [6615, end)
slot update_slots: id  3 | task 8150 | prompt processing progress, n_tokens = 6679, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8150 | prompt done, n_tokens = 6679, batch.n_tokens = 64
slot init_sampler: id  3 | task 8150 | init sampler, took 1.02 ms, tokens: text = 6679, total = 6679
slot update_slots: id  3 | task 8150 | erasing old context checkpoint (pos_min = 3757, pos_max = 4360, size = 14.163 MiB)
slot update_slots: id  3 | task 8150 | created context checkpoint 8 of 8 (pos_min = 5591, pos_max = 6614, size = 24.012 MiB)
slot print_timing: id  3 | task 8150 | 
prompt eval time =     691.53 ms /   514 tokens (    1.35 ms per token,   743.28 tokens per second)
       eval time =    4711.51 ms /   197 tokens (   23.92 ms per token,    41.81 tokens per second)
      total time =    5403.04 ms /   711 tokens
slot      release: id  3 | task 8150 | stop processing: n_tokens = 6875, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.161 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8349 | processing task, is_child = 0
slot update_slots: id  3 | task 8349 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 41613
slot update_slots: id  3 | task 8349 | n_tokens = 6679, memory_seq_rm [6679, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 8727, batch.n_tokens = 2048, progress = 0.209718
slot update_slots: id  3 | task 8349 | n_tokens = 8727, memory_seq_rm [8727, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 10775, batch.n_tokens = 2048, progress = 0.258934
slot update_slots: id  3 | task 8349 | n_tokens = 10775, memory_seq_rm [10775, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 12823, batch.n_tokens = 2048, progress = 0.308149
slot update_slots: id  3 | task 8349 | n_tokens = 12823, memory_seq_rm [12823, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 14871, batch.n_tokens = 2048, progress = 0.357364
slot update_slots: id  3 | task 8349 | n_tokens = 14871, memory_seq_rm [14871, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 16919, batch.n_tokens = 2048, progress = 0.406580
slot update_slots: id  3 | task 8349 | n_tokens = 16919, memory_seq_rm [16919, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 18967, batch.n_tokens = 2048, progress = 0.455795
slot update_slots: id  3 | task 8349 | n_tokens = 18967, memory_seq_rm [18967, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 21015, batch.n_tokens = 2048, progress = 0.505010
slot update_slots: id  3 | task 8349 | n_tokens = 21015, memory_seq_rm [21015, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 23063, batch.n_tokens = 2048, progress = 0.554226
slot update_slots: id  3 | task 8349 | n_tokens = 23063, memory_seq_rm [23063, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 25111, batch.n_tokens = 2048, progress = 0.603441
slot update_slots: id  3 | task 8349 | n_tokens = 25111, memory_seq_rm [25111, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 27159, batch.n_tokens = 2048, progress = 0.652657
slot update_slots: id  3 | task 8349 | n_tokens = 27159, memory_seq_rm [27159, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 29207, batch.n_tokens = 2048, progress = 0.701872
slot update_slots: id  3 | task 8349 | n_tokens = 29207, memory_seq_rm [29207, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 31255, batch.n_tokens = 2048, progress = 0.751087
slot update_slots: id  3 | task 8349 | n_tokens = 31255, memory_seq_rm [31255, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 33303, batch.n_tokens = 2048, progress = 0.800303
slot update_slots: id  3 | task 8349 | n_tokens = 33303, memory_seq_rm [33303, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 35351, batch.n_tokens = 2048, progress = 0.849518
slot update_slots: id  3 | task 8349 | n_tokens = 35351, memory_seq_rm [35351, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 37399, batch.n_tokens = 2048, progress = 0.898734
slot update_slots: id  3 | task 8349 | n_tokens = 37399, memory_seq_rm [37399, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 39447, batch.n_tokens = 2048, progress = 0.947949
slot update_slots: id  3 | task 8349 | n_tokens = 39447, memory_seq_rm [39447, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 41495, batch.n_tokens = 2048, progress = 0.997164
slot update_slots: id  3 | task 8349 | n_tokens = 41495, memory_seq_rm [41495, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 41549, batch.n_tokens = 54, progress = 0.998462
slot update_slots: id  3 | task 8349 | n_tokens = 41549, memory_seq_rm [41549, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 41613, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8349 | prompt done, n_tokens = 41613, batch.n_tokens = 64
slot init_sampler: id  3 | task 8349 | init sampler, took 5.89 ms, tokens: text = 41613, total = 41613
slot update_slots: id  3 | task 8349 | erasing old context checkpoint (pos_min = 3757, pos_max = 4432, size = 15.852 MiB)
slot update_slots: id  3 | task 8349 | created context checkpoint 8 of 8 (pos_min = 40525, pos_max = 41548, size = 24.012 MiB)
slot print_timing: id  3 | task 8349 | 
prompt eval time =   40798.66 ms / 34934 tokens (    1.17 ms per token,   856.25 tokens per second)
       eval time =    6303.89 ms /   219 tokens (   28.78 ms per token,    34.74 tokens per second)
      total time =   47102.56 ms / 35153 tokens
slot      release: id  3 | task 8349 | stop processing: n_tokens = 41831, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8587 | processing task, is_child = 0
slot update_slots: id  3 | task 8587 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 41950
slot update_slots: id  3 | task 8587 | n_tokens = 41613, memory_seq_rm [41613, end)
slot update_slots: id  3 | task 8587 | prompt processing progress, n_tokens = 41886, batch.n_tokens = 273, progress = 0.998474
slot update_slots: id  3 | task 8587 | n_tokens = 41886, memory_seq_rm [41886, end)
slot update_slots: id  3 | task 8587 | prompt processing progress, n_tokens = 41950, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8587 | prompt done, n_tokens = 41950, batch.n_tokens = 64
slot init_sampler: id  3 | task 8587 | init sampler, took 6.10 ms, tokens: text = 41950, total = 41950
slot update_slots: id  3 | task 8587 | erasing old context checkpoint (pos_min = 3757, pos_max = 4536, size = 18.290 MiB)
slot update_slots: id  3 | task 8587 | created context checkpoint 8 of 8 (pos_min = 40862, pos_max = 41885, size = 24.012 MiB)
slot print_timing: id  3 | task 8587 | 
prompt eval time =     775.30 ms /   337 tokens (    2.30 ms per token,   434.67 tokens per second)
       eval time =    1766.68 ms /    63 tokens (   28.04 ms per token,    35.66 tokens per second)
      total time =    2541.97 ms /   400 tokens
slot      release: id  3 | task 8587 | stop processing: n_tokens = 42012, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8652 | processing task, is_child = 0
slot update_slots: id  3 | task 8652 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 42123
slot update_slots: id  3 | task 8652 | n_tokens = 41950, memory_seq_rm [41950, end)
slot update_slots: id  3 | task 8652 | prompt processing progress, n_tokens = 42059, batch.n_tokens = 109, progress = 0.998481
slot update_slots: id  3 | task 8652 | n_tokens = 42059, memory_seq_rm [42059, end)
slot update_slots: id  3 | task 8652 | prompt processing progress, n_tokens = 42123, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8652 | prompt done, n_tokens = 42123, batch.n_tokens = 64
slot init_sampler: id  3 | task 8652 | init sampler, took 5.84 ms, tokens: text = 42123, total = 42123
slot update_slots: id  3 | task 8652 | erasing old context checkpoint (pos_min = 3757, pos_max = 4608, size = 19.979 MiB)
slot update_slots: id  3 | task 8652 | created context checkpoint 8 of 8 (pos_min = 41035, pos_max = 42058, size = 24.012 MiB)
slot print_timing: id  3 | task 8652 | 
prompt eval time =     578.59 ms /   173 tokens (    3.34 ms per token,   299.00 tokens per second)
       eval time =    6612.50 ms /   235 tokens (   28.14 ms per token,    35.54 tokens per second)
      total time =    7191.09 ms /   408 tokens
slot      release: id  3 | task 8652 | stop processing: n_tokens = 42357, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8889 | processing task, is_child = 0
slot update_slots: id  3 | task 8889 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 42324
slot update_slots: id  3 | task 8889 | n_tokens = 42123, memory_seq_rm [42123, end)
slot update_slots: id  3 | task 8889 | prompt processing progress, n_tokens = 42260, batch.n_tokens = 137, progress = 0.998488
slot update_slots: id  3 | task 8889 | n_tokens = 42260, memory_seq_rm [42260, end)
slot update_slots: id  3 | task 8889 | prompt processing progress, n_tokens = 42324, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8889 | prompt done, n_tokens = 42324, batch.n_tokens = 64
slot init_sampler: id  3 | task 8889 | init sampler, took 6.05 ms, tokens: text = 42324, total = 42324
slot update_slots: id  3 | task 8889 | erasing old context checkpoint (pos_min = 3824, pos_max = 4680, size = 20.096 MiB)
slot update_slots: id  3 | task 8889 | created context checkpoint 8 of 8 (pos_min = 41333, pos_max = 42259, size = 21.737 MiB)
slot print_timing: id  3 | task 8889 | 
prompt eval time =     619.14 ms /   201 tokens (    3.08 ms per token,   324.65 tokens per second)
       eval time =    8135.57 ms /   290 tokens (   28.05 ms per token,    35.65 tokens per second)
      total time =    8754.71 ms /   491 tokens
slot      release: id  3 | task 8889 | stop processing: n_tokens = 42613, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9181 | processing task, is_child = 0
slot update_slots: id  3 | task 9181 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 42532
slot update_slots: id  3 | task 9181 | n_tokens = 42324, memory_seq_rm [42324, end)
slot update_slots: id  3 | task 9181 | prompt processing progress, n_tokens = 42468, batch.n_tokens = 144, progress = 0.998495
slot update_slots: id  3 | task 9181 | n_tokens = 42468, memory_seq_rm [42468, end)
slot update_slots: id  3 | task 9181 | prompt processing progress, n_tokens = 42532, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9181 | prompt done, n_tokens = 42532, batch.n_tokens = 64
slot init_sampler: id  3 | task 9181 | init sampler, took 5.98 ms, tokens: text = 42532, total = 42532
slot update_slots: id  3 | task 9181 | erasing old context checkpoint (pos_min = 3958, pos_max = 4754, size = 18.689 MiB)
slot update_slots: id  3 | task 9181 | created context checkpoint 8 of 8 (pos_min = 41639, pos_max = 42467, size = 19.439 MiB)
slot print_timing: id  3 | task 9181 | 
prompt eval time =     571.73 ms /   208 tokens (    2.75 ms per token,   363.81 tokens per second)
       eval time =   14262.63 ms /   500 tokens (   28.53 ms per token,    35.06 tokens per second)
      total time =   14834.37 ms /   708 tokens
slot      release: id  3 | task 9181 | stop processing: n_tokens = 43031, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9683 | processing task, is_child = 0
slot update_slots: id  3 | task 9683 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 42758
slot update_slots: id  3 | task 9683 | n_tokens = 42532, memory_seq_rm [42532, end)
slot update_slots: id  3 | task 9683 | prompt processing progress, n_tokens = 42694, batch.n_tokens = 162, progress = 0.998503
slot update_slots: id  3 | task 9683 | n_tokens = 42694, memory_seq_rm [42694, end)
slot update_slots: id  3 | task 9683 | prompt processing progress, n_tokens = 42758, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9683 | prompt done, n_tokens = 42758, batch.n_tokens = 64
slot init_sampler: id  3 | task 9683 | init sampler, took 6.91 ms, tokens: text = 42758, total = 42758
slot update_slots: id  3 | task 9683 | erasing old context checkpoint (pos_min = 5005, pos_max = 6028, size = 24.012 MiB)
slot update_slots: id  3 | task 9683 | created context checkpoint 8 of 8 (pos_min = 42202, pos_max = 42693, size = 11.537 MiB)
slot print_timing: id  3 | task 9683 | 
prompt eval time =     707.24 ms /   226 tokens (    3.13 ms per token,   319.55 tokens per second)
       eval time =   10508.37 ms /   365 tokens (   28.79 ms per token,    34.73 tokens per second)
      total time =   11215.61 ms /   591 tokens
slot      release: id  3 | task 9683 | stop processing: n_tokens = 43122, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10050 | processing task, is_child = 0
slot update_slots: id  3 | task 10050 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 42941
slot update_slots: id  3 | task 10050 | n_tokens = 42758, memory_seq_rm [42758, end)
slot update_slots: id  3 | task 10050 | prompt processing progress, n_tokens = 42877, batch.n_tokens = 119, progress = 0.998510
slot update_slots: id  3 | task 10050 | n_tokens = 42877, memory_seq_rm [42877, end)
slot update_slots: id  3 | task 10050 | prompt processing progress, n_tokens = 42941, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10050 | prompt done, n_tokens = 42941, batch.n_tokens = 64
slot init_sampler: id  3 | task 10050 | init sampler, took 6.46 ms, tokens: text = 42941, total = 42941
slot update_slots: id  3 | task 10050 | erasing old context checkpoint (pos_min = 5148, pos_max = 6100, size = 22.347 MiB)
slot update_slots: id  3 | task 10050 | created context checkpoint 8 of 8 (pos_min = 42293, pos_max = 42876, size = 13.694 MiB)
slot print_timing: id  3 | task 10050 | 
prompt eval time =     688.46 ms /   183 tokens (    3.76 ms per token,   265.81 tokens per second)
       eval time =    8487.61 ms /   293 tokens (   28.97 ms per token,    34.52 tokens per second)
      total time =    9176.08 ms /   476 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 10050 | stop processing: n_tokens = 43233, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10345 | processing task, is_child = 0
slot update_slots: id  3 | task 10345 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 43105
slot update_slots: id  3 | task 10345 | n_tokens = 42941, memory_seq_rm [42941, end)
slot update_slots: id  3 | task 10345 | prompt processing progress, n_tokens = 43041, batch.n_tokens = 100, progress = 0.998515
slot update_slots: id  3 | task 10345 | n_tokens = 43041, memory_seq_rm [43041, end)
slot update_slots: id  3 | task 10345 | prompt processing progress, n_tokens = 43105, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10345 | prompt done, n_tokens = 43105, batch.n_tokens = 64
slot init_sampler: id  3 | task 10345 | init sampler, took 6.11 ms, tokens: text = 43105, total = 43105
slot update_slots: id  3 | task 10345 | erasing old context checkpoint (pos_min = 5591, pos_max = 6614, size = 24.012 MiB)
slot update_slots: id  3 | task 10345 | created context checkpoint 8 of 8 (pos_min = 42324, pos_max = 43040, size = 16.813 MiB)
slot print_timing: id  3 | task 10345 | 
prompt eval time =     635.24 ms /   164 tokens (    3.87 ms per token,   258.17 tokens per second)
       eval time =    8151.49 ms /   280 tokens (   29.11 ms per token,    34.35 tokens per second)
      total time =    8786.73 ms /   444 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 10345 | stop processing: n_tokens = 43384, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.393 (> 0.100 thold), f_keep = 0.039
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 43384, total state size = 1041.321 MiB
srv          load:  - looking for better prompt, base f_keep = 0.039, sim = 0.393
srv        update:  - cache state: 2 prompts, 1559.454 MiB (limits: 8192.000 MiB, 64000 tokens, 269485 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv  get_availabl: prompt cache update took 1382.15 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10627 | processing task, is_child = 0
slot update_slots: id  3 | task 10627 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4288
slot update_slots: id  3 | task 10627 | n_past = 1685, slot.prompt.tokens.size() = 43384, seq_id = 3, pos_min = 42360, n_swa = 128
slot update_slots: id  3 | task 10627 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 40525, pos_max = 41548, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 40862, pos_max = 41885, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 41035, pos_max = 42058, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 41333, pos_max = 42259, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 41639, pos_max = 42467, n_swa = 128, size = 19.439 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 42202, pos_max = 42693, n_swa = 128, size = 11.537 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 42293, pos_max = 42876, n_swa = 128, size = 13.694 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 42324, pos_max = 43040, n_swa = 128, size = 16.813 MiB)
slot update_slots: id  3 | task 10627 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 10627 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.477612
slot update_slots: id  3 | task 10627 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 10627 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.955224
slot update_slots: id  3 | task 10627 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 10627 | prompt processing progress, n_tokens = 4224, batch.n_tokens = 128, progress = 0.985075
slot update_slots: id  3 | task 10627 | n_tokens = 4224, memory_seq_rm [4224, end)
slot update_slots: id  3 | task 10627 | prompt processing progress, n_tokens = 4288, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10627 | prompt done, n_tokens = 4288, batch.n_tokens = 64
slot init_sampler: id  3 | task 10627 | init sampler, took 0.72 ms, tokens: text = 4288, total = 4288
slot update_slots: id  3 | task 10627 | created context checkpoint 1 of 8 (pos_min = 3200, pos_max = 4223, size = 24.012 MiB)
slot print_timing: id  3 | task 10627 | 
prompt eval time =    4285.18 ms /  4288 tokens (    1.00 ms per token,  1000.66 tokens per second)
       eval time =    1478.73 ms /    62 tokens (   23.85 ms per token,    41.93 tokens per second)
      total time =    5763.91 ms /  4350 tokens
slot      release: id  3 | task 10627 | stop processing: n_tokens = 4349, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.872 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10693 | processing task, is_child = 0
slot update_slots: id  3 | task 10693 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4915
slot update_slots: id  3 | task 10693 | n_tokens = 4288, memory_seq_rm [4288, end)
slot update_slots: id  3 | task 10693 | prompt processing progress, n_tokens = 4851, batch.n_tokens = 563, progress = 0.986979
slot update_slots: id  3 | task 10693 | n_tokens = 4851, memory_seq_rm [4851, end)
slot update_slots: id  3 | task 10693 | prompt processing progress, n_tokens = 4915, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10693 | prompt done, n_tokens = 4915, batch.n_tokens = 64
slot init_sampler: id  3 | task 10693 | init sampler, took 0.80 ms, tokens: text = 4915, total = 4915
slot update_slots: id  3 | task 10693 | created context checkpoint 2 of 8 (pos_min = 3827, pos_max = 4850, size = 24.012 MiB)
slot print_timing: id  3 | task 10693 | 
prompt eval time =     834.97 ms /   627 tokens (    1.33 ms per token,   750.93 tokens per second)
       eval time =    1096.53 ms /    45 tokens (   24.37 ms per token,    41.04 tokens per second)
      total time =    1931.50 ms /   672 tokens
slot      release: id  3 | task 10693 | stop processing: n_tokens = 4959, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10740 | processing task, is_child = 0
slot update_slots: id  3 | task 10740 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4967
slot update_slots: id  3 | task 10740 | n_tokens = 4915, memory_seq_rm [4915, end)
slot update_slots: id  3 | task 10740 | prompt processing progress, n_tokens = 4967, batch.n_tokens = 52, progress = 1.000000
slot update_slots: id  3 | task 10740 | prompt done, n_tokens = 4967, batch.n_tokens = 52
slot init_sampler: id  3 | task 10740 | init sampler, took 0.80 ms, tokens: text = 4967, total = 4967
slot print_timing: id  3 | task 10740 | 
prompt eval time =     147.66 ms /    52 tokens (    2.84 ms per token,   352.15 tokens per second)
       eval time =    1293.91 ms /    53 tokens (   24.41 ms per token,    40.96 tokens per second)
      total time =    1441.58 ms /   105 tokens
slot      release: id  3 | task 10740 | stop processing: n_tokens = 5019, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10794 | processing task, is_child = 0
slot update_slots: id  3 | task 10794 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5049
slot update_slots: id  3 | task 10794 | n_tokens = 4967, memory_seq_rm [4967, end)
slot update_slots: id  3 | task 10794 | prompt processing progress, n_tokens = 4985, batch.n_tokens = 18, progress = 0.987324
slot update_slots: id  3 | task 10794 | n_tokens = 4985, memory_seq_rm [4985, end)
slot update_slots: id  3 | task 10794 | prompt processing progress, n_tokens = 5049, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10794 | prompt done, n_tokens = 5049, batch.n_tokens = 64
slot init_sampler: id  3 | task 10794 | init sampler, took 0.93 ms, tokens: text = 5049, total = 5049
slot update_slots: id  3 | task 10794 | created context checkpoint 3 of 8 (pos_min = 3995, pos_max = 4984, size = 23.215 MiB)
slot print_timing: id  3 | task 10794 | 
prompt eval time =     261.22 ms /    82 tokens (    3.19 ms per token,   313.91 tokens per second)
       eval time =   17281.97 ms /   700 tokens (   24.69 ms per token,    40.50 tokens per second)
      total time =   17543.19 ms /   782 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 10794 | stop processing: n_tokens = 5748, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.925 (> 0.100 thold), f_keep = 0.878
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11496 | processing task, is_child = 0
slot update_slots: id  3 | task 11496 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5456
slot update_slots: id  3 | task 11496 | n_tokens = 5049, memory_seq_rm [5049, end)
slot update_slots: id  3 | task 11496 | prompt processing progress, n_tokens = 5392, batch.n_tokens = 343, progress = 0.988270
slot update_slots: id  3 | task 11496 | n_tokens = 5392, memory_seq_rm [5392, end)
slot update_slots: id  3 | task 11496 | prompt processing progress, n_tokens = 5456, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11496 | prompt done, n_tokens = 5456, batch.n_tokens = 64
slot init_sampler: id  3 | task 11496 | init sampler, took 1.12 ms, tokens: text = 5456, total = 5456
slot update_slots: id  3 | task 11496 | created context checkpoint 4 of 8 (pos_min = 4724, pos_max = 5391, size = 15.664 MiB)
slot print_timing: id  3 | task 11496 | 
prompt eval time =     678.13 ms /   407 tokens (    1.67 ms per token,   600.18 tokens per second)
       eval time =    1349.19 ms /    52 tokens (   25.95 ms per token,    38.54 tokens per second)
      total time =    2027.33 ms /   459 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 11496 | stop processing: n_tokens = 5507, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11550 | processing task, is_child = 0
slot update_slots: id  3 | task 11550 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5514
slot update_slots: id  3 | task 11550 | n_tokens = 5456, memory_seq_rm [5456, end)
slot update_slots: id  3 | task 11550 | prompt processing progress, n_tokens = 5514, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 11550 | prompt done, n_tokens = 5514, batch.n_tokens = 58
slot init_sampler: id  3 | task 11550 | init sampler, took 0.84 ms, tokens: text = 5514, total = 5514
slot print_timing: id  3 | task 11550 | 
prompt eval time =     245.25 ms /    58 tokens (    4.23 ms per token,   236.50 tokens per second)
       eval time =    7266.28 ms /   288 tokens (   25.23 ms per token,    39.64 tokens per second)
      total time =    7511.53 ms /   346 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 11550 | stop processing: n_tokens = 5801, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.951
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11839 | processing task, is_child = 0
slot update_slots: id  3 | task 11839 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5763
slot update_slots: id  3 | task 11839 | n_tokens = 5514, memory_seq_rm [5514, end)
slot update_slots: id  3 | task 11839 | prompt processing progress, n_tokens = 5699, batch.n_tokens = 185, progress = 0.988895
slot update_slots: id  3 | task 11839 | n_tokens = 5699, memory_seq_rm [5699, end)
slot update_slots: id  3 | task 11839 | prompt processing progress, n_tokens = 5763, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11839 | prompt done, n_tokens = 5763, batch.n_tokens = 64
slot init_sampler: id  3 | task 11839 | init sampler, took 1.47 ms, tokens: text = 5763, total = 5763
slot update_slots: id  3 | task 11839 | created context checkpoint 5 of 8 (pos_min = 4848, pos_max = 5698, size = 19.955 MiB)
slot print_timing: id  3 | task 11839 | 
prompt eval time =     462.82 ms /   249 tokens (    1.86 ms per token,   538.00 tokens per second)
       eval time =    1338.54 ms /    50 tokens (   26.77 ms per token,    37.35 tokens per second)
      total time =    1801.36 ms /   299 tokens
slot      release: id  3 | task 11839 | stop processing: n_tokens = 5812, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11891 | processing task, is_child = 0
slot update_slots: id  3 | task 11891 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5873
slot update_slots: id  3 | task 11891 | n_tokens = 5763, memory_seq_rm [5763, end)
slot update_slots: id  3 | task 11891 | prompt processing progress, n_tokens = 5809, batch.n_tokens = 46, progress = 0.989103
slot update_slots: id  3 | task 11891 | n_tokens = 5809, memory_seq_rm [5809, end)
slot update_slots: id  3 | task 11891 | prompt processing progress, n_tokens = 5873, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11891 | prompt done, n_tokens = 5873, batch.n_tokens = 64
slot init_sampler: id  3 | task 11891 | init sampler, took 0.90 ms, tokens: text = 5873, total = 5873
slot update_slots: id  3 | task 11891 | created context checkpoint 6 of 8 (pos_min = 4859, pos_max = 5808, size = 22.277 MiB)
slot print_timing: id  3 | task 11891 | 
prompt eval time =     380.94 ms /   110 tokens (    3.46 ms per token,   288.76 tokens per second)
       eval time =    4436.25 ms /   185 tokens (   23.98 ms per token,    41.70 tokens per second)
      total time =    4817.19 ms /   295 tokens
slot      release: id  3 | task 11891 | stop processing: n_tokens = 6057, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12078 | processing task, is_child = 0
slot update_slots: id  3 | task 12078 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6046
slot update_slots: id  3 | task 12078 | n_tokens = 5873, memory_seq_rm [5873, end)
slot update_slots: id  3 | task 12078 | prompt processing progress, n_tokens = 5982, batch.n_tokens = 109, progress = 0.989415
slot update_slots: id  3 | task 12078 | n_tokens = 5982, memory_seq_rm [5982, end)
slot update_slots: id  3 | task 12078 | prompt processing progress, n_tokens = 6046, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12078 | prompt done, n_tokens = 6046, batch.n_tokens = 64
slot init_sampler: id  3 | task 12078 | init sampler, took 0.91 ms, tokens: text = 6046, total = 6046
slot update_slots: id  3 | task 12078 | created context checkpoint 7 of 8 (pos_min = 5049, pos_max = 5981, size = 21.878 MiB)
slot print_timing: id  3 | task 12078 | 
prompt eval time =     422.68 ms /   173 tokens (    2.44 ms per token,   409.30 tokens per second)
       eval time =    6492.61 ms /   262 tokens (   24.78 ms per token,    40.35 tokens per second)
      total time =    6915.29 ms /   435 tokens
slot      release: id  3 | task 12078 | stop processing: n_tokens = 6307, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12342 | processing task, is_child = 0
slot update_slots: id  3 | task 12342 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6294
slot update_slots: id  3 | task 12342 | n_tokens = 6046, memory_seq_rm [6046, end)
slot update_slots: id  3 | task 12342 | prompt processing progress, n_tokens = 6230, batch.n_tokens = 184, progress = 0.989832
slot update_slots: id  3 | task 12342 | n_tokens = 6230, memory_seq_rm [6230, end)
slot update_slots: id  3 | task 12342 | prompt processing progress, n_tokens = 6294, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12342 | prompt done, n_tokens = 6294, batch.n_tokens = 64
slot init_sampler: id  3 | task 12342 | init sampler, took 1.17 ms, tokens: text = 6294, total = 6294
slot update_slots: id  3 | task 12342 | created context checkpoint 8 of 8 (pos_min = 5283, pos_max = 6229, size = 22.206 MiB)
slot print_timing: id  3 | task 12342 | 
prompt eval time =     461.81 ms /   248 tokens (    1.86 ms per token,   537.02 tokens per second)
       eval time =    1035.94 ms /    40 tokens (   25.90 ms per token,    38.61 tokens per second)
      total time =    1497.75 ms /   288 tokens
slot      release: id  3 | task 12342 | stop processing: n_tokens = 6333, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12384 | processing task, is_child = 0
slot update_slots: id  3 | task 12384 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6402
slot update_slots: id  3 | task 12384 | n_tokens = 6294, memory_seq_rm [6294, end)
slot update_slots: id  3 | task 12384 | prompt processing progress, n_tokens = 6338, batch.n_tokens = 44, progress = 0.990003
slot update_slots: id  3 | task 12384 | n_tokens = 6338, memory_seq_rm [6338, end)
slot update_slots: id  3 | task 12384 | prompt processing progress, n_tokens = 6402, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12384 | prompt done, n_tokens = 6402, batch.n_tokens = 64
slot init_sampler: id  3 | task 12384 | init sampler, took 0.96 ms, tokens: text = 6402, total = 6402
slot update_slots: id  3 | task 12384 | erasing old context checkpoint (pos_min = 3200, pos_max = 4223, size = 24.012 MiB)
slot update_slots: id  3 | task 12384 | created context checkpoint 8 of 8 (pos_min = 5341, pos_max = 6337, size = 23.379 MiB)
slot print_timing: id  3 | task 12384 | 
prompt eval time =     417.53 ms /   108 tokens (    3.87 ms per token,   258.66 tokens per second)
       eval time =    3619.20 ms /   149 tokens (   24.29 ms per token,    41.17 tokens per second)
      total time =    4036.73 ms /   257 tokens
slot      release: id  3 | task 12384 | stop processing: n_tokens = 6550, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12535 | processing task, is_child = 0
slot update_slots: id  3 | task 12535 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6500
slot update_slots: id  3 | task 12535 | n_tokens = 6402, memory_seq_rm [6402, end)
slot update_slots: id  3 | task 12535 | prompt processing progress, n_tokens = 6436, batch.n_tokens = 34, progress = 0.990154
slot update_slots: id  3 | task 12535 | n_tokens = 6436, memory_seq_rm [6436, end)
slot update_slots: id  3 | task 12535 | prompt processing progress, n_tokens = 6500, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12535 | prompt done, n_tokens = 6500, batch.n_tokens = 64
slot init_sampler: id  3 | task 12535 | init sampler, took 1.10 ms, tokens: text = 6500, total = 6500
slot update_slots: id  3 | task 12535 | erasing old context checkpoint (pos_min = 3827, pos_max = 4850, size = 24.012 MiB)
slot update_slots: id  3 | task 12535 | created context checkpoint 8 of 8 (pos_min = 5553, pos_max = 6435, size = 20.706 MiB)
slot print_timing: id  3 | task 12535 | 
prompt eval time =     298.60 ms /    98 tokens (    3.05 ms per token,   328.20 tokens per second)
       eval time =    2875.19 ms /   118 tokens (   24.37 ms per token,    41.04 tokens per second)
      total time =    3173.79 ms /   216 tokens
slot      release: id  3 | task 12535 | stop processing: n_tokens = 6617, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12655 | processing task, is_child = 0
slot update_slots: id  3 | task 12655 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6697
slot update_slots: id  3 | task 12655 | n_tokens = 6500, memory_seq_rm [6500, end)
slot update_slots: id  3 | task 12655 | prompt processing progress, n_tokens = 6633, batch.n_tokens = 133, progress = 0.990443
slot update_slots: id  3 | task 12655 | n_tokens = 6633, memory_seq_rm [6633, end)
slot update_slots: id  3 | task 12655 | prompt processing progress, n_tokens = 6697, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12655 | prompt done, n_tokens = 6697, batch.n_tokens = 64
slot init_sampler: id  3 | task 12655 | init sampler, took 3.18 ms, tokens: text = 6697, total = 6697
slot update_slots: id  3 | task 12655 | erasing old context checkpoint (pos_min = 3995, pos_max = 4984, size = 23.215 MiB)
slot update_slots: id  3 | task 12655 | created context checkpoint 8 of 8 (pos_min = 5636, pos_max = 6632, size = 23.379 MiB)
slot print_timing: id  3 | task 12655 | 
prompt eval time =     393.18 ms /   197 tokens (    2.00 ms per token,   501.04 tokens per second)
       eval time =    6217.42 ms /   248 tokens (   25.07 ms per token,    39.89 tokens per second)
      total time =    6610.60 ms /   445 tokens
slot      release: id  3 | task 12655 | stop processing: n_tokens = 6944, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.363 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12905 | processing task, is_child = 0
slot update_slots: id  3 | task 12905 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18439
slot update_slots: id  3 | task 12905 | n_tokens = 6697, memory_seq_rm [6697, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 8745, batch.n_tokens = 2048, progress = 0.474266
slot update_slots: id  3 | task 12905 | n_tokens = 8745, memory_seq_rm [8745, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 10793, batch.n_tokens = 2048, progress = 0.585335
slot update_slots: id  3 | task 12905 | n_tokens = 10793, memory_seq_rm [10793, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 12841, batch.n_tokens = 2048, progress = 0.696404
slot update_slots: id  3 | task 12905 | n_tokens = 12841, memory_seq_rm [12841, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 14889, batch.n_tokens = 2048, progress = 0.807473
slot update_slots: id  3 | task 12905 | n_tokens = 14889, memory_seq_rm [14889, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 16937, batch.n_tokens = 2048, progress = 0.918542
slot update_slots: id  3 | task 12905 | n_tokens = 16937, memory_seq_rm [16937, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 18375, batch.n_tokens = 1438, progress = 0.996529
slot update_slots: id  3 | task 12905 | n_tokens = 18375, memory_seq_rm [18375, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 18439, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12905 | prompt done, n_tokens = 18439, batch.n_tokens = 64
slot init_sampler: id  3 | task 12905 | init sampler, took 3.41 ms, tokens: text = 18439, total = 18439
slot update_slots: id  3 | task 12905 | erasing old context checkpoint (pos_min = 4724, pos_max = 5391, size = 15.664 MiB)
slot update_slots: id  3 | task 12905 | created context checkpoint 8 of 8 (pos_min = 17351, pos_max = 18374, size = 24.012 MiB)
slot print_timing: id  3 | task 12905 | 
prompt eval time =   12715.05 ms / 11742 tokens (    1.08 ms per token,   923.47 tokens per second)
       eval time =    2118.73 ms /    80 tokens (   26.48 ms per token,    37.76 tokens per second)
      total time =   14833.78 ms / 11822 tokens
slot      release: id  3 | task 12905 | stop processing: n_tokens = 18518, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12992 | processing task, is_child = 0
slot update_slots: id  3 | task 12992 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19151
slot update_slots: id  3 | task 12992 | n_tokens = 18439, memory_seq_rm [18439, end)
slot update_slots: id  3 | task 12992 | prompt processing progress, n_tokens = 19087, batch.n_tokens = 648, progress = 0.996658
slot update_slots: id  3 | task 12992 | n_tokens = 19087, memory_seq_rm [19087, end)
slot update_slots: id  3 | task 12992 | prompt processing progress, n_tokens = 19151, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12992 | prompt done, n_tokens = 19151, batch.n_tokens = 64
slot init_sampler: id  3 | task 12992 | init sampler, took 3.71 ms, tokens: text = 19151, total = 19151
slot update_slots: id  3 | task 12992 | erasing old context checkpoint (pos_min = 4848, pos_max = 5698, size = 19.955 MiB)
slot update_slots: id  3 | task 12992 | created context checkpoint 8 of 8 (pos_min = 18063, pos_max = 19086, size = 24.012 MiB)
slot print_timing: id  3 | task 12992 | 
prompt eval time =    1096.52 ms /   712 tokens (    1.54 ms per token,   649.32 tokens per second)
       eval time =   14636.69 ms /   565 tokens (   25.91 ms per token,    38.60 tokens per second)
      total time =   15733.22 ms /  1277 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 12992 | stop processing: n_tokens = 19715, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13559 | processing task, is_child = 0
slot update_slots: id  3 | task 13559 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19578
slot update_slots: id  3 | task 13559 | n_tokens = 19151, memory_seq_rm [19151, end)
slot update_slots: id  3 | task 13559 | prompt processing progress, n_tokens = 19514, batch.n_tokens = 363, progress = 0.996731
slot update_slots: id  3 | task 13559 | n_tokens = 19514, memory_seq_rm [19514, end)
slot update_slots: id  3 | task 13559 | prompt processing progress, n_tokens = 19578, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13559 | prompt done, n_tokens = 19578, batch.n_tokens = 64
slot init_sampler: id  3 | task 13559 | init sampler, took 3.98 ms, tokens: text = 19578, total = 19578
slot update_slots: id  3 | task 13559 | erasing old context checkpoint (pos_min = 4859, pos_max = 5808, size = 22.277 MiB)
slot update_slots: id  3 | task 13559 | created context checkpoint 8 of 8 (pos_min = 18697, pos_max = 19513, size = 19.158 MiB)
slot print_timing: id  3 | task 13559 | 
prompt eval time =     720.93 ms /   427 tokens (    1.69 ms per token,   592.29 tokens per second)
       eval time =   10549.17 ms /   410 tokens (   25.73 ms per token,    38.87 tokens per second)
      total time =   11270.10 ms /   837 tokens
slot      release: id  3 | task 13559 | stop processing: n_tokens = 19987, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13971 | processing task, is_child = 0
slot update_slots: id  3 | task 13971 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19993
slot update_slots: id  3 | task 13971 | n_tokens = 19578, memory_seq_rm [19578, end)
slot update_slots: id  3 | task 13971 | prompt processing progress, n_tokens = 19929, batch.n_tokens = 351, progress = 0.996799
slot update_slots: id  3 | task 13971 | n_tokens = 19929, memory_seq_rm [19929, end)
slot update_slots: id  3 | task 13971 | prompt processing progress, n_tokens = 19993, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13971 | prompt done, n_tokens = 19993, batch.n_tokens = 64
slot init_sampler: id  3 | task 13971 | init sampler, took 3.77 ms, tokens: text = 19993, total = 19993
slot update_slots: id  3 | task 13971 | erasing old context checkpoint (pos_min = 5049, pos_max = 5981, size = 21.878 MiB)
slot update_slots: id  3 | task 13971 | created context checkpoint 8 of 8 (pos_min = 19151, pos_max = 19928, size = 18.244 MiB)
slot print_timing: id  3 | task 13971 | 
prompt eval time =     687.87 ms /   415 tokens (    1.66 ms per token,   603.31 tokens per second)
       eval time =   10788.81 ms /   413 tokens (   26.12 ms per token,    38.28 tokens per second)
      total time =   11476.68 ms /   828 tokens
slot      release: id  3 | task 13971 | stop processing: n_tokens = 20405, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14386 | processing task, is_child = 0
slot update_slots: id  3 | task 14386 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20381
slot update_slots: id  3 | task 14386 | n_tokens = 19993, memory_seq_rm [19993, end)
slot update_slots: id  3 | task 14386 | prompt processing progress, n_tokens = 20317, batch.n_tokens = 324, progress = 0.996860
slot update_slots: id  3 | task 14386 | n_tokens = 20317, memory_seq_rm [20317, end)
slot update_slots: id  3 | task 14386 | prompt processing progress, n_tokens = 20381, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 14386 | prompt done, n_tokens = 20381, batch.n_tokens = 64
slot init_sampler: id  3 | task 14386 | init sampler, took 2.88 ms, tokens: text = 20381, total = 20381
slot update_slots: id  3 | task 14386 | erasing old context checkpoint (pos_min = 5283, pos_max = 6229, size = 22.206 MiB)
slot update_slots: id  3 | task 14386 | created context checkpoint 8 of 8 (pos_min = 19475, pos_max = 20316, size = 19.744 MiB)
slot print_timing: id  3 | task 14386 | 
prompt eval time =     648.69 ms /   388 tokens (    1.67 ms per token,   598.12 tokens per second)
       eval time =    1123.78 ms /    43 tokens (   26.13 ms per token,    38.26 tokens per second)
      total time =    1772.47 ms /   431 tokens
slot      release: id  3 | task 14386 | stop processing: n_tokens = 20423, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14431 | processing task, is_child = 0
slot update_slots: id  3 | task 14431 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20489
slot update_slots: id  3 | task 14431 | n_tokens = 20381, memory_seq_rm [20381, end)
slot update_slots: id  3 | task 14431 | prompt processing progress, n_tokens = 20425, batch.n_tokens = 44, progress = 0.996876
slot update_slots: id  3 | task 14431 | n_tokens = 20425, memory_seq_rm [20425, end)
slot update_slots: id  3 | task 14431 | prompt processing progress, n_tokens = 20489, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 14431 | prompt done, n_tokens = 20489, batch.n_tokens = 64
slot init_sampler: id  3 | task 14431 | init sampler, took 3.90 ms, tokens: text = 20489, total = 20489
slot update_slots: id  3 | task 14431 | erasing old context checkpoint (pos_min = 5341, pos_max = 6337, size = 23.379 MiB)
slot update_slots: id  3 | task 14431 | created context checkpoint 8 of 8 (pos_min = 19583, pos_max = 20424, size = 19.744 MiB)
slot print_timing: id  3 | task 14431 | 
prompt eval time =     450.85 ms /   108 tokens (    4.17 ms per token,   239.55 tokens per second)
       eval time =    4390.95 ms /   166 tokens (   26.45 ms per token,    37.81 tokens per second)
      total time =    4841.80 ms /   274 tokens
slot      release: id  3 | task 14431 | stop processing: n_tokens = 20654, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 14599 | processing task, is_child = 0
slot update_slots: id  2 | task 14599 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 301477
srv    send_error: task id = 14599, error: request (301477 tokens) exceeds the available context size (64000 tokens), try increasing it
slot      release: id  2 | task 14599 | stop processing: n_tokens = 0, truncated = 0
srv  update_slots: no tokens to decode
srv  update_slots: all slots are idle
srv          stop: cancel task, id_task = 14599
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 400
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.605 (> 0.100 thold), f_keep = 0.207
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 20654, total state size = 504.059 MiB
srv          load:  - looking for better prompt, base f_keep = 0.207, sim = 0.605
srv        update:  - cache state: 3 prompts, 2232.512 MiB (limits: 8192.000 MiB, 64000 tokens, 264028 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv  get_availabl: prompt cache update took 523.99 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14602 | processing task, is_child = 0
slot update_slots: id  3 | task 14602 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7053
slot update_slots: id  3 | task 14602 | n_past = 4270, slot.prompt.tokens.size() = 20654, seq_id = 3, pos_min = 19812, n_swa = 128
slot update_slots: id  3 | task 14602 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 5553, pos_max = 6435, n_swa = 128, size = 20.706 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 5636, pos_max = 6632, n_swa = 128, size = 23.379 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 17351, pos_max = 18374, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 18063, pos_max = 19086, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 18697, pos_max = 19513, n_swa = 128, size = 19.158 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 19151, pos_max = 19928, n_swa = 128, size = 18.244 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 19475, pos_max = 20316, n_swa = 128, size = 19.744 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 19583, pos_max = 20424, n_swa = 128, size = 19.744 MiB)
slot update_slots: id  3 | task 14602 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 14602 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.290373
slot update_slots: id  3 | task 14602 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 14602 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.580746
slot update_slots: id  3 | task 14602 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 14602 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.871119
slot update_slots: id  3 | task 14602 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  3 | task 14602 | prompt processing progress, n_tokens = 6989, batch.n_tokens = 845, progress = 0.990926
slot update_slots: id  3 | task 14602 | n_tokens = 6989, memory_seq_rm [6989, end)
slot update_slots: id  3 | task 14602 | prompt processing progress, n_tokens = 7053, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 14602 | prompt done, n_tokens = 7053, batch.n_tokens = 64
slot init_sampler: id  3 | task 14602 | init sampler, took 1.27 ms, tokens: text = 7053, total = 7053
slot update_slots: id  3 | task 14602 | created context checkpoint 1 of 8 (pos_min = 5965, pos_max = 6988, size = 24.012 MiB)
slot print_timing: id  3 | task 14602 | 
prompt eval time =    6849.42 ms /  7053 tokens (    0.97 ms per token,  1029.72 tokens per second)
       eval time =    1632.94 ms /    66 tokens (   24.74 ms per token,    40.42 tokens per second)
      total time =    8482.36 ms /  7119 tokens
slot      release: id  3 | task 14602 | stop processing: n_tokens = 7118, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14673 | processing task, is_child = 0
slot update_slots: id  3 | task 14673 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7107
slot update_slots: id  3 | task 14673 | n_tokens = 7053, memory_seq_rm [7053, end)
slot update_slots: id  3 | task 14673 | prompt processing progress, n_tokens = 7107, batch.n_tokens = 54, progress = 1.000000
slot update_slots: id  3 | task 14673 | prompt done, n_tokens = 7107, batch.n_tokens = 54
slot init_sampler: id  3 | task 14673 | init sampler, took 2.27 ms, tokens: text = 7107, total = 7107
slot print_timing: id  3 | task 14673 | 
prompt eval time =     153.37 ms /    54 tokens (    2.84 ms per token,   352.09 tokens per second)
       eval time =    2550.48 ms /   106 tokens (   24.06 ms per token,    41.56 tokens per second)
      total time =    2703.85 ms /   160 tokens
slot      release: id  3 | task 14673 | stop processing: n_tokens = 7212, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14780 | processing task, is_child = 0
slot update_slots: id  3 | task 14780 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7202
slot update_slots: id  3 | task 14780 | n_tokens = 7040, memory_seq_rm [7040, end)
slot update_slots: id  3 | task 14780 | prompt processing progress, n_tokens = 7138, batch.n_tokens = 98, progress = 0.991114
slot update_slots: id  3 | task 14780 | n_tokens = 7138, memory_seq_rm [7138, end)
slot update_slots: id  3 | task 14780 | prompt processing progress, n_tokens = 7202, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 14780 | prompt done, n_tokens = 7202, batch.n_tokens = 64
slot init_sampler: id  3 | task 14780 | init sampler, took 1.35 ms, tokens: text = 7202, total = 7202
slot update_slots: id  3 | task 14780 | created context checkpoint 2 of 8 (pos_min = 6242, pos_max = 7137, size = 21.011 MiB)
slot print_timing: id  3 | task 14780 | 
prompt eval time =     528.07 ms /   162 tokens (    3.26 ms per token,   306.78 tokens per second)
       eval time =   11865.44 ms /   511 tokens (   23.22 ms per token,    43.07 tokens per second)
      total time =   12393.51 ms /   673 tokens
slot      release: id  3 | task 14780 | stop processing: n_tokens = 7712, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.955 (> 0.100 thold), f_keep = 0.934
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15293 | processing task, is_child = 0
slot update_slots: id  3 | task 15293 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7544
slot update_slots: id  3 | task 15293 | n_tokens = 7202, memory_seq_rm [7202, end)
slot update_slots: id  3 | task 15293 | prompt processing progress, n_tokens = 7480, batch.n_tokens = 278, progress = 0.991516
slot update_slots: id  3 | task 15293 | n_tokens = 7480, memory_seq_rm [7480, end)
slot update_slots: id  3 | task 15293 | prompt processing progress, n_tokens = 7544, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15293 | prompt done, n_tokens = 7544, batch.n_tokens = 64
slot init_sampler: id  3 | task 15293 | init sampler, took 1.61 ms, tokens: text = 7544, total = 7544
slot update_slots: id  3 | task 15293 | created context checkpoint 3 of 8 (pos_min = 6816, pos_max = 7479, size = 15.570 MiB)
slot print_timing: id  3 | task 15293 | 
prompt eval time =     504.55 ms /   342 tokens (    1.48 ms per token,   677.83 tokens per second)
       eval time =    1200.82 ms /    49 tokens (   24.51 ms per token,    40.81 tokens per second)
      total time =    1705.37 ms /   391 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 15293 | stop processing: n_tokens = 7592, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.914 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15344 | processing task, is_child = 0
slot update_slots: id  3 | task 15344 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8256
slot update_slots: id  3 | task 15344 | n_tokens = 7544, memory_seq_rm [7544, end)
slot update_slots: id  3 | task 15344 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 648, progress = 0.992248
slot update_slots: id  3 | task 15344 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  3 | task 15344 | prompt processing progress, n_tokens = 8256, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15344 | prompt done, n_tokens = 8256, batch.n_tokens = 64
slot init_sampler: id  3 | task 15344 | init sampler, took 1.72 ms, tokens: text = 8256, total = 8256
slot update_slots: id  3 | task 15344 | created context checkpoint 4 of 8 (pos_min = 7168, pos_max = 8191, size = 24.012 MiB)
slot print_timing: id  3 | task 15344 | 
prompt eval time =     933.05 ms /   712 tokens (    1.31 ms per token,   763.09 tokens per second)
       eval time =    8465.07 ms /   360 tokens (   23.51 ms per token,    42.53 tokens per second)
      total time =    9398.12 ms /  1072 tokens
slot      release: id  3 | task 15344 | stop processing: n_tokens = 8615, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15706 | processing task, is_child = 0
slot update_slots: id  3 | task 15706 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8402
slot update_slots: id  3 | task 15706 | n_tokens = 8256, memory_seq_rm [8256, end)
slot update_slots: id  3 | task 15706 | prompt processing progress, n_tokens = 8338, batch.n_tokens = 82, progress = 0.992383
slot update_slots: id  3 | task 15706 | n_tokens = 8338, memory_seq_rm [8338, end)
slot update_slots: id  3 | task 15706 | prompt processing progress, n_tokens = 8402, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15706 | prompt done, n_tokens = 8402, batch.n_tokens = 64
slot init_sampler: id  3 | task 15706 | init sampler, took 1.24 ms, tokens: text = 8402, total = 8402
slot update_slots: id  3 | task 15706 | created context checkpoint 5 of 8 (pos_min = 7591, pos_max = 8337, size = 17.517 MiB)
slot print_timing: id  3 | task 15706 | 
prompt eval time =     380.28 ms /   146 tokens (    2.60 ms per token,   383.92 tokens per second)
       eval time =     844.91 ms /    36 tokens (   23.47 ms per token,    42.61 tokens per second)
      total time =    1225.20 ms /   182 tokens
slot      release: id  3 | task 15706 | stop processing: n_tokens = 8437, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15744 | processing task, is_child = 0
slot update_slots: id  3 | task 15744 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8525
slot update_slots: id  3 | task 15744 | n_tokens = 8402, memory_seq_rm [8402, end)
slot update_slots: id  3 | task 15744 | prompt processing progress, n_tokens = 8461, batch.n_tokens = 59, progress = 0.992493
slot update_slots: id  3 | task 15744 | n_tokens = 8461, memory_seq_rm [8461, end)
slot update_slots: id  3 | task 15744 | prompt processing progress, n_tokens = 8525, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15744 | prompt done, n_tokens = 8525, batch.n_tokens = 64
slot init_sampler: id  3 | task 15744 | init sampler, took 1.60 ms, tokens: text = 8525, total = 8525
slot update_slots: id  3 | task 15744 | created context checkpoint 6 of 8 (pos_min = 7591, pos_max = 8460, size = 20.401 MiB)
slot print_timing: id  3 | task 15744 | 
prompt eval time =     406.20 ms /   123 tokens (    3.30 ms per token,   302.81 tokens per second)
       eval time =    1283.50 ms /    52 tokens (   24.68 ms per token,    40.51 tokens per second)
      total time =    1689.70 ms /   175 tokens
slot      release: id  3 | task 15744 | stop processing: n_tokens = 8576, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15798 | processing task, is_child = 0
slot update_slots: id  3 | task 15798 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8584
slot update_slots: id  3 | task 15798 | n_tokens = 8525, memory_seq_rm [8525, end)
slot update_slots: id  3 | task 15798 | prompt processing progress, n_tokens = 8584, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 15798 | prompt done, n_tokens = 8584, batch.n_tokens = 59
slot init_sampler: id  3 | task 15798 | init sampler, took 2.04 ms, tokens: text = 8584, total = 8584
slot print_timing: id  3 | task 15798 | 
prompt eval time =     154.33 ms /    59 tokens (    2.62 ms per token,   382.29 tokens per second)
       eval time =    2014.37 ms /    85 tokens (   23.70 ms per token,    42.20 tokens per second)
      total time =    2168.70 ms /   144 tokens
slot      release: id  3 | task 15798 | stop processing: n_tokens = 8668, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15884 | processing task, is_child = 0
slot update_slots: id  3 | task 15884 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8677
slot update_slots: id  3 | task 15884 | n_tokens = 8584, memory_seq_rm [8584, end)
slot update_slots: id  3 | task 15884 | prompt processing progress, n_tokens = 8613, batch.n_tokens = 29, progress = 0.992624
slot update_slots: id  3 | task 15884 | n_tokens = 8613, memory_seq_rm [8613, end)
slot update_slots: id  3 | task 15884 | prompt processing progress, n_tokens = 8677, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15884 | prompt done, n_tokens = 8677, batch.n_tokens = 64
slot init_sampler: id  3 | task 15884 | init sampler, took 1.46 ms, tokens: text = 8677, total = 8677
slot update_slots: id  3 | task 15884 | created context checkpoint 7 of 8 (pos_min = 7644, pos_max = 8612, size = 22.722 MiB)
slot print_timing: id  3 | task 15884 | 
prompt eval time =     279.41 ms /    93 tokens (    3.00 ms per token,   332.84 tokens per second)
       eval time =    4968.99 ms /   210 tokens (   23.66 ms per token,    42.26 tokens per second)
      total time =    5248.40 ms /   303 tokens
slot      release: id  3 | task 15884 | stop processing: n_tokens = 8886, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 16096 | processing task, is_child = 0
slot update_slots: id  3 | task 16096 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8772
slot update_slots: id  3 | task 16096 | n_tokens = 8677, memory_seq_rm [8677, end)
slot update_slots: id  3 | task 16096 | prompt processing progress, n_tokens = 8708, batch.n_tokens = 31, progress = 0.992704
slot update_slots: id  3 | task 16096 | n_tokens = 8708, memory_seq_rm [8708, end)
slot update_slots: id  3 | task 16096 | prompt processing progress, n_tokens = 8772, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 16096 | prompt done, n_tokens = 8772, batch.n_tokens = 64
slot init_sampler: id  3 | task 16096 | init sampler, took 1.29 ms, tokens: text = 8772, total = 8772
slot update_slots: id  3 | task 16096 | created context checkpoint 8 of 8 (pos_min = 7862, pos_max = 8707, size = 19.838 MiB)
slot print_timing: id  3 | task 16096 | 
prompt eval time =     289.70 ms /    95 tokens (    3.05 ms per token,   327.92 tokens per second)
       eval time =    2429.25 ms /    99 tokens (   24.54 ms per token,    40.75 tokens per second)
      total time =    2718.96 ms /   194 tokens
slot      release: id  3 | task 16096 | stop processing: n_tokens = 8870, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 16197 | processing task, is_child = 0
slot update_slots: id  3 | task 16197 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8866
slot update_slots: id  3 | task 16197 | n_tokens = 8772, memory_seq_rm [8772, end)
slot update_slots: id  3 | task 16197 | prompt processing progress, n_tokens = 8802, batch.n_tokens = 30, progress = 0.992781
slot update_slots: id  3 | task 16197 | n_tokens = 8802, memory_seq_rm [8802, end)
slot update_slots: id  3 | task 16197 | prompt processing progress, n_tokens = 8866, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 16197 | prompt done, n_tokens = 8866, batch.n_tokens = 64
slot init_sampler: id  3 | task 16197 | init sampler, took 1.75 ms, tokens: text = 8866, total = 8866
slot update_slots: id  3 | task 16197 | erasing old context checkpoint (pos_min = 5965, pos_max = 6988, size = 24.012 MiB)
slot update_slots: id  3 | task 16197 | created context checkpoint 8 of 8 (pos_min = 7862, pos_max = 8801, size = 22.042 MiB)
slot print_timing: id  3 | task 16197 | 
prompt eval time =     398.01 ms /    94 tokens (    4.23 ms per token,   236.18 tokens per second)
       eval time =    5654.35 ms /   237 tokens (   23.86 ms per token,    41.91 tokens per second)
      total time =    6052.36 ms /   331 tokens
slot      release: id  3 | task 16197 | stop processing: n_tokens = 9102, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 16436 | processing task, is_child = 0
slot update_slots: id  3 | task 16436 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9066
slot update_slots: id  3 | task 16436 | n_tokens = 8866, memory_seq_rm [8866, end)
slot update_slots: id  3 | task 16436 | prompt processing progress, n_tokens = 9002, batch.n_tokens = 136, progress = 0.992941
slot update_slots: id  3 | task 16436 | n_tokens = 9002, memory_seq_rm [9002, end)
slot update_slots: id  3 | task 16436 | prompt processing progress, n_tokens = 9066, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 16436 | prompt done, n_tokens = 9066, batch.n_tokens = 64
slot init_sampler: id  3 | task 16436 | init sampler, took 1.30 ms, tokens: text = 9066, total = 9066
slot update_slots: id  3 | task 16436 | erasing old context checkpoint (pos_min = 6242, pos_max = 7137, size = 21.011 MiB)
slot update_slots: id  3 | task 16436 | created context checkpoint 8 of 8 (pos_min = 8200, pos_max = 9001, size = 18.806 MiB)
slot print_timing: id  3 | task 16436 | 
prompt eval time =     386.36 ms /   200 tokens (    1.93 ms per token,   517.66 tokens per second)
       eval time =    5476.19 ms /   230 tokens (   23.81 ms per token,    42.00 tokens per second)
      total time =    5862.55 ms /   430 tokens
slot      release: id  3 | task 16436 | stop processing: n_tokens = 9295, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 16668 | processing task, is_child = 0
slot update_slots: id  3 | task 16668 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9229
slot update_slots: id  3 | task 16668 | n_tokens = 9066, memory_seq_rm [9066, end)
slot update_slots: id  3 | task 16668 | prompt processing progress, n_tokens = 9165, batch.n_tokens = 99, progress = 0.993065
slot update_slots: id  3 | task 16668 | n_tokens = 9165, memory_seq_rm [9165, end)
slot update_slots: id  3 | task 16668 | prompt processing progress, n_tokens = 9229, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 16668 | prompt done, n_tokens = 9229, batch.n_tokens = 64
slot init_sampler: id  3 | task 16668 | init sampler, took 1.66 ms, tokens: text = 9229, total = 9229
slot update_slots: id  3 | task 16668 | erasing old context checkpoint (pos_min = 6816, pos_max = 7479, size = 15.570 MiB)
slot update_slots: id  3 | task 16668 | created context checkpoint 8 of 8 (pos_min = 8493, pos_max = 9164, size = 15.758 MiB)
slot print_timing: id  3 | task 16668 | 
prompt eval time =     518.04 ms /   163 tokens (    3.18 ms per token,   314.64 tokens per second)
       eval time =   14779.58 ms /   614 tokens (   24.07 ms per token,    41.54 tokens per second)
      total time =   15297.63 ms /   777 tokens
slot      release: id  3 | task 16668 | stop processing: n_tokens = 9842, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.938
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17284 | processing task, is_child = 0
slot update_slots: id  3 | task 17284 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9564
slot update_slots: id  3 | task 17284 | n_tokens = 9229, memory_seq_rm [9229, end)
slot update_slots: id  3 | task 17284 | prompt processing progress, n_tokens = 9500, batch.n_tokens = 271, progress = 0.993308
slot update_slots: id  3 | task 17284 | n_tokens = 9500, memory_seq_rm [9500, end)
slot update_slots: id  3 | task 17284 | prompt processing progress, n_tokens = 9564, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17284 | prompt done, n_tokens = 9564, batch.n_tokens = 64
slot init_sampler: id  3 | task 17284 | init sampler, took 1.93 ms, tokens: text = 9564, total = 9564
slot update_slots: id  3 | task 17284 | erasing old context checkpoint (pos_min = 7168, pos_max = 8191, size = 24.012 MiB)
slot update_slots: id  3 | task 17284 | created context checkpoint 8 of 8 (pos_min = 8866, pos_max = 9499, size = 14.867 MiB)
slot print_timing: id  3 | task 17284 | 
prompt eval time =     524.57 ms /   335 tokens (    1.57 ms per token,   638.61 tokens per second)
       eval time =     563.19 ms /    22 tokens (   25.60 ms per token,    39.06 tokens per second)
      total time =    1087.76 ms /   357 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 17284 | stop processing: n_tokens = 9585, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17308 | processing task, is_child = 0
slot update_slots: id  3 | task 17308 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9622
slot update_slots: id  3 | task 17308 | n_tokens = 9564, memory_seq_rm [9564, end)
slot update_slots: id  3 | task 17308 | prompt processing progress, n_tokens = 9622, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 17308 | prompt done, n_tokens = 9622, batch.n_tokens = 58
slot init_sampler: id  3 | task 17308 | init sampler, took 1.37 ms, tokens: text = 9622, total = 9622
slot print_timing: id  3 | task 17308 | 
prompt eval time =     274.52 ms /    58 tokens (    4.73 ms per token,   211.28 tokens per second)
       eval time =   12959.54 ms /   525 tokens (   24.68 ms per token,    40.51 tokens per second)
      total time =   13234.06 ms /   583 tokens
slot      release: id  3 | task 17308 | stop processing: n_tokens = 10146, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.948
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17834 | processing task, is_child = 0
slot update_slots: id  3 | task 17834 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10131
slot update_slots: id  3 | task 17834 | n_tokens = 9622, memory_seq_rm [9622, end)
slot update_slots: id  3 | task 17834 | prompt processing progress, n_tokens = 10067, batch.n_tokens = 445, progress = 0.993683
slot update_slots: id  3 | task 17834 | n_tokens = 10067, memory_seq_rm [10067, end)
slot update_slots: id  3 | task 17834 | prompt processing progress, n_tokens = 10131, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17834 | prompt done, n_tokens = 10131, batch.n_tokens = 64
slot init_sampler: id  3 | task 17834 | init sampler, took 1.96 ms, tokens: text = 10131, total = 10131
slot update_slots: id  3 | task 17834 | erasing old context checkpoint (pos_min = 7591, pos_max = 8337, size = 17.517 MiB)
slot update_slots: id  3 | task 17834 | created context checkpoint 8 of 8 (pos_min = 9311, pos_max = 10066, size = 17.728 MiB)
slot print_timing: id  3 | task 17834 | 
prompt eval time =     656.34 ms /   509 tokens (    1.29 ms per token,   775.51 tokens per second)
       eval time =    1780.00 ms /    73 tokens (   24.38 ms per token,    41.01 tokens per second)
      total time =    2436.34 ms /   582 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 17834 | stop processing: n_tokens = 10203, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.061
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 10203, total state size = 256.978 MiB
srv          load:  - looking for better prompt, base f_keep = 0.061, sim = 0.958
srv        update:  - cache state: 4 prompts, 2641.652 MiB (limits: 8192.000 MiB, 64000 tokens, 254776 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv  get_availabl: prompt cache update took 408.90 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17909 | processing task, is_child = 0
slot update_slots: id  3 | task 17909 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 648
slot update_slots: id  3 | task 17909 | n_past = 621, slot.prompt.tokens.size() = 10203, seq_id = 3, pos_min = 9447, n_swa = 128
slot update_slots: id  3 | task 17909 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 7591, pos_max = 8460, n_swa = 128, size = 20.401 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 7644, pos_max = 8612, n_swa = 128, size = 22.722 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 7862, pos_max = 8707, n_swa = 128, size = 19.838 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 7862, pos_max = 8801, n_swa = 128, size = 22.042 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 8200, pos_max = 9001, n_swa = 128, size = 18.806 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 8493, pos_max = 9164, n_swa = 128, size = 15.758 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 8866, pos_max = 9499, n_swa = 128, size = 14.867 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 9311, pos_max = 10066, n_swa = 128, size = 17.728 MiB)
slot update_slots: id  3 | task 17909 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 17909 | prompt processing progress, n_tokens = 584, batch.n_tokens = 584, progress = 0.901235
slot update_slots: id  3 | task 17909 | n_tokens = 584, memory_seq_rm [584, end)
slot update_slots: id  3 | task 17909 | prompt processing progress, n_tokens = 648, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17909 | prompt done, n_tokens = 648, batch.n_tokens = 64
slot init_sampler: id  3 | task 17909 | init sampler, took 0.11 ms, tokens: text = 648, total = 648
slot update_slots: id  3 | task 17909 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 583, size = 13.694 MiB)
slot print_timing: id  3 | task 17909 | 
prompt eval time =     793.32 ms /   648 tokens (    1.22 ms per token,   816.82 tokens per second)
       eval time =    2554.02 ms /   108 tokens (   23.65 ms per token,    42.29 tokens per second)
      total time =    3347.33 ms /   756 tokens
slot      release: id  3 | task 17909 | stop processing: n_tokens = 755, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.893 (> 0.100 thold), f_keep = 0.858
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18019 | processing task, is_child = 0
slot update_slots: id  3 | task 18019 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 726
slot update_slots: id  3 | task 18019 | n_tokens = 648, memory_seq_rm [648, end)
slot update_slots: id  3 | task 18019 | prompt processing progress, n_tokens = 662, batch.n_tokens = 14, progress = 0.911846
slot update_slots: id  3 | task 18019 | n_tokens = 662, memory_seq_rm [662, end)
slot update_slots: id  3 | task 18019 | prompt processing progress, n_tokens = 726, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18019 | prompt done, n_tokens = 726, batch.n_tokens = 64
slot init_sampler: id  3 | task 18019 | init sampler, took 0.14 ms, tokens: text = 726, total = 726
slot update_slots: id  3 | task 18019 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 661, size = 15.523 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18022 | processing task, is_child = 0
slot update_slots: id  1 | task 18022 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10249
slot update_slots: id  1 | task 18022 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 2047, batch.n_tokens = 2048, progress = 0.199727
slot update_slots: id  1 | task 18022 | n_tokens = 2047, memory_seq_rm [2047, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 4094, batch.n_tokens = 2048, progress = 0.399454
slot update_slots: id  1 | task 18022 | n_tokens = 4094, memory_seq_rm [4094, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 6141, batch.n_tokens = 2048, progress = 0.599180
slot update_slots: id  1 | task 18022 | n_tokens = 6141, memory_seq_rm [6141, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 8188, batch.n_tokens = 2048, progress = 0.798907
slot update_slots: id  1 | task 18022 | n_tokens = 8188, memory_seq_rm [8188, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 10185, batch.n_tokens = 1998, progress = 0.993755
slot update_slots: id  1 | task 18022 | n_tokens = 10185, memory_seq_rm [10185, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 10249, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 18022 | prompt done, n_tokens = 10249, batch.n_tokens = 65
slot init_sampler: id  1 | task 18022 | init sampler, took 1.82 ms, tokens: text = 10249, total = 10249
slot update_slots: id  1 | task 18022 | created context checkpoint 1 of 8 (pos_min = 9288, pos_max = 10184, size = 21.034 MiB)
slot print_timing: id  1 | task 18022 | 
prompt eval time =   10320.58 ms / 10249 tokens (    1.01 ms per token,   993.06 tokens per second)
       eval time =    5097.21 ms /   116 tokens (   43.94 ms per token,    22.76 tokens per second)
      total time =   15417.79 ms / 10365 tokens
slot      release: id  1 | task 18022 | stop processing: n_tokens = 10364, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 18019 | 
prompt eval time =     226.30 ms /    78 tokens (    2.90 ms per token,   344.68 tokens per second)
       eval time =   15768.90 ms /   136 tokens (  115.95 ms per token,     8.62 tokens per second)
      total time =   15995.20 ms /   214 tokens
slot      release: id  3 | task 18019 | stop processing: n_tokens = 861, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.902 (> 0.100 thold), f_keep = 0.843
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18158 | processing task, is_child = 0
slot update_slots: id  3 | task 18158 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 805
slot update_slots: id  3 | task 18158 | n_past = 726, slot.prompt.tokens.size() = 861, seq_id = 3, pos_min = 604, n_swa = 128
slot update_slots: id  3 | task 18158 | restored context checkpoint (pos_min = 0, pos_max = 661, size = 15.523 MiB)
slot update_slots: id  3 | task 18158 | n_tokens = 661, memory_seq_rm [661, end)
slot update_slots: id  3 | task 18158 | prompt processing progress, n_tokens = 741, batch.n_tokens = 80, progress = 0.920497
slot update_slots: id  3 | task 18158 | n_tokens = 741, memory_seq_rm [741, end)
slot update_slots: id  3 | task 18158 | prompt processing progress, n_tokens = 805, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18158 | prompt done, n_tokens = 805, batch.n_tokens = 64
slot init_sampler: id  3 | task 18158 | init sampler, took 0.15 ms, tokens: text = 805, total = 805
slot update_slots: id  3 | task 18158 | created context checkpoint 3 of 8 (pos_min = 0, pos_max = 740, size = 17.376 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18192 | processing task, is_child = 0
slot update_slots: id  1 | task 18192 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10747
slot update_slots: id  1 | task 18192 | n_past = 10249, slot.prompt.tokens.size() = 10364, seq_id = 1, pos_min = 10237, n_swa = 128
slot update_slots: id  1 | task 18192 | restored context checkpoint (pos_min = 9288, pos_max = 10184, size = 21.034 MiB)
slot update_slots: id  1 | task 18192 | n_tokens = 10184, memory_seq_rm [10184, end)
slot update_slots: id  1 | task 18192 | prompt processing progress, n_tokens = 10683, batch.n_tokens = 500, progress = 0.994045
slot update_slots: id  1 | task 18192 | n_tokens = 10683, memory_seq_rm [10683, end)
slot update_slots: id  1 | task 18192 | prompt processing progress, n_tokens = 10747, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 18192 | prompt done, n_tokens = 10747, batch.n_tokens = 65
slot init_sampler: id  1 | task 18192 | init sampler, took 5.34 ms, tokens: text = 10747, total = 10747
slot update_slots: id  1 | task 18192 | created context checkpoint 2 of 8 (pos_min = 9787, pos_max = 10682, size = 21.011 MiB)
slot print_timing: id  3 | task 18158 | 
prompt eval time =     608.77 ms /   144 tokens (    4.23 ms per token,   236.54 tokens per second)
       eval time =    5063.66 ms /   107 tokens (   47.32 ms per token,    21.13 tokens per second)
      total time =    5672.43 ms /   251 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 18158 | stop processing: n_tokens = 911, truncated = 0
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.927 (> 0.100 thold), f_keep = 0.884
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18279 | processing task, is_child = 0
slot update_slots: id  3 | task 18279 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 868
slot update_slots: id  3 | task 18279 | n_past = 805, slot.prompt.tokens.size() = 911, seq_id = 3, pos_min = 709, n_swa = 128
slot update_slots: id  3 | task 18279 | restored context checkpoint (pos_min = 0, pos_max = 740, size = 17.376 MiB)
slot update_slots: id  3 | task 18279 | n_tokens = 740, memory_seq_rm [740, end)
slot update_slots: id  3 | task 18279 | prompt processing progress, n_tokens = 804, batch.n_tokens = 65, progress = 0.926267
slot update_slots: id  3 | task 18279 | n_tokens = 804, memory_seq_rm [804, end)
slot update_slots: id  3 | task 18279 | prompt processing progress, n_tokens = 868, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 18279 | prompt done, n_tokens = 868, batch.n_tokens = 65
slot init_sampler: id  3 | task 18279 | init sampler, took 0.17 ms, tokens: text = 868, total = 868
slot print_timing: id  1 | task 18192 | 
prompt eval time =     981.99 ms /   563 tokens (    1.74 ms per token,   573.33 tokens per second)
       eval time =    6146.05 ms /   129 tokens (   47.64 ms per token,    20.99 tokens per second)
      total time =    7128.04 ms /   692 tokens
slot      release: id  1 | task 18192 | stop processing: n_tokens = 10875, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 18279 | 
prompt eval time =     624.85 ms /   128 tokens (    4.88 ms per token,   204.85 tokens per second)
       eval time =    3812.42 ms /   113 tokens (   33.74 ms per token,    29.64 tokens per second)
      total time =    4437.26 ms /   241 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 18279 | stop processing: n_tokens = 980, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.886
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18394 | processing task, is_child = 0
slot update_slots: id  3 | task 18394 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 931
slot update_slots: id  3 | task 18394 | n_tokens = 868, memory_seq_rm [868, end)
slot update_slots: id  3 | task 18394 | prompt processing progress, n_tokens = 931, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 18394 | prompt done, n_tokens = 931, batch.n_tokens = 63
slot init_sampler: id  3 | task 18394 | init sampler, took 0.20 ms, tokens: text = 931, total = 931
slot update_slots: id  3 | task 18394 | created context checkpoint 4 of 8 (pos_min = 127, pos_max = 867, size = 17.376 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18400 | processing task, is_child = 0
slot update_slots: id  1 | task 18400 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10888
slot update_slots: id  1 | task 18400 | n_past = 10747, slot.prompt.tokens.size() = 10875, seq_id = 1, pos_min = 10704, n_swa = 128
slot update_slots: id  1 | task 18400 | restored context checkpoint (pos_min = 9787, pos_max = 10682, size = 21.011 MiB)
slot update_slots: id  1 | task 18400 | n_tokens = 10682, memory_seq_rm [10682, end)
slot update_slots: id  1 | task 18400 | prompt processing progress, n_tokens = 10824, batch.n_tokens = 143, progress = 0.994122
slot update_slots: id  1 | task 18400 | n_tokens = 10824, memory_seq_rm [10824, end)
slot update_slots: id  1 | task 18400 | prompt processing progress, n_tokens = 10888, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 18400 | prompt done, n_tokens = 10888, batch.n_tokens = 65
slot init_sampler: id  1 | task 18400 | init sampler, took 2.00 ms, tokens: text = 10888, total = 10888
slot update_slots: id  1 | task 18400 | created context checkpoint 3 of 8 (pos_min = 9929, pos_max = 10823, size = 20.987 MiB)
slot print_timing: id  1 | task 18400 | 
prompt eval time =     656.09 ms /   206 tokens (    3.18 ms per token,   313.98 tokens per second)
       eval time =    2027.09 ms /    45 tokens (   45.05 ms per token,    22.20 tokens per second)
      total time =    2683.18 ms /   251 tokens
slot      release: id  1 | task 18400 | stop processing: n_tokens = 10932, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 18394 | 
prompt eval time =     206.84 ms /    63 tokens (    3.28 ms per token,   304.59 tokens per second)
       eval time =    7321.87 ms /   229 tokens (   31.97 ms per token,    31.28 tokens per second)
      total time =    7528.70 ms /   292 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 18394 | stop processing: n_tokens = 1159, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.937 (> 0.100 thold), f_keep = 0.803
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18625 | processing task, is_child = 0
slot update_slots: id  3 | task 18625 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 994
slot update_slots: id  3 | task 18625 | n_past = 931, slot.prompt.tokens.size() = 1159, seq_id = 3, pos_min = 888, n_swa = 128
slot update_slots: id  3 | task 18625 | restored context checkpoint (pos_min = 127, pos_max = 867, size = 17.376 MiB)
slot update_slots: id  3 | task 18625 | n_tokens = 867, memory_seq_rm [867, end)
slot update_slots: id  3 | task 18625 | prompt processing progress, n_tokens = 930, batch.n_tokens = 63, progress = 0.935614
slot update_slots: id  3 | task 18625 | n_tokens = 930, memory_seq_rm [930, end)
slot update_slots: id  3 | task 18625 | prompt processing progress, n_tokens = 994, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18625 | prompt done, n_tokens = 994, batch.n_tokens = 64
slot init_sampler: id  3 | task 18625 | init sampler, took 0.18 ms, tokens: text = 994, total = 994
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18708 | processing task, is_child = 0
slot update_slots: id  1 | task 18708 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11021
slot update_slots: id  1 | task 18708 | n_past = 10888, slot.prompt.tokens.size() = 10932, seq_id = 1, pos_min = 10805, n_swa = 128
slot update_slots: id  1 | task 18708 | restored context checkpoint (pos_min = 9929, pos_max = 10823, size = 20.987 MiB)
slot update_slots: id  1 | task 18708 | n_tokens = 10823, memory_seq_rm [10823, end)
slot update_slots: id  1 | task 18708 | prompt processing progress, n_tokens = 10957, batch.n_tokens = 135, progress = 0.994193
slot update_slots: id  1 | task 18708 | n_tokens = 10957, memory_seq_rm [10957, end)
slot update_slots: id  1 | task 18708 | prompt processing progress, n_tokens = 11021, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 18708 | prompt done, n_tokens = 11021, batch.n_tokens = 65
slot init_sampler: id  1 | task 18708 | init sampler, took 2.31 ms, tokens: text = 11021, total = 11021
slot update_slots: id  1 | task 18708 | created context checkpoint 4 of 8 (pos_min = 10063, pos_max = 10956, size = 20.964 MiB)
slot print_timing: id  1 | task 18708 | 
prompt eval time =     485.29 ms /   198 tokens (    2.45 ms per token,   408.00 tokens per second)
       eval time =    1900.94 ms /    40 tokens (   47.52 ms per token,    21.04 tokens per second)
      total time =    2386.23 ms /   238 tokens
slot      release: id  1 | task 18708 | stop processing: n_tokens = 11060, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 18625 | 
prompt eval time =     699.37 ms /   127 tokens (    5.51 ms per token,   181.59 tokens per second)
       eval time =    5460.35 ms /   154 tokens (   35.46 ms per token,    28.20 tokens per second)
      total time =    6159.72 ms /   281 tokens
slot      release: id  3 | task 18625 | stop processing: n_tokens = 1147, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.940 (> 0.100 thold), f_keep = 0.867
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18782 | processing task, is_child = 0
slot update_slots: id  3 | task 18782 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1057
slot update_slots: id  3 | task 18782 | n_past = 994, slot.prompt.tokens.size() = 1147, seq_id = 3, pos_min = 984, n_swa = 128
slot update_slots: id  3 | task 18782 | restored context checkpoint (pos_min = 127, pos_max = 867, size = 17.376 MiB)
slot update_slots: id  3 | task 18782 | n_tokens = 867, memory_seq_rm [867, end)
slot update_slots: id  3 | task 18782 | prompt processing progress, n_tokens = 993, batch.n_tokens = 126, progress = 0.939451
slot update_slots: id  3 | task 18782 | n_tokens = 993, memory_seq_rm [993, end)
slot update_slots: id  3 | task 18782 | prompt processing progress, n_tokens = 1057, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18782 | prompt done, n_tokens = 1057, batch.n_tokens = 64
slot init_sampler: id  3 | task 18782 | init sampler, took 0.19 ms, tokens: text = 1057, total = 1057
slot update_slots: id  3 | task 18782 | created context checkpoint 5 of 8 (pos_min = 127, pos_max = 992, size = 20.307 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.755 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18824 | processing task, is_child = 0
slot update_slots: id  1 | task 18824 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14604
slot update_slots: id  1 | task 18824 | n_past = 11021, slot.prompt.tokens.size() = 11060, seq_id = 1, pos_min = 10933, n_swa = 128
slot update_slots: id  1 | task 18824 | restored context checkpoint (pos_min = 10063, pos_max = 10956, size = 20.964 MiB)
slot update_slots: id  1 | task 18824 | n_tokens = 10956, memory_seq_rm [10956, end)
slot update_slots: id  1 | task 18824 | prompt processing progress, n_tokens = 13003, batch.n_tokens = 2048, progress = 0.890373
slot update_slots: id  1 | task 18824 | n_tokens = 13003, memory_seq_rm [13003, end)
slot update_slots: id  1 | task 18824 | prompt processing progress, n_tokens = 14540, batch.n_tokens = 1538, progress = 0.995618
slot update_slots: id  1 | task 18824 | n_tokens = 14540, memory_seq_rm [14540, end)
slot update_slots: id  1 | task 18824 | prompt processing progress, n_tokens = 14604, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 18824 | prompt done, n_tokens = 14604, batch.n_tokens = 65
slot init_sampler: id  1 | task 18824 | init sampler, took 2.76 ms, tokens: text = 14604, total = 14604
slot update_slots: id  1 | task 18824 | created context checkpoint 5 of 8 (pos_min = 13643, pos_max = 14539, size = 21.034 MiB)
slot print_timing: id  3 | task 18782 | 
prompt eval time =     748.34 ms /   190 tokens (    3.94 ms per token,   253.90 tokens per second)
       eval time =   13876.85 ms /   223 tokens (   62.23 ms per token,    16.07 tokens per second)
      total time =   14625.19 ms /   413 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 18782 | stop processing: n_tokens = 1279, truncated = 0
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.826
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19026 | processing task, is_child = 0
slot update_slots: id  3 | task 19026 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1132
slot update_slots: id  3 | task 19026 | n_past = 1057, slot.prompt.tokens.size() = 1279, seq_id = 3, pos_min = 1011, n_swa = 128
slot update_slots: id  3 | task 19026 | restored context checkpoint (pos_min = 127, pos_max = 992, size = 20.307 MiB)
slot update_slots: id  3 | task 19026 | n_tokens = 992, memory_seq_rm [992, end)
slot update_slots: id  3 | task 19026 | prompt processing progress, n_tokens = 1068, batch.n_tokens = 77, progress = 0.943463
slot update_slots: id  3 | task 19026 | n_tokens = 1068, memory_seq_rm [1068, end)
slot update_slots: id  3 | task 19026 | prompt processing progress, n_tokens = 1132, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 19026 | prompt done, n_tokens = 1132, batch.n_tokens = 65
slot init_sampler: id  3 | task 19026 | init sampler, took 0.21 ms, tokens: text = 1132, total = 1132
slot update_slots: id  3 | task 19026 | created context checkpoint 6 of 8 (pos_min = 172, pos_max = 1067, size = 21.011 MiB)
slot print_timing: id  3 | task 19026 | 
prompt eval time =     736.34 ms /   140 tokens (    5.26 ms per token,   190.13 tokens per second)
       eval time =   12634.90 ms /   278 tokens (   45.45 ms per token,    22.00 tokens per second)
      total time =   13371.24 ms /   418 tokens
slot      release: id  3 | task 19026 | stop processing: n_tokens = 1409, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.803
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19316 | processing task, is_child = 0
slot update_slots: id  3 | task 19316 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1205
slot update_slots: id  3 | task 19316 | n_tokens = 1132, memory_seq_rm [1132, end)
slot update_slots: id  3 | task 19316 | prompt processing progress, n_tokens = 1141, batch.n_tokens = 10, progress = 0.946888
slot update_slots: id  3 | task 19316 | n_tokens = 1141, memory_seq_rm [1141, end)
slot update_slots: id  3 | task 19316 | prompt processing progress, n_tokens = 1205, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 19316 | prompt done, n_tokens = 1205, batch.n_tokens = 65
slot init_sampler: id  3 | task 19316 | init sampler, took 0.23 ms, tokens: text = 1205, total = 1205
slot update_slots: id  3 | task 19316 | created context checkpoint 7 of 8 (pos_min = 674, pos_max = 1140, size = 10.951 MiB)
slot print_timing: id  1 | task 18824 | 
prompt eval time =    4425.31 ms /  3648 tokens (    1.21 ms per token,   824.35 tokens per second)
       eval time =   28424.30 ms /   609 tokens (   46.67 ms per token,    21.43 tokens per second)
      total time =   32849.61 ms /  4257 tokens
slot      release: id  1 | task 18824 | stop processing: n_tokens = 15212, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.960
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19531 | processing task, is_child = 0
slot update_slots: id  1 | task 19531 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14925
slot update_slots: id  1 | task 19531 | n_past = 14604, slot.prompt.tokens.size() = 15212, seq_id = 1, pos_min = 15008, n_swa = 128
slot update_slots: id  1 | task 19531 | restored context checkpoint (pos_min = 13643, pos_max = 14539, size = 21.034 MiB)
slot update_slots: id  1 | task 19531 | n_tokens = 14539, memory_seq_rm [14539, end)
slot update_slots: id  1 | task 19531 | prompt processing progress, n_tokens = 14861, batch.n_tokens = 323, progress = 0.995712
slot update_slots: id  1 | task 19531 | n_tokens = 14861, memory_seq_rm [14861, end)
slot update_slots: id  1 | task 19531 | prompt processing progress, n_tokens = 14925, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 19531 | prompt done, n_tokens = 14925, batch.n_tokens = 65
slot init_sampler: id  1 | task 19531 | init sampler, took 4.43 ms, tokens: text = 14925, total = 14925
slot update_slots: id  1 | task 19531 | created context checkpoint 6 of 8 (pos_min = 13965, pos_max = 14860, size = 21.011 MiB)
slot print_timing: id  3 | task 19316 | 
prompt eval time =     309.32 ms /    73 tokens (    4.24 ms per token,   236.00 tokens per second)
       eval time =    9472.05 ms /   230 tokens (   41.18 ms per token,    24.28 tokens per second)
      total time =    9781.37 ms /   303 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 19316 | stop processing: n_tokens = 1434, truncated = 0
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.840
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19560 | processing task, is_child = 0
slot update_slots: id  3 | task 19560 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1268
slot update_slots: id  3 | task 19560 | n_past = 1205, slot.prompt.tokens.size() = 1434, seq_id = 3, pos_min = 1291, n_swa = 128
slot update_slots: id  3 | task 19560 | restored context checkpoint (pos_min = 674, pos_max = 1140, size = 10.951 MiB)
slot update_slots: id  3 | task 19560 | n_tokens = 1140, memory_seq_rm [1140, end)
slot update_slots: id  3 | task 19560 | prompt processing progress, n_tokens = 1204, batch.n_tokens = 65, progress = 0.949527
slot update_slots: id  3 | task 19560 | n_tokens = 1204, memory_seq_rm [1204, end)
slot update_slots: id  3 | task 19560 | prompt processing progress, n_tokens = 1268, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 19560 | prompt done, n_tokens = 1268, batch.n_tokens = 65
slot init_sampler: id  3 | task 19560 | init sampler, took 0.22 ms, tokens: text = 1268, total = 1268
slot print_timing: id  1 | task 19531 | 
prompt eval time =     844.10 ms /   386 tokens (    2.19 ms per token,   457.29 tokens per second)
       eval time =    6991.28 ms /   149 tokens (   46.92 ms per token,    21.31 tokens per second)
      total time =    7835.38 ms /   535 tokens
slot      release: id  1 | task 19531 | stop processing: n_tokens = 15073, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 19560 | 
prompt eval time =     528.65 ms /   128 tokens (    4.13 ms per token,   242.13 tokens per second)
       eval time =    6683.89 ms /   161 tokens (   41.51 ms per token,    24.09 tokens per second)
      total time =    7212.54 ms /   289 tokens
slot      release: id  3 | task 19560 | stop processing: n_tokens = 1428, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.946 (> 0.100 thold), f_keep = 0.888
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19723 | processing task, is_child = 0
slot update_slots: id  3 | task 19723 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1341
slot update_slots: id  3 | task 19723 | n_tokens = 1268, memory_seq_rm [1268, end)
slot update_slots: id  3 | task 19723 | prompt processing progress, n_tokens = 1277, batch.n_tokens = 9, progress = 0.952274
slot update_slots: id  3 | task 19723 | n_tokens = 1277, memory_seq_rm [1277, end)
slot update_slots: id  3 | task 19723 | prompt processing progress, n_tokens = 1341, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 19723 | prompt done, n_tokens = 1341, batch.n_tokens = 64
slot init_sampler: id  3 | task 19723 | init sampler, took 0.26 ms, tokens: text = 1341, total = 1341
slot update_slots: id  3 | task 19723 | created context checkpoint 8 of 8 (pos_min = 1061, pos_max = 1276, size = 5.065 MiB)
slot print_timing: id  3 | task 19723 | 
prompt eval time =     284.93 ms /    73 tokens (    3.90 ms per token,   256.20 tokens per second)
       eval time =    4240.36 ms /   145 tokens (   29.24 ms per token,    34.20 tokens per second)
      total time =    4525.29 ms /   218 tokens
slot      release: id  3 | task 19723 | stop processing: n_tokens = 1485, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.948 (> 0.100 thold), f_keep = 0.903
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19870 | processing task, is_child = 0
slot update_slots: id  3 | task 19870 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1414
slot update_slots: id  3 | task 19870 | n_past = 1341, slot.prompt.tokens.size() = 1485, seq_id = 3, pos_min = 1236, n_swa = 128
slot update_slots: id  3 | task 19870 | restored context checkpoint (pos_min = 1061, pos_max = 1276, size = 5.065 MiB)
slot update_slots: id  3 | task 19870 | n_tokens = 1276, memory_seq_rm [1276, end)
slot update_slots: id  3 | task 19870 | prompt processing progress, n_tokens = 1350, batch.n_tokens = 74, progress = 0.954738
slot update_slots: id  3 | task 19870 | n_tokens = 1350, memory_seq_rm [1350, end)
slot update_slots: id  3 | task 19870 | prompt processing progress, n_tokens = 1414, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 19870 | prompt done, n_tokens = 1414, batch.n_tokens = 64
slot init_sampler: id  3 | task 19870 | init sampler, took 1.30 ms, tokens: text = 1414, total = 1414
slot update_slots: id  3 | task 19870 | erasing old context checkpoint (pos_min = 0, pos_max = 583, size = 13.694 MiB)
slot update_slots: id  3 | task 19870 | created context checkpoint 8 of 8 (pos_min = 1134, pos_max = 1349, size = 5.065 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 20075 | processing task, is_child = 0
slot update_slots: id  1 | task 20075 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14995
slot update_slots: id  1 | task 20075 | n_past = 14925, slot.prompt.tokens.size() = 15073, seq_id = 1, pos_min = 14937, n_swa = 128
slot update_slots: id  1 | task 20075 | restored context checkpoint (pos_min = 13965, pos_max = 14860, size = 21.011 MiB)
slot update_slots: id  1 | task 20075 | n_tokens = 14860, memory_seq_rm [14860, end)
slot update_slots: id  1 | task 20075 | prompt processing progress, n_tokens = 14931, batch.n_tokens = 72, progress = 0.995732
slot update_slots: id  1 | task 20075 | n_tokens = 14931, memory_seq_rm [14931, end)
slot update_slots: id  1 | task 20075 | prompt processing progress, n_tokens = 14995, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 20075 | prompt done, n_tokens = 14995, batch.n_tokens = 65
slot init_sampler: id  1 | task 20075 | init sampler, took 2.68 ms, tokens: text = 14995, total = 14995
slot update_slots: id  1 | task 20075 | created context checkpoint 7 of 8 (pos_min = 14036, pos_max = 14930, size = 20.987 MiB)
slot print_timing: id  3 | task 19870 | 
prompt eval time =     473.80 ms /   138 tokens (    3.43 ms per token,   291.26 tokens per second)
       eval time =    6371.15 ms /   214 tokens (   29.77 ms per token,    33.59 tokens per second)
      total time =    6844.95 ms /   352 tokens
slot      release: id  3 | task 19870 | stop processing: n_tokens = 1627, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.946 (> 0.100 thold), f_keep = 0.869
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 20100 | processing task, is_child = 0
slot update_slots: id  3 | task 20100 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1494
slot update_slots: id  3 | task 20100 | n_past = 1414, slot.prompt.tokens.size() = 1627, seq_id = 3, pos_min = 1489, n_swa = 128
slot update_slots: id  3 | task 20100 | restored context checkpoint (pos_min = 1134, pos_max = 1349, size = 5.065 MiB)
slot update_slots: id  3 | task 20100 | n_tokens = 1349, memory_seq_rm [1349, end)
slot update_slots: id  3 | task 20100 | prompt processing progress, n_tokens = 1430, batch.n_tokens = 82, progress = 0.957162
slot update_slots: id  3 | task 20100 | n_tokens = 1430, memory_seq_rm [1430, end)
slot update_slots: id  3 | task 20100 | prompt processing progress, n_tokens = 1494, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 20100 | prompt done, n_tokens = 1494, batch.n_tokens = 65
slot init_sampler: id  3 | task 20100 | init sampler, took 0.29 ms, tokens: text = 1494, total = 1494
slot update_slots: id  3 | task 20100 | erasing old context checkpoint (pos_min = 0, pos_max = 661, size = 15.523 MiB)
slot update_slots: id  3 | task 20100 | created context checkpoint 8 of 8 (pos_min = 1215, pos_max = 1429, size = 5.042 MiB)
slot print_timing: id  1 | task 20075 | 
prompt eval time =     610.47 ms /   135 tokens (    4.52 ms per token,   221.14 tokens per second)
       eval time =    3300.26 ms /    70 tokens (   47.15 ms per token,    21.21 tokens per second)
      total time =    3910.72 ms /   205 tokens
slot      release: id  1 | task 20075 | stop processing: n_tokens = 15064, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 20100 | 
prompt eval time =     473.70 ms /   145 tokens (    3.27 ms per token,   306.10 tokens per second)
       eval time =    7489.21 ms /   247 tokens (   30.32 ms per token,    32.98 tokens per second)
      total time =    7962.91 ms /   392 tokens
slot      release: id  3 | task 20100 | stop processing: n_tokens = 1740, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.859
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 20349 | processing task, is_child = 0
slot update_slots: id  3 | task 20349 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1567
slot update_slots: id  3 | task 20349 | n_tokens = 1494, memory_seq_rm [1494, end)
slot update_slots: id  3 | task 20349 | prompt processing progress, n_tokens = 1503, batch.n_tokens = 9, progress = 0.959158
slot update_slots: id  3 | task 20349 | n_tokens = 1503, memory_seq_rm [1503, end)
slot update_slots: id  3 | task 20349 | prompt processing progress, n_tokens = 1567, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 20349 | prompt done, n_tokens = 1567, batch.n_tokens = 64
slot init_sampler: id  3 | task 20349 | init sampler, took 0.26 ms, tokens: text = 1567, total = 1567
slot update_slots: id  3 | task 20349 | erasing old context checkpoint (pos_min = 0, pos_max = 740, size = 17.376 MiB)
slot update_slots: id  3 | task 20349 | created context checkpoint 8 of 8 (pos_min = 1334, pos_max = 1502, size = 3.963 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 20481 | processing task, is_child = 0
slot update_slots: id  1 | task 20481 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15128
slot update_slots: id  1 | task 20481 | n_tokens = 14995, memory_seq_rm [14995, end)
slot update_slots: id  1 | task 20481 | prompt processing progress, n_tokens = 15064, batch.n_tokens = 70, progress = 0.995769
slot update_slots: id  1 | task 20481 | n_tokens = 15064, memory_seq_rm [15064, end)
slot update_slots: id  1 | task 20481 | prompt processing progress, n_tokens = 15128, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 20481 | prompt done, n_tokens = 15128, batch.n_tokens = 65
slot init_sampler: id  1 | task 20481 | init sampler, took 2.57 ms, tokens: text = 15128, total = 15128
slot update_slots: id  1 | task 20481 | created context checkpoint 8 of 8 (pos_min = 14859, pos_max = 15063, size = 4.807 MiB)
slot print_timing: id  3 | task 20349 | 
prompt eval time =     265.05 ms /    73 tokens (    3.63 ms per token,   275.42 tokens per second)
       eval time =    6333.96 ms /   185 tokens (   34.24 ms per token,    29.21 tokens per second)
      total time =    6599.00 ms /   258 tokens
slot      release: id  3 | task 20349 | stop processing: n_tokens = 1751, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.895
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 20553 | processing task, is_child = 0
slot update_slots: id  3 | task 20553 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1647
slot update_slots: id  3 | task 20553 | n_past = 1567, slot.prompt.tokens.size() = 1751, seq_id = 3, pos_min = 1570, n_swa = 128
slot update_slots: id  3 | task 20553 | restored context checkpoint (pos_min = 1334, pos_max = 1502, size = 3.963 MiB)
slot update_slots: id  3 | task 20553 | n_tokens = 1502, memory_seq_rm [1502, end)
slot update_slots: id  3 | task 20553 | prompt processing progress, n_tokens = 1583, batch.n_tokens = 82, progress = 0.961141
slot update_slots: id  3 | task 20553 | n_tokens = 1583, memory_seq_rm [1583, end)
slot update_slots: id  3 | task 20553 | prompt processing progress, n_tokens = 1647, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 20553 | prompt done, n_tokens = 1647, batch.n_tokens = 65
slot init_sampler: id  3 | task 20553 | init sampler, took 0.31 ms, tokens: text = 1647, total = 1647
slot update_slots: id  3 | task 20553 | erasing old context checkpoint (pos_min = 127, pos_max = 867, size = 17.376 MiB)
slot update_slots: id  3 | task 20553 | created context checkpoint 8 of 8 (pos_min = 1375, pos_max = 1582, size = 4.878 MiB)
slot print_timing: id  1 | task 20481 | 
prompt eval time =     461.02 ms /   133 tokens (    3.47 ms per token,   288.49 tokens per second)
       eval time =   12035.69 ms /   259 tokens (   46.47 ms per token,    21.52 tokens per second)
      total time =   12496.71 ms /   392 tokens
slot      release: id  1 | task 20481 | stop processing: n_tokens = 15386, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 20553 | 
prompt eval time =     503.11 ms /   145 tokens (    3.47 ms per token,   288.21 tokens per second)
       eval time =   26520.87 ms /   876 tokens (   30.27 ms per token,    33.03 tokens per second)
      total time =   27023.98 ms /  1021 tokens
slot      release: id  3 | task 20553 | stop processing: n_tokens = 2522, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.653
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21431 | processing task, is_child = 0
slot update_slots: id  3 | task 21431 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1716
slot update_slots: id  3 | task 21431 | n_past = 1647, slot.prompt.tokens.size() = 2522, seq_id = 3, pos_min = 1666, n_swa = 128
slot update_slots: id  3 | task 21431 | restored context checkpoint (pos_min = 1375, pos_max = 1582, size = 4.878 MiB)
slot update_slots: id  3 | task 21431 | n_tokens = 1582, memory_seq_rm [1582, end)
slot update_slots: id  3 | task 21431 | prompt processing progress, n_tokens = 1652, batch.n_tokens = 70, progress = 0.962704
slot update_slots: id  3 | task 21431 | n_tokens = 1652, memory_seq_rm [1652, end)
slot update_slots: id  3 | task 21431 | prompt processing progress, n_tokens = 1716, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21431 | prompt done, n_tokens = 1716, batch.n_tokens = 64
slot init_sampler: id  3 | task 21431 | init sampler, took 0.30 ms, tokens: text = 1716, total = 1716
slot update_slots: id  3 | task 21431 | erasing old context checkpoint (pos_min = 127, pos_max = 992, size = 20.307 MiB)
slot update_slots: id  3 | task 21431 | created context checkpoint 8 of 8 (pos_min = 1375, pos_max = 1651, size = 6.496 MiB)
slot print_timing: id  3 | task 21431 | 
prompt eval time =     417.03 ms /   134 tokens (    3.11 ms per token,   321.32 tokens per second)
       eval time =    6090.25 ms /   238 tokens (   25.59 ms per token,    39.08 tokens per second)
      total time =    6507.28 ms /   372 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 21431 | stop processing: n_tokens = 1953, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.955 (> 0.100 thold), f_keep = 0.879
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21671 | processing task, is_child = 0
slot update_slots: id  3 | task 21671 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1796
slot update_slots: id  3 | task 21671 | n_tokens = 1716, memory_seq_rm [1716, end)
slot update_slots: id  3 | task 21671 | prompt processing progress, n_tokens = 1732, batch.n_tokens = 16, progress = 0.964365
slot update_slots: id  3 | task 21671 | n_tokens = 1732, memory_seq_rm [1732, end)
slot update_slots: id  3 | task 21671 | prompt processing progress, n_tokens = 1796, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21671 | prompt done, n_tokens = 1796, batch.n_tokens = 64
slot init_sampler: id  3 | task 21671 | init sampler, took 0.33 ms, tokens: text = 1796, total = 1796
slot update_slots: id  3 | task 21671 | erasing old context checkpoint (pos_min = 172, pos_max = 1067, size = 21.011 MiB)
slot update_slots: id  3 | task 21671 | created context checkpoint 8 of 8 (pos_min = 1375, pos_max = 1731, size = 8.372 MiB)
slot print_timing: id  3 | task 21671 | 
prompt eval time =     274.29 ms /    80 tokens (    3.43 ms per token,   291.67 tokens per second)
       eval time =    5057.12 ms /   196 tokens (   25.80 ms per token,    38.76 tokens per second)
      total time =    5331.40 ms /   276 tokens
slot      release: id  3 | task 21671 | stop processing: n_tokens = 1991, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.720 (> 0.100 thold), f_keep = 0.465
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 15386, total state size = 363.764 MiB
srv          load:  - looking for better prompt, base f_keep = 0.465, sim = 0.720
srv        update:  - cache state: 5 prompts, 3157.251 MiB (limits: 8192.000 MiB, 64000 tokens, 253091 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv  get_availabl: prompt cache update took 616.16 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 21869 | processing task, is_child = 0
slot update_slots: id  1 | task 21869 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9942
slot update_slots: id  1 | task 21869 | n_past = 7159, slot.prompt.tokens.size() = 15386, seq_id = 1, pos_min = 15259, n_swa = 128
slot update_slots: id  1 | task 21869 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 9288, pos_max = 10184, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 9787, pos_max = 10682, n_swa = 128, size = 21.011 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 9929, pos_max = 10823, n_swa = 128, size = 20.987 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 10063, pos_max = 10956, n_swa = 128, size = 20.964 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 13643, pos_max = 14539, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 13965, pos_max = 14860, n_swa = 128, size = 21.011 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 14036, pos_max = 14930, n_swa = 128, size = 20.987 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 14859, pos_max = 15063, n_swa = 128, size = 4.807 MiB)
slot update_slots: id  1 | task 21869 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.205995
slot update_slots: id  1 | task 21869 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.411990
slot update_slots: id  1 | task 21869 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.617984
slot update_slots: id  1 | task 21869 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.823979
slot update_slots: id  1 | task 21869 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 9878, batch.n_tokens = 1686, progress = 0.993563
slot update_slots: id  1 | task 21869 | n_tokens = 9878, memory_seq_rm [9878, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 9942, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 21869 | prompt done, n_tokens = 9942, batch.n_tokens = 64
slot init_sampler: id  1 | task 21869 | init sampler, took 1.48 ms, tokens: text = 9942, total = 9942
slot update_slots: id  1 | task 21869 | created context checkpoint 1 of 8 (pos_min = 8981, pos_max = 9877, size = 21.034 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.359 (> 0.100 thold), f_keep = 0.312
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1991, total state size = 49.665 MiB
srv          load:  - looking for better prompt, base f_keep = 0.312, sim = 0.359
srv        update:  - cache state: 6 prompts, 3256.747 MiB (limits: 8192.000 MiB, 64000 tokens, 250367 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x57387f4603d0:    1991 tokens, checkpoints:  8,    99.497 MiB
srv  get_availabl: prompt cache update took 213.85 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22581 | processing task, is_child = 0
slot update_slots: id  3 | task 22581 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1731
slot update_slots: id  3 | task 22581 | n_past = 621, slot.prompt.tokens.size() = 1991, seq_id = 3, pos_min = 1864, n_swa = 128
slot update_slots: id  3 | task 22581 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 674, pos_max = 1140, n_swa = 128, size = 10.951 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1061, pos_max = 1276, n_swa = 128, size = 5.065 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1134, pos_max = 1349, n_swa = 128, size = 5.065 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1215, pos_max = 1429, n_swa = 128, size = 5.042 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1334, pos_max = 1502, n_swa = 128, size = 3.963 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1582, n_swa = 128, size = 4.878 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1651, n_swa = 128, size = 6.496 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1731, n_swa = 128, size = 8.372 MiB)
slot update_slots: id  3 | task 22581 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 22581 | prompt processing progress, n_tokens = 1667, batch.n_tokens = 1668, progress = 0.963027
slot update_slots: id  3 | task 22581 | n_tokens = 1667, memory_seq_rm [1667, end)
slot update_slots: id  3 | task 22581 | prompt processing progress, n_tokens = 1731, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 22581 | prompt done, n_tokens = 1731, batch.n_tokens = 65
slot init_sampler: id  3 | task 22581 | init sampler, took 0.29 ms, tokens: text = 1731, total = 1731
slot update_slots: id  3 | task 22581 | created context checkpoint 1 of 8 (pos_min = 770, pos_max = 1666, size = 21.034 MiB)
slot print_timing: id  3 | task 22581 | 
prompt eval time =    2161.79 ms /  1731 tokens (    1.25 ms per token,   800.72 tokens per second)
       eval time =   33780.89 ms /   737 tokens (   45.84 ms per token,    21.82 tokens per second)
      total time =   35942.68 ms /  2468 tokens
slot      release: id  3 | task 22581 | stop processing: n_tokens = 2467, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  1 | task 21869 | 
prompt eval time =   11814.54 ms /  9942 tokens (    1.19 ms per token,   841.51 tokens per second)
       eval time =   55791.46 ms /  1495 tokens (   37.32 ms per token,    26.80 tokens per second)
      total time =   67606.00 ms / 11437 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 21869 | stop processing: n_tokens = 11436, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.894 (> 0.100 thold), f_keep = 0.864
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23371 | processing task, is_child = 0
slot update_slots: id  1 | task 23371 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11050
slot update_slots: id  1 | task 23371 | n_past = 9884, slot.prompt.tokens.size() = 11436, seq_id = 1, pos_min = 10899, n_swa = 128
slot update_slots: id  1 | task 23371 | restored context checkpoint (pos_min = 8981, pos_max = 9877, size = 21.034 MiB)
slot update_slots: id  1 | task 23371 | n_tokens = 9877, memory_seq_rm [9877, end)
slot update_slots: id  1 | task 23371 | prompt processing progress, n_tokens = 10986, batch.n_tokens = 1109, progress = 0.994208
slot update_slots: id  1 | task 23371 | n_tokens = 10986, memory_seq_rm [10986, end)
slot update_slots: id  1 | task 23371 | prompt processing progress, n_tokens = 11050, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23371 | prompt done, n_tokens = 11050, batch.n_tokens = 64
slot init_sampler: id  1 | task 23371 | init sampler, took 1.57 ms, tokens: text = 11050, total = 11050
slot update_slots: id  1 | task 23371 | created context checkpoint 2 of 8 (pos_min = 10089, pos_max = 10985, size = 21.034 MiB)
slot print_timing: id  1 | task 23371 | 
prompt eval time =    1799.35 ms /  1173 tokens (    1.53 ms per token,   651.90 tokens per second)
       eval time =   26223.42 ms /  1035 tokens (   25.34 ms per token,    39.47 tokens per second)
      total time =   28022.77 ms /  2208 tokens
slot      release: id  1 | task 23371 | stop processing: n_tokens = 12084, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.914 (> 0.100 thold), f_keep = 0.914
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24408 | processing task, is_child = 0
slot update_slots: id  1 | task 24408 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12091
slot update_slots: id  1 | task 24408 | n_past = 11050, slot.prompt.tokens.size() = 12084, seq_id = 1, pos_min = 11187, n_swa = 128
slot update_slots: id  1 | task 24408 | restored context checkpoint (pos_min = 10089, pos_max = 10985, size = 21.034 MiB)
slot update_slots: id  1 | task 24408 | n_tokens = 10985, memory_seq_rm [10985, end)
slot update_slots: id  1 | task 24408 | prompt processing progress, n_tokens = 12027, batch.n_tokens = 1042, progress = 0.994707
slot update_slots: id  1 | task 24408 | n_tokens = 12027, memory_seq_rm [12027, end)
slot update_slots: id  1 | task 24408 | prompt processing progress, n_tokens = 12091, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24408 | prompt done, n_tokens = 12091, batch.n_tokens = 64
slot init_sampler: id  1 | task 24408 | init sampler, took 1.75 ms, tokens: text = 12091, total = 12091
slot update_slots: id  1 | task 24408 | created context checkpoint 3 of 8 (pos_min = 11130, pos_max = 12026, size = 21.034 MiB)
slot print_timing: id  1 | task 24408 | 
prompt eval time =    1623.69 ms /  1106 tokens (    1.47 ms per token,   681.16 tokens per second)
       eval time =   27560.17 ms /  1087 tokens (   25.35 ms per token,    39.44 tokens per second)
      total time =   29183.86 ms /  2193 tokens
slot      release: id  1 | task 24408 | stop processing: n_tokens = 13177, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.917 (> 0.100 thold), f_keep = 0.918
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25497 | processing task, is_child = 0
slot update_slots: id  1 | task 25497 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13186
slot update_slots: id  1 | task 25497 | n_past = 12091, slot.prompt.tokens.size() = 13177, seq_id = 1, pos_min = 12280, n_swa = 128
slot update_slots: id  1 | task 25497 | restored context checkpoint (pos_min = 11130, pos_max = 12026, size = 21.034 MiB)
slot update_slots: id  1 | task 25497 | n_tokens = 12026, memory_seq_rm [12026, end)
slot update_slots: id  1 | task 25497 | prompt processing progress, n_tokens = 13122, batch.n_tokens = 1096, progress = 0.995146
slot update_slots: id  1 | task 25497 | n_tokens = 13122, memory_seq_rm [13122, end)
slot update_slots: id  1 | task 25497 | prompt processing progress, n_tokens = 13186, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25497 | prompt done, n_tokens = 13186, batch.n_tokens = 64
slot init_sampler: id  1 | task 25497 | init sampler, took 1.89 ms, tokens: text = 13186, total = 13186
slot update_slots: id  1 | task 25497 | created context checkpoint 4 of 8 (pos_min = 12225, pos_max = 13121, size = 21.034 MiB)
slot print_timing: id  1 | task 25497 | 
prompt eval time =    1742.84 ms /  1160 tokens (    1.50 ms per token,   665.58 tokens per second)
       eval time =     596.91 ms /    25 tokens (   23.88 ms per token,    41.88 tokens per second)
      total time =    2339.75 ms /  1185 tokens
slot      release: id  1 | task 25497 | stop processing: n_tokens = 13210, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.722 (> 0.100 thold), f_keep = 0.678
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25524 | processing task, is_child = 0
slot update_slots: id  3 | task 25524 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2318
slot update_slots: id  3 | task 25524 | n_past = 1673, slot.prompt.tokens.size() = 2467, seq_id = 3, pos_min = 2340, n_swa = 128
slot update_slots: id  3 | task 25524 | restored context checkpoint (pos_min = 770, pos_max = 1666, size = 21.034 MiB)
slot update_slots: id  3 | task 25524 | n_tokens = 1666, memory_seq_rm [1666, end)
slot update_slots: id  3 | task 25524 | prompt processing progress, n_tokens = 2254, batch.n_tokens = 588, progress = 0.972390
slot update_slots: id  3 | task 25524 | n_tokens = 2254, memory_seq_rm [2254, end)
slot update_slots: id  3 | task 25524 | prompt processing progress, n_tokens = 2318, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25524 | prompt done, n_tokens = 2318, batch.n_tokens = 64
slot init_sampler: id  3 | task 25524 | init sampler, took 0.40 ms, tokens: text = 2318, total = 2318
slot update_slots: id  3 | task 25524 | created context checkpoint 2 of 8 (pos_min = 1357, pos_max = 2253, size = 21.034 MiB)
slot print_timing: id  3 | task 25524 | 
prompt eval time =    1209.85 ms /   652 tokens (    1.86 ms per token,   538.91 tokens per second)
       eval time =    1757.46 ms /    72 tokens (   24.41 ms per token,    40.97 tokens per second)
      total time =    2967.31 ms /   724 tokens
slot      release: id  3 | task 25524 | stop processing: n_tokens = 2389, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25598 | processing task, is_child = 0
slot update_slots: id  3 | task 25598 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2386
slot update_slots: id  3 | task 25598 | n_tokens = 2318, memory_seq_rm [2318, end)
slot update_slots: id  3 | task 25598 | prompt processing progress, n_tokens = 2322, batch.n_tokens = 4, progress = 0.973177
slot update_slots: id  3 | task 25598 | n_tokens = 2322, memory_seq_rm [2322, end)
slot update_slots: id  3 | task 25598 | prompt processing progress, n_tokens = 2386, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25598 | prompt done, n_tokens = 2386, batch.n_tokens = 64
slot init_sampler: id  3 | task 25598 | init sampler, took 0.36 ms, tokens: text = 2386, total = 2386
slot update_slots: id  3 | task 25598 | created context checkpoint 3 of 8 (pos_min = 1492, pos_max = 2321, size = 19.463 MiB)
slot print_timing: id  3 | task 25598 | 
prompt eval time =     224.83 ms /    68 tokens (    3.31 ms per token,   302.46 tokens per second)
       eval time =   14847.60 ms /   586 tokens (   25.34 ms per token,    39.47 tokens per second)
      total time =   15072.43 ms /   654 tokens
slot      release: id  3 | task 25598 | stop processing: n_tokens = 2971, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.856 (> 0.100 thold), f_keep = 0.803
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26186 | processing task, is_child = 0
slot update_slots: id  3 | task 26186 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2789
slot update_slots: id  3 | task 26186 | n_tokens = 2386, memory_seq_rm [2386, end)
slot update_slots: id  3 | task 26186 | prompt processing progress, n_tokens = 2725, batch.n_tokens = 339, progress = 0.977053
slot update_slots: id  3 | task 26186 | n_tokens = 2725, memory_seq_rm [2725, end)
slot update_slots: id  3 | task 26186 | prompt processing progress, n_tokens = 2789, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 26186 | prompt done, n_tokens = 2789, batch.n_tokens = 64
slot init_sampler: id  3 | task 26186 | init sampler, took 0.45 ms, tokens: text = 2789, total = 2789
slot update_slots: id  3 | task 26186 | created context checkpoint 4 of 8 (pos_min = 2084, pos_max = 2724, size = 15.031 MiB)
slot print_timing: id  3 | task 26186 | 
prompt eval time =     671.99 ms /   403 tokens (    1.67 ms per token,   599.71 tokens per second)
       eval time =     847.57 ms /    34 tokens (   24.93 ms per token,    40.11 tokens per second)
      total time =    1519.56 ms /   437 tokens
slot      release: id  3 | task 26186 | stop processing: n_tokens = 2822, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.835 (> 0.100 thold), f_keep = 0.832
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26222 | processing task, is_child = 0
slot update_slots: id  1 | task 26222 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13169
slot update_slots: id  1 | task 26222 | n_past = 10992, slot.prompt.tokens.size() = 13210, seq_id = 1, pos_min = 13083, n_swa = 128
slot update_slots: id  1 | task 26222 | restored context checkpoint (pos_min = 10089, pos_max = 10985, size = 21.034 MiB)
slot update_slots: id  1 | task 26222 | erased invalidated context checkpoint (pos_min = 11130, pos_max = 12026, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 26222 | erased invalidated context checkpoint (pos_min = 12225, pos_max = 13121, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 26222 | n_tokens = 10985, memory_seq_rm [10985, end)
slot update_slots: id  1 | task 26222 | prompt processing progress, n_tokens = 13033, batch.n_tokens = 2048, progress = 0.989673
slot update_slots: id  1 | task 26222 | n_tokens = 13033, memory_seq_rm [13033, end)
slot update_slots: id  1 | task 26222 | prompt processing progress, n_tokens = 13105, batch.n_tokens = 72, progress = 0.995140
slot update_slots: id  1 | task 26222 | n_tokens = 13105, memory_seq_rm [13105, end)
slot update_slots: id  1 | task 26222 | prompt processing progress, n_tokens = 13169, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26222 | prompt done, n_tokens = 13169, batch.n_tokens = 64
slot init_sampler: id  1 | task 26222 | init sampler, took 2.40 ms, tokens: text = 13169, total = 13169
slot update_slots: id  1 | task 26222 | created context checkpoint 3 of 8 (pos_min = 12208, pos_max = 13104, size = 21.034 MiB)
slot print_timing: id  1 | task 26222 | 
prompt eval time =    3015.41 ms /  2184 tokens (    1.38 ms per token,   724.28 tokens per second)
       eval time =   27556.51 ms /  1073 tokens (   25.68 ms per token,    38.94 tokens per second)
      total time =   30571.92 ms /  3257 tokens
slot      release: id  1 | task 26222 | stop processing: n_tokens = 14241, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.936 (> 0.100 thold), f_keep = 0.925
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27298 | processing task, is_child = 0
slot update_slots: id  1 | task 27298 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14072
slot update_slots: id  1 | task 27298 | n_past = 13169, slot.prompt.tokens.size() = 14241, seq_id = 1, pos_min = 13344, n_swa = 128
slot update_slots: id  1 | task 27298 | restored context checkpoint (pos_min = 12208, pos_max = 13104, size = 21.034 MiB)
slot update_slots: id  1 | task 27298 | n_tokens = 13104, memory_seq_rm [13104, end)
slot update_slots: id  1 | task 27298 | prompt processing progress, n_tokens = 14008, batch.n_tokens = 904, progress = 0.995452
slot update_slots: id  1 | task 27298 | n_tokens = 14008, memory_seq_rm [14008, end)
slot update_slots: id  1 | task 27298 | prompt processing progress, n_tokens = 14072, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27298 | prompt done, n_tokens = 14072, batch.n_tokens = 64
slot init_sampler: id  1 | task 27298 | init sampler, took 3.00 ms, tokens: text = 14072, total = 14072
slot update_slots: id  1 | task 27298 | created context checkpoint 4 of 8 (pos_min = 13111, pos_max = 14007, size = 21.034 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.817 (> 0.100 thold), f_keep = 0.794
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27841 | processing task, is_child = 0
slot update_slots: id  3 | task 27841 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2744
slot update_slots: id  3 | task 27841 | n_past = 2241, slot.prompt.tokens.size() = 2822, seq_id = 3, pos_min = 2695, n_swa = 128
slot update_slots: id  3 | task 27841 | restored context checkpoint (pos_min = 2084, pos_max = 2724, size = 15.031 MiB)
slot update_slots: id  3 | task 27841 | n_tokens = 2241, memory_seq_rm [2241, end)
slot update_slots: id  3 | task 27841 | prompt processing progress, n_tokens = 2680, batch.n_tokens = 440, progress = 0.976676
slot update_slots: id  3 | task 27841 | n_tokens = 2680, memory_seq_rm [2680, end)
slot update_slots: id  3 | task 27841 | prompt processing progress, n_tokens = 2744, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 27841 | prompt done, n_tokens = 2744, batch.n_tokens = 65
slot init_sampler: id  3 | task 27841 | init sampler, took 0.48 ms, tokens: text = 2744, total = 2744
slot print_timing: id  3 | task 27841 | 
prompt eval time =     754.32 ms /   503 tokens (    1.50 ms per token,   666.83 tokens per second)
       eval time =   16327.60 ms /   352 tokens (   46.39 ms per token,    21.56 tokens per second)
      total time =   17081.92 ms /   855 tokens
slot      release: id  3 | task 27841 | stop processing: n_tokens = 3095, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.887
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28210 | processing task, is_child = 0
slot update_slots: id  3 | task 28210 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2943
slot update_slots: id  3 | task 28210 | n_tokens = 2744, memory_seq_rm [2744, end)
slot update_slots: id  3 | task 28210 | prompt processing progress, n_tokens = 2879, batch.n_tokens = 136, progress = 0.978253
slot update_slots: id  3 | task 28210 | n_tokens = 2879, memory_seq_rm [2879, end)
slot update_slots: id  3 | task 28210 | prompt processing progress, n_tokens = 2943, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 28210 | prompt done, n_tokens = 2943, batch.n_tokens = 65
slot init_sampler: id  3 | task 28210 | init sampler, took 0.56 ms, tokens: text = 2943, total = 2943
slot update_slots: id  3 | task 28210 | created context checkpoint 5 of 8 (pos_min = 2565, pos_max = 2878, size = 7.363 MiB)
slot print_timing: id  1 | task 27298 | 
prompt eval time =    1557.30 ms /   968 tokens (    1.61 ms per token,   621.59 tokens per second)
       eval time =   31916.81 ms /   913 tokens (   34.96 ms per token,    28.61 tokens per second)
      total time =   33474.11 ms /  1881 tokens
slot      release: id  1 | task 27298 | stop processing: n_tokens = 14984, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 28210 | 
prompt eval time =     505.71 ms /   199 tokens (    2.54 ms per token,   393.51 tokens per second)
       eval time =     772.98 ms /    30 tokens (   25.77 ms per token,    38.81 tokens per second)
      total time =    1278.69 ms /   229 tokens
slot      release: id  3 | task 28210 | stop processing: n_tokens = 2972, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.940 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28242 | processing task, is_child = 0
slot update_slots: id  1 | task 28242 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14975
slot update_slots: id  1 | task 28242 | n_past = 14072, slot.prompt.tokens.size() = 14984, seq_id = 1, pos_min = 14849, n_swa = 128
slot update_slots: id  1 | task 28242 | restored context checkpoint (pos_min = 13111, pos_max = 14007, size = 21.034 MiB)
slot update_slots: id  1 | task 28242 | n_tokens = 14007, memory_seq_rm [14007, end)
slot update_slots: id  1 | task 28242 | prompt processing progress, n_tokens = 14911, batch.n_tokens = 904, progress = 0.995726
slot update_slots: id  1 | task 28242 | n_tokens = 14911, memory_seq_rm [14911, end)
slot update_slots: id  1 | task 28242 | prompt processing progress, n_tokens = 14975, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28242 | prompt done, n_tokens = 14975, batch.n_tokens = 64
slot init_sampler: id  1 | task 28242 | init sampler, took 2.20 ms, tokens: text = 14975, total = 14975
slot update_slots: id  1 | task 28242 | created context checkpoint 5 of 8 (pos_min = 14141, pos_max = 14910, size = 18.056 MiB)
slot print_timing: id  1 | task 28242 | 
prompt eval time =    1438.60 ms /   968 tokens (    1.49 ms per token,   672.88 tokens per second)
       eval time =   23910.69 ms /   936 tokens (   25.55 ms per token,    39.15 tokens per second)
      total time =   25349.29 ms /  1904 tokens
slot      release: id  1 | task 28242 | stop processing: n_tokens = 15910, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.943 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29180 | processing task, is_child = 0
slot update_slots: id  1 | task 29180 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15887
slot update_slots: id  1 | task 29180 | n_past = 14975, slot.prompt.tokens.size() = 15910, seq_id = 1, pos_min = 15013, n_swa = 128
slot update_slots: id  1 | task 29180 | restored context checkpoint (pos_min = 14141, pos_max = 14910, size = 18.056 MiB)
slot update_slots: id  1 | task 29180 | n_tokens = 14910, memory_seq_rm [14910, end)
slot update_slots: id  1 | task 29180 | prompt processing progress, n_tokens = 15823, batch.n_tokens = 913, progress = 0.995972
slot update_slots: id  1 | task 29180 | n_tokens = 15823, memory_seq_rm [15823, end)
slot update_slots: id  1 | task 29180 | prompt processing progress, n_tokens = 15887, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29180 | prompt done, n_tokens = 15887, batch.n_tokens = 64
slot init_sampler: id  1 | task 29180 | init sampler, took 2.28 ms, tokens: text = 15887, total = 15887
slot update_slots: id  1 | task 29180 | created context checkpoint 6 of 8 (pos_min = 14926, pos_max = 15822, size = 21.034 MiB)
slot print_timing: id  1 | task 29180 | 
prompt eval time =    1448.98 ms /   977 tokens (    1.48 ms per token,   674.27 tokens per second)
       eval time =   25436.53 ms /   985 tokens (   25.82 ms per token,    38.72 tokens per second)
      total time =   26885.51 ms /  1962 tokens
slot      release: id  1 | task 29180 | stop processing: n_tokens = 16871, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.945 (> 0.100 thold), f_keep = 0.942
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30167 | processing task, is_child = 0
slot update_slots: id  1 | task 30167 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16804
slot update_slots: id  1 | task 30167 | n_past = 15887, slot.prompt.tokens.size() = 16871, seq_id = 1, pos_min = 15974, n_swa = 128
slot update_slots: id  1 | task 30167 | restored context checkpoint (pos_min = 14926, pos_max = 15822, size = 21.034 MiB)
slot update_slots: id  1 | task 30167 | n_tokens = 15822, memory_seq_rm [15822, end)
slot update_slots: id  1 | task 30167 | prompt processing progress, n_tokens = 16740, batch.n_tokens = 918, progress = 0.996191
slot update_slots: id  1 | task 30167 | n_tokens = 16740, memory_seq_rm [16740, end)
slot update_slots: id  1 | task 30167 | prompt processing progress, n_tokens = 16804, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30167 | prompt done, n_tokens = 16804, batch.n_tokens = 64
slot init_sampler: id  1 | task 30167 | init sampler, took 2.53 ms, tokens: text = 16804, total = 16804
slot update_slots: id  1 | task 30167 | created context checkpoint 7 of 8 (pos_min = 15843, pos_max = 16739, size = 21.034 MiB)
slot print_timing: id  1 | task 30167 | 
prompt eval time =    1578.34 ms /   982 tokens (    1.61 ms per token,   622.17 tokens per second)
       eval time =    1075.81 ms /    43 tokens (   25.02 ms per token,    39.97 tokens per second)
      total time =    2654.14 ms /  1025 tokens
slot      release: id  1 | task 30167 | stop processing: n_tokens = 16846, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.942 (> 0.100 thold), f_keep = 0.037
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 16846, total state size = 416.055 MiB
srv          load:  - looking for better prompt, base f_keep = 0.037, sim = 0.942
srv          load:  - found better prompt with f_keep = 0.313, sim = 0.945
srv        update:  - cache state: 6 prompts, 3717.566 MiB (limits: 8192.000 MiB, 64000 tokens, 252066 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv  get_availabl: prompt cache update took 1167.86 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30212 | processing task, is_child = 0
slot update_slots: id  1 | task 30212 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 659
slot update_slots: id  1 | task 30212 | n_past = 623, slot.prompt.tokens.size() = 1991, seq_id = 1, pos_min = 1864, n_swa = 128
slot update_slots: id  1 | task 30212 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 674, pos_max = 1140, n_swa = 128, size = 10.951 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1061, pos_max = 1276, n_swa = 128, size = 5.065 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1134, pos_max = 1349, n_swa = 128, size = 5.065 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1215, pos_max = 1429, n_swa = 128, size = 5.042 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1334, pos_max = 1502, n_swa = 128, size = 3.963 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1582, n_swa = 128, size = 4.878 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1651, n_swa = 128, size = 6.496 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1731, n_swa = 128, size = 8.372 MiB)
slot update_slots: id  1 | task 30212 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 30212 | prompt processing progress, n_tokens = 595, batch.n_tokens = 595, progress = 0.902883
slot update_slots: id  1 | task 30212 | n_tokens = 595, memory_seq_rm [595, end)
slot update_slots: id  1 | task 30212 | prompt processing progress, n_tokens = 659, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30212 | prompt done, n_tokens = 659, batch.n_tokens = 64
slot init_sampler: id  1 | task 30212 | init sampler, took 0.13 ms, tokens: text = 659, total = 659
slot update_slots: id  1 | task 30212 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 594, size = 13.952 MiB)
slot print_timing: id  1 | task 30212 | 
prompt eval time =    1075.84 ms /   659 tokens (    1.63 ms per token,   612.55 tokens per second)
       eval time =    1097.47 ms /    44 tokens (   24.94 ms per token,    40.09 tokens per second)
      total time =    2173.31 ms /   703 tokens
slot      release: id  1 | task 30212 | stop processing: n_tokens = 702, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.459 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30258 | processing task, is_child = 0
slot update_slots: id  1 | task 30258 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1437
slot update_slots: id  1 | task 30258 | n_tokens = 659, memory_seq_rm [659, end)
slot update_slots: id  1 | task 30258 | prompt processing progress, n_tokens = 1373, batch.n_tokens = 714, progress = 0.955463
slot update_slots: id  1 | task 30258 | n_tokens = 1373, memory_seq_rm [1373, end)
slot update_slots: id  1 | task 30258 | prompt processing progress, n_tokens = 1437, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30258 | prompt done, n_tokens = 1437, batch.n_tokens = 64
slot init_sampler: id  1 | task 30258 | init sampler, took 0.35 ms, tokens: text = 1437, total = 1437
slot update_slots: id  1 | task 30258 | created context checkpoint 2 of 8 (pos_min = 476, pos_max = 1372, size = 21.034 MiB)
slot print_timing: id  1 | task 30258 | 
prompt eval time =    1108.22 ms /   778 tokens (    1.42 ms per token,   702.03 tokens per second)
       eval time =    1363.49 ms /    53 tokens (   25.73 ms per token,    38.87 tokens per second)
      total time =    2471.71 ms /   831 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 30258 | stop processing: n_tokens = 1489, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 30313 | processing task, is_child = 0
slot update_slots: id  0 | task 30313 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 40397
slot update_slots: id  0 | task 30313 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.050697
slot update_slots: id  0 | task 30313 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.101394
slot update_slots: id  0 | task 30313 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.152091
slot update_slots: id  0 | task 30313 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.202787
slot update_slots: id  0 | task 30313 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.253484
slot update_slots: id  0 | task 30313 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.304181
slot update_slots: id  0 | task 30313 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.354878
slot update_slots: id  0 | task 30313 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.405575
slot update_slots: id  0 | task 30313 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.456271
slot update_slots: id  0 | task 30313 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 2048, progress = 0.506968
slot update_slots: id  0 | task 30313 | n_tokens = 20480, memory_seq_rm [20480, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 22528, batch.n_tokens = 2048, progress = 0.557665
slot update_slots: id  0 | task 30313 | n_tokens = 22528, memory_seq_rm [22528, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 24576, batch.n_tokens = 2048, progress = 0.608362
slot update_slots: id  0 | task 30313 | n_tokens = 24576, memory_seq_rm [24576, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 26624, batch.n_tokens = 2048, progress = 0.659059
slot update_slots: id  0 | task 30313 | n_tokens = 26624, memory_seq_rm [26624, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 28672, batch.n_tokens = 2048, progress = 0.709756
slot update_slots: id  0 | task 30313 | n_tokens = 28672, memory_seq_rm [28672, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 30720, batch.n_tokens = 2048, progress = 0.760453
slot update_slots: id  0 | task 30313 | n_tokens = 30720, memory_seq_rm [30720, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 32768, batch.n_tokens = 2048, progress = 0.811149
slot update_slots: id  0 | task 30313 | n_tokens = 32768, memory_seq_rm [32768, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 34816, batch.n_tokens = 2048, progress = 0.861846
slot update_slots: id  0 | task 30313 | n_tokens = 34816, memory_seq_rm [34816, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 36864, batch.n_tokens = 2048, progress = 0.912543
slot update_slots: id  0 | task 30313 | n_tokens = 36864, memory_seq_rm [36864, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 38912, batch.n_tokens = 2048, progress = 0.963240
slot update_slots: id  0 | task 30313 | n_tokens = 38912, memory_seq_rm [38912, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 40333, batch.n_tokens = 1421, progress = 0.998416
slot update_slots: id  0 | task 30313 | n_tokens = 40333, memory_seq_rm [40333, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 40397, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 30313 | prompt done, n_tokens = 40397, batch.n_tokens = 64
slot init_sampler: id  0 | task 30313 | init sampler, took 5.61 ms, tokens: text = 40397, total = 40397
slot update_slots: id  0 | task 30313 | created context checkpoint 1 of 8 (pos_min = 39563, pos_max = 40332, size = 18.056 MiB)
slot print_timing: id  0 | task 30313 | 
prompt eval time =   52297.10 ms / 40397 tokens (    1.29 ms per token,   772.45 tokens per second)
       eval time =    2311.90 ms /    83 tokens (   27.85 ms per token,    35.90 tokens per second)
      total time =   54609.01 ms / 40480 tokens
slot      release: id  0 | task 30313 | stop processing: n_tokens = 40479, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.536 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 30417 | processing task, is_child = 0
slot update_slots: id  0 | task 30417 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 75329
srv    send_error: task id = 30417, error: request (75329 tokens) exceeds the available context size (64000 tokens), try increasing it
slot      release: id  0 | task 30417 | stop processing: n_tokens = 40479, truncated = 0
srv  update_slots: no tokens to decode
srv  update_slots: all slots are idle
srv          stop: cancel task, id_task = 30417
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 400
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.530 (> 0.100 thold), f_keep = 0.210
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2972, total state size = 72.669 MiB
srv          load:  - looking for better prompt, base f_keep = 0.210, sim = 0.530
srv        update:  - cache state: 7 prompts, 3874.160 MiB (limits: 8192.000 MiB, 64000 tokens, 248162 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv  get_availabl: prompt cache update took 427.05 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 30420 | processing task, is_child = 0
slot update_slots: id  3 | task 30420 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1179
slot update_slots: id  3 | task 30420 | n_past = 625, slot.prompt.tokens.size() = 2972, seq_id = 3, pos_min = 2845, n_swa = 128
slot update_slots: id  3 | task 30420 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 30420 | erased invalidated context checkpoint (pos_min = 770, pos_max = 1666, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 30420 | erased invalidated context checkpoint (pos_min = 1357, pos_max = 2253, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 30420 | erased invalidated context checkpoint (pos_min = 1492, pos_max = 2321, n_swa = 128, size = 19.463 MiB)
slot update_slots: id  3 | task 30420 | erased invalidated context checkpoint (pos_min = 2084, pos_max = 2724, n_swa = 128, size = 15.031 MiB)
slot update_slots: id  3 | task 30420 | erased invalidated context checkpoint (pos_min = 2565, pos_max = 2878, n_swa = 128, size = 7.363 MiB)
slot update_slots: id  3 | task 30420 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 30420 | prompt processing progress, n_tokens = 1115, batch.n_tokens = 1115, progress = 0.945717
slot update_slots: id  3 | task 30420 | n_tokens = 1115, memory_seq_rm [1115, end)
slot update_slots: id  3 | task 30420 | prompt processing progress, n_tokens = 1179, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 30420 | prompt done, n_tokens = 1179, batch.n_tokens = 64
slot init_sampler: id  3 | task 30420 | init sampler, took 0.25 ms, tokens: text = 1179, total = 1179
slot update_slots: id  3 | task 30420 | created context checkpoint 1 of 8 (pos_min = 345, pos_max = 1114, size = 18.056 MiB)
slot print_timing: id  3 | task 30420 | 
prompt eval time =    2195.21 ms /  1179 tokens (    1.86 ms per token,   537.08 tokens per second)
       eval time =    2417.54 ms /    85 tokens (   28.44 ms per token,    35.16 tokens per second)
      total time =    4612.75 ms /  1264 tokens
slot      release: id  3 | task 30420 | stop processing: n_tokens = 1263, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.417 (> 0.100 thold), f_keep = 0.933
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 30507 | processing task, is_child = 0
slot update_slots: id  3 | task 30507 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2825
slot update_slots: id  3 | task 30507 | n_tokens = 1179, memory_seq_rm [1179, end)
slot update_slots: id  3 | task 30507 | prompt processing progress, n_tokens = 2761, batch.n_tokens = 1582, progress = 0.977345
slot update_slots: id  3 | task 30507 | n_tokens = 2761, memory_seq_rm [2761, end)
slot update_slots: id  3 | task 30507 | prompt processing progress, n_tokens = 2825, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 30507 | prompt done, n_tokens = 2825, batch.n_tokens = 64
slot init_sampler: id  3 | task 30507 | init sampler, took 0.53 ms, tokens: text = 2825, total = 2825
slot update_slots: id  3 | task 30507 | created context checkpoint 2 of 8 (pos_min = 1991, pos_max = 2760, size = 18.056 MiB)
slot print_timing: id  3 | task 30507 | 
prompt eval time =    2883.78 ms /  1646 tokens (    1.75 ms per token,   570.78 tokens per second)
       eval time =   27310.11 ms /   957 tokens (   28.54 ms per token,    35.04 tokens per second)
      total time =   30193.88 ms /  2603 tokens
slot      release: id  3 | task 30507 | stop processing: n_tokens = 3781, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.786 (> 0.100 thold), f_keep = 0.747
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31466 | processing task, is_child = 0
slot update_slots: id  3 | task 31466 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3593
slot update_slots: id  3 | task 31466 | n_past = 2825, slot.prompt.tokens.size() = 3781, seq_id = 3, pos_min = 3011, n_swa = 128
slot update_slots: id  3 | task 31466 | restored context checkpoint (pos_min = 1991, pos_max = 2760, size = 18.056 MiB)
slot update_slots: id  3 | task 31466 | n_tokens = 2760, memory_seq_rm [2760, end)
slot update_slots: id  3 | task 31466 | prompt processing progress, n_tokens = 3529, batch.n_tokens = 769, progress = 0.982188
slot update_slots: id  3 | task 31466 | n_tokens = 3529, memory_seq_rm [3529, end)
slot update_slots: id  3 | task 31466 | prompt processing progress, n_tokens = 3593, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31466 | prompt done, n_tokens = 3593, batch.n_tokens = 64
slot init_sampler: id  3 | task 31466 | init sampler, took 0.61 ms, tokens: text = 3593, total = 3593
slot update_slots: id  3 | task 31466 | created context checkpoint 3 of 8 (pos_min = 2759, pos_max = 3528, size = 18.056 MiB)
slot print_timing: id  3 | task 31466 | 
prompt eval time =    1739.61 ms /   833 tokens (    2.09 ms per token,   478.84 tokens per second)
       eval time =    2433.92 ms /    84 tokens (   28.98 ms per token,    34.51 tokens per second)
      total time =    4173.53 ms /   917 tokens
slot      release: id  3 | task 31466 | stop processing: n_tokens = 3676, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31552 | processing task, is_child = 0
slot update_slots: id  3 | task 31552 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3664
slot update_slots: id  3 | task 31552 | n_tokens = 3593, memory_seq_rm [3593, end)
slot update_slots: id  3 | task 31552 | prompt processing progress, n_tokens = 3600, batch.n_tokens = 7, progress = 0.982533
slot update_slots: id  3 | task 31552 | n_tokens = 3600, memory_seq_rm [3600, end)
slot update_slots: id  3 | task 31552 | prompt processing progress, n_tokens = 3664, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31552 | prompt done, n_tokens = 3664, batch.n_tokens = 64
slot init_sampler: id  3 | task 31552 | init sampler, took 0.57 ms, tokens: text = 3664, total = 3664
slot update_slots: id  3 | task 31552 | created context checkpoint 4 of 8 (pos_min = 2906, pos_max = 3599, size = 16.274 MiB)
slot print_timing: id  3 | task 31552 | 
prompt eval time =     319.81 ms /    71 tokens (    4.50 ms per token,   222.01 tokens per second)
       eval time =     969.81 ms /    33 tokens (   29.39 ms per token,    34.03 tokens per second)
      total time =    1289.61 ms /   104 tokens
slot      release: id  3 | task 31552 | stop processing: n_tokens = 3696, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.825 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31587 | processing task, is_child = 0
slot update_slots: id  3 | task 31587 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4442
slot update_slots: id  3 | task 31587 | n_tokens = 3664, memory_seq_rm [3664, end)
slot update_slots: id  3 | task 31587 | prompt processing progress, n_tokens = 4378, batch.n_tokens = 714, progress = 0.985592
slot update_slots: id  3 | task 31587 | n_tokens = 4378, memory_seq_rm [4378, end)
slot update_slots: id  3 | task 31587 | prompt processing progress, n_tokens = 4442, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31587 | prompt done, n_tokens = 4442, batch.n_tokens = 64
slot init_sampler: id  3 | task 31587 | init sampler, took 0.69 ms, tokens: text = 4442, total = 4442
slot update_slots: id  3 | task 31587 | created context checkpoint 5 of 8 (pos_min = 3608, pos_max = 4377, size = 18.056 MiB)
slot print_timing: id  3 | task 31587 | 
prompt eval time =    1548.36 ms /   778 tokens (    1.99 ms per token,   502.47 tokens per second)
       eval time =   10662.35 ms /   365 tokens (   29.21 ms per token,    34.23 tokens per second)
      total time =   12210.71 ms /  1143 tokens
slot      release: id  3 | task 31587 | stop processing: n_tokens = 4806, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.924
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31954 | processing task, is_child = 0
slot update_slots: id  3 | task 31954 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4626
slot update_slots: id  3 | task 31954 | n_tokens = 4442, memory_seq_rm [4442, end)
slot update_slots: id  3 | task 31954 | prompt processing progress, n_tokens = 4562, batch.n_tokens = 120, progress = 0.986165
slot update_slots: id  3 | task 31954 | n_tokens = 4562, memory_seq_rm [4562, end)
slot update_slots: id  3 | task 31954 | prompt processing progress, n_tokens = 4626, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31954 | prompt done, n_tokens = 4626, batch.n_tokens = 64
slot init_sampler: id  3 | task 31954 | init sampler, took 0.70 ms, tokens: text = 4626, total = 4626
slot update_slots: id  3 | task 31954 | created context checkpoint 6 of 8 (pos_min = 4036, pos_max = 4561, size = 12.334 MiB)
slot print_timing: id  3 | task 31954 | 
prompt eval time =     661.45 ms /   184 tokens (    3.59 ms per token,   278.18 tokens per second)
       eval time =    4232.25 ms /   147 tokens (   28.79 ms per token,    34.73 tokens per second)
      total time =    4893.69 ms /   331 tokens
slot      release: id  3 | task 31954 | stop processing: n_tokens = 4772, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32103 | processing task, is_child = 0
slot update_slots: id  3 | task 32103 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4688
slot update_slots: id  3 | task 32103 | n_tokens = 4626, memory_seq_rm [4626, end)
slot update_slots: id  3 | task 32103 | prompt processing progress, n_tokens = 4688, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 32103 | prompt done, n_tokens = 4688, batch.n_tokens = 62
slot init_sampler: id  3 | task 32103 | init sampler, took 0.85 ms, tokens: text = 4688, total = 4688
slot print_timing: id  3 | task 32103 | 
prompt eval time =     343.63 ms /    62 tokens (    5.54 ms per token,   180.43 tokens per second)
       eval time =   15818.68 ms /   543 tokens (   29.13 ms per token,    34.33 tokens per second)
      total time =   16162.32 ms /   605 tokens
slot      release: id  3 | task 32103 | stop processing: n_tokens = 5230, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.927 (> 0.100 thold), f_keep = 0.896
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32647 | processing task, is_child = 0
slot update_slots: id  3 | task 32647 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5058
slot update_slots: id  3 | task 32647 | n_tokens = 4688, memory_seq_rm [4688, end)
slot update_slots: id  3 | task 32647 | prompt processing progress, n_tokens = 4994, batch.n_tokens = 306, progress = 0.987347
slot update_slots: id  3 | task 32647 | n_tokens = 4994, memory_seq_rm [4994, end)
slot update_slots: id  3 | task 32647 | prompt processing progress, n_tokens = 5058, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 32647 | prompt done, n_tokens = 5058, batch.n_tokens = 64
slot init_sampler: id  3 | task 32647 | init sampler, took 0.89 ms, tokens: text = 5058, total = 5058
slot update_slots: id  3 | task 32647 | created context checkpoint 7 of 8 (pos_min = 4561, pos_max = 4993, size = 10.154 MiB)
slot print_timing: id  3 | task 32647 | 
prompt eval time =     824.91 ms /   370 tokens (    2.23 ms per token,   448.53 tokens per second)
       eval time =    1967.87 ms /    68 tokens (   28.94 ms per token,    34.56 tokens per second)
      total time =    2792.78 ms /   438 tokens
slot      release: id  3 | task 32647 | stop processing: n_tokens = 5125, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32717 | processing task, is_child = 0
slot update_slots: id  3 | task 32717 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5125
slot update_slots: id  3 | task 32717 | n_tokens = 5058, memory_seq_rm [5058, end)
slot update_slots: id  3 | task 32717 | prompt processing progress, n_tokens = 5061, batch.n_tokens = 3, progress = 0.987512
slot update_slots: id  3 | task 32717 | n_tokens = 5061, memory_seq_rm [5061, end)
slot update_slots: id  3 | task 32717 | prompt processing progress, n_tokens = 5125, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 32717 | prompt done, n_tokens = 5125, batch.n_tokens = 64
slot init_sampler: id  3 | task 32717 | init sampler, took 0.94 ms, tokens: text = 5125, total = 5125
slot update_slots: id  3 | task 32717 | created context checkpoint 8 of 8 (pos_min = 4561, pos_max = 5060, size = 11.725 MiB)
slot print_timing: id  3 | task 32717 | 
prompt eval time =     297.85 ms /    67 tokens (    4.45 ms per token,   224.94 tokens per second)
       eval time =    4575.96 ms /   152 tokens (   30.11 ms per token,    33.22 tokens per second)
      total time =    4873.82 ms /   219 tokens
slot      release: id  3 | task 32717 | stop processing: n_tokens = 5276, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.749 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32871 | processing task, is_child = 0
slot update_slots: id  3 | task 32871 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6842
slot update_slots: id  3 | task 32871 | n_tokens = 5125, memory_seq_rm [5125, end)
slot update_slots: id  3 | task 32871 | prompt processing progress, n_tokens = 6778, batch.n_tokens = 1653, progress = 0.990646
slot update_slots: id  3 | task 32871 | n_tokens = 6778, memory_seq_rm [6778, end)
slot update_slots: id  3 | task 32871 | prompt processing progress, n_tokens = 6842, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 32871 | prompt done, n_tokens = 6842, batch.n_tokens = 64
slot init_sampler: id  3 | task 32871 | init sampler, took 3.31 ms, tokens: text = 6842, total = 6842
slot update_slots: id  3 | task 32871 | erasing old context checkpoint (pos_min = 345, pos_max = 1114, size = 18.056 MiB)
slot update_slots: id  3 | task 32871 | created context checkpoint 8 of 8 (pos_min = 6008, pos_max = 6777, size = 18.056 MiB)
slot print_timing: id  3 | task 32871 | 
prompt eval time =    3208.19 ms /  1717 tokens (    1.87 ms per token,   535.19 tokens per second)
       eval time =   23784.14 ms /   797 tokens (   29.84 ms per token,    33.51 tokens per second)
      total time =   26992.33 ms /  2514 tokens
slot      release: id  3 | task 32871 | stop processing: n_tokens = 7638, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.922 (> 0.100 thold), f_keep = 0.896
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33670 | processing task, is_child = 0
slot update_slots: id  3 | task 33670 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7422
slot update_slots: id  3 | task 33670 | n_past = 6842, slot.prompt.tokens.size() = 7638, seq_id = 3, pos_min = 6868, n_swa = 128
slot update_slots: id  3 | task 33670 | restored context checkpoint (pos_min = 6008, pos_max = 6777, size = 18.056 MiB)
slot update_slots: id  3 | task 33670 | n_tokens = 6777, memory_seq_rm [6777, end)
slot update_slots: id  3 | task 33670 | prompt processing progress, n_tokens = 7358, batch.n_tokens = 581, progress = 0.991377
slot update_slots: id  3 | task 33670 | n_tokens = 7358, memory_seq_rm [7358, end)
slot update_slots: id  3 | task 33670 | prompt processing progress, n_tokens = 7422, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33670 | prompt done, n_tokens = 7422, batch.n_tokens = 64
slot init_sampler: id  3 | task 33670 | init sampler, took 1.31 ms, tokens: text = 7422, total = 7422
slot update_slots: id  3 | task 33670 | erasing old context checkpoint (pos_min = 1991, pos_max = 2760, size = 18.056 MiB)
slot update_slots: id  3 | task 33670 | created context checkpoint 8 of 8 (pos_min = 6588, pos_max = 7357, size = 18.056 MiB)
slot print_timing: id  3 | task 33670 | 
prompt eval time =    1592.48 ms /   645 tokens (    2.47 ms per token,   405.03 tokens per second)
       eval time =    5706.09 ms /   177 tokens (   32.24 ms per token,    31.02 tokens per second)
      total time =    7298.56 ms /   822 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 33670 | stop processing: n_tokens = 7598, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33849 | processing task, is_child = 0
slot update_slots: id  3 | task 33849 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7479
slot update_slots: id  3 | task 33849 | n_tokens = 7422, memory_seq_rm [7422, end)
slot update_slots: id  3 | task 33849 | prompt processing progress, n_tokens = 7479, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 33849 | prompt done, n_tokens = 7479, batch.n_tokens = 57
slot init_sampler: id  3 | task 33849 | init sampler, took 1.11 ms, tokens: text = 7479, total = 7479
slot print_timing: id  3 | task 33849 | 
prompt eval time =     304.48 ms /    57 tokens (    5.34 ms per token,   187.21 tokens per second)
       eval time =    2088.21 ms /    60 tokens (   34.80 ms per token,    28.73 tokens per second)
      total time =    2392.69 ms /   117 tokens
slot      release: id  3 | task 33849 | stop processing: n_tokens = 7538, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33910 | processing task, is_child = 0
slot update_slots: id  3 | task 33910 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7553
slot update_slots: id  3 | task 33910 | n_tokens = 7479, memory_seq_rm [7479, end)
slot update_slots: id  3 | task 33910 | prompt processing progress, n_tokens = 7489, batch.n_tokens = 10, progress = 0.991527
slot update_slots: id  3 | task 33910 | n_tokens = 7489, memory_seq_rm [7489, end)
slot update_slots: id  3 | task 33910 | prompt processing progress, n_tokens = 7553, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33910 | prompt done, n_tokens = 7553, batch.n_tokens = 64
slot init_sampler: id  3 | task 33910 | init sampler, took 2.21 ms, tokens: text = 7553, total = 7553
slot update_slots: id  3 | task 33910 | erasing old context checkpoint (pos_min = 2759, pos_max = 3528, size = 18.056 MiB)
slot update_slots: id  3 | task 33910 | created context checkpoint 8 of 8 (pos_min = 6828, pos_max = 7488, size = 15.500 MiB)
slot print_timing: id  3 | task 33910 | 
prompt eval time =     391.19 ms /    74 tokens (    5.29 ms per token,   189.17 tokens per second)
       eval time =    2368.83 ms /    82 tokens (   28.89 ms per token,    34.62 tokens per second)
      total time =    2760.02 ms /   156 tokens
slot      release: id  3 | task 33910 | stop processing: n_tokens = 7634, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.015
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 40479, total state size = 952.168 MiB
srv          load:  - looking for better prompt, base f_keep = 0.015, sim = 0.976
srv        update:  - cache state: 8 prompts, 4844.384 MiB (limits: 8192.000 MiB, 64000 tokens, 266912 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv  get_availabl: prompt cache update took 1567.73 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 33994 | processing task, is_child = 0
slot update_slots: id  0 | task 33994 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 638
slot update_slots: id  0 | task 33994 | n_past = 623, slot.prompt.tokens.size() = 40479, seq_id = 0, pos_min = 40352, n_swa = 128
slot update_slots: id  0 | task 33994 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 33994 | erased invalidated context checkpoint (pos_min = 39563, pos_max = 40332, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 33994 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 33994 | prompt processing progress, n_tokens = 574, batch.n_tokens = 574, progress = 0.899687
slot update_slots: id  0 | task 33994 | n_tokens = 574, memory_seq_rm [574, end)
slot update_slots: id  0 | task 33994 | prompt processing progress, n_tokens = 638, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 33994 | prompt done, n_tokens = 638, batch.n_tokens = 64
slot init_sampler: id  0 | task 33994 | init sampler, took 0.12 ms, tokens: text = 638, total = 638
slot update_slots: id  0 | task 33994 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 573, size = 13.460 MiB)
slot print_timing: id  0 | task 33994 | 
prompt eval time =    1486.91 ms /   638 tokens (    2.33 ms per token,   429.08 tokens per second)
       eval time =     976.94 ms /    31 tokens (   31.51 ms per token,    31.73 tokens per second)
      total time =    2463.85 ms /   669 tokens
slot      release: id  0 | task 33994 | stop processing: n_tokens = 668, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34027 | processing task, is_child = 0
slot update_slots: id  0 | task 34027 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 685
slot update_slots: id  0 | task 34027 | n_tokens = 638, memory_seq_rm [638, end)
slot update_slots: id  0 | task 34027 | prompt processing progress, n_tokens = 685, batch.n_tokens = 47, progress = 1.000000
slot update_slots: id  0 | task 34027 | prompt done, n_tokens = 685, batch.n_tokens = 47
slot init_sampler: id  0 | task 34027 | init sampler, took 0.13 ms, tokens: text = 685, total = 685
slot print_timing: id  0 | task 34027 | 
prompt eval time =     315.69 ms /    47 tokens (    6.72 ms per token,   148.88 tokens per second)
       eval time =    2432.48 ms /    74 tokens (   32.87 ms per token,    30.42 tokens per second)
      total time =    2748.17 ms /   121 tokens
slot      release: id  0 | task 34027 | stop processing: n_tokens = 758, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.923 (> 0.100 thold), f_keep = 0.904
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34102 | processing task, is_child = 0
slot update_slots: id  0 | task 34102 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 742
slot update_slots: id  0 | task 34102 | n_tokens = 685, memory_seq_rm [685, end)
slot update_slots: id  0 | task 34102 | prompt processing progress, n_tokens = 742, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  0 | task 34102 | prompt done, n_tokens = 742, batch.n_tokens = 57
slot init_sampler: id  0 | task 34102 | init sampler, took 0.14 ms, tokens: text = 742, total = 742
slot update_slots: id  0 | task 34102 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 684, size = 16.063 MiB)
slot print_timing: id  0 | task 34102 | 
prompt eval time =     281.37 ms /    57 tokens (    4.94 ms per token,   202.58 tokens per second)
       eval time =    1860.30 ms /    62 tokens (   30.00 ms per token,    33.33 tokens per second)
      total time =    2141.67 ms /   119 tokens
slot      release: id  0 | task 34102 | stop processing: n_tokens = 803, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.487 (> 0.100 thold), f_keep = 0.924
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34165 | processing task, is_child = 0
slot update_slots: id  0 | task 34165 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1524
slot update_slots: id  0 | task 34165 | n_tokens = 742, memory_seq_rm [742, end)
slot update_slots: id  0 | task 34165 | prompt processing progress, n_tokens = 1460, batch.n_tokens = 718, progress = 0.958005
slot update_slots: id  0 | task 34165 | n_tokens = 1460, memory_seq_rm [1460, end)
slot update_slots: id  0 | task 34165 | prompt processing progress, n_tokens = 1524, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34165 | prompt done, n_tokens = 1524, batch.n_tokens = 64
slot init_sampler: id  0 | task 34165 | init sampler, took 0.35 ms, tokens: text = 1524, total = 1524
slot update_slots: id  0 | task 34165 | created context checkpoint 3 of 8 (pos_min = 690, pos_max = 1459, size = 18.056 MiB)
slot print_timing: id  0 | task 34165 | 
prompt eval time =    1578.77 ms /   782 tokens (    2.02 ms per token,   495.32 tokens per second)
       eval time =    1877.47 ms /    63 tokens (   29.80 ms per token,    33.56 tokens per second)
      total time =    3456.23 ms /   845 tokens
slot      release: id  0 | task 34165 | stop processing: n_tokens = 1586, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.393
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1586, total state size = 55.246 MiB
srv          load:  - looking for better prompt, base f_keep = 0.393, sim = 0.978
srv        update:  - cache state: 9 prompts, 4947.209 MiB (limits: 8192.000 MiB, 64000 tokens, 263990 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv  get_availabl: prompt cache update took 84.40 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34230 | processing task, is_child = 0
slot update_slots: id  0 | task 34230 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 638
slot update_slots: id  0 | task 34230 | n_past = 624, slot.prompt.tokens.size() = 1586, seq_id = 0, pos_min = 816, n_swa = 128
slot update_slots: id  0 | task 34230 | restored context checkpoint (pos_min = 0, pos_max = 684, size = 16.063 MiB)
slot update_slots: id  0 | task 34230 | erased invalidated context checkpoint (pos_min = 690, pos_max = 1459, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 34230 | n_tokens = 624, memory_seq_rm [624, end)
slot update_slots: id  0 | task 34230 | prompt processing progress, n_tokens = 638, batch.n_tokens = 14, progress = 1.000000
slot update_slots: id  0 | task 34230 | prompt done, n_tokens = 638, batch.n_tokens = 14
slot init_sampler: id  0 | task 34230 | init sampler, took 0.11 ms, tokens: text = 638, total = 638
slot print_timing: id  0 | task 34230 | 
prompt eval time =     260.54 ms /    14 tokens (   18.61 ms per token,    53.73 tokens per second)
       eval time =     821.08 ms /    29 tokens (   28.31 ms per token,    35.32 tokens per second)
      total time =    1081.63 ms /    43 tokens
slot      release: id  0 | task 34230 | stop processing: n_tokens = 666, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.906 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34260 | processing task, is_child = 0
slot update_slots: id  0 | task 34260 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 704
slot update_slots: id  0 | task 34260 | n_tokens = 638, memory_seq_rm [638, end)
slot update_slots: id  0 | task 34260 | prompt processing progress, n_tokens = 640, batch.n_tokens = 2, progress = 0.909091
slot update_slots: id  0 | task 34260 | n_tokens = 640, memory_seq_rm [640, end)
slot update_slots: id  0 | task 34260 | prompt processing progress, n_tokens = 704, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34260 | prompt done, n_tokens = 704, batch.n_tokens = 64
slot init_sampler: id  0 | task 34260 | init sampler, took 0.12 ms, tokens: text = 704, total = 704
slot print_timing: id  0 | task 34260 | 
prompt eval time =     332.58 ms /    66 tokens (    5.04 ms per token,   198.45 tokens per second)
       eval time =    1355.85 ms /    48 tokens (   28.25 ms per token,    35.40 tokens per second)
      total time =    1688.43 ms /   114 tokens
slot      release: id  0 | task 34260 | stop processing: n_tokens = 751, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.853 (> 0.100 thold), f_keep = 0.830
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34310 | processing task, is_child = 0
slot update_slots: id  0 | task 34310 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 730
slot update_slots: id  0 | task 34310 | n_tokens = 623, memory_seq_rm [623, end)
slot update_slots: id  0 | task 34310 | prompt processing progress, n_tokens = 666, batch.n_tokens = 43, progress = 0.912329
slot update_slots: id  0 | task 34310 | n_tokens = 666, memory_seq_rm [666, end)
slot update_slots: id  0 | task 34310 | prompt processing progress, n_tokens = 730, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34310 | prompt done, n_tokens = 730, batch.n_tokens = 64
slot init_sampler: id  0 | task 34310 | init sampler, took 0.13 ms, tokens: text = 730, total = 730
slot print_timing: id  0 | task 34310 | 
prompt eval time =     499.11 ms /   107 tokens (    4.66 ms per token,   214.38 tokens per second)
       eval time =    1313.28 ms /    46 tokens (   28.55 ms per token,    35.03 tokens per second)
      total time =    1812.39 ms /   153 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 34310 | stop processing: n_tokens = 775, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.483 (> 0.100 thold), f_keep = 0.942
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34358 | processing task, is_child = 0
slot update_slots: id  0 | task 34358 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1512
slot update_slots: id  0 | task 34358 | n_tokens = 730, memory_seq_rm [730, end)
slot update_slots: id  0 | task 34358 | prompt processing progress, n_tokens = 1448, batch.n_tokens = 718, progress = 0.957672
slot update_slots: id  0 | task 34358 | n_tokens = 1448, memory_seq_rm [1448, end)
slot update_slots: id  0 | task 34358 | prompt processing progress, n_tokens = 1512, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34358 | prompt done, n_tokens = 1512, batch.n_tokens = 64
slot init_sampler: id  0 | task 34358 | init sampler, took 0.29 ms, tokens: text = 1512, total = 1512
slot update_slots: id  0 | task 34358 | created context checkpoint 3 of 8 (pos_min = 718, pos_max = 1447, size = 17.118 MiB)
slot print_timing: id  0 | task 34358 | 
prompt eval time =    1572.77 ms /   782 tokens (    2.01 ms per token,   497.21 tokens per second)
       eval time =    2266.47 ms /    76 tokens (   29.82 ms per token,    33.53 tokens per second)
      total time =    3839.23 ms /   858 tokens
slot      release: id  0 | task 34358 | stop processing: n_tokens = 1587, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.406 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34436 | processing task, is_child = 0
slot update_slots: id  0 | task 34436 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3722
slot update_slots: id  0 | task 34436 | n_tokens = 1512, memory_seq_rm [1512, end)
slot update_slots: id  0 | task 34436 | prompt processing progress, n_tokens = 3560, batch.n_tokens = 2048, progress = 0.956475
slot update_slots: id  0 | task 34436 | n_tokens = 3560, memory_seq_rm [3560, end)
slot update_slots: id  0 | task 34436 | prompt processing progress, n_tokens = 3658, batch.n_tokens = 98, progress = 0.982805
slot update_slots: id  0 | task 34436 | n_tokens = 3658, memory_seq_rm [3658, end)
slot update_slots: id  0 | task 34436 | prompt processing progress, n_tokens = 3722, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34436 | prompt done, n_tokens = 3722, batch.n_tokens = 64
slot init_sampler: id  0 | task 34436 | init sampler, took 0.55 ms, tokens: text = 3722, total = 3722
slot update_slots: id  0 | task 34436 | created context checkpoint 4 of 8 (pos_min = 2888, pos_max = 3657, size = 18.056 MiB)
slot print_timing: id  0 | task 34436 | 
prompt eval time =    4074.40 ms /  2210 tokens (    1.84 ms per token,   542.41 tokens per second)
       eval time =   18897.72 ms /   640 tokens (   29.53 ms per token,    33.87 tokens per second)
      total time =   22972.12 ms /  2850 tokens
slot      release: id  0 | task 34436 | stop processing: n_tokens = 4361, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.896 (> 0.100 thold), f_keep = 0.853
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35079 | processing task, is_child = 0
slot update_slots: id  0 | task 35079 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4153
slot update_slots: id  0 | task 35079 | n_tokens = 3722, memory_seq_rm [3722, end)
slot update_slots: id  0 | task 35079 | prompt processing progress, n_tokens = 4089, batch.n_tokens = 367, progress = 0.984589
slot update_slots: id  0 | task 35079 | n_tokens = 4089, memory_seq_rm [4089, end)
slot update_slots: id  0 | task 35079 | prompt processing progress, n_tokens = 4153, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35079 | prompt done, n_tokens = 4153, batch.n_tokens = 64
slot init_sampler: id  0 | task 35079 | init sampler, took 0.68 ms, tokens: text = 4153, total = 4153
slot update_slots: id  0 | task 35079 | created context checkpoint 5 of 8 (pos_min = 3591, pos_max = 4088, size = 11.678 MiB)
slot print_timing: id  0 | task 35079 | 
prompt eval time =     956.12 ms /   431 tokens (    2.22 ms per token,   450.78 tokens per second)
       eval time =    1561.65 ms /    53 tokens (   29.47 ms per token,    33.94 tokens per second)
      total time =    2517.78 ms /   484 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 35079 | stop processing: n_tokens = 4205, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.813 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35134 | processing task, is_child = 0
slot update_slots: id  0 | task 35134 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5107
slot update_slots: id  0 | task 35134 | n_tokens = 4153, memory_seq_rm [4153, end)
slot update_slots: id  0 | task 35134 | prompt processing progress, n_tokens = 5043, batch.n_tokens = 890, progress = 0.987468
slot update_slots: id  0 | task 35134 | n_tokens = 5043, memory_seq_rm [5043, end)
slot update_slots: id  0 | task 35134 | prompt processing progress, n_tokens = 5107, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35134 | prompt done, n_tokens = 5107, batch.n_tokens = 64
slot init_sampler: id  0 | task 35134 | init sampler, took 0.95 ms, tokens: text = 5107, total = 5107
slot update_slots: id  0 | task 35134 | created context checkpoint 6 of 8 (pos_min = 4273, pos_max = 5042, size = 18.056 MiB)
slot print_timing: id  0 | task 35134 | 
prompt eval time =    1821.42 ms /   954 tokens (    1.91 ms per token,   523.77 tokens per second)
       eval time =    1707.62 ms /    58 tokens (   29.44 ms per token,    33.97 tokens per second)
      total time =    3529.05 ms /  1012 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 35134 | stop processing: n_tokens = 5164, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.897 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35194 | processing task, is_child = 0
slot update_slots: id  0 | task 35194 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5691
slot update_slots: id  0 | task 35194 | n_tokens = 5107, memory_seq_rm [5107, end)
slot update_slots: id  0 | task 35194 | prompt processing progress, n_tokens = 5627, batch.n_tokens = 520, progress = 0.988754
slot update_slots: id  0 | task 35194 | n_tokens = 5627, memory_seq_rm [5627, end)
slot update_slots: id  0 | task 35194 | prompt processing progress, n_tokens = 5691, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35194 | prompt done, n_tokens = 5691, batch.n_tokens = 64
slot init_sampler: id  0 | task 35194 | init sampler, took 0.84 ms, tokens: text = 5691, total = 5691
slot update_slots: id  0 | task 35194 | created context checkpoint 7 of 8 (pos_min = 4857, pos_max = 5626, size = 18.056 MiB)
slot print_timing: id  0 | task 35194 | 
prompt eval time =    1197.31 ms /   584 tokens (    2.05 ms per token,   487.76 tokens per second)
       eval time =    1961.13 ms /    68 tokens (   28.84 ms per token,    34.67 tokens per second)
      total time =    3158.44 ms /   652 tokens
slot      release: id  0 | task 35194 | stop processing: n_tokens = 5758, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.919 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35264 | processing task, is_child = 0
slot update_slots: id  0 | task 35264 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6194
slot update_slots: id  0 | task 35264 | n_tokens = 5691, memory_seq_rm [5691, end)
slot update_slots: id  0 | task 35264 | prompt processing progress, n_tokens = 6130, batch.n_tokens = 439, progress = 0.989667
slot update_slots: id  0 | task 35264 | n_tokens = 6130, memory_seq_rm [6130, end)
slot update_slots: id  0 | task 35264 | prompt processing progress, n_tokens = 6194, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35264 | prompt done, n_tokens = 6194, batch.n_tokens = 64
slot init_sampler: id  0 | task 35264 | init sampler, took 0.95 ms, tokens: text = 6194, total = 6194
slot update_slots: id  0 | task 35264 | created context checkpoint 8 of 8 (pos_min = 5360, pos_max = 6129, size = 18.056 MiB)
slot print_timing: id  0 | task 35264 | 
prompt eval time =     994.31 ms /   503 tokens (    1.98 ms per token,   505.88 tokens per second)
       eval time =    1838.65 ms /    64 tokens (   28.73 ms per token,    34.81 tokens per second)
      total time =    2832.97 ms /   567 tokens
slot      release: id  0 | task 35264 | stop processing: n_tokens = 6257, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.937 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35330 | processing task, is_child = 0
slot update_slots: id  0 | task 35330 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6610
slot update_slots: id  0 | task 35330 | n_tokens = 6194, memory_seq_rm [6194, end)
slot update_slots: id  0 | task 35330 | prompt processing progress, n_tokens = 6546, batch.n_tokens = 352, progress = 0.990318
slot update_slots: id  0 | task 35330 | n_tokens = 6546, memory_seq_rm [6546, end)
slot update_slots: id  0 | task 35330 | prompt processing progress, n_tokens = 6610, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35330 | prompt done, n_tokens = 6610, batch.n_tokens = 64
slot init_sampler: id  0 | task 35330 | init sampler, took 1.00 ms, tokens: text = 6610, total = 6610
slot update_slots: id  0 | task 35330 | erasing old context checkpoint (pos_min = 0, pos_max = 573, size = 13.460 MiB)
slot update_slots: id  0 | task 35330 | created context checkpoint 8 of 8 (pos_min = 5776, pos_max = 6545, size = 18.056 MiB)
slot print_timing: id  0 | task 35330 | 
prompt eval time =     898.55 ms /   416 tokens (    2.16 ms per token,   462.97 tokens per second)
       eval time =    1881.29 ms /    66 tokens (   28.50 ms per token,    35.08 tokens per second)
      total time =    2779.84 ms /   482 tokens
slot      release: id  0 | task 35330 | stop processing: n_tokens = 6675, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35398 | processing task, is_child = 0
slot update_slots: id  0 | task 35398 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6967
slot update_slots: id  0 | task 35398 | n_tokens = 6610, memory_seq_rm [6610, end)
slot update_slots: id  0 | task 35398 | prompt processing progress, n_tokens = 6903, batch.n_tokens = 293, progress = 0.990814
slot update_slots: id  0 | task 35398 | n_tokens = 6903, memory_seq_rm [6903, end)
slot update_slots: id  0 | task 35398 | prompt processing progress, n_tokens = 6967, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35398 | prompt done, n_tokens = 6967, batch.n_tokens = 64
slot init_sampler: id  0 | task 35398 | init sampler, took 1.04 ms, tokens: text = 6967, total = 6967
slot update_slots: id  0 | task 35398 | erasing old context checkpoint (pos_min = 0, pos_max = 684, size = 16.063 MiB)
slot update_slots: id  0 | task 35398 | created context checkpoint 8 of 8 (pos_min = 6133, pos_max = 6902, size = 18.056 MiB)
slot print_timing: id  0 | task 35398 | 
prompt eval time =     797.14 ms /   357 tokens (    2.23 ms per token,   447.85 tokens per second)
       eval time =    1995.21 ms /    68 tokens (   29.34 ms per token,    34.08 tokens per second)
      total time =    2792.35 ms /   425 tokens
slot      release: id  0 | task 35398 | stop processing: n_tokens = 7034, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.945 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35468 | processing task, is_child = 0
slot update_slots: id  0 | task 35468 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7372
slot update_slots: id  0 | task 35468 | n_tokens = 6967, memory_seq_rm [6967, end)
slot update_slots: id  0 | task 35468 | prompt processing progress, n_tokens = 7308, batch.n_tokens = 341, progress = 0.991319
slot update_slots: id  0 | task 35468 | n_tokens = 7308, memory_seq_rm [7308, end)
slot update_slots: id  0 | task 35468 | prompt processing progress, n_tokens = 7372, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35468 | prompt done, n_tokens = 7372, batch.n_tokens = 64
slot init_sampler: id  0 | task 35468 | init sampler, took 1.39 ms, tokens: text = 7372, total = 7372
slot update_slots: id  0 | task 35468 | erasing old context checkpoint (pos_min = 718, pos_max = 1447, size = 17.118 MiB)
slot update_slots: id  0 | task 35468 | created context checkpoint 8 of 8 (pos_min = 6538, pos_max = 7307, size = 18.056 MiB)
slot print_timing: id  0 | task 35468 | 
prompt eval time =     883.96 ms /   405 tokens (    2.18 ms per token,   458.17 tokens per second)
       eval time =    2441.80 ms /    84 tokens (   29.07 ms per token,    34.40 tokens per second)
      total time =    3325.76 ms /   489 tokens
slot      release: id  0 | task 35468 | stop processing: n_tokens = 7455, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35554 | processing task, is_child = 0
slot update_slots: id  0 | task 35554 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7788
slot update_slots: id  0 | task 35554 | n_tokens = 7372, memory_seq_rm [7372, end)
slot update_slots: id  0 | task 35554 | prompt processing progress, n_tokens = 7724, batch.n_tokens = 352, progress = 0.991782
slot update_slots: id  0 | task 35554 | n_tokens = 7724, memory_seq_rm [7724, end)
slot update_slots: id  0 | task 35554 | prompt processing progress, n_tokens = 7788, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35554 | prompt done, n_tokens = 7788, batch.n_tokens = 64
slot init_sampler: id  0 | task 35554 | init sampler, took 1.14 ms, tokens: text = 7788, total = 7788
slot update_slots: id  0 | task 35554 | erasing old context checkpoint (pos_min = 2888, pos_max = 3657, size = 18.056 MiB)
slot update_slots: id  0 | task 35554 | created context checkpoint 8 of 8 (pos_min = 6954, pos_max = 7723, size = 18.056 MiB)
slot print_timing: id  0 | task 35554 | 
prompt eval time =     889.38 ms /   416 tokens (    2.14 ms per token,   467.74 tokens per second)
       eval time =    1665.78 ms /    58 tokens (   28.72 ms per token,    34.82 tokens per second)
      total time =    2555.17 ms /   474 tokens
slot      release: id  0 | task 35554 | stop processing: n_tokens = 7845, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35614 | processing task, is_child = 0
slot update_slots: id  0 | task 35614 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8204
slot update_slots: id  0 | task 35614 | n_tokens = 7788, memory_seq_rm [7788, end)
slot update_slots: id  0 | task 35614 | prompt processing progress, n_tokens = 8140, batch.n_tokens = 352, progress = 0.992199
slot update_slots: id  0 | task 35614 | n_tokens = 8140, memory_seq_rm [8140, end)
slot update_slots: id  0 | task 35614 | prompt processing progress, n_tokens = 8204, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35614 | prompt done, n_tokens = 8204, batch.n_tokens = 64
slot init_sampler: id  0 | task 35614 | init sampler, took 1.21 ms, tokens: text = 8204, total = 8204
slot update_slots: id  0 | task 35614 | erasing old context checkpoint (pos_min = 3591, pos_max = 4088, size = 11.678 MiB)
slot update_slots: id  0 | task 35614 | created context checkpoint 8 of 8 (pos_min = 7370, pos_max = 8139, size = 18.056 MiB)
slot print_timing: id  0 | task 35614 | 
prompt eval time =     881.91 ms /   416 tokens (    2.12 ms per token,   471.70 tokens per second)
       eval time =    1485.43 ms /    52 tokens (   28.57 ms per token,    35.01 tokens per second)
      total time =    2367.34 ms /   468 tokens
slot      release: id  0 | task 35614 | stop processing: n_tokens = 8255, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35668 | processing task, is_child = 0
slot update_slots: id  0 | task 35668 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8788
slot update_slots: id  0 | task 35668 | n_tokens = 8204, memory_seq_rm [8204, end)
slot update_slots: id  0 | task 35668 | prompt processing progress, n_tokens = 8724, batch.n_tokens = 520, progress = 0.992717
slot update_slots: id  0 | task 35668 | n_tokens = 8724, memory_seq_rm [8724, end)
slot update_slots: id  0 | task 35668 | prompt processing progress, n_tokens = 8788, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35668 | prompt done, n_tokens = 8788, batch.n_tokens = 64
slot init_sampler: id  0 | task 35668 | init sampler, took 1.40 ms, tokens: text = 8788, total = 8788
slot update_slots: id  0 | task 35668 | erasing old context checkpoint (pos_min = 4273, pos_max = 5042, size = 18.056 MiB)
slot update_slots: id  0 | task 35668 | created context checkpoint 8 of 8 (pos_min = 7954, pos_max = 8723, size = 18.056 MiB)
slot print_timing: id  0 | task 35668 | 
prompt eval time =    1163.30 ms /   584 tokens (    1.99 ms per token,   502.02 tokens per second)
       eval time =    1911.52 ms /    67 tokens (   28.53 ms per token,    35.05 tokens per second)
      total time =    3074.81 ms /   651 tokens
slot      release: id  0 | task 35668 | stop processing: n_tokens = 8854, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35737 | processing task, is_child = 0
slot update_slots: id  0 | task 35737 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9410
slot update_slots: id  0 | task 35737 | n_tokens = 8788, memory_seq_rm [8788, end)
slot update_slots: id  0 | task 35737 | prompt processing progress, n_tokens = 9346, batch.n_tokens = 558, progress = 0.993199
slot update_slots: id  0 | task 35737 | n_tokens = 9346, memory_seq_rm [9346, end)
slot update_slots: id  0 | task 35737 | prompt processing progress, n_tokens = 9410, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35737 | prompt done, n_tokens = 9410, batch.n_tokens = 64
slot init_sampler: id  0 | task 35737 | init sampler, took 1.36 ms, tokens: text = 9410, total = 9410
slot update_slots: id  0 | task 35737 | erasing old context checkpoint (pos_min = 4857, pos_max = 5626, size = 18.056 MiB)
slot update_slots: id  0 | task 35737 | created context checkpoint 8 of 8 (pos_min = 8576, pos_max = 9345, size = 18.056 MiB)
slot print_timing: id  0 | task 35737 | 
prompt eval time =    1260.89 ms /   622 tokens (    2.03 ms per token,   493.30 tokens per second)
       eval time =    1740.67 ms /    60 tokens (   29.01 ms per token,    34.47 tokens per second)
      total time =    3001.56 ms /   682 tokens
slot      release: id  0 | task 35737 | stop processing: n_tokens = 9469, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.942 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35799 | processing task, is_child = 0
slot update_slots: id  0 | task 35799 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9994
slot update_slots: id  0 | task 35799 | n_tokens = 9410, memory_seq_rm [9410, end)
slot update_slots: id  0 | task 35799 | prompt processing progress, n_tokens = 9930, batch.n_tokens = 520, progress = 0.993596
slot update_slots: id  0 | task 35799 | n_tokens = 9930, memory_seq_rm [9930, end)
slot update_slots: id  0 | task 35799 | prompt processing progress, n_tokens = 9994, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35799 | prompt done, n_tokens = 9994, batch.n_tokens = 64
slot init_sampler: id  0 | task 35799 | init sampler, took 1.82 ms, tokens: text = 9994, total = 9994
slot update_slots: id  0 | task 35799 | erasing old context checkpoint (pos_min = 5360, pos_max = 6129, size = 18.056 MiB)
slot update_slots: id  0 | task 35799 | created context checkpoint 8 of 8 (pos_min = 9160, pos_max = 9929, size = 18.056 MiB)
slot print_timing: id  0 | task 35799 | 
prompt eval time =    1167.14 ms /   584 tokens (    2.00 ms per token,   500.37 tokens per second)
       eval time =    1695.16 ms /    58 tokens (   29.23 ms per token,    34.22 tokens per second)
      total time =    2862.30 ms /   642 tokens
slot      release: id  0 | task 35799 | stop processing: n_tokens = 10051, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35859 | processing task, is_child = 0
slot update_slots: id  0 | task 35859 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10410
slot update_slots: id  0 | task 35859 | n_tokens = 9994, memory_seq_rm [9994, end)
slot update_slots: id  0 | task 35859 | prompt processing progress, n_tokens = 10346, batch.n_tokens = 352, progress = 0.993852
slot update_slots: id  0 | task 35859 | n_tokens = 10346, memory_seq_rm [10346, end)
slot update_slots: id  0 | task 35859 | prompt processing progress, n_tokens = 10410, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35859 | prompt done, n_tokens = 10410, batch.n_tokens = 64
slot init_sampler: id  0 | task 35859 | init sampler, took 1.49 ms, tokens: text = 10410, total = 10410
slot update_slots: id  0 | task 35859 | erasing old context checkpoint (pos_min = 5776, pos_max = 6545, size = 18.056 MiB)
slot update_slots: id  0 | task 35859 | created context checkpoint 8 of 8 (pos_min = 9576, pos_max = 10345, size = 18.056 MiB)
slot print_timing: id  0 | task 35859 | 
prompt eval time =     886.97 ms /   416 tokens (    2.13 ms per token,   469.01 tokens per second)
       eval time =    2210.83 ms /    77 tokens (   28.71 ms per token,    34.83 tokens per second)
      total time =    3097.80 ms /   493 tokens
slot      release: id  0 | task 35859 | stop processing: n_tokens = 10486, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35938 | processing task, is_child = 0
slot update_slots: id  0 | task 35938 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10913
slot update_slots: id  0 | task 35938 | n_tokens = 10410, memory_seq_rm [10410, end)
slot update_slots: id  0 | task 35938 | prompt processing progress, n_tokens = 10849, batch.n_tokens = 439, progress = 0.994135
slot update_slots: id  0 | task 35938 | n_tokens = 10849, memory_seq_rm [10849, end)
slot update_slots: id  0 | task 35938 | prompt processing progress, n_tokens = 10913, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35938 | prompt done, n_tokens = 10913, batch.n_tokens = 64
slot init_sampler: id  0 | task 35938 | init sampler, took 1.88 ms, tokens: text = 10913, total = 10913
slot update_slots: id  0 | task 35938 | erasing old context checkpoint (pos_min = 6133, pos_max = 6902, size = 18.056 MiB)
slot update_slots: id  0 | task 35938 | created context checkpoint 8 of 8 (pos_min = 10079, pos_max = 10848, size = 18.056 MiB)
slot print_timing: id  0 | task 35938 | 
prompt eval time =     979.02 ms /   503 tokens (    1.95 ms per token,   513.78 tokens per second)
       eval time =   14633.52 ms /   506 tokens (   28.92 ms per token,    34.58 tokens per second)
      total time =   15612.54 ms /  1009 tokens
slot      release: id  0 | task 35938 | stop processing: n_tokens = 11418, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.968 (> 0.100 thold), f_keep = 0.956
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36446 | processing task, is_child = 0
slot update_slots: id  0 | task 36446 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11271
slot update_slots: id  0 | task 36446 | n_tokens = 10913, memory_seq_rm [10913, end)
slot update_slots: id  0 | task 36446 | prompt processing progress, n_tokens = 11207, batch.n_tokens = 294, progress = 0.994322
slot update_slots: id  0 | task 36446 | n_tokens = 11207, memory_seq_rm [11207, end)
slot update_slots: id  0 | task 36446 | prompt processing progress, n_tokens = 11271, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36446 | prompt done, n_tokens = 11271, batch.n_tokens = 64
slot init_sampler: id  0 | task 36446 | init sampler, took 1.63 ms, tokens: text = 11271, total = 11271
slot update_slots: id  0 | task 36446 | erasing old context checkpoint (pos_min = 6538, pos_max = 7307, size = 18.056 MiB)
slot update_slots: id  0 | task 36446 | created context checkpoint 8 of 8 (pos_min = 10648, pos_max = 11206, size = 13.108 MiB)
slot print_timing: id  0 | task 36446 | 
prompt eval time =     823.73 ms /   358 tokens (    2.30 ms per token,   434.61 tokens per second)
       eval time =    1644.06 ms /    57 tokens (   28.84 ms per token,    34.67 tokens per second)
      total time =    2467.79 ms /   415 tokens
slot      release: id  0 | task 36446 | stop processing: n_tokens = 11327, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36505 | processing task, is_child = 0
slot update_slots: id  0 | task 36505 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11584
slot update_slots: id  0 | task 36505 | n_tokens = 11271, memory_seq_rm [11271, end)
slot update_slots: id  0 | task 36505 | prompt processing progress, n_tokens = 11520, batch.n_tokens = 249, progress = 0.994475
slot update_slots: id  0 | task 36505 | n_tokens = 11520, memory_seq_rm [11520, end)
slot update_slots: id  0 | task 36505 | prompt processing progress, n_tokens = 11584, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36505 | prompt done, n_tokens = 11584, batch.n_tokens = 64
slot init_sampler: id  0 | task 36505 | init sampler, took 2.39 ms, tokens: text = 11584, total = 11584
slot update_slots: id  0 | task 36505 | erasing old context checkpoint (pos_min = 6954, pos_max = 7723, size = 18.056 MiB)
slot update_slots: id  0 | task 36505 | created context checkpoint 8 of 8 (pos_min = 10750, pos_max = 11519, size = 18.056 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 36505
slot      release: id  0 | task 36505 | stop processing: n_tokens = 11695, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.871 (> 0.100 thold), f_keep = 0.054
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 11695, total state size = 292.292 MiB
srv          load:  - looking for better prompt, base f_keep = 0.054, sim = 0.871
srv        update:  - cache state: 10 prompts, 5379.001 MiB (limits: 8192.000 MiB, 64000 tokens, 260610 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv  get_availabl: prompt cache update took 548.94 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36620 | processing task, is_child = 0
slot update_slots: id  0 | task 36620 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 726
slot update_slots: id  0 | task 36620 | n_past = 632, slot.prompt.tokens.size() = 11695, seq_id = 0, pos_min = 10925, n_swa = 128
slot update_slots: id  0 | task 36620 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 36620 | erased invalidated context checkpoint (pos_min = 7370, pos_max = 8139, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 36620 | erased invalidated context checkpoint (pos_min = 7954, pos_max = 8723, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 36620 | erased invalidated context checkpoint (pos_min = 8576, pos_max = 9345, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 36620 | erased invalidated context checkpoint (pos_min = 9160, pos_max = 9929, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 36620 | erased invalidated context checkpoint (pos_min = 9576, pos_max = 10345, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 36620 | erased invalidated context checkpoint (pos_min = 10079, pos_max = 10848, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 36620 | erased invalidated context checkpoint (pos_min = 10648, pos_max = 11206, n_swa = 128, size = 13.108 MiB)
slot update_slots: id  0 | task 36620 | erased invalidated context checkpoint (pos_min = 10750, pos_max = 11519, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 36620 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 36620 | prompt processing progress, n_tokens = 662, batch.n_tokens = 662, progress = 0.911846
slot update_slots: id  0 | task 36620 | n_tokens = 662, memory_seq_rm [662, end)
slot update_slots: id  0 | task 36620 | prompt processing progress, n_tokens = 726, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36620 | prompt done, n_tokens = 726, batch.n_tokens = 64
slot init_sampler: id  0 | task 36620 | init sampler, took 0.14 ms, tokens: text = 726, total = 726
slot update_slots: id  0 | task 36620 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 661, size = 15.523 MiB)
slot print_timing: id  0 | task 36620 | 
prompt eval time =    1541.89 ms /   726 tokens (    2.12 ms per token,   470.85 tokens per second)
       eval time =    1147.83 ms /    41 tokens (   28.00 ms per token,    35.72 tokens per second)
      total time =    2689.72 ms /   767 tokens
slot      release: id  0 | task 36620 | stop processing: n_tokens = 766, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.483 (> 0.100 thold), f_keep = 0.948
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36663 | processing task, is_child = 0
slot update_slots: id  0 | task 36663 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1504
slot update_slots: id  0 | task 36663 | n_tokens = 726, memory_seq_rm [726, end)
slot update_slots: id  0 | task 36663 | prompt processing progress, n_tokens = 1440, batch.n_tokens = 714, progress = 0.957447
slot update_slots: id  0 | task 36663 | n_tokens = 1440, memory_seq_rm [1440, end)
slot update_slots: id  0 | task 36663 | prompt processing progress, n_tokens = 1504, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36663 | prompt done, n_tokens = 1504, batch.n_tokens = 64
slot init_sampler: id  0 | task 36663 | init sampler, took 0.26 ms, tokens: text = 1504, total = 1504
slot update_slots: id  0 | task 36663 | created context checkpoint 2 of 8 (pos_min = 670, pos_max = 1439, size = 18.056 MiB)
slot print_timing: id  0 | task 36663 | 
prompt eval time =    1536.92 ms /   778 tokens (    1.98 ms per token,   506.21 tokens per second)
       eval time =    1367.02 ms /    48 tokens (   28.48 ms per token,    35.11 tokens per second)
      total time =    2903.94 ms /   826 tokens
slot      release: id  0 | task 36663 | stop processing: n_tokens = 1551, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.417 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36713 | processing task, is_child = 0
slot update_slots: id  0 | task 36713 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3606
slot update_slots: id  0 | task 36713 | n_tokens = 1504, memory_seq_rm [1504, end)
slot update_slots: id  0 | task 36713 | prompt processing progress, n_tokens = 3542, batch.n_tokens = 2038, progress = 0.982252
slot update_slots: id  0 | task 36713 | n_tokens = 3542, memory_seq_rm [3542, end)
slot update_slots: id  0 | task 36713 | prompt processing progress, n_tokens = 3606, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36713 | prompt done, n_tokens = 3606, batch.n_tokens = 64
slot init_sampler: id  0 | task 36713 | init sampler, took 0.71 ms, tokens: text = 3606, total = 3606
slot update_slots: id  0 | task 36713 | created context checkpoint 3 of 8 (pos_min = 2772, pos_max = 3541, size = 18.056 MiB)
slot print_timing: id  0 | task 36713 | 
prompt eval time =    3706.24 ms /  2102 tokens (    1.76 ms per token,   567.15 tokens per second)
       eval time =   19023.41 ms /   646 tokens (   29.45 ms per token,    33.96 tokens per second)
      total time =   22729.66 ms /  2748 tokens
slot      release: id  0 | task 36713 | stop processing: n_tokens = 4251, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.903 (> 0.100 thold), f_keep = 0.848
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37361 | processing task, is_child = 0
slot update_slots: id  0 | task 37361 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3995
slot update_slots: id  0 | task 37361 | n_past = 3606, slot.prompt.tokens.size() = 4251, seq_id = 0, pos_min = 3481, n_swa = 128
slot update_slots: id  0 | task 37361 | restored context checkpoint (pos_min = 2772, pos_max = 3541, size = 18.056 MiB)
slot update_slots: id  0 | task 37361 | n_tokens = 3541, memory_seq_rm [3541, end)
slot update_slots: id  0 | task 37361 | prompt processing progress, n_tokens = 3931, batch.n_tokens = 390, progress = 0.983980
slot update_slots: id  0 | task 37361 | n_tokens = 3931, memory_seq_rm [3931, end)
slot update_slots: id  0 | task 37361 | prompt processing progress, n_tokens = 3995, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37361 | prompt done, n_tokens = 3995, batch.n_tokens = 64
slot init_sampler: id  0 | task 37361 | init sampler, took 0.65 ms, tokens: text = 3995, total = 3995
slot update_slots: id  0 | task 37361 | created context checkpoint 4 of 8 (pos_min = 3161, pos_max = 3930, size = 18.056 MiB)
slot print_timing: id  0 | task 37361 | 
prompt eval time =    1116.20 ms /   454 tokens (    2.46 ms per token,   406.74 tokens per second)
       eval time =   10758.57 ms /   368 tokens (   29.24 ms per token,    34.21 tokens per second)
      total time =   11874.77 ms /   822 tokens
slot      release: id  0 | task 37361 | stop processing: n_tokens = 4362, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.833 (> 0.100 thold), f_keep = 0.143
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4362, total state size = 120.341 MiB
srv          load:  - looking for better prompt, base f_keep = 0.143, sim = 0.833
srv        update:  - cache state: 11 prompts, 5569.033 MiB (limits: 8192.000 MiB, 64000 tokens, 258133 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv  get_availabl: prompt cache update took 153.73 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37731 | processing task, is_child = 0
slot update_slots: id  0 | task 37731 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 748
slot update_slots: id  0 | task 37731 | n_past = 623, slot.prompt.tokens.size() = 4362, seq_id = 0, pos_min = 3592, n_swa = 128
slot update_slots: id  0 | task 37731 | restored context checkpoint (pos_min = 0, pos_max = 661, size = 15.523 MiB)
slot update_slots: id  0 | task 37731 | erased invalidated context checkpoint (pos_min = 670, pos_max = 1439, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 37731 | erased invalidated context checkpoint (pos_min = 2772, pos_max = 3541, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 37731 | erased invalidated context checkpoint (pos_min = 3161, pos_max = 3930, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 37731 | n_tokens = 623, memory_seq_rm [623, end)
slot update_slots: id  0 | task 37731 | prompt processing progress, n_tokens = 684, batch.n_tokens = 61, progress = 0.914438
slot update_slots: id  0 | task 37731 | n_tokens = 684, memory_seq_rm [684, end)
slot update_slots: id  0 | task 37731 | prompt processing progress, n_tokens = 748, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37731 | prompt done, n_tokens = 748, batch.n_tokens = 64
slot init_sampler: id  0 | task 37731 | init sampler, took 0.14 ms, tokens: text = 748, total = 748
slot print_timing: id  0 | task 37731 | 
prompt eval time =     727.52 ms /   125 tokens (    5.82 ms per token,   171.82 tokens per second)
       eval time =   35635.31 ms /  1210 tokens (   29.45 ms per token,    33.96 tokens per second)
      total time =   36362.83 ms /  1335 tokens
slot      release: id  0 | task 37731 | stop processing: n_tokens = 1957, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.246 (> 0.100 thold), f_keep = 0.081
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7634, total state size = 181.988 MiB
srv          load:  - looking for better prompt, base f_keep = 0.081, sim = 0.246
srv        update:  - cache state: 12 prompts, 5871.176 MiB (limits: 8192.000 MiB, 64000 tokens, 255501 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv        update:    - prompt 0x573890f29600:    7634 tokens, checkpoints:  8,   302.143 MiB
srv  get_availabl: prompt cache update took 465.95 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 38943 | processing task, is_child = 0
slot update_slots: id  3 | task 38943 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2524
slot update_slots: id  3 | task 38943 | n_past = 622, slot.prompt.tokens.size() = 7634, seq_id = 3, pos_min = 7507, n_swa = 128
slot update_slots: id  3 | task 38943 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 38943 | erased invalidated context checkpoint (pos_min = 2906, pos_max = 3599, n_swa = 128, size = 16.274 MiB)
slot update_slots: id  3 | task 38943 | erased invalidated context checkpoint (pos_min = 3608, pos_max = 4377, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 38943 | erased invalidated context checkpoint (pos_min = 4036, pos_max = 4561, n_swa = 128, size = 12.334 MiB)
slot update_slots: id  3 | task 38943 | erased invalidated context checkpoint (pos_min = 4561, pos_max = 4993, n_swa = 128, size = 10.154 MiB)
slot update_slots: id  3 | task 38943 | erased invalidated context checkpoint (pos_min = 4561, pos_max = 5060, n_swa = 128, size = 11.725 MiB)
slot update_slots: id  3 | task 38943 | erased invalidated context checkpoint (pos_min = 6008, pos_max = 6777, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 38943 | erased invalidated context checkpoint (pos_min = 6588, pos_max = 7357, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 38943 | erased invalidated context checkpoint (pos_min = 6828, pos_max = 7488, n_swa = 128, size = 15.500 MiB)
slot update_slots: id  3 | task 38943 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 38943 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.811410
slot update_slots: id  3 | task 38943 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 38943 | prompt processing progress, n_tokens = 2460, batch.n_tokens = 412, progress = 0.974643
slot update_slots: id  3 | task 38943 | n_tokens = 2460, memory_seq_rm [2460, end)
slot update_slots: id  3 | task 38943 | prompt processing progress, n_tokens = 2524, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 38943 | prompt done, n_tokens = 2524, batch.n_tokens = 64
slot init_sampler: id  3 | task 38943 | init sampler, took 0.61 ms, tokens: text = 2524, total = 2524
slot update_slots: id  3 | task 38943 | created context checkpoint 1 of 8 (pos_min = 1690, pos_max = 2459, size = 18.056 MiB)
slot print_timing: id  3 | task 38943 | 
prompt eval time =    2583.36 ms /  2524 tokens (    1.02 ms per token,   977.02 tokens per second)
       eval time =   18942.36 ms /   782 tokens (   24.22 ms per token,    41.28 tokens per second)
      total time =   21525.72 ms /  3306 tokens
slot      release: id  3 | task 38943 | stop processing: n_tokens = 3305, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.402 (> 0.100 thold), f_keep = 0.469
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3305, total state size = 95.555 MiB
srv          load:  - looking for better prompt, base f_keep = 0.469, sim = 0.402
srv        update:  - cache state: 13 prompts, 5984.787 MiB (limits: 8192.000 MiB, 64000 tokens, 255175 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv        update:    - prompt 0x573890f29600:    7634 tokens, checkpoints:  8,   302.143 MiB
srv        update:    - prompt 0x5738800dfa90:    3305 tokens, checkpoints:  1,   113.611 MiB
srv  get_availabl: prompt cache update took 93.14 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 39728 | processing task, is_child = 0
slot update_slots: id  3 | task 39728 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3854
slot update_slots: id  3 | task 39728 | n_past = 1549, slot.prompt.tokens.size() = 3305, seq_id = 3, pos_min = 2535, n_swa = 128
slot update_slots: id  3 | task 39728 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 39728 | erased invalidated context checkpoint (pos_min = 1690, pos_max = 2459, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 39728 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 39728 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.531396
slot update_slots: id  3 | task 39728 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 39728 | prompt processing progress, n_tokens = 3790, batch.n_tokens = 1742, progress = 0.983394
slot update_slots: id  3 | task 39728 | n_tokens = 3790, memory_seq_rm [3790, end)
slot update_slots: id  3 | task 39728 | prompt processing progress, n_tokens = 3854, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 39728 | prompt done, n_tokens = 3854, batch.n_tokens = 64
slot init_sampler: id  3 | task 39728 | init sampler, took 0.73 ms, tokens: text = 3854, total = 3854
slot update_slots: id  3 | task 39728 | created context checkpoint 1 of 8 (pos_min = 3020, pos_max = 3789, size = 18.056 MiB)
slot print_timing: id  3 | task 39728 | 
prompt eval time =    4025.96 ms /  3854 tokens (    1.04 ms per token,   957.29 tokens per second)
       eval time =    3996.55 ms /   165 tokens (   24.22 ms per token,    41.29 tokens per second)
      total time =    8022.51 ms /  4019 tokens
slot      release: id  3 | task 39728 | stop processing: n_tokens = 4018, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.831 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 39896 | processing task, is_child = 0
slot update_slots: id  3 | task 39896 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4637
slot update_slots: id  3 | task 39896 | n_tokens = 3854, memory_seq_rm [3854, end)
slot update_slots: id  3 | task 39896 | prompt processing progress, n_tokens = 4573, batch.n_tokens = 719, progress = 0.986198
slot update_slots: id  3 | task 39896 | n_tokens = 4573, memory_seq_rm [4573, end)
slot update_slots: id  3 | task 39896 | prompt processing progress, n_tokens = 4637, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 39896 | prompt done, n_tokens = 4637, batch.n_tokens = 64
slot init_sampler: id  3 | task 39896 | init sampler, took 0.69 ms, tokens: text = 4637, total = 4637
slot update_slots: id  3 | task 39896 | created context checkpoint 2 of 8 (pos_min = 3803, pos_max = 4572, size = 18.056 MiB)
slot print_timing: id  3 | task 39896 | 
prompt eval time =     999.98 ms /   783 tokens (    1.28 ms per token,   783.01 tokens per second)
       eval time =    2052.34 ms /    84 tokens (   24.43 ms per token,    40.93 tokens per second)
      total time =    3052.32 ms /   867 tokens
slot      release: id  3 | task 39896 | stop processing: n_tokens = 4720, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 39982 | processing task, is_child = 0
slot update_slots: id  3 | task 39982 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4704
slot update_slots: id  3 | task 39982 | n_tokens = 4637, memory_seq_rm [4637, end)
slot update_slots: id  3 | task 39982 | prompt processing progress, n_tokens = 4640, batch.n_tokens = 3, progress = 0.986395
slot update_slots: id  3 | task 39982 | n_tokens = 4640, memory_seq_rm [4640, end)
slot update_slots: id  3 | task 39982 | prompt processing progress, n_tokens = 4704, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 39982 | prompt done, n_tokens = 4704, batch.n_tokens = 64
slot init_sampler: id  3 | task 39982 | init sampler, took 0.86 ms, tokens: text = 4704, total = 4704
slot update_slots: id  3 | task 39982 | created context checkpoint 3 of 8 (pos_min = 3950, pos_max = 4639, size = 16.180 MiB)
slot print_timing: id  3 | task 39982 | 
prompt eval time =     211.90 ms /    67 tokens (    3.16 ms per token,   316.19 tokens per second)
       eval time =    5655.99 ms /   227 tokens (   24.92 ms per token,    40.13 tokens per second)
      total time =    5867.89 ms /   294 tokens
slot      release: id  3 | task 39982 | stop processing: n_tokens = 4930, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40211 | processing task, is_child = 0
slot update_slots: id  3 | task 40211 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4770
slot update_slots: id  3 | task 40211 | n_tokens = 4704, memory_seq_rm [4704, end)
slot update_slots: id  3 | task 40211 | prompt processing progress, n_tokens = 4706, batch.n_tokens = 2, progress = 0.986583
slot update_slots: id  3 | task 40211 | n_tokens = 4706, memory_seq_rm [4706, end)
slot update_slots: id  3 | task 40211 | prompt processing progress, n_tokens = 4770, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 40211 | prompt done, n_tokens = 4770, batch.n_tokens = 64
slot init_sampler: id  3 | task 40211 | init sampler, took 0.89 ms, tokens: text = 4770, total = 4770
slot update_slots: id  3 | task 40211 | created context checkpoint 4 of 8 (pos_min = 4160, pos_max = 4705, size = 12.803 MiB)
slot print_timing: id  3 | task 40211 | 
prompt eval time =     217.39 ms /    66 tokens (    3.29 ms per token,   303.61 tokens per second)
       eval time =    1284.62 ms /    50 tokens (   25.69 ms per token,    38.92 tokens per second)
      total time =    1502.00 ms /   116 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 40211 | stop processing: n_tokens = 4819, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.802 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40263 | processing task, is_child = 0
slot update_slots: id  3 | task 40263 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5950
slot update_slots: id  3 | task 40263 | n_tokens = 4770, memory_seq_rm [4770, end)
slot update_slots: id  3 | task 40263 | prompt processing progress, n_tokens = 5886, batch.n_tokens = 1116, progress = 0.989244
slot update_slots: id  3 | task 40263 | n_tokens = 5886, memory_seq_rm [5886, end)
slot update_slots: id  3 | task 40263 | prompt processing progress, n_tokens = 5950, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 40263 | prompt done, n_tokens = 5950, batch.n_tokens = 64
slot init_sampler: id  3 | task 40263 | init sampler, took 0.89 ms, tokens: text = 5950, total = 5950
slot update_slots: id  3 | task 40263 | created context checkpoint 5 of 8 (pos_min = 5116, pos_max = 5885, size = 18.056 MiB)
slot print_timing: id  3 | task 40263 | 
prompt eval time =    1471.37 ms /  1180 tokens (    1.25 ms per token,   801.97 tokens per second)
       eval time =    5537.76 ms /   223 tokens (   24.83 ms per token,    40.27 tokens per second)
      total time =    7009.13 ms /  1403 tokens
slot      release: id  3 | task 40263 | stop processing: n_tokens = 6172, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40488 | processing task, is_child = 0
slot update_slots: id  3 | task 40488 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6286
slot update_slots: id  3 | task 40488 | n_tokens = 5950, memory_seq_rm [5950, end)
slot update_slots: id  3 | task 40488 | prompt processing progress, n_tokens = 6222, batch.n_tokens = 272, progress = 0.989819
slot update_slots: id  3 | task 40488 | n_tokens = 6222, memory_seq_rm [6222, end)
slot update_slots: id  3 | task 40488 | prompt processing progress, n_tokens = 6286, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 40488 | prompt done, n_tokens = 6286, batch.n_tokens = 64
slot init_sampler: id  3 | task 40488 | init sampler, took 0.94 ms, tokens: text = 6286, total = 6286
slot update_slots: id  3 | task 40488 | created context checkpoint 6 of 8 (pos_min = 5662, pos_max = 6221, size = 13.132 MiB)
slot print_timing: id  3 | task 40488 | 
prompt eval time =     531.57 ms /   336 tokens (    1.58 ms per token,   632.10 tokens per second)
       eval time =    3156.81 ms /   127 tokens (   24.86 ms per token,    40.23 tokens per second)
      total time =    3688.38 ms /   463 tokens
slot      release: id  3 | task 40488 | stop processing: n_tokens = 6412, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.906 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40617 | processing task, is_child = 0
slot update_slots: id  3 | task 40617 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6936
slot update_slots: id  3 | task 40617 | n_tokens = 6286, memory_seq_rm [6286, end)
slot update_slots: id  3 | task 40617 | prompt processing progress, n_tokens = 6872, batch.n_tokens = 586, progress = 0.990773
slot update_slots: id  3 | task 40617 | n_tokens = 6872, memory_seq_rm [6872, end)
slot update_slots: id  3 | task 40617 | prompt processing progress, n_tokens = 6936, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 40617 | prompt done, n_tokens = 6936, batch.n_tokens = 64
slot init_sampler: id  3 | task 40617 | init sampler, took 1.30 ms, tokens: text = 6936, total = 6936
slot update_slots: id  3 | task 40617 | created context checkpoint 7 of 8 (pos_min = 6102, pos_max = 6871, size = 18.056 MiB)
slot print_timing: id  3 | task 40617 | 
prompt eval time =     943.20 ms /   650 tokens (    1.45 ms per token,   689.14 tokens per second)
       eval time =    6242.79 ms /   250 tokens (   24.97 ms per token,    40.05 tokens per second)
      total time =    7185.99 ms /   900 tokens
slot      release: id  3 | task 40617 | stop processing: n_tokens = 7185, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40869 | processing task, is_child = 0
slot update_slots: id  3 | task 40869 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7003
slot update_slots: id  3 | task 40869 | n_tokens = 6936, memory_seq_rm [6936, end)
slot update_slots: id  3 | task 40869 | prompt processing progress, n_tokens = 6939, batch.n_tokens = 3, progress = 0.990861
slot update_slots: id  3 | task 40869 | n_tokens = 6939, memory_seq_rm [6939, end)
slot update_slots: id  3 | task 40869 | prompt processing progress, n_tokens = 7003, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 40869 | prompt done, n_tokens = 7003, batch.n_tokens = 64
slot init_sampler: id  3 | task 40869 | init sampler, took 1.02 ms, tokens: text = 7003, total = 7003
slot update_slots: id  3 | task 40869 | created context checkpoint 8 of 8 (pos_min = 6415, pos_max = 6938, size = 12.288 MiB)
slot print_timing: id  3 | task 40869 | 
prompt eval time =     224.80 ms /    67 tokens (    3.36 ms per token,   298.04 tokens per second)
       eval time =    3327.43 ms /   136 tokens (   24.47 ms per token,    40.87 tokens per second)
      total time =    3552.23 ms /   203 tokens
slot      release: id  3 | task 40869 | stop processing: n_tokens = 7138, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.878 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41007 | processing task, is_child = 0
slot update_slots: id  3 | task 41007 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7978
slot update_slots: id  3 | task 41007 | n_tokens = 7003, memory_seq_rm [7003, end)
slot update_slots: id  3 | task 41007 | prompt processing progress, n_tokens = 7914, batch.n_tokens = 911, progress = 0.991978
slot update_slots: id  3 | task 41007 | n_tokens = 7914, memory_seq_rm [7914, end)
slot update_slots: id  3 | task 41007 | prompt processing progress, n_tokens = 7978, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 41007 | prompt done, n_tokens = 7978, batch.n_tokens = 64
slot init_sampler: id  3 | task 41007 | init sampler, took 1.49 ms, tokens: text = 7978, total = 7978
slot update_slots: id  3 | task 41007 | erasing old context checkpoint (pos_min = 3020, pos_max = 3789, size = 18.056 MiB)
slot update_slots: id  3 | task 41007 | created context checkpoint 8 of 8 (pos_min = 7144, pos_max = 7913, size = 18.056 MiB)
slot print_timing: id  3 | task 41007 | 
prompt eval time =    1176.48 ms /   975 tokens (    1.21 ms per token,   828.75 tokens per second)
       eval time =    3024.33 ms /   123 tokens (   24.59 ms per token,    40.67 tokens per second)
      total time =    4200.81 ms /  1098 tokens
slot      release: id  3 | task 41007 | stop processing: n_tokens = 8100, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.822 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41132 | processing task, is_child = 0
slot update_slots: id  3 | task 41132 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9705
slot update_slots: id  3 | task 41132 | n_tokens = 7978, memory_seq_rm [7978, end)
slot update_slots: id  3 | task 41132 | prompt processing progress, n_tokens = 9641, batch.n_tokens = 1663, progress = 0.993405
slot update_slots: id  3 | task 41132 | n_tokens = 9641, memory_seq_rm [9641, end)
slot update_slots: id  3 | task 41132 | prompt processing progress, n_tokens = 9705, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 41132 | prompt done, n_tokens = 9705, batch.n_tokens = 64
slot init_sampler: id  3 | task 41132 | init sampler, took 1.82 ms, tokens: text = 9705, total = 9705
slot update_slots: id  3 | task 41132 | erasing old context checkpoint (pos_min = 3803, pos_max = 4572, size = 18.056 MiB)
slot update_slots: id  3 | task 41132 | created context checkpoint 8 of 8 (pos_min = 8871, pos_max = 9640, size = 18.056 MiB)
slot print_timing: id  3 | task 41132 | 
prompt eval time =    2266.48 ms /  1727 tokens (    1.31 ms per token,   761.97 tokens per second)
       eval time =    3740.87 ms /   150 tokens (   24.94 ms per token,    40.10 tokens per second)
      total time =    6007.35 ms /  1877 tokens
slot      release: id  3 | task 41132 | stop processing: n_tokens = 9854, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41284 | processing task, is_child = 0
slot update_slots: id  3 | task 41284 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9761
slot update_slots: id  3 | task 41284 | n_tokens = 9705, memory_seq_rm [9705, end)
slot update_slots: id  3 | task 41284 | prompt processing progress, n_tokens = 9761, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  3 | task 41284 | prompt done, n_tokens = 9761, batch.n_tokens = 56
slot init_sampler: id  3 | task 41284 | init sampler, took 1.40 ms, tokens: text = 9761, total = 9761
slot print_timing: id  3 | task 41284 | 
prompt eval time =     159.17 ms /    56 tokens (    2.84 ms per token,   351.83 tokens per second)
       eval time =   37271.76 ms /  1499 tokens (   24.86 ms per token,    40.22 tokens per second)
      total time =   37430.93 ms /  1555 tokens
slot      release: id  3 | task 41284 | stop processing: n_tokens = 11259, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.900 (> 0.100 thold), f_keep = 0.867
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 42784 | processing task, is_child = 0
slot update_slots: id  3 | task 42784 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10844
slot update_slots: id  3 | task 42784 | n_past = 9761, slot.prompt.tokens.size() = 11259, seq_id = 3, pos_min = 10489, n_swa = 128
slot update_slots: id  3 | task 42784 | restored context checkpoint (pos_min = 8871, pos_max = 9640, size = 18.056 MiB)
slot update_slots: id  3 | task 42784 | n_tokens = 9640, memory_seq_rm [9640, end)
slot update_slots: id  3 | task 42784 | prompt processing progress, n_tokens = 10780, batch.n_tokens = 1140, progress = 0.994098
slot update_slots: id  3 | task 42784 | n_tokens = 10780, memory_seq_rm [10780, end)
slot update_slots: id  3 | task 42784 | prompt processing progress, n_tokens = 10844, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 42784 | prompt done, n_tokens = 10844, batch.n_tokens = 64
slot init_sampler: id  3 | task 42784 | init sampler, took 2.29 ms, tokens: text = 10844, total = 10844
slot update_slots: id  3 | task 42784 | erasing old context checkpoint (pos_min = 3950, pos_max = 4639, size = 16.180 MiB)
slot update_slots: id  3 | task 42784 | created context checkpoint 8 of 8 (pos_min = 10010, pos_max = 10779, size = 18.056 MiB)
slot print_timing: id  3 | task 42784 | 
prompt eval time =    1810.75 ms /  1204 tokens (    1.50 ms per token,   664.92 tokens per second)
       eval time =    2008.20 ms /    79 tokens (   25.42 ms per token,    39.34 tokens per second)
      total time =    3818.95 ms /  1283 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 42784 | stop processing: n_tokens = 10922, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 42865 | processing task, is_child = 0
slot update_slots: id  3 | task 42865 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10921
slot update_slots: id  3 | task 42865 | n_tokens = 10844, memory_seq_rm [10844, end)
slot update_slots: id  3 | task 42865 | prompt processing progress, n_tokens = 10857, batch.n_tokens = 13, progress = 0.994140
slot update_slots: id  3 | task 42865 | n_tokens = 10857, memory_seq_rm [10857, end)
slot update_slots: id  3 | task 42865 | prompt processing progress, n_tokens = 10921, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 42865 | prompt done, n_tokens = 10921, batch.n_tokens = 64
slot init_sampler: id  3 | task 42865 | init sampler, took 2.15 ms, tokens: text = 10921, total = 10921
slot update_slots: id  3 | task 42865 | erasing old context checkpoint (pos_min = 4160, pos_max = 4705, size = 12.803 MiB)
slot update_slots: id  3 | task 42865 | created context checkpoint 8 of 8 (pos_min = 10152, pos_max = 10856, size = 16.532 MiB)
slot print_timing: id  3 | task 42865 | 
prompt eval time =     286.37 ms /    77 tokens (    3.72 ms per token,   268.88 tokens per second)
       eval time =   27965.44 ms /  1111 tokens (   25.17 ms per token,    39.73 tokens per second)
      total time =   28251.81 ms /  1188 tokens
slot      release: id  3 | task 42865 | stop processing: n_tokens = 12031, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.907 (> 0.100 thold), f_keep = 0.908
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 43978 | processing task, is_child = 0
slot update_slots: id  3 | task 43978 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12039
slot update_slots: id  3 | task 43978 | n_past = 10921, slot.prompt.tokens.size() = 12031, seq_id = 3, pos_min = 11261, n_swa = 128
slot update_slots: id  3 | task 43978 | restored context checkpoint (pos_min = 10152, pos_max = 10856, size = 16.532 MiB)
slot update_slots: id  3 | task 43978 | n_tokens = 10856, memory_seq_rm [10856, end)
slot update_slots: id  3 | task 43978 | prompt processing progress, n_tokens = 11975, batch.n_tokens = 1119, progress = 0.994684
slot update_slots: id  3 | task 43978 | n_tokens = 11975, memory_seq_rm [11975, end)
slot update_slots: id  3 | task 43978 | prompt processing progress, n_tokens = 12039, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 43978 | prompt done, n_tokens = 12039, batch.n_tokens = 64
slot init_sampler: id  3 | task 43978 | init sampler, took 2.27 ms, tokens: text = 12039, total = 12039
slot update_slots: id  3 | task 43978 | erasing old context checkpoint (pos_min = 5116, pos_max = 5885, size = 18.056 MiB)
slot update_slots: id  3 | task 43978 | created context checkpoint 8 of 8 (pos_min = 11205, pos_max = 11974, size = 18.056 MiB)
slot print_timing: id  3 | task 43978 | 
prompt eval time =    1738.44 ms /  1183 tokens (    1.47 ms per token,   680.50 tokens per second)
       eval time =    1275.05 ms /    50 tokens (   25.50 ms per token,    39.21 tokens per second)
      total time =    3013.49 ms /  1233 tokens
slot      release: id  3 | task 43978 | stop processing: n_tokens = 12088, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 44030 | processing task, is_child = 0
slot update_slots: id  3 | task 44030 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13024
slot update_slots: id  3 | task 44030 | n_tokens = 12039, memory_seq_rm [12039, end)
slot update_slots: id  3 | task 44030 | prompt processing progress, n_tokens = 12960, batch.n_tokens = 921, progress = 0.995086
slot update_slots: id  3 | task 44030 | n_tokens = 12960, memory_seq_rm [12960, end)
slot update_slots: id  3 | task 44030 | prompt processing progress, n_tokens = 13024, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 44030 | prompt done, n_tokens = 13024, batch.n_tokens = 64
slot init_sampler: id  3 | task 44030 | init sampler, took 1.86 ms, tokens: text = 13024, total = 13024
slot update_slots: id  3 | task 44030 | erasing old context checkpoint (pos_min = 5662, pos_max = 6221, size = 13.132 MiB)
slot update_slots: id  3 | task 44030 | created context checkpoint 8 of 8 (pos_min = 12190, pos_max = 12959, size = 18.056 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 44030
slot      release: id  3 | task 44030 | stop processing: n_tokens = 13068, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.858 (> 0.100 thold), f_keep = 0.318
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1957, total state size = 48.868 MiB
srv          load:  - looking for better prompt, base f_keep = 0.318, sim = 0.858
srv        update:  - cache state: 14 prompts, 6049.178 MiB (limits: 8192.000 MiB, 64000 tokens, 255109 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv        update:    - prompt 0x573890f29600:    7634 tokens, checkpoints:  8,   302.143 MiB
srv        update:    - prompt 0x5738800dfa90:    3305 tokens, checkpoints:  1,   113.611 MiB
srv        update:    - prompt 0x5738c7258990:    1957 tokens, checkpoints:  1,    64.392 MiB
srv  get_availabl: prompt cache update took 73.53 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44078 | processing task, is_child = 0
slot update_slots: id  0 | task 44078 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 726
slot update_slots: id  0 | task 44078 | n_past = 623, slot.prompt.tokens.size() = 1957, seq_id = 0, pos_min = 1830, n_swa = 128
slot update_slots: id  0 | task 44078 | restored context checkpoint (pos_min = 0, pos_max = 661, size = 15.523 MiB)
slot update_slots: id  0 | task 44078 | n_tokens = 623, memory_seq_rm [623, end)
slot update_slots: id  0 | task 44078 | prompt processing progress, n_tokens = 662, batch.n_tokens = 39, progress = 0.911846
slot update_slots: id  0 | task 44078 | n_tokens = 662, memory_seq_rm [662, end)
slot update_slots: id  0 | task 44078 | prompt processing progress, n_tokens = 726, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44078 | prompt done, n_tokens = 726, batch.n_tokens = 64
slot init_sampler: id  0 | task 44078 | init sampler, took 0.15 ms, tokens: text = 726, total = 726
slot print_timing: id  0 | task 44078 | 
prompt eval time =     515.18 ms /   103 tokens (    5.00 ms per token,   199.93 tokens per second)
       eval time =    1343.64 ms /    55 tokens (   24.43 ms per token,    40.93 tokens per second)
      total time =    1858.82 ms /   158 tokens
slot      release: id  0 | task 44078 | stop processing: n_tokens = 780, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.480 (> 0.100 thold), f_keep = 0.931
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44135 | processing task, is_child = 0
slot update_slots: id  0 | task 44135 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1513
slot update_slots: id  0 | task 44135 | n_tokens = 726, memory_seq_rm [726, end)
slot update_slots: id  0 | task 44135 | prompt processing progress, n_tokens = 1449, batch.n_tokens = 723, progress = 0.957700
slot update_slots: id  0 | task 44135 | n_tokens = 1449, memory_seq_rm [1449, end)
slot update_slots: id  0 | task 44135 | prompt processing progress, n_tokens = 1513, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44135 | prompt done, n_tokens = 1513, batch.n_tokens = 64
slot init_sampler: id  0 | task 44135 | init sampler, took 0.28 ms, tokens: text = 1513, total = 1513
slot update_slots: id  0 | task 44135 | created context checkpoint 2 of 8 (pos_min = 679, pos_max = 1448, size = 18.056 MiB)
slot print_timing: id  0 | task 44135 | 
prompt eval time =    1081.99 ms /   787 tokens (    1.37 ms per token,   727.37 tokens per second)
       eval time =    1903.46 ms /    76 tokens (   25.05 ms per token,    39.93 tokens per second)
      total time =    2985.45 ms /   863 tokens
slot      release: id  0 | task 44135 | stop processing: n_tokens = 1588, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = 12051635082
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44213 | processing task, is_child = 0
slot update_slots: id  2 | task 44213 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 40473
slot update_slots: id  2 | task 44213 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.050602
slot update_slots: id  2 | task 44213 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.101203
slot update_slots: id  2 | task 44213 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.151805
slot update_slots: id  2 | task 44213 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.202407
slot update_slots: id  2 | task 44213 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.253008
slot update_slots: id  2 | task 44213 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.303610
slot update_slots: id  2 | task 44213 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.354211
slot update_slots: id  2 | task 44213 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.404813
slot update_slots: id  2 | task 44213 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.455415
slot update_slots: id  2 | task 44213 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 2048, progress = 0.506016
slot update_slots: id  2 | task 44213 | n_tokens = 20480, memory_seq_rm [20480, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 22528, batch.n_tokens = 2048, progress = 0.556618
slot update_slots: id  2 | task 44213 | n_tokens = 22528, memory_seq_rm [22528, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 24576, batch.n_tokens = 2048, progress = 0.607220
slot update_slots: id  2 | task 44213 | n_tokens = 24576, memory_seq_rm [24576, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 26624, batch.n_tokens = 2048, progress = 0.657821
slot update_slots: id  2 | task 44213 | n_tokens = 26624, memory_seq_rm [26624, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 28672, batch.n_tokens = 2048, progress = 0.708423
slot update_slots: id  2 | task 44213 | n_tokens = 28672, memory_seq_rm [28672, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 30720, batch.n_tokens = 2048, progress = 0.759025
slot update_slots: id  2 | task 44213 | n_tokens = 30720, memory_seq_rm [30720, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 32768, batch.n_tokens = 2048, progress = 0.809626
slot update_slots: id  2 | task 44213 | n_tokens = 32768, memory_seq_rm [32768, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 34816, batch.n_tokens = 2048, progress = 0.860228
slot update_slots: id  2 | task 44213 | n_tokens = 34816, memory_seq_rm [34816, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 36864, batch.n_tokens = 2048, progress = 0.910829
slot update_slots: id  2 | task 44213 | n_tokens = 36864, memory_seq_rm [36864, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 38912, batch.n_tokens = 2048, progress = 0.961431
slot update_slots: id  2 | task 44213 | n_tokens = 38912, memory_seq_rm [38912, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 40409, batch.n_tokens = 1497, progress = 0.998419
slot update_slots: id  2 | task 44213 | n_tokens = 40409, memory_seq_rm [40409, end)
slot update_slots: id  2 | task 44213 | prompt processing progress, n_tokens = 40473, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44213 | prompt done, n_tokens = 40473, batch.n_tokens = 64
slot init_sampler: id  2 | task 44213 | init sampler, took 5.89 ms, tokens: text = 40473, total = 40473
slot update_slots: id  2 | task 44213 | created context checkpoint 1 of 8 (pos_min = 39766, pos_max = 40408, size = 15.078 MiB)
slot print_timing: id  2 | task 44213 | 
prompt eval time =   58561.69 ms / 40473 tokens (    1.45 ms per token,   691.12 tokens per second)
       eval time =    3733.80 ms /   126 tokens (   29.63 ms per token,    33.75 tokens per second)
      total time =   62295.50 ms / 40599 tokens
slot      release: id  2 | task 44213 | stop processing: n_tokens = 40598, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.537 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44360 | processing task, is_child = 0
slot update_slots: id  2 | task 44360 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 75405
srv    send_error: task id = 44360, error: request (75405 tokens) exceeds the available context size (64000 tokens), try increasing it
slot      release: id  2 | task 44360 | stop processing: n_tokens = 40598, truncated = 0
srv  update_slots: no tokens to decode
srv  update_slots: all slots are idle
srv          stop: cancel task, id_task = 44360
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 400
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.891 (> 0.100 thold), f_keep = 0.405
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1588, total state size = 40.216 MiB
srv          load:  - looking for better prompt, base f_keep = 0.405, sim = 0.891
srv        update:  - cache state: 15 prompts, 6122.973 MiB (limits: 8192.000 MiB, 64000 tokens, 254159 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv        update:    - prompt 0x573890f29600:    7634 tokens, checkpoints:  8,   302.143 MiB
srv        update:    - prompt 0x5738800dfa90:    3305 tokens, checkpoints:  1,   113.611 MiB
srv        update:    - prompt 0x5738c7258990:    1957 tokens, checkpoints:  1,    64.392 MiB
srv        update:    - prompt 0x57389a068030:    1588 tokens, checkpoints:  2,    73.795 MiB
srv  get_availabl: prompt cache update took 53.76 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44363 | processing task, is_child = 0
slot update_slots: id  0 | task 44363 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 722
slot update_slots: id  0 | task 44363 | n_past = 643, slot.prompt.tokens.size() = 1588, seq_id = 0, pos_min = 1461, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  0 | task 44363 | failed to restore context checkpoint (pos_min = 0, pos_max = 661, size = 15.523 MiB)
slot update_slots: id  0 | task 44363 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 44363 | erased invalidated context checkpoint (pos_min = 679, pos_max = 1448, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 44363 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 44363 | prompt processing progress, n_tokens = 658, batch.n_tokens = 658, progress = 0.911357
slot update_slots: id  0 | task 44363 | n_tokens = 658, memory_seq_rm [658, end)
slot update_slots: id  0 | task 44363 | prompt processing progress, n_tokens = 722, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44363 | prompt done, n_tokens = 722, batch.n_tokens = 64
slot init_sampler: id  0 | task 44363 | init sampler, took 0.17 ms, tokens: text = 722, total = 722
slot print_timing: id  0 | task 44363 | 
prompt eval time =    1614.17 ms /   722 tokens (    2.24 ms per token,   447.29 tokens per second)
       eval time =    1296.96 ms /    45 tokens (   28.82 ms per token,    34.70 tokens per second)
      total time =    2911.13 ms /   767 tokens
slot      release: id  0 | task 44363 | stop processing: n_tokens = 766, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.488 (> 0.100 thold), f_keep = 0.943
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44410 | processing task, is_child = 0
slot update_slots: id  0 | task 44410 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1479
slot update_slots: id  0 | task 44410 | n_tokens = 722, memory_seq_rm [722, end)
slot update_slots: id  0 | task 44410 | prompt processing progress, n_tokens = 1415, batch.n_tokens = 693, progress = 0.956728
slot update_slots: id  0 | task 44410 | n_tokens = 1415, memory_seq_rm [1415, end)
slot update_slots: id  0 | task 44410 | prompt processing progress, n_tokens = 1479, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44410 | prompt done, n_tokens = 1479, batch.n_tokens = 64
slot init_sampler: id  0 | task 44410 | init sampler, took 0.26 ms, tokens: text = 1479, total = 1479
slot update_slots: id  0 | task 44410 | created context checkpoint 2 of 8 (pos_min = 772, pos_max = 1414, size = 15.078 MiB)
slot print_timing: id  0 | task 44410 | 
prompt eval time =    1588.46 ms /   757 tokens (    2.10 ms per token,   476.56 tokens per second)
       eval time =    1425.60 ms /    49 tokens (   29.09 ms per token,    34.37 tokens per second)
      total time =    3014.06 ms /   806 tokens
slot      release: id  0 | task 44410 | stop processing: n_tokens = 1527, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.140 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44461 | processing task, is_child = 0
slot update_slots: id  0 | task 44461 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10576
slot update_slots: id  0 | task 44461 | n_tokens = 1479, memory_seq_rm [1479, end)
slot update_slots: id  0 | task 44461 | prompt processing progress, n_tokens = 3527, batch.n_tokens = 2048, progress = 0.333491
slot update_slots: id  0 | task 44461 | n_tokens = 3527, memory_seq_rm [3527, end)
slot update_slots: id  0 | task 44461 | prompt processing progress, n_tokens = 5575, batch.n_tokens = 2048, progress = 0.527137
slot update_slots: id  0 | task 44461 | n_tokens = 5575, memory_seq_rm [5575, end)
slot update_slots: id  0 | task 44461 | prompt processing progress, n_tokens = 7623, batch.n_tokens = 2048, progress = 0.720783
slot update_slots: id  0 | task 44461 | n_tokens = 7623, memory_seq_rm [7623, end)
slot update_slots: id  0 | task 44461 | prompt processing progress, n_tokens = 9671, batch.n_tokens = 2048, progress = 0.914429
decode: failed to find a memory slot for batch of size 2048
srv  try_clear_id: purging slot 1 with 1489 tokens
slot prompt_clear: id  1 | task -1 | clearing prompt with 1489 tokens
srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 2048, ret = 1
slot update_slots: id  0 | task 44461 | n_tokens = 9671, memory_seq_rm [9671, end)
slot update_slots: id  0 | task 44461 | prompt processing progress, n_tokens = 10512, batch.n_tokens = 841, progress = 0.993949
decode: failed to find a memory slot for batch of size 841
srv  try_clear_id: purging slot 2 with 40598 tokens
slot prompt_clear: id  2 | task -1 | clearing prompt with 40598 tokens
srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 2048, ret = 1
slot update_slots: id  0 | task 44461 | n_tokens = 10512, memory_seq_rm [10512, end)
slot update_slots: id  0 | task 44461 | prompt processing progress, n_tokens = 10576, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44461 | prompt done, n_tokens = 10576, batch.n_tokens = 64
slot init_sampler: id  0 | task 44461 | init sampler, took 1.59 ms, tokens: text = 10576, total = 10576
slot update_slots: id  0 | task 44461 | created context checkpoint 3 of 8 (pos_min = 9742, pos_max = 10511, size = 18.056 MiB)
slot print_timing: id  0 | task 44461 | 
prompt eval time =   16890.65 ms /  9097 tokens (    1.86 ms per token,   538.58 tokens per second)
       eval time =    7466.48 ms /   238 tokens (   31.37 ms per token,    31.88 tokens per second)
      total time =   24357.12 ms /  9335 tokens
slot      release: id  0 | task 44461 | stop processing: n_tokens = 10813, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44705 | processing task, is_child = 0
slot update_slots: id  0 | task 44705 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10834
slot update_slots: id  0 | task 44705 | n_tokens = 10576, memory_seq_rm [10576, end)
slot update_slots: id  0 | task 44705 | prompt processing progress, n_tokens = 10770, batch.n_tokens = 194, progress = 0.994093
slot update_slots: id  0 | task 44705 | n_tokens = 10770, memory_seq_rm [10770, end)
slot update_slots: id  0 | task 44705 | prompt processing progress, n_tokens = 10834, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44705 | prompt done, n_tokens = 10834, batch.n_tokens = 64
slot init_sampler: id  0 | task 44705 | init sampler, took 1.82 ms, tokens: text = 10834, total = 10834
slot update_slots: id  0 | task 44705 | created context checkpoint 4 of 8 (pos_min = 9916, pos_max = 10769, size = 20.026 MiB)
slot print_timing: id  0 | task 44705 | 
prompt eval time =     792.57 ms /   258 tokens (    3.07 ms per token,   325.52 tokens per second)
       eval time =    1639.71 ms /    53 tokens (   30.94 ms per token,    32.32 tokens per second)
      total time =    2432.28 ms /   311 tokens
slot      release: id  0 | task 44705 | stop processing: n_tokens = 10886, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.218 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44760 | processing task, is_child = 0
slot update_slots: id  0 | task 44760 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 49794
slot update_slots: id  0 | task 44760 | n_tokens = 10834, memory_seq_rm [10834, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 12882, batch.n_tokens = 2048, progress = 0.258706
slot update_slots: id  0 | task 44760 | n_tokens = 12882, memory_seq_rm [12882, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 14930, batch.n_tokens = 2048, progress = 0.299835
slot update_slots: id  0 | task 44760 | n_tokens = 14930, memory_seq_rm [14930, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 16978, batch.n_tokens = 2048, progress = 0.340965
slot update_slots: id  0 | task 44760 | n_tokens = 16978, memory_seq_rm [16978, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 19026, batch.n_tokens = 2048, progress = 0.382094
slot update_slots: id  0 | task 44760 | n_tokens = 19026, memory_seq_rm [19026, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 21074, batch.n_tokens = 2048, progress = 0.423224
slot update_slots: id  0 | task 44760 | n_tokens = 21074, memory_seq_rm [21074, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 23122, batch.n_tokens = 2048, progress = 0.464353
slot update_slots: id  0 | task 44760 | n_tokens = 23122, memory_seq_rm [23122, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 25170, batch.n_tokens = 2048, progress = 0.505483
slot update_slots: id  0 | task 44760 | n_tokens = 25170, memory_seq_rm [25170, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 27218, batch.n_tokens = 2048, progress = 0.546612
slot update_slots: id  0 | task 44760 | n_tokens = 27218, memory_seq_rm [27218, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 29266, batch.n_tokens = 2048, progress = 0.587741
slot update_slots: id  0 | task 44760 | n_tokens = 29266, memory_seq_rm [29266, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 31314, batch.n_tokens = 2048, progress = 0.628871
slot update_slots: id  0 | task 44760 | n_tokens = 31314, memory_seq_rm [31314, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 33362, batch.n_tokens = 2048, progress = 0.670000
slot update_slots: id  0 | task 44760 | n_tokens = 33362, memory_seq_rm [33362, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 35410, batch.n_tokens = 2048, progress = 0.711130
slot update_slots: id  0 | task 44760 | n_tokens = 35410, memory_seq_rm [35410, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 37458, batch.n_tokens = 2048, progress = 0.752259
slot update_slots: id  0 | task 44760 | n_tokens = 37458, memory_seq_rm [37458, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 39506, batch.n_tokens = 2048, progress = 0.793389
slot update_slots: id  0 | task 44760 | n_tokens = 39506, memory_seq_rm [39506, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 41554, batch.n_tokens = 2048, progress = 0.834518
slot update_slots: id  0 | task 44760 | n_tokens = 41554, memory_seq_rm [41554, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 43602, batch.n_tokens = 2048, progress = 0.875648
slot update_slots: id  0 | task 44760 | n_tokens = 43602, memory_seq_rm [43602, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 45650, batch.n_tokens = 2048, progress = 0.916777
slot update_slots: id  0 | task 44760 | n_tokens = 45650, memory_seq_rm [45650, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 47698, batch.n_tokens = 2048, progress = 0.957907
slot update_slots: id  0 | task 44760 | n_tokens = 47698, memory_seq_rm [47698, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 49730, batch.n_tokens = 2032, progress = 0.998715
slot update_slots: id  0 | task 44760 | n_tokens = 49730, memory_seq_rm [49730, end)
slot update_slots: id  0 | task 44760 | prompt processing progress, n_tokens = 49794, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44760 | prompt done, n_tokens = 49794, batch.n_tokens = 64
slot init_sampler: id  0 | task 44760 | init sampler, took 6.87 ms, tokens: text = 49794, total = 49794
slot update_slots: id  0 | task 44760 | created context checkpoint 5 of 8 (pos_min = 48833, pos_max = 49729, size = 21.034 MiB)
slot print_timing: id  0 | task 44760 | 
prompt eval time =   72346.35 ms / 38960 tokens (    1.86 ms per token,   538.52 tokens per second)
       eval time =    6335.23 ms /   208 tokens (   30.46 ms per token,    32.83 tokens per second)
      total time =   78681.58 ms / 39168 tokens
slot      release: id  0 | task 44760 | stop processing: n_tokens = 50001, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44988 | processing task, is_child = 0
slot update_slots: id  0 | task 44988 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 50046
slot update_slots: id  0 | task 44988 | n_tokens = 49794, memory_seq_rm [49794, end)
slot update_slots: id  0 | task 44988 | prompt processing progress, n_tokens = 49982, batch.n_tokens = 188, progress = 0.998721
slot update_slots: id  0 | task 44988 | n_tokens = 49982, memory_seq_rm [49982, end)
slot update_slots: id  0 | task 44988 | prompt processing progress, n_tokens = 50046, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44988 | prompt done, n_tokens = 50046, batch.n_tokens = 64
slot init_sampler: id  0 | task 44988 | init sampler, took 15.94 ms, tokens: text = 50046, total = 50046
slot update_slots: id  0 | task 44988 | created context checkpoint 6 of 8 (pos_min = 49104, pos_max = 49981, size = 20.588 MiB)
slot print_timing: id  0 | task 44988 | 
prompt eval time =     753.90 ms /   252 tokens (    2.99 ms per token,   334.26 tokens per second)
       eval time =    3621.83 ms /   117 tokens (   30.96 ms per token,    32.30 tokens per second)
      total time =    4375.73 ms /   369 tokens
slot      release: id  0 | task 44988 | stop processing: n_tokens = 50162, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45107 | processing task, is_child = 0
slot update_slots: id  0 | task 45107 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 50298
slot update_slots: id  0 | task 45107 | n_tokens = 50046, memory_seq_rm [50046, end)
slot update_slots: id  0 | task 45107 | prompt processing progress, n_tokens = 50234, batch.n_tokens = 188, progress = 0.998728
slot update_slots: id  0 | task 45107 | n_tokens = 50234, memory_seq_rm [50234, end)
slot update_slots: id  0 | task 45107 | prompt processing progress, n_tokens = 50298, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45107 | prompt done, n_tokens = 50298, batch.n_tokens = 64
slot init_sampler: id  0 | task 45107 | init sampler, took 7.12 ms, tokens: text = 50298, total = 50298
slot update_slots: id  0 | task 45107 | created context checkpoint 7 of 8 (pos_min = 49343, pos_max = 50233, size = 20.893 MiB)
slot print_timing: id  0 | task 45107 | 
prompt eval time =     750.04 ms /   252 tokens (    2.98 ms per token,   335.98 tokens per second)
       eval time =    4202.38 ms /   137 tokens (   30.67 ms per token,    32.60 tokens per second)
      total time =    4952.41 ms /   389 tokens
slot      release: id  0 | task 45107 | stop processing: n_tokens = 50434, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45246 | processing task, is_child = 0
slot update_slots: id  0 | task 45246 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 50354
slot update_slots: id  0 | task 45246 | n_tokens = 50298, memory_seq_rm [50298, end)
slot update_slots: id  0 | task 45246 | prompt processing progress, n_tokens = 50354, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  0 | task 45246 | prompt done, n_tokens = 50354, batch.n_tokens = 56
slot init_sampler: id  0 | task 45246 | init sampler, took 6.94 ms, tokens: text = 50354, total = 50354
slot print_timing: id  0 | task 45246 | 
prompt eval time =     224.16 ms /    56 tokens (    4.00 ms per token,   249.82 tokens per second)
       eval time =    4256.78 ms /   140 tokens (   30.41 ms per token,    32.89 tokens per second)
      total time =    4480.94 ms /   196 tokens
slot      release: id  0 | task 45246 | stop processing: n_tokens = 50493, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.871 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45387 | processing task, is_child = 0
slot update_slots: id  0 | task 45387 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 57839
slot update_slots: id  0 | task 45387 | n_tokens = 50354, memory_seq_rm [50354, end)
slot update_slots: id  0 | task 45387 | prompt processing progress, n_tokens = 52402, batch.n_tokens = 2048, progress = 0.905998
decode: failed to find a memory slot for batch of size 2048
srv  try_clear_id: purging slot 3 with 13068 tokens
slot prompt_clear: id  3 | task -1 | clearing prompt with 13068 tokens
srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 2048, ret = 1
slot update_slots: id  0 | task 45387 | n_tokens = 52402, memory_seq_rm [52402, end)
slot update_slots: id  0 | task 45387 | prompt processing progress, n_tokens = 54450, batch.n_tokens = 2048, progress = 0.941406
slot update_slots: id  0 | task 45387 | n_tokens = 54450, memory_seq_rm [54450, end)
slot update_slots: id  0 | task 45387 | prompt processing progress, n_tokens = 56498, batch.n_tokens = 2048, progress = 0.976815
slot update_slots: id  0 | task 45387 | n_tokens = 56498, memory_seq_rm [56498, end)
slot update_slots: id  0 | task 45387 | prompt processing progress, n_tokens = 57775, batch.n_tokens = 1277, progress = 0.998893
slot update_slots: id  0 | task 45387 | n_tokens = 57775, memory_seq_rm [57775, end)
slot update_slots: id  0 | task 45387 | prompt processing progress, n_tokens = 57839, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45387 | prompt done, n_tokens = 57839, batch.n_tokens = 64
slot init_sampler: id  0 | task 45387 | init sampler, took 8.04 ms, tokens: text = 57839, total = 57839
slot update_slots: id  0 | task 45387 | created context checkpoint 8 of 8 (pos_min = 56751, pos_max = 57774, size = 24.012 MiB)
slot print_timing: id  0 | task 45387 | 
prompt eval time =   14290.00 ms /  7485 tokens (    1.91 ms per token,   523.79 tokens per second)
       eval time =    3758.61 ms /   120 tokens (   31.32 ms per token,    31.93 tokens per second)
      total time =   18048.61 ms /  7605 tokens
slot      release: id  0 | task 45387 | stop processing: n_tokens = 57958, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45512 | processing task, is_child = 0
slot update_slots: id  0 | task 45512 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 58532
slot update_slots: id  0 | task 45512 | n_tokens = 57839, memory_seq_rm [57839, end)
slot update_slots: id  0 | task 45512 | prompt processing progress, n_tokens = 58468, batch.n_tokens = 629, progress = 0.998907
slot update_slots: id  0 | task 45512 | n_tokens = 58468, memory_seq_rm [58468, end)
slot update_slots: id  0 | task 45512 | prompt processing progress, n_tokens = 58532, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45512 | prompt done, n_tokens = 58532, batch.n_tokens = 64
slot init_sampler: id  0 | task 45512 | init sampler, took 8.35 ms, tokens: text = 58532, total = 58532
slot update_slots: id  0 | task 45512 | erasing old context checkpoint (pos_min = 0, pos_max = 661, size = 15.523 MiB)
slot update_slots: id  0 | task 45512 | created context checkpoint 8 of 8 (pos_min = 57444, pos_max = 58467, size = 24.012 MiB)
slot print_timing: id  0 | task 45512 | 
prompt eval time =    1672.99 ms /   693 tokens (    2.41 ms per token,   414.23 tokens per second)
       eval time =   17437.42 ms /   567 tokens (   30.75 ms per token,    32.52 tokens per second)
      total time =   19110.41 ms /  1260 tokens
slot      release: id  0 | task 45512 | stop processing: n_tokens = 59098, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 46081 | processing task, is_child = 0
slot update_slots: id  0 | task 46081 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 60652
slot update_slots: id  0 | task 46081 | n_tokens = 58532, memory_seq_rm [58532, end)
slot update_slots: id  0 | task 46081 | prompt processing progress, n_tokens = 60580, batch.n_tokens = 2048, progress = 0.998813
slot update_slots: id  0 | task 46081 | n_tokens = 60580, memory_seq_rm [60580, end)
slot update_slots: id  0 | task 46081 | prompt processing progress, n_tokens = 60588, batch.n_tokens = 8, progress = 0.998945
slot update_slots: id  0 | task 46081 | n_tokens = 60588, memory_seq_rm [60588, end)
slot update_slots: id  0 | task 46081 | prompt processing progress, n_tokens = 60652, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 46081 | prompt done, n_tokens = 60652, batch.n_tokens = 64
slot init_sampler: id  0 | task 46081 | init sampler, took 8.42 ms, tokens: text = 60652, total = 60652
slot update_slots: id  0 | task 46081 | erasing old context checkpoint (pos_min = 772, pos_max = 1414, size = 15.078 MiB)
slot update_slots: id  0 | task 46081 | created context checkpoint 8 of 8 (pos_min = 59564, pos_max = 60587, size = 24.012 MiB)
slot print_timing: id  0 | task 46081 | 
prompt eval time =    4248.70 ms /  2120 tokens (    2.00 ms per token,   498.98 tokens per second)
       eval time =    9517.52 ms /   309 tokens (   30.80 ms per token,    32.47 tokens per second)
      total time =   13766.22 ms /  2429 tokens
slot      release: id  0 | task 46081 | stop processing: n_tokens = 60960, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.948 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 46393 | processing task, is_child = 0
slot update_slots: id  0 | task 46393 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 63999
slot update_slots: id  0 | task 46393 | n_tokens = 60652, memory_seq_rm [60652, end)
slot update_slots: id  0 | task 46393 | prompt processing progress, n_tokens = 62700, batch.n_tokens = 2048, progress = 0.979703
slot update_slots: id  0 | task 46393 | n_tokens = 62700, memory_seq_rm [62700, end)
slot update_slots: id  0 | task 46393 | prompt processing progress, n_tokens = 63935, batch.n_tokens = 1235, progress = 0.999000
slot update_slots: id  0 | task 46393 | n_tokens = 63935, memory_seq_rm [63935, end)
slot update_slots: id  0 | task 46393 | prompt processing progress, n_tokens = 63999, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 46393 | prompt done, n_tokens = 63999, batch.n_tokens = 64
slot init_sampler: id  0 | task 46393 | init sampler, took 11.33 ms, tokens: text = 63999, total = 63999
slot update_slots: id  0 | task 46393 | erasing old context checkpoint (pos_min = 9742, pos_max = 10511, size = 18.056 MiB)
slot update_slots: id  0 | task 46393 | created context checkpoint 8 of 8 (pos_min = 62911, pos_max = 63934, size = 24.012 MiB)
slot print_timing: id  0 | task 46393 | 
prompt eval time =    6543.67 ms /  3347 tokens (    1.96 ms per token,   511.49 tokens per second)
       eval time =       0.00 ms /     1 tokens (    0.00 ms per token, 1000000.00 tokens per second)
      total time =    6543.67 ms /  3348 tokens
slot      release: id  0 | task 46393 | stop processing: n_tokens = 63999, truncated = 1
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.922 (> 0.100 thold), f_keep = 0.010
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 63999, total state size = 1524.721 MiB
srv          load:  - looking for better prompt, base f_keep = 0.010, sim = 0.922
srv        update:  - cache state: 16 prompts, 7826.284 MiB (limits: 8192.000 MiB, 64000 tokens, 265833 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv        update:    - prompt 0x573890f29600:    7634 tokens, checkpoints:  8,   302.143 MiB
srv        update:    - prompt 0x5738800dfa90:    3305 tokens, checkpoints:  1,   113.611 MiB
srv        update:    - prompt 0x5738c7258990:    1957 tokens, checkpoints:  1,    64.392 MiB
srv        update:    - prompt 0x57389a068030:    1588 tokens, checkpoints:  2,    73.795 MiB
srv        update:    - prompt 0x57388390fe00:   63999 tokens, checkpoints:  8,  1703.311 MiB
srv  get_availabl: prompt cache update took 1544.66 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 46397 | processing task, is_child = 0
slot update_slots: id  0 | task 46397 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 676
slot update_slots: id  0 | task 46397 | n_past = 623, slot.prompt.tokens.size() = 63999, seq_id = 0, pos_min = 62975, n_swa = 128
slot update_slots: id  0 | task 46397 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 46397 | erased invalidated context checkpoint (pos_min = 9916, pos_max = 10769, n_swa = 128, size = 20.026 MiB)
slot update_slots: id  0 | task 46397 | erased invalidated context checkpoint (pos_min = 48833, pos_max = 49729, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  0 | task 46397 | erased invalidated context checkpoint (pos_min = 49104, pos_max = 49981, n_swa = 128, size = 20.588 MiB)
slot update_slots: id  0 | task 46397 | erased invalidated context checkpoint (pos_min = 49343, pos_max = 50233, n_swa = 128, size = 20.893 MiB)
slot update_slots: id  0 | task 46397 | erased invalidated context checkpoint (pos_min = 56751, pos_max = 57774, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  0 | task 46397 | erased invalidated context checkpoint (pos_min = 57444, pos_max = 58467, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  0 | task 46397 | erased invalidated context checkpoint (pos_min = 59564, pos_max = 60587, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  0 | task 46397 | erased invalidated context checkpoint (pos_min = 62911, pos_max = 63934, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  0 | task 46397 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 46397 | prompt processing progress, n_tokens = 612, batch.n_tokens = 612, progress = 0.905325
slot update_slots: id  0 | task 46397 | n_tokens = 612, memory_seq_rm [612, end)
slot update_slots: id  0 | task 46397 | prompt processing progress, n_tokens = 676, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 46397 | prompt done, n_tokens = 676, batch.n_tokens = 64
slot init_sampler: id  0 | task 46397 | init sampler, took 0.13 ms, tokens: text = 676, total = 676
slot update_slots: id  0 | task 46397 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 611, size = 14.351 MiB)
slot print_timing: id  0 | task 46397 | 
prompt eval time =     881.34 ms /   676 tokens (    1.30 ms per token,   767.02 tokens per second)
       eval time =    3324.15 ms /   150 tokens (   22.16 ms per token,    45.12 tokens per second)
      total time =    4205.49 ms /   826 tokens
slot      release: id  0 | task 46397 | stop processing: n_tokens = 825, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.893 (> 0.100 thold), f_keep = 0.819
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 46549 | processing task, is_child = 0
slot update_slots: id  0 | task 46549 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 757
slot update_slots: id  0 | task 46549 | n_tokens = 676, memory_seq_rm [676, end)
slot update_slots: id  0 | task 46549 | prompt processing progress, n_tokens = 693, batch.n_tokens = 17, progress = 0.915456
slot update_slots: id  0 | task 46549 | n_tokens = 693, memory_seq_rm [693, end)
slot update_slots: id  0 | task 46549 | prompt processing progress, n_tokens = 757, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 46549 | prompt done, n_tokens = 757, batch.n_tokens = 64
slot init_sampler: id  0 | task 46549 | init sampler, took 0.15 ms, tokens: text = 757, total = 757
slot update_slots: id  0 | task 46549 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 692, size = 16.250 MiB)
slot print_timing: id  0 | task 46549 | 
prompt eval time =     222.99 ms /    81 tokens (    2.75 ms per token,   363.25 tokens per second)
       eval time =     935.78 ms /    41 tokens (   22.82 ms per token,    43.81 tokens per second)
      total time =    1158.77 ms /   122 tokens
slot      release: id  0 | task 46549 | stop processing: n_tokens = 797, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.499 (> 0.100 thold), f_keep = 0.950
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 46592 | processing task, is_child = 0
slot update_slots: id  0 | task 46592 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1517
slot update_slots: id  0 | task 46592 | n_tokens = 757, memory_seq_rm [757, end)
slot update_slots: id  0 | task 46592 | prompt processing progress, n_tokens = 1453, batch.n_tokens = 696, progress = 0.957811
slot update_slots: id  0 | task 46592 | n_tokens = 1453, memory_seq_rm [1453, end)
slot update_slots: id  0 | task 46592 | prompt processing progress, n_tokens = 1517, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 46592 | prompt done, n_tokens = 1517, batch.n_tokens = 64
slot init_sampler: id  0 | task 46592 | init sampler, took 0.44 ms, tokens: text = 1517, total = 1517
slot update_slots: id  0 | task 46592 | created context checkpoint 3 of 8 (pos_min = 429, pos_max = 1452, size = 24.012 MiB)
slot print_timing: id  0 | task 46592 | 
prompt eval time =     812.09 ms /   760 tokens (    1.07 ms per token,   935.86 tokens per second)
       eval time =    1414.34 ms /    61 tokens (   23.19 ms per token,    43.13 tokens per second)
      total time =    2226.43 ms /   821 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 46592 | stop processing: n_tokens = 1577, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = 14371836660
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 46655 | processing task, is_child = 0
slot update_slots: id  1 | task 46655 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 40475
slot update_slots: id  1 | task 46655 | erased invalidated context checkpoint (pos_min = 476, pos_max = 1372, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 46655 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.050599
slot update_slots: id  1 | task 46655 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.101198
slot update_slots: id  1 | task 46655 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.151797
slot update_slots: id  1 | task 46655 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.202397
slot update_slots: id  1 | task 46655 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.252996
slot update_slots: id  1 | task 46655 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.303595
slot update_slots: id  1 | task 46655 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.354194
slot update_slots: id  1 | task 46655 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.404793
slot update_slots: id  1 | task 46655 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.455392
slot update_slots: id  1 | task 46655 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 2048, progress = 0.505991
slot update_slots: id  1 | task 46655 | n_tokens = 20480, memory_seq_rm [20480, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 22528, batch.n_tokens = 2048, progress = 0.556590
slot update_slots: id  1 | task 46655 | n_tokens = 22528, memory_seq_rm [22528, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 24576, batch.n_tokens = 2048, progress = 0.607190
slot update_slots: id  1 | task 46655 | n_tokens = 24576, memory_seq_rm [24576, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 26624, batch.n_tokens = 2048, progress = 0.657789
slot update_slots: id  1 | task 46655 | n_tokens = 26624, memory_seq_rm [26624, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 28672, batch.n_tokens = 2048, progress = 0.708388
slot update_slots: id  1 | task 46655 | n_tokens = 28672, memory_seq_rm [28672, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 30720, batch.n_tokens = 2048, progress = 0.758987
slot update_slots: id  1 | task 46655 | n_tokens = 30720, memory_seq_rm [30720, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 32768, batch.n_tokens = 2048, progress = 0.809586
slot update_slots: id  1 | task 46655 | n_tokens = 32768, memory_seq_rm [32768, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 34816, batch.n_tokens = 2048, progress = 0.860185
slot update_slots: id  1 | task 46655 | n_tokens = 34816, memory_seq_rm [34816, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 36864, batch.n_tokens = 2048, progress = 0.910784
slot update_slots: id  1 | task 46655 | n_tokens = 36864, memory_seq_rm [36864, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 38912, batch.n_tokens = 2048, progress = 0.961384
slot update_slots: id  1 | task 46655 | n_tokens = 38912, memory_seq_rm [38912, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 40411, batch.n_tokens = 1499, progress = 0.998419
slot update_slots: id  1 | task 46655 | n_tokens = 40411, memory_seq_rm [40411, end)
slot update_slots: id  1 | task 46655 | prompt processing progress, n_tokens = 40475, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 46655 | prompt done, n_tokens = 40475, batch.n_tokens = 64
slot init_sampler: id  1 | task 46655 | init sampler, took 5.70 ms, tokens: text = 40475, total = 40475
slot update_slots: id  1 | task 46655 | created context checkpoint 2 of 8 (pos_min = 39514, pos_max = 40410, size = 21.034 MiB)
slot print_timing: id  1 | task 46655 | 
prompt eval time =   47100.56 ms / 40475 tokens (    1.16 ms per token,   859.33 tokens per second)
       eval time =    2554.51 ms /    94 tokens (   27.18 ms per token,    36.80 tokens per second)
      total time =   49655.07 ms / 40569 tokens
slot      release: id  1 | task 46655 | stop processing: n_tokens = 40568, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.642 (> 0.100 thold), f_keep = 0.394
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1577, total state size = 39.958 MiB
srv          load:  - looking for better prompt, base f_keep = 0.394, sim = 0.642
srv        update:  - cache state: 17 prompts, 7920.855 MiB (limits: 8192.000 MiB, 64000 tokens, 264290 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv        update:    - prompt 0x573890f29600:    7634 tokens, checkpoints:  8,   302.143 MiB
srv        update:    - prompt 0x5738800dfa90:    3305 tokens, checkpoints:  1,   113.611 MiB
srv        update:    - prompt 0x5738c7258990:    1957 tokens, checkpoints:  1,    64.392 MiB
srv        update:    - prompt 0x57389a068030:    1588 tokens, checkpoints:  2,    73.795 MiB
srv        update:    - prompt 0x57388390fe00:   63999 tokens, checkpoints:  8,  1703.311 MiB
srv        update:    - prompt 0x57389a067d70:    1577 tokens, checkpoints:  3,    94.571 MiB
srv  get_availabl: prompt cache update took 37.41 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 46770 | processing task, is_child = 0
slot update_slots: id  0 | task 46770 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 968
slot update_slots: id  0 | task 46770 | n_past = 621, slot.prompt.tokens.size() = 1577, seq_id = 0, pos_min = 1450, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  0 | task 46770 | failed to restore context checkpoint (pos_min = 429, pos_max = 1452, size = 24.012 MiB)
slot update_slots: id  0 | task 46770 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 46770 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 46770 | prompt processing progress, n_tokens = 904, batch.n_tokens = 904, progress = 0.933884
slot update_slots: id  0 | task 46770 | n_tokens = 904, memory_seq_rm [904, end)
slot update_slots: id  0 | task 46770 | prompt processing progress, n_tokens = 968, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 46770 | prompt done, n_tokens = 968, batch.n_tokens = 64
slot init_sampler: id  0 | task 46770 | init sampler, took 0.17 ms, tokens: text = 968, total = 968
slot print_timing: id  0 | task 46770 | 
prompt eval time =    1704.12 ms /   968 tokens (    1.76 ms per token,   568.03 tokens per second)
       eval time =    1864.24 ms /    68 tokens (   27.42 ms per token,    36.48 tokens per second)
      total time =    3568.37 ms /  1036 tokens
slot      release: id  0 | task 46770 | stop processing: n_tokens = 1035, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = 15459932779
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46840 | processing task, is_child = 0
slot update_slots: id  3 | task 46840 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 90727
srv    send_error: task id = 46840, error: request (90727 tokens) exceeds the available context size (64000 tokens), try increasing it
slot      release: id  3 | task 46840 | stop processing: n_tokens = 0, truncated = 0
srv  update_slots: no tokens to decode
srv          stop: cancel task, id_task = 46840
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 400
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.902 (> 0.100 thold), f_keep = 0.015
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 40568, total state size = 954.255 MiB
srv          load:  - looking for better prompt, base f_keep = 0.015, sim = 0.902
srv        update:  - cache size limit reached, removing oldest entry (size = 362.875 MiB)
srv        update:  - cache size limit reached, removing oldest entry (size = 1196.579 MiB)
srv        update:  - cache state: 16 prompts, 7350.642 MiB (limits: 8192.000 MiB, 64000 tokens, 272832 est)
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv        update:    - prompt 0x573890f29600:    7634 tokens, checkpoints:  8,   302.143 MiB
srv        update:    - prompt 0x5738800dfa90:    3305 tokens, checkpoints:  1,   113.611 MiB
srv        update:    - prompt 0x5738c7258990:    1957 tokens, checkpoints:  1,    64.392 MiB
srv        update:    - prompt 0x57389a068030:    1588 tokens, checkpoints:  2,    73.795 MiB
srv        update:    - prompt 0x57388390fe00:   63999 tokens, checkpoints:  8,  1703.311 MiB
srv        update:    - prompt 0x57389a067d70:    1577 tokens, checkpoints:  3,    94.571 MiB
srv        update:    - prompt 0x573883ba85f0:   40568 tokens, checkpoints:  2,   989.242 MiB
srv  get_availabl: prompt cache update took 956.92 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 46843 | processing task, is_child = 0
slot update_slots: id  1 | task 46843 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 692
slot update_slots: id  1 | task 46843 | n_past = 624, slot.prompt.tokens.size() = 40568, seq_id = 1, pos_min = 40441, n_swa = 128
slot update_slots: id  1 | task 46843 | restored context checkpoint (pos_min = 0, pos_max = 594, size = 13.952 MiB)
slot update_slots: id  1 | task 46843 | erased invalidated context checkpoint (pos_min = 39514, pos_max = 40410, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 46843 | n_tokens = 594, memory_seq_rm [594, end)
slot update_slots: id  1 | task 46843 | prompt processing progress, n_tokens = 628, batch.n_tokens = 34, progress = 0.907514
slot update_slots: id  1 | task 46843 | n_tokens = 628, memory_seq_rm [628, end)
slot update_slots: id  1 | task 46843 | prompt processing progress, n_tokens = 692, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 46843 | prompt done, n_tokens = 692, batch.n_tokens = 64
slot init_sampler: id  1 | task 46843 | init sampler, took 0.14 ms, tokens: text = 692, total = 692
slot print_timing: id  1 | task 46843 | 
prompt eval time =     481.26 ms /    98 tokens (    4.91 ms per token,   203.63 tokens per second)
       eval time =    1668.91 ms /    78 tokens (   21.40 ms per token,    46.74 tokens per second)
      total time =    2150.17 ms /   176 tokens
slot      release: id  1 | task 46843 | stop processing: n_tokens = 769, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.476 (> 0.100 thold), f_keep = 0.900
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 46923 | processing task, is_child = 0
slot update_slots: id  1 | task 46923 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1453
slot update_slots: id  1 | task 46923 | n_tokens = 692, memory_seq_rm [692, end)
slot update_slots: id  1 | task 46923 | prompt processing progress, n_tokens = 1389, batch.n_tokens = 697, progress = 0.955953
slot update_slots: id  1 | task 46923 | n_tokens = 1389, memory_seq_rm [1389, end)
slot update_slots: id  1 | task 46923 | prompt processing progress, n_tokens = 1453, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 46923 | prompt done, n_tokens = 1453, batch.n_tokens = 64
slot init_sampler: id  1 | task 46923 | init sampler, took 0.29 ms, tokens: text = 1453, total = 1453
slot update_slots: id  1 | task 46923 | created context checkpoint 2 of 8 (pos_min = 492, pos_max = 1388, size = 21.034 MiB)
slot print_timing: id  1 | task 46923 | 
prompt eval time =     791.30 ms /   761 tokens (    1.04 ms per token,   961.70 tokens per second)
       eval time =    1357.45 ms /    63 tokens (   21.55 ms per token,    46.41 tokens per second)
      total time =    2148.76 ms /   824 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 46923 | stop processing: n_tokens = 1515, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.323 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 46988 | processing task, is_child = 0
slot update_slots: id  1 | task 46988 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4504
slot update_slots: id  1 | task 46988 | n_tokens = 1453, memory_seq_rm [1453, end)
slot update_slots: id  1 | task 46988 | prompt processing progress, n_tokens = 3501, batch.n_tokens = 2048, progress = 0.777309
slot update_slots: id  1 | task 46988 | n_tokens = 3501, memory_seq_rm [3501, end)
slot update_slots: id  1 | task 46988 | prompt processing progress, n_tokens = 4440, batch.n_tokens = 939, progress = 0.985790
slot update_slots: id  1 | task 46988 | n_tokens = 4440, memory_seq_rm [4440, end)
slot update_slots: id  1 | task 46988 | prompt processing progress, n_tokens = 4504, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 46988 | prompt done, n_tokens = 4504, batch.n_tokens = 64
slot init_sampler: id  1 | task 46988 | init sampler, took 0.73 ms, tokens: text = 4504, total = 4504
slot update_slots: id  1 | task 46988 | created context checkpoint 3 of 8 (pos_min = 3543, pos_max = 4439, size = 21.034 MiB)
slot print_timing: id  1 | task 46988 | 
prompt eval time =    2715.06 ms /  3051 tokens (    0.89 ms per token,  1123.73 tokens per second)
       eval time =    1618.63 ms /    73 tokens (   22.17 ms per token,    45.10 tokens per second)
      total time =    4333.69 ms /  3124 tokens
slot      release: id  1 | task 46988 | stop processing: n_tokens = 4576, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.853 (> 0.100 thold), f_keep = 0.137
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4576, total state size = 128.337 MiB
srv          load:  - looking for better prompt, base f_keep = 0.137, sim = 0.853
srv        update:  - cache state: 17 prompts, 7535.000 MiB (limits: 8192.000 MiB, 64000 tokens, 271131 est)
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv        update:    - prompt 0x573890f29600:    7634 tokens, checkpoints:  8,   302.143 MiB
srv        update:    - prompt 0x5738800dfa90:    3305 tokens, checkpoints:  1,   113.611 MiB
srv        update:    - prompt 0x5738c7258990:    1957 tokens, checkpoints:  1,    64.392 MiB
srv        update:    - prompt 0x57389a068030:    1588 tokens, checkpoints:  2,    73.795 MiB
srv        update:    - prompt 0x57388390fe00:   63999 tokens, checkpoints:  8,  1703.311 MiB
srv        update:    - prompt 0x57389a067d70:    1577 tokens, checkpoints:  3,    94.571 MiB
srv        update:    - prompt 0x573883ba85f0:   40568 tokens, checkpoints:  2,   989.242 MiB
srv        update:    - prompt 0x573883b86cb0:    4576 tokens, checkpoints:  3,   184.357 MiB
srv  get_availabl: prompt cache update took 78.79 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 47064 | processing task, is_child = 0
slot update_slots: id  1 | task 47064 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 734
slot update_slots: id  1 | task 47064 | n_past = 626, slot.prompt.tokens.size() = 4576, seq_id = 1, pos_min = 3679, n_swa = 128
slot update_slots: id  1 | task 47064 | restored context checkpoint (pos_min = 492, pos_max = 1388, size = 21.034 MiB)
slot update_slots: id  1 | task 47064 | erased invalidated context checkpoint (pos_min = 3543, pos_max = 4439, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 47064 | n_tokens = 626, memory_seq_rm [626, end)
slot update_slots: id  1 | task 47064 | prompt processing progress, n_tokens = 670, batch.n_tokens = 44, progress = 0.912807
slot update_slots: id  1 | task 47064 | n_tokens = 670, memory_seq_rm [670, end)
slot update_slots: id  1 | task 47064 | prompt processing progress, n_tokens = 734, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 47064 | prompt done, n_tokens = 734, batch.n_tokens = 64
slot init_sampler: id  1 | task 47064 | init sampler, took 0.15 ms, tokens: text = 734, total = 734
slot print_timing: id  1 | task 47064 | 
prompt eval time =     600.17 ms /   108 tokens (    5.56 ms per token,   179.95 tokens per second)
       eval time =    2064.32 ms /    93 tokens (   22.20 ms per token,    45.05 tokens per second)
      total time =    2664.49 ms /   201 tokens
slot      release: id  1 | task 47064 | stop processing: n_tokens = 826, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = 15616868670
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47159 | processing task, is_child = 0
slot update_slots: id  2 | task 47159 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18982
slot update_slots: id  2 | task 47159 | erased invalidated context checkpoint (pos_min = 39766, pos_max = 40408, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 47159 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 47159 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.107892
slot update_slots: id  2 | task 47159 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 47159 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.215783
slot update_slots: id  2 | task 47159 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 47159 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.323675
slot update_slots: id  2 | task 47159 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 47159 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.431567
slot update_slots: id  2 | task 47159 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 47159 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.539458
slot update_slots: id  2 | task 47159 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 47159 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.647350
slot update_slots: id  2 | task 47159 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  2 | task 47159 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.755242
slot update_slots: id  2 | task 47159 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  2 | task 47159 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.863133
slot update_slots: id  2 | task 47159 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  2 | task 47159 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.971025
slot update_slots: id  2 | task 47159 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  2 | task 47159 | prompt processing progress, n_tokens = 18918, batch.n_tokens = 486, progress = 0.996628
slot update_slots: id  2 | task 47159 | n_tokens = 18918, memory_seq_rm [18918, end)
slot update_slots: id  2 | task 47159 | prompt processing progress, n_tokens = 18982, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47159 | prompt done, n_tokens = 18982, batch.n_tokens = 64
slot init_sampler: id  2 | task 47159 | init sampler, took 2.70 ms, tokens: text = 18982, total = 18982
slot update_slots: id  2 | task 47159 | created context checkpoint 1 of 8 (pos_min = 18148, pos_max = 18917, size = 18.056 MiB)
slot print_timing: id  2 | task 47159 | 
prompt eval time =   18930.67 ms / 18982 tokens (    1.00 ms per token,  1002.71 tokens per second)
       eval time =    2357.14 ms /    95 tokens (   24.81 ms per token,    40.30 tokens per second)
      total time =   21287.81 ms / 19077 tokens
slot      release: id  2 | task 47159 | stop processing: n_tokens = 19076, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47265 | processing task, is_child = 0
slot update_slots: id  2 | task 47265 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19045
slot update_slots: id  2 | task 47265 | n_tokens = 18982, memory_seq_rm [18982, end)
slot update_slots: id  2 | task 47265 | prompt processing progress, n_tokens = 19045, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  2 | task 47265 | prompt done, n_tokens = 19045, batch.n_tokens = 63
slot init_sampler: id  2 | task 47265 | init sampler, took 3.06 ms, tokens: text = 19045, total = 19045
slot print_timing: id  2 | task 47265 | 
prompt eval time =     189.27 ms /    63 tokens (    3.00 ms per token,   332.85 tokens per second)
       eval time =    2242.29 ms /    90 tokens (   24.91 ms per token,    40.14 tokens per second)
      total time =    2431.56 ms /   153 tokens
slot      release: id  2 | task 47265 | stop processing: n_tokens = 19134, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47356 | processing task, is_child = 0
slot update_slots: id  2 | task 47356 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19106
slot update_slots: id  2 | task 47356 | n_tokens = 19045, memory_seq_rm [19045, end)
slot update_slots: id  2 | task 47356 | prompt processing progress, n_tokens = 19106, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  2 | task 47356 | prompt done, n_tokens = 19106, batch.n_tokens = 61
slot init_sampler: id  2 | task 47356 | init sampler, took 2.71 ms, tokens: text = 19106, total = 19106
slot update_slots: id  2 | task 47356 | created context checkpoint 2 of 8 (pos_min = 18432, pos_max = 19044, size = 14.374 MiB)
slot print_timing: id  2 | task 47356 | 
prompt eval time =     204.06 ms /    61 tokens (    3.35 ms per token,   298.92 tokens per second)
       eval time =    1536.14 ms /    61 tokens (   25.18 ms per token,    39.71 tokens per second)
      total time =    1740.21 ms /   122 tokens
slot      release: id  2 | task 47356 | stop processing: n_tokens = 19166, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.891 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47418 | processing task, is_child = 0
slot update_slots: id  2 | task 47418 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21436
slot update_slots: id  2 | task 47418 | n_tokens = 19106, memory_seq_rm [19106, end)
slot update_slots: id  2 | task 47418 | prompt processing progress, n_tokens = 21154, batch.n_tokens = 2048, progress = 0.986845
slot update_slots: id  2 | task 47418 | n_tokens = 21154, memory_seq_rm [21154, end)
slot update_slots: id  2 | task 47418 | prompt processing progress, n_tokens = 21372, batch.n_tokens = 218, progress = 0.997014
slot update_slots: id  2 | task 47418 | n_tokens = 21372, memory_seq_rm [21372, end)
slot update_slots: id  2 | task 47418 | prompt processing progress, n_tokens = 21436, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47418 | prompt done, n_tokens = 21436, batch.n_tokens = 64
slot init_sampler: id  2 | task 47418 | init sampler, took 4.04 ms, tokens: text = 21436, total = 21436
slot update_slots: id  2 | task 47418 | created context checkpoint 3 of 8 (pos_min = 20602, pos_max = 21371, size = 18.056 MiB)
slot print_timing: id  2 | task 47418 | 
prompt eval time =    2991.36 ms /  2330 tokens (    1.28 ms per token,   778.91 tokens per second)
       eval time =    2408.12 ms /    93 tokens (   25.89 ms per token,    38.62 tokens per second)
      total time =    5399.47 ms /  2423 tokens
slot      release: id  2 | task 47418 | stop processing: n_tokens = 21528, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47514 | processing task, is_child = 0
slot update_slots: id  2 | task 47514 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21495
slot update_slots: id  2 | task 47514 | n_tokens = 21436, memory_seq_rm [21436, end)
slot update_slots: id  2 | task 47514 | prompt processing progress, n_tokens = 21495, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  2 | task 47514 | prompt done, n_tokens = 21495, batch.n_tokens = 59
slot init_sampler: id  2 | task 47514 | init sampler, took 4.94 ms, tokens: text = 21495, total = 21495
slot print_timing: id  2 | task 47514 | 
prompt eval time =     179.39 ms /    59 tokens (    3.04 ms per token,   328.90 tokens per second)
       eval time =    1917.27 ms /    75 tokens (   25.56 ms per token,    39.12 tokens per second)
      total time =    2096.66 ms /   134 tokens
slot      release: id  2 | task 47514 | stop processing: n_tokens = 21569, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.763 (> 0.100 thold), f_keep = 0.754
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 47590 | processing task, is_child = 0
slot update_slots: id  1 | task 47590 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 816
slot update_slots: id  1 | task 47590 | n_past = 623, slot.prompt.tokens.size() = 826, seq_id = 1, pos_min = 699, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  1 | task 47590 | failed to restore context checkpoint (pos_min = 492, pos_max = 1388, size = 21.034 MiB)
slot update_slots: id  1 | task 47590 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 47590 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 47590 | prompt processing progress, n_tokens = 752, batch.n_tokens = 752, progress = 0.921569
slot update_slots: id  1 | task 47590 | n_tokens = 752, memory_seq_rm [752, end)
slot update_slots: id  1 | task 47590 | prompt processing progress, n_tokens = 816, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 47590 | prompt done, n_tokens = 816, batch.n_tokens = 64
slot init_sampler: id  1 | task 47590 | init sampler, took 0.19 ms, tokens: text = 816, total = 816
slot print_timing: id  1 | task 47590 | 
prompt eval time =    1518.17 ms /   816 tokens (    1.86 ms per token,   537.49 tokens per second)
       eval time =    1586.49 ms /    62 tokens (   25.59 ms per token,    39.08 tokens per second)
      total time =    3104.66 ms /   878 tokens
slot      release: id  1 | task 47590 | stop processing: n_tokens = 877, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 16115832402
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1035, total state size = 27.248 MiB
srv          load:  - looking for better prompt, base f_keep = 0.600, sim = 0.033
srv        update:  - cache state: 18 prompts, 7616.861 MiB (limits: 8192.000 MiB, 64000 tokens, 269331 est)
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv        update:    - prompt 0x573890f29600:    7634 tokens, checkpoints:  8,   302.143 MiB
srv        update:    - prompt 0x5738800dfa90:    3305 tokens, checkpoints:  1,   113.611 MiB
srv        update:    - prompt 0x5738c7258990:    1957 tokens, checkpoints:  1,    64.392 MiB
srv        update:    - prompt 0x57389a068030:    1588 tokens, checkpoints:  2,    73.795 MiB
srv        update:    - prompt 0x57388390fe00:   63999 tokens, checkpoints:  8,  1703.311 MiB
srv        update:    - prompt 0x57389a067d70:    1577 tokens, checkpoints:  3,    94.571 MiB
srv        update:    - prompt 0x573883ba85f0:   40568 tokens, checkpoints:  2,   989.242 MiB
srv        update:    - prompt 0x573883b86cb0:    4576 tokens, checkpoints:  3,   184.357 MiB
srv        update:    - prompt 0x5738a40753e0:    1035 tokens, checkpoints:  3,    81.862 MiB
srv  get_availabl: prompt cache update took 55.82 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 47654 | processing task, is_child = 0
slot update_slots: id  0 | task 47654 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19064
slot update_slots: id  0 | task 47654 | n_past = 621, slot.prompt.tokens.size() = 1035, seq_id = 0, pos_min = 908, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  0 | task 47654 | failed to restore context checkpoint (pos_min = 429, pos_max = 1452, size = 24.012 MiB)
slot update_slots: id  0 | task 47654 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 47654 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 47654 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.107428
slot update_slots: id  0 | task 47654 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 47654 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.214855
slot update_slots: id  0 | task 47654 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 47654 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.322283
slot update_slots: id  0 | task 47654 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 47654 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.429710
slot update_slots: id  0 | task 47654 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 47654 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.537138
slot update_slots: id  0 | task 47654 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  0 | task 47654 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.644566
slot update_slots: id  0 | task 47654 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  0 | task 47654 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.751993
slot update_slots: id  0 | task 47654 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  0 | task 47654 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.859421
slot update_slots: id  0 | task 47654 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  0 | task 47654 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.966848
slot update_slots: id  0 | task 47654 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  0 | task 47654 | prompt processing progress, n_tokens = 19000, batch.n_tokens = 568, progress = 0.996643
slot update_slots: id  0 | task 47654 | n_tokens = 19000, memory_seq_rm [19000, end)
slot update_slots: id  0 | task 47654 | prompt processing progress, n_tokens = 19064, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 47654 | prompt done, n_tokens = 19064, batch.n_tokens = 64
slot init_sampler: id  0 | task 47654 | init sampler, took 3.81 ms, tokens: text = 19064, total = 19064
slot update_slots: id  0 | task 47654 | created context checkpoint 4 of 8 (pos_min = 18230, pos_max = 18999, size = 18.056 MiB)
slot print_timing: id  0 | task 47654 | 
prompt eval time =   24704.68 ms / 19064 tokens (    1.30 ms per token,   771.68 tokens per second)
       eval time =    3305.21 ms /   122 tokens (   27.09 ms per token,    36.91 tokens per second)
      total time =   28009.89 ms / 19186 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 47654 | stop processing: n_tokens = 19185, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 47787 | processing task, is_child = 0
slot update_slots: id  0 | task 47787 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19128
slot update_slots: id  0 | task 47787 | n_tokens = 19064, memory_seq_rm [19064, end)
slot update_slots: id  0 | task 47787 | prompt processing progress, n_tokens = 19128, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 47787 | prompt done, n_tokens = 19128, batch.n_tokens = 64
slot init_sampler: id  0 | task 47787 | init sampler, took 4.61 ms, tokens: text = 19128, total = 19128
slot print_timing: id  0 | task 47787 | 
prompt eval time =     232.45 ms /    64 tokens (    3.63 ms per token,   275.33 tokens per second)
       eval time =    2261.64 ms /    79 tokens (   28.63 ms per token,    34.93 tokens per second)
      total time =    2494.09 ms /   143 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 47787 | stop processing: n_tokens = 19206, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.789 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 47867 | processing task, is_child = 0
slot update_slots: id  0 | task 47867 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24257
slot update_slots: id  0 | task 47867 | n_tokens = 19128, memory_seq_rm [19128, end)
slot update_slots: id  0 | task 47867 | prompt processing progress, n_tokens = 21176, batch.n_tokens = 2048, progress = 0.872985
slot update_slots: id  0 | task 47867 | n_tokens = 21176, memory_seq_rm [21176, end)
slot update_slots: id  0 | task 47867 | prompt processing progress, n_tokens = 23224, batch.n_tokens = 2048, progress = 0.957414
slot update_slots: id  0 | task 47867 | n_tokens = 23224, memory_seq_rm [23224, end)
slot update_slots: id  0 | task 47867 | prompt processing progress, n_tokens = 24193, batch.n_tokens = 969, progress = 0.997362
slot update_slots: id  0 | task 47867 | n_tokens = 24193, memory_seq_rm [24193, end)
slot update_slots: id  0 | task 47867 | prompt processing progress, n_tokens = 24257, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 47867 | prompt done, n_tokens = 24257, batch.n_tokens = 64
slot init_sampler: id  0 | task 47867 | init sampler, took 4.31 ms, tokens: text = 24257, total = 24257
slot update_slots: id  0 | task 47867 | created context checkpoint 5 of 8 (pos_min = 23423, pos_max = 24192, size = 18.056 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 47867
slot      release: id  0 | task 47867 | stop processing: n_tokens = 24301, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.640 (> 0.100 thold), f_keep = 0.026
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 24301, total state size = 587.889 MiB
srv          load:  - looking for better prompt, base f_keep = 0.026, sim = 0.640
srv          load:  - found better prompt with f_keep = 0.614, sim = 0.655
srv        update:  - cache size limit reached, removing oldest entry (size = 673.058 MiB)
srv        update:  - cache state: 17 prompts, 7540.556 MiB (limits: 8192.000 MiB, 64000 tokens, 274894 est)
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv        update:    - prompt 0x573890f29600:    7634 tokens, checkpoints:  8,   302.143 MiB
srv        update:    - prompt 0x5738800dfa90:    3305 tokens, checkpoints:  1,   113.611 MiB
srv        update:    - prompt 0x5738c7258990:    1957 tokens, checkpoints:  1,    64.392 MiB
srv        update:    - prompt 0x57389a068030:    1588 tokens, checkpoints:  2,    73.795 MiB
srv        update:    - prompt 0x57388390fe00:   63999 tokens, checkpoints:  8,  1703.311 MiB
srv        update:    - prompt 0x57389a067d70:    1577 tokens, checkpoints:  3,    94.571 MiB
srv        update:    - prompt 0x573883ba85f0:   40568 tokens, checkpoints:  2,   989.242 MiB
srv        update:    - prompt 0x573883b86cb0:    4576 tokens, checkpoints:  3,   184.357 MiB
srv        update:    - prompt 0x57387f6f7f90:   24301 tokens, checkpoints:  5,   678.614 MiB
srv  get_availabl: prompt cache update took 720.61 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 47917 | processing task, is_child = 0
slot update_slots: id  0 | task 47917 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 970
slot update_slots: id  0 | task 47917 | n_past = 635, slot.prompt.tokens.size() = 1035, seq_id = 0, pos_min = 908, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  0 | task 47917 | failed to restore context checkpoint (pos_min = 429, pos_max = 1452, size = 24.012 MiB)
slot update_slots: id  0 | task 47917 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 47917 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 47917 | prompt processing progress, n_tokens = 906, batch.n_tokens = 906, progress = 0.934021
slot update_slots: id  0 | task 47917 | n_tokens = 906, memory_seq_rm [906, end)
slot update_slots: id  0 | task 47917 | prompt processing progress, n_tokens = 970, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 47917 | prompt done, n_tokens = 970, batch.n_tokens = 64
slot init_sampler: id  0 | task 47917 | init sampler, took 0.33 ms, tokens: text = 970, total = 970
slot print_timing: id  0 | task 47917 | 
prompt eval time =    1393.72 ms /   970 tokens (    1.44 ms per token,   695.98 tokens per second)
       eval time =    4156.35 ms /   161 tokens (   25.82 ms per token,    38.74 tokens per second)
      total time =    5550.07 ms /  1131 tokens
slot      release: id  0 | task 47917 | stop processing: n_tokens = 1130, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = 16116668073
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 48080 | processing task, is_child = 0
slot update_slots: id  3 | task 48080 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21750
slot update_slots: id  3 | task 48080 | erased invalidated context checkpoint (pos_min = 6102, pos_max = 6871, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 48080 | erased invalidated context checkpoint (pos_min = 6415, pos_max = 6938, n_swa = 128, size = 12.288 MiB)
slot update_slots: id  3 | task 48080 | erased invalidated context checkpoint (pos_min = 7144, pos_max = 7913, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 48080 | erased invalidated context checkpoint (pos_min = 8871, pos_max = 9640, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 48080 | erased invalidated context checkpoint (pos_min = 10010, pos_max = 10779, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 48080 | erased invalidated context checkpoint (pos_min = 10152, pos_max = 10856, n_swa = 128, size = 16.532 MiB)
slot update_slots: id  3 | task 48080 | erased invalidated context checkpoint (pos_min = 11205, pos_max = 11974, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 48080 | erased invalidated context checkpoint (pos_min = 12190, pos_max = 12959, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 48080 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.094161
slot update_slots: id  3 | task 48080 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.188322
slot update_slots: id  3 | task 48080 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.282483
slot update_slots: id  3 | task 48080 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.376644
slot update_slots: id  3 | task 48080 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.470805
slot update_slots: id  3 | task 48080 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.564966
slot update_slots: id  3 | task 48080 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.659126
slot update_slots: id  3 | task 48080 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.753287
slot update_slots: id  3 | task 48080 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.847448
slot update_slots: id  3 | task 48080 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 2048, progress = 0.941609
slot update_slots: id  3 | task 48080 | n_tokens = 20480, memory_seq_rm [20480, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 21686, batch.n_tokens = 1206, progress = 0.997057
slot update_slots: id  3 | task 48080 | n_tokens = 21686, memory_seq_rm [21686, end)
slot update_slots: id  3 | task 48080 | prompt processing progress, n_tokens = 21750, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 48080 | prompt done, n_tokens = 21750, batch.n_tokens = 64
slot init_sampler: id  3 | task 48080 | init sampler, took 4.63 ms, tokens: text = 21750, total = 21750
slot update_slots: id  3 | task 48080 | created context checkpoint 1 of 8 (pos_min = 21043, pos_max = 21685, size = 15.078 MiB)
slot print_timing: id  3 | task 48080 | 
prompt eval time =   29076.01 ms / 21750 tokens (    1.34 ms per token,   748.04 tokens per second)
       eval time =    3281.59 ms /   116 tokens (   28.29 ms per token,    35.35 tokens per second)
      total time =   32357.60 ms / 21866 tokens
slot      release: id  3 | task 48080 | stop processing: n_tokens = 21865, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.482 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 48208 | processing task, is_child = 0
slot update_slots: id  3 | task 48208 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 45146
slot update_slots: id  3 | task 48208 | n_tokens = 21750, memory_seq_rm [21750, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 23798, batch.n_tokens = 2048, progress = 0.527134
slot update_slots: id  3 | task 48208 | n_tokens = 23798, memory_seq_rm [23798, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 25846, batch.n_tokens = 2048, progress = 0.572498
slot update_slots: id  3 | task 48208 | n_tokens = 25846, memory_seq_rm [25846, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 27894, batch.n_tokens = 2048, progress = 0.617862
slot update_slots: id  3 | task 48208 | n_tokens = 27894, memory_seq_rm [27894, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 29942, batch.n_tokens = 2048, progress = 0.663226
slot update_slots: id  3 | task 48208 | n_tokens = 29942, memory_seq_rm [29942, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 31990, batch.n_tokens = 2048, progress = 0.708590
slot update_slots: id  3 | task 48208 | n_tokens = 31990, memory_seq_rm [31990, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 34038, batch.n_tokens = 2048, progress = 0.753954
slot update_slots: id  3 | task 48208 | n_tokens = 34038, memory_seq_rm [34038, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 36086, batch.n_tokens = 2048, progress = 0.799318
slot update_slots: id  3 | task 48208 | n_tokens = 36086, memory_seq_rm [36086, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 38134, batch.n_tokens = 2048, progress = 0.844682
slot update_slots: id  3 | task 48208 | n_tokens = 38134, memory_seq_rm [38134, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 40182, batch.n_tokens = 2048, progress = 0.890046
slot update_slots: id  3 | task 48208 | n_tokens = 40182, memory_seq_rm [40182, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 42230, batch.n_tokens = 2048, progress = 0.935410
decode: failed to find a memory slot for batch of size 2048
srv  try_clear_id: purging slot 0 with 1130 tokens
slot prompt_clear: id  0 | task -1 | clearing prompt with 1130 tokens
srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 2048, ret = 1
decode: failed to find a memory slot for batch of size 2048
srv  try_clear_id: purging slot 1 with 877 tokens
slot prompt_clear: id  1 | task -1 | clearing prompt with 877 tokens
srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 2048, ret = 1
slot update_slots: id  3 | task 48208 | n_tokens = 42230, memory_seq_rm [42230, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 44278, batch.n_tokens = 2048, progress = 0.980774
decode: failed to find a memory slot for batch of size 2048
srv  try_clear_id: purging slot 2 with 21569 tokens
slot prompt_clear: id  2 | task -1 | clearing prompt with 21569 tokens
srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 2048, ret = 1
slot update_slots: id  3 | task 48208 | n_tokens = 44278, memory_seq_rm [44278, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 45082, batch.n_tokens = 804, progress = 0.998582
slot update_slots: id  3 | task 48208 | n_tokens = 45082, memory_seq_rm [45082, end)
slot update_slots: id  3 | task 48208 | prompt processing progress, n_tokens = 45146, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 48208 | prompt done, n_tokens = 45146, batch.n_tokens = 64
slot init_sampler: id  3 | task 48208 | init sampler, took 11.86 ms, tokens: text = 45146, total = 45146
slot update_slots: id  3 | task 48208 | created context checkpoint 2 of 8 (pos_min = 44058, pos_max = 45081, size = 24.012 MiB)
slot print_timing: id  3 | task 48208 | 
prompt eval time =   39724.37 ms / 23396 tokens (    1.70 ms per token,   588.96 tokens per second)
       eval time =    6358.78 ms /   207 tokens (   30.72 ms per token,    32.55 tokens per second)
      total time =   46083.14 ms / 23603 tokens
slot      release: id  3 | task 48208 | stop processing: n_tokens = 45352, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.702 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 48428 | processing task, is_child = 0
slot update_slots: id  3 | task 48428 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 64300
srv    send_error: task id = 48428, error: request (64300 tokens) exceeds the available context size (64000 tokens), try increasing it
slot      release: id  3 | task 48428 | stop processing: n_tokens = 45352, truncated = 0
srv          stop: cancel task, id_task = 48428
srv  update_slots: no tokens to decode
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 400
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.826 (> 0.100 thold), f_keep = 0.017
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 45352, total state size = 1087.469 MiB
srv          load:  - looking for better prompt, base f_keep = 0.017, sim = 0.826
srv        update:  - cache size limit reached, removing oldest entry (size = 409.140 MiB)
srv        update:  - cache size limit reached, removing oldest entry (size = 515.598 MiB)
srv        update:  - cache state: 16 prompts, 7742.376 MiB (limits: 8192.000 MiB, 64000 tokens, 288639 est)
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv        update:    - prompt 0x5738a5017040:   40479 tokens, checkpoints:  1,   970.224 MiB
srv        update:    - prompt 0x573890dd5080:    1586 tokens, checkpoints:  3,   102.825 MiB
srv        update:    - prompt 0x5738c71d5760:   11695 tokens, checkpoints:  8,   431.792 MiB
srv        update:    - prompt 0x5738a4f709d0:    4362 tokens, checkpoints:  4,   190.032 MiB
srv        update:    - prompt 0x573890f29600:    7634 tokens, checkpoints:  8,   302.143 MiB
srv        update:    - prompt 0x5738800dfa90:    3305 tokens, checkpoints:  1,   113.611 MiB
srv        update:    - prompt 0x5738c7258990:    1957 tokens, checkpoints:  1,    64.392 MiB
srv        update:    - prompt 0x57389a068030:    1588 tokens, checkpoints:  2,    73.795 MiB
srv        update:    - prompt 0x57388390fe00:   63999 tokens, checkpoints:  8,  1703.311 MiB
srv        update:    - prompt 0x57389a067d70:    1577 tokens, checkpoints:  3,    94.571 MiB
srv        update:    - prompt 0x573883ba85f0:   40568 tokens, checkpoints:  2,   989.242 MiB
srv        update:    - prompt 0x573883b86cb0:    4576 tokens, checkpoints:  3,   184.357 MiB
srv        update:    - prompt 0x57387f6f7f90:   24301 tokens, checkpoints:  5,   678.614 MiB
srv        update:    - prompt 0x57387f660660:   45352 tokens, checkpoints:  2,  1126.559 MiB
srv  get_availabl: prompt cache update took 1220.38 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 48431 | processing task, is_child = 0
slot update_slots: id  3 | task 48431 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 941
slot update_slots: id  3 | task 48431 | n_past = 777, slot.prompt.tokens.size() = 45352, seq_id = 3, pos_min = 44328, n_swa = 128
slot update_slots: id  3 | task 48431 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 48431 | erased invalidated context checkpoint (pos_min = 21043, pos_max = 21685, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 48431 | erased invalidated context checkpoint (pos_min = 44058, pos_max = 45081, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 48431 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 48431 | prompt processing progress, n_tokens = 877, batch.n_tokens = 877, progress = 0.931987
slot update_slots: id  3 | task 48431 | n_tokens = 877, memory_seq_rm [877, end)
slot update_slots: id  3 | task 48431 | prompt processing progress, n_tokens = 941, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 48431 | prompt done, n_tokens = 941, batch.n_tokens = 64
slot init_sampler: id  3 | task 48431 | init sampler, took 0.19 ms, tokens: text = 941, total = 941
slot update_slots: id  3 | task 48431 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 876, size = 20.565 MiB)
slot print_timing: id  3 | task 48431 | 
prompt eval time =    1155.75 ms /   941 tokens (    1.23 ms per token,   814.19 tokens per second)
       eval time =     910.76 ms /    40 tokens (   22.77 ms per token,    43.92 tokens per second)
      total time =    2066.51 ms /   981 tokens
slot      release: id  3 | task 48431 | stop processing: n_tokens = 980, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.553 (> 0.100 thold), f_keep = 0.960
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 48473 | processing task, is_child = 0
slot update_slots: id  3 | task 48473 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1701
slot update_slots: id  3 | task 48473 | n_tokens = 941, memory_seq_rm [941, end)
slot update_slots: id  3 | task 48473 | prompt processing progress, n_tokens = 1637, batch.n_tokens = 696, progress = 0.962375
slot update_slots: id  3 | task 48473 | n_tokens = 1637, memory_seq_rm [1637, end)
slot update_slots: id  3 | task 48473 | prompt processing progress, n_tokens = 1701, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 48473 | prompt done, n_tokens = 1701, batch.n_tokens = 64
slot init_sampler: id  3 | task 48473 | init sampler, took 0.31 ms, tokens: text = 1701, total = 1701
slot update_slots: id  3 | task 48473 | created context checkpoint 2 of 8 (pos_min = 613, pos_max = 1636, size = 24.012 MiB)
slot print_timing: id  3 | task 48473 | 
prompt eval time =     826.43 ms /   760 tokens (    1.09 ms per token,   919.62 tokens per second)
       eval time =    1082.64 ms /    49 tokens (   22.09 ms per token,    45.26 tokens per second)
      total time =    1909.07 ms /   809 tokens
slot      release: id  3 | task 48473 | stop processing: n_tokens = 1749, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 48524 | processing task, is_child = 0
slot update_slots: id  3 | task 48524 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1760
slot update_slots: id  3 | task 48524 | n_tokens = 1701, memory_seq_rm [1701, end)
slot update_slots: id  3 | task 48524 | prompt processing progress, n_tokens = 1760, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 48524 | prompt done, n_tokens = 1760, batch.n_tokens = 59
slot init_sampler: id  3 | task 48524 | init sampler, took 0.33 ms, tokens: text = 1760, total = 1760
slot print_timing: id  3 | task 48524 | 
prompt eval time =     287.39 ms /    59 tokens (    4.87 ms per token,   205.30 tokens per second)
       eval time =    1601.01 ms /    63 tokens (   25.41 ms per token,    39.35 tokens per second)
      total time =    1888.40 ms /   122 tokens
slot      release: id  3 | task 48524 | stop processing: n_tokens = 1822, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
