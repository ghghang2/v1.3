ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 3 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 13395 MiB of device memory vs. 14807 MiB of free device memory
llama_params_fit_impl: will leave 1412 >= 1024 MiB of free device memory, no changes needed
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.43 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14807 MiB free
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
warning: failed to mlock 1158266880-byte buffer (after previously locking 0 bytes): Cannot allocate memory
Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
.srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 40192
llama_context: n_ctx_seq     = 40192
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (40192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.77 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 40192 cells
llama_kv_cache:      CUDA0 KV buffer size =   942.00 MiB
llama_kv_cache: size =  942.00 MiB ( 40192 cells,  12 layers,  1/1 seqs), K (f16):  471.00 MiB, V (f16):  471.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 768 cells
llama_kv_cache:      CUDA0 KV buffer size =    18.00 MiB
llama_kv_cache: size =   18.00 MiB (   768 cells,  12 layers,  1/1 seqs), K (f16):    9.00 MiB, V (f16):    9.00 MiB
sched_reserve: reserving ...
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =    85.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 57.72 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 1
slot   load_model: id  0 | task -1 | new slot, n_ctx = 40192
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-28

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 0 | processing task, is_child = 0
slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 740
slot update_slots: id  0 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 0 | prompt processing progress, n_tokens = 676, batch.n_tokens = 676, progress = 0.913514
slot update_slots: id  0 | task 0 | n_tokens = 676, memory_seq_rm [676, end)
slot update_slots: id  0 | task 0 | prompt processing progress, n_tokens = 740, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 0 | prompt done, n_tokens = 740, batch.n_tokens = 64
slot init_sampler: id  0 | task 0 | init sampler, took 0.12 ms, tokens: text = 740, total = 740
slot update_slots: id  0 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 675, size = 15.852 MiB)
slot print_timing: id  0 | task 0 | 
prompt eval time =    1166.14 ms /   740 tokens (    1.58 ms per token,   634.57 tokens per second)
       eval time =    1360.78 ms /    63 tokens (   21.60 ms per token,    46.30 tokens per second)
      total time =    2526.92 ms /   803 tokens
slot      release: id  0 | task 0 | stop processing: n_tokens = 802, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.600 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 65 | processing task, is_child = 0
slot update_slots: id  0 | task 65 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1306
slot update_slots: id  0 | task 65 | n_tokens = 783, memory_seq_rm [783, end)
slot update_slots: id  0 | task 65 | prompt processing progress, n_tokens = 1242, batch.n_tokens = 459, progress = 0.950995
slot update_slots: id  0 | task 65 | n_tokens = 1242, memory_seq_rm [1242, end)
slot update_slots: id  0 | task 65 | prompt processing progress, n_tokens = 1306, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 65 | prompt done, n_tokens = 1306, batch.n_tokens = 64
slot init_sampler: id  0 | task 65 | init sampler, took 0.28 ms, tokens: text = 1306, total = 1306
slot update_slots: id  0 | task 65 | created context checkpoint 2 of 8 (pos_min = 474, pos_max = 1241, size = 18.009 MiB)
slot print_timing: id  0 | task 65 | 
prompt eval time =     563.51 ms /   523 tokens (    1.08 ms per token,   928.11 tokens per second)
       eval time =     971.08 ms /    43 tokens (   22.58 ms per token,    44.28 tokens per second)
      total time =    1534.59 ms /   566 tokens
slot      release: id  0 | task 65 | stop processing: n_tokens = 1348, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.402 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 110 | processing task, is_child = 0
slot update_slots: id  0 | task 110 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3280
slot update_slots: id  0 | task 110 | n_tokens = 1317, memory_seq_rm [1317, end)
slot update_slots: id  0 | task 110 | prompt processing progress, n_tokens = 3216, batch.n_tokens = 1899, progress = 0.980488
slot update_slots: id  0 | task 110 | n_tokens = 3216, memory_seq_rm [3216, end)
slot update_slots: id  0 | task 110 | prompt processing progress, n_tokens = 3280, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 110 | prompt done, n_tokens = 3280, batch.n_tokens = 64
slot init_sampler: id  0 | task 110 | init sampler, took 0.47 ms, tokens: text = 3280, total = 3280
slot update_slots: id  0 | task 110 | created context checkpoint 3 of 8 (pos_min = 2448, pos_max = 3215, size = 18.009 MiB)
slot print_timing: id  0 | task 110 | 
prompt eval time =    1935.64 ms /  1963 tokens (    0.99 ms per token,  1014.13 tokens per second)
       eval time =     636.46 ms /    28 tokens (   22.73 ms per token,    43.99 tokens per second)
      total time =    2572.10 ms /  1991 tokens
slot      release: id  0 | task 110 | stop processing: n_tokens = 3307, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.903 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 140 | processing task, is_child = 0
slot update_slots: id  0 | task 140 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3631
slot update_slots: id  0 | task 140 | n_tokens = 3280, memory_seq_rm [3280, end)
slot update_slots: id  0 | task 140 | prompt processing progress, n_tokens = 3567, batch.n_tokens = 287, progress = 0.982374
slot update_slots: id  0 | task 140 | n_tokens = 3567, memory_seq_rm [3567, end)
slot update_slots: id  0 | task 140 | prompt processing progress, n_tokens = 3631, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 140 | prompt done, n_tokens = 3631, batch.n_tokens = 64
slot init_sampler: id  0 | task 140 | init sampler, took 0.54 ms, tokens: text = 3631, total = 3631
slot update_slots: id  0 | task 140 | created context checkpoint 4 of 8 (pos_min = 2799, pos_max = 3566, size = 18.009 MiB)
slot print_timing: id  0 | task 140 | 
prompt eval time =     463.87 ms /   351 tokens (    1.32 ms per token,   756.68 tokens per second)
       eval time =     914.34 ms /    40 tokens (   22.86 ms per token,    43.75 tokens per second)
      total time =    1378.21 ms /   391 tokens
slot      release: id  0 | task 140 | stop processing: n_tokens = 3670, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.854 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 182 | processing task, is_child = 0
slot update_slots: id  0 | task 182 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4264
slot update_slots: id  0 | task 182 | n_tokens = 3643, memory_seq_rm [3643, end)
slot update_slots: id  0 | task 182 | prompt processing progress, n_tokens = 4200, batch.n_tokens = 557, progress = 0.984991
slot update_slots: id  0 | task 182 | n_tokens = 4200, memory_seq_rm [4200, end)
slot update_slots: id  0 | task 182 | prompt processing progress, n_tokens = 4264, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 182 | prompt done, n_tokens = 4264, batch.n_tokens = 64
slot init_sampler: id  0 | task 182 | init sampler, took 1.86 ms, tokens: text = 4264, total = 4264
slot update_slots: id  0 | task 182 | created context checkpoint 5 of 8 (pos_min = 3432, pos_max = 4199, size = 18.009 MiB)
slot print_timing: id  0 | task 182 | 
prompt eval time =     768.69 ms /   621 tokens (    1.24 ms per token,   807.87 tokens per second)
       eval time =     923.91 ms /    39 tokens (   23.69 ms per token,    42.21 tokens per second)
      total time =    1692.60 ms /   660 tokens
slot      release: id  0 | task 182 | stop processing: n_tokens = 4302, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 223 | processing task, is_child = 0
slot update_slots: id  0 | task 223 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4412
slot update_slots: id  0 | task 223 | n_tokens = 4276, memory_seq_rm [4276, end)
slot update_slots: id  0 | task 223 | prompt processing progress, n_tokens = 4348, batch.n_tokens = 72, progress = 0.985494
slot update_slots: id  0 | task 223 | n_tokens = 4348, memory_seq_rm [4348, end)
slot update_slots: id  0 | task 223 | prompt processing progress, n_tokens = 4412, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 223 | prompt done, n_tokens = 4412, batch.n_tokens = 64
slot init_sampler: id  0 | task 223 | init sampler, took 0.88 ms, tokens: text = 4412, total = 4412
slot update_slots: id  0 | task 223 | created context checkpoint 6 of 8 (pos_min = 3580, pos_max = 4347, size = 18.009 MiB)
slot print_timing: id  0 | task 223 | 
prompt eval time =     345.08 ms /   136 tokens (    2.54 ms per token,   394.11 tokens per second)
       eval time =     886.59 ms /    37 tokens (   23.96 ms per token,    41.73 tokens per second)
      total time =    1231.67 ms /   173 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 223 | stop processing: n_tokens = 4448, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.803 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 262 | processing task, is_child = 0
slot update_slots: id  0 | task 262 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5509
slot update_slots: id  0 | task 262 | n_tokens = 4422, memory_seq_rm [4422, end)
slot update_slots: id  0 | task 262 | prompt processing progress, n_tokens = 5445, batch.n_tokens = 1023, progress = 0.988383
slot update_slots: id  0 | task 262 | n_tokens = 5445, memory_seq_rm [5445, end)
slot update_slots: id  0 | task 262 | prompt processing progress, n_tokens = 5509, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 262 | prompt done, n_tokens = 5509, batch.n_tokens = 64
slot init_sampler: id  0 | task 262 | init sampler, took 0.78 ms, tokens: text = 5509, total = 5509
slot update_slots: id  0 | task 262 | created context checkpoint 7 of 8 (pos_min = 4677, pos_max = 5444, size = 18.009 MiB)
slot print_timing: id  0 | task 262 | 
prompt eval time =    1123.96 ms /  1087 tokens (    1.03 ms per token,   967.11 tokens per second)
       eval time =    1059.06 ms /    46 tokens (   23.02 ms per token,    43.43 tokens per second)
      total time =    2183.02 ms /  1133 tokens
slot      release: id  0 | task 262 | stop processing: n_tokens = 5554, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 310 | processing task, is_child = 0
slot update_slots: id  0 | task 310 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5583
slot update_slots: id  0 | task 310 | n_tokens = 5526, memory_seq_rm [5526, end)
slot update_slots: id  0 | task 310 | prompt processing progress, n_tokens = 5583, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  0 | task 310 | prompt done, n_tokens = 5583, batch.n_tokens = 57
slot init_sampler: id  0 | task 310 | init sampler, took 1.08 ms, tokens: text = 5583, total = 5583
slot update_slots: id  0 | task 310 | created context checkpoint 8 of 8 (pos_min = 4786, pos_max = 5525, size = 17.353 MiB)
slot print_timing: id  0 | task 310 | 
prompt eval time =     266.34 ms /    57 tokens (    4.67 ms per token,   214.01 tokens per second)
       eval time =    1080.15 ms /    45 tokens (   24.00 ms per token,    41.66 tokens per second)
      total time =    1346.49 ms /   102 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 310 | stop processing: n_tokens = 5627, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 356 | processing task, is_child = 0
slot update_slots: id  0 | task 356 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5820
slot update_slots: id  0 | task 356 | n_tokens = 5601, memory_seq_rm [5601, end)
slot update_slots: id  0 | task 356 | prompt processing progress, n_tokens = 5756, batch.n_tokens = 155, progress = 0.989003
slot update_slots: id  0 | task 356 | n_tokens = 5756, memory_seq_rm [5756, end)
slot update_slots: id  0 | task 356 | prompt processing progress, n_tokens = 5820, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 356 | prompt done, n_tokens = 5820, batch.n_tokens = 64
slot init_sampler: id  0 | task 356 | init sampler, took 1.14 ms, tokens: text = 5820, total = 5820
slot update_slots: id  0 | task 356 | erasing old context checkpoint (pos_min = 0, pos_max = 675, size = 15.852 MiB)
slot update_slots: id  0 | task 356 | created context checkpoint 8 of 8 (pos_min = 4988, pos_max = 5755, size = 18.009 MiB)
slot print_timing: id  0 | task 356 | 
prompt eval time =     397.54 ms /   219 tokens (    1.82 ms per token,   550.88 tokens per second)
       eval time =    1000.14 ms /    41 tokens (   24.39 ms per token,    40.99 tokens per second)
      total time =    1397.68 ms /   260 tokens
slot      release: id  0 | task 356 | stop processing: n_tokens = 5860, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.746 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 399 | processing task, is_child = 0
slot update_slots: id  0 | task 399 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7817
slot update_slots: id  0 | task 399 | n_tokens = 5830, memory_seq_rm [5830, end)
slot update_slots: id  0 | task 399 | prompt processing progress, n_tokens = 7753, batch.n_tokens = 1923, progress = 0.991813
slot update_slots: id  0 | task 399 | n_tokens = 7753, memory_seq_rm [7753, end)
slot update_slots: id  0 | task 399 | prompt processing progress, n_tokens = 7817, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 399 | prompt done, n_tokens = 7817, batch.n_tokens = 64
slot init_sampler: id  0 | task 399 | init sampler, took 1.49 ms, tokens: text = 7817, total = 7817
slot update_slots: id  0 | task 399 | erasing old context checkpoint (pos_min = 474, pos_max = 1241, size = 18.009 MiB)
slot update_slots: id  0 | task 399 | created context checkpoint 8 of 8 (pos_min = 6985, pos_max = 7752, size = 18.009 MiB)
slot print_timing: id  0 | task 399 | 
prompt eval time =    2039.88 ms /  1987 tokens (    1.03 ms per token,   974.08 tokens per second)
       eval time =    1180.38 ms /    48 tokens (   24.59 ms per token,    40.66 tokens per second)
      total time =    3220.26 ms /  2035 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 399 | stop processing: n_tokens = 7864, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 449 | processing task, is_child = 0
slot update_slots: id  0 | task 449 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8042
slot update_slots: id  0 | task 449 | n_tokens = 7839, memory_seq_rm [7839, end)
slot update_slots: id  0 | task 449 | prompt processing progress, n_tokens = 7978, batch.n_tokens = 139, progress = 0.992042
slot update_slots: id  0 | task 449 | n_tokens = 7978, memory_seq_rm [7978, end)
slot update_slots: id  0 | task 449 | prompt processing progress, n_tokens = 8042, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 449 | prompt done, n_tokens = 8042, batch.n_tokens = 64
slot init_sampler: id  0 | task 449 | init sampler, took 1.48 ms, tokens: text = 8042, total = 8042
slot update_slots: id  0 | task 449 | erasing old context checkpoint (pos_min = 2448, pos_max = 3215, size = 18.009 MiB)
slot update_slots: id  0 | task 449 | created context checkpoint 8 of 8 (pos_min = 7210, pos_max = 7977, size = 18.009 MiB)
slot print_timing: id  0 | task 449 | 
prompt eval time =     414.97 ms /   203 tokens (    2.04 ms per token,   489.19 tokens per second)
       eval time =     873.84 ms /    28 tokens (   31.21 ms per token,    32.04 tokens per second)
      total time =    1288.81 ms /   231 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 449 | stop processing: n_tokens = 8069, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.935 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 479 | processing task, is_child = 0
slot update_slots: id  0 | task 479 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8604
slot update_slots: id  0 | task 479 | n_tokens = 8042, memory_seq_rm [8042, end)
slot update_slots: id  0 | task 479 | prompt processing progress, n_tokens = 8540, batch.n_tokens = 498, progress = 0.992562
slot update_slots: id  0 | task 479 | n_tokens = 8540, memory_seq_rm [8540, end)
slot update_slots: id  0 | task 479 | prompt processing progress, n_tokens = 8604, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 479 | prompt done, n_tokens = 8604, batch.n_tokens = 64
slot init_sampler: id  0 | task 479 | init sampler, took 1.67 ms, tokens: text = 8604, total = 8604
slot update_slots: id  0 | task 479 | erasing old context checkpoint (pos_min = 2799, pos_max = 3566, size = 18.009 MiB)
slot update_slots: id  0 | task 479 | created context checkpoint 8 of 8 (pos_min = 7772, pos_max = 8539, size = 18.009 MiB)
slot print_timing: id  0 | task 479 | 
prompt eval time =     713.65 ms /   562 tokens (    1.27 ms per token,   787.51 tokens per second)
       eval time =    1028.82 ms /    39 tokens (   26.38 ms per token,    37.91 tokens per second)
      total time =    1742.47 ms /   601 tokens
slot      release: id  0 | task 479 | stop processing: n_tokens = 8642, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.901 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 520 | processing task, is_child = 0
slot update_slots: id  0 | task 520 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 9564
slot update_slots: id  0 | task 520 | n_tokens = 8615, memory_seq_rm [8615, end)
slot update_slots: id  0 | task 520 | prompt processing progress, n_tokens = 9500, batch.n_tokens = 885, progress = 0.993308
slot update_slots: id  0 | task 520 | n_tokens = 9500, memory_seq_rm [9500, end)
slot update_slots: id  0 | task 520 | prompt processing progress, n_tokens = 9564, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 520 | prompt done, n_tokens = 9564, batch.n_tokens = 64
slot init_sampler: id  0 | task 520 | init sampler, took 1.86 ms, tokens: text = 9564, total = 9564
slot update_slots: id  0 | task 520 | erasing old context checkpoint (pos_min = 3432, pos_max = 4199, size = 18.009 MiB)
slot update_slots: id  0 | task 520 | created context checkpoint 8 of 8 (pos_min = 8732, pos_max = 9499, size = 18.009 MiB)
slot print_timing: id  0 | task 520 | 
prompt eval time =    1164.34 ms /   949 tokens (    1.23 ms per token,   815.05 tokens per second)
       eval time =  109340.42 ms /  3958 tokens (   27.63 ms per token,    36.20 tokens per second)
      total time =  110504.76 ms /  4907 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 520 | stop processing: n_tokens = 13521, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 968366755
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 13521, total state size = 335.063 MiB
srv          load:  - looking for better prompt, base f_keep = 0.004, sim = 0.007
srv        update:  - cache state: 1 prompts, 478.479 MiB (limits: 8192.000 MiB, 40192 tokens, 231492 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv  get_availabl: prompt cache update took 929.06 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 4480 | processing task, is_child = 0
slot update_slots: id  0 | task 4480 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8109
slot update_slots: id  0 | task 4480 | n_past = 59, slot.prompt.tokens.size() = 13521, seq_id = 0, pos_min = 12753, n_swa = 128
slot update_slots: id  0 | task 4480 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 3580, pos_max = 4347, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 4677, pos_max = 5444, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 4786, pos_max = 5525, n_swa = 128, size = 17.353 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 4988, pos_max = 5755, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 6985, pos_max = 7752, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 7210, pos_max = 7977, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 7772, pos_max = 8539, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 8732, pos_max = 9499, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 4480 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.252559
slot update_slots: id  0 | task 4480 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 4480 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.505118
slot update_slots: id  0 | task 4480 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 4480 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.757677
slot update_slots: id  0 | task 4480 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 4480 | prompt processing progress, n_tokens = 8045, batch.n_tokens = 1901, progress = 0.992108
slot update_slots: id  0 | task 4480 | n_tokens = 8045, memory_seq_rm [8045, end)
slot update_slots: id  0 | task 4480 | prompt processing progress, n_tokens = 8109, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 4480 | prompt done, n_tokens = 8109, batch.n_tokens = 64
slot init_sampler: id  0 | task 4480 | init sampler, took 1.62 ms, tokens: text = 8109, total = 8109
slot update_slots: id  0 | task 4480 | created context checkpoint 1 of 8 (pos_min = 7277, pos_max = 8044, size = 18.009 MiB)
slot print_timing: id  0 | task 4480 | 
prompt eval time =    8384.61 ms /  8109 tokens (    1.03 ms per token,   967.13 tokens per second)
       eval time =  116798.06 ms /  4096 tokens (   28.52 ms per token,    35.07 tokens per second)
      total time =  125182.67 ms / 12205 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 4480 | stop processing: n_tokens = 12204, truncated = 0
srv  update_slots: all slots are idle
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 1112026055
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 12204, total state size = 304.180 MiB
srv          load:  - looking for better prompt, base f_keep = 0.005, sim = 0.072
srv        update:  - cache state: 2 prompts, 800.668 MiB (limits: 8192.000 MiB, 40192 tokens, 263204 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv  get_availabl: prompt cache update took 333.32 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8581 | processing task, is_child = 0
slot update_slots: id  0 | task 8581 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 822
slot update_slots: id  0 | task 8581 | n_past = 59, slot.prompt.tokens.size() = 12204, seq_id = 0, pos_min = 11436, n_swa = 128
slot update_slots: id  0 | task 8581 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 8581 | erased invalidated context checkpoint (pos_min = 7277, pos_max = 8044, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 8581 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 8581 | prompt processing progress, n_tokens = 758, batch.n_tokens = 758, progress = 0.922141
slot update_slots: id  0 | task 8581 | n_tokens = 758, memory_seq_rm [758, end)
slot update_slots: id  0 | task 8581 | prompt processing progress, n_tokens = 822, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8581 | prompt done, n_tokens = 822, batch.n_tokens = 64
slot init_sampler: id  0 | task 8581 | init sampler, took 0.16 ms, tokens: text = 822, total = 822
slot update_slots: id  0 | task 8581 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 757, size = 17.775 MiB)
slot print_timing: id  0 | task 8581 | 
prompt eval time =     987.10 ms /   822 tokens (    1.20 ms per token,   832.74 tokens per second)
       eval time =    3251.90 ms /   141 tokens (   23.06 ms per token,    43.36 tokens per second)
      total time =    4238.99 ms /   963 tokens
slot      release: id  0 | task 8581 | stop processing: n_tokens = 962, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.643 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8724 | processing task, is_child = 0
slot update_slots: id  0 | task 8724 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1466
slot update_slots: id  0 | task 8724 | n_tokens = 943, memory_seq_rm [943, end)
slot update_slots: id  0 | task 8724 | prompt processing progress, n_tokens = 1402, batch.n_tokens = 459, progress = 0.956344
slot update_slots: id  0 | task 8724 | n_tokens = 1402, memory_seq_rm [1402, end)
slot update_slots: id  0 | task 8724 | prompt processing progress, n_tokens = 1466, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8724 | prompt done, n_tokens = 1466, batch.n_tokens = 64
slot init_sampler: id  0 | task 8724 | init sampler, took 0.27 ms, tokens: text = 1466, total = 1466
slot update_slots: id  0 | task 8724 | created context checkpoint 2 of 8 (pos_min = 634, pos_max = 1401, size = 18.009 MiB)
slot print_timing: id  0 | task 8724 | 
prompt eval time =     591.61 ms /   523 tokens (    1.13 ms per token,   884.02 tokens per second)
       eval time =     926.09 ms /    39 tokens (   23.75 ms per token,    42.11 tokens per second)
      total time =    1517.70 ms /   562 tokens
slot      release: id  0 | task 8724 | stop processing: n_tokens = 1504, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8765 | processing task, is_child = 0
slot update_slots: id  0 | task 8765 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1559
slot update_slots: id  0 | task 8765 | n_tokens = 1477, memory_seq_rm [1477, end)
slot update_slots: id  0 | task 8765 | prompt processing progress, n_tokens = 1495, batch.n_tokens = 18, progress = 0.958948
slot update_slots: id  0 | task 8765 | n_tokens = 1495, memory_seq_rm [1495, end)
slot update_slots: id  0 | task 8765 | prompt processing progress, n_tokens = 1559, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8765 | prompt done, n_tokens = 1559, batch.n_tokens = 64
slot init_sampler: id  0 | task 8765 | init sampler, took 0.33 ms, tokens: text = 1559, total = 1559
slot update_slots: id  0 | task 8765 | created context checkpoint 3 of 8 (pos_min = 736, pos_max = 1494, size = 17.798 MiB)
slot print_timing: id  0 | task 8765 | 
prompt eval time =     316.37 ms /    82 tokens (    3.86 ms per token,   259.19 tokens per second)
       eval time =    3209.01 ms /   129 tokens (   24.88 ms per token,    40.20 tokens per second)
      total time =    3525.38 ms /   211 tokens
slot      release: id  0 | task 8765 | stop processing: n_tokens = 1687, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.458 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8896 | processing task, is_child = 0
slot update_slots: id  0 | task 8896 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3619
slot update_slots: id  0 | task 8896 | n_tokens = 1656, memory_seq_rm [1656, end)
slot update_slots: id  0 | task 8896 | prompt processing progress, n_tokens = 3555, batch.n_tokens = 1899, progress = 0.982316
slot update_slots: id  0 | task 8896 | n_tokens = 3555, memory_seq_rm [3555, end)
slot update_slots: id  0 | task 8896 | prompt processing progress, n_tokens = 3619, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8896 | prompt done, n_tokens = 3619, batch.n_tokens = 64
slot init_sampler: id  0 | task 8896 | init sampler, took 0.68 ms, tokens: text = 3619, total = 3619
slot update_slots: id  0 | task 8896 | created context checkpoint 4 of 8 (pos_min = 2787, pos_max = 3554, size = 18.009 MiB)
slot print_timing: id  0 | task 8896 | 
prompt eval time =    2046.18 ms /  1963 tokens (    1.04 ms per token,   959.35 tokens per second)
       eval time =     970.28 ms /    40 tokens (   24.26 ms per token,    41.23 tokens per second)
      total time =    3016.46 ms /  2003 tokens
slot      release: id  0 | task 8896 | stop processing: n_tokens = 3658, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.912 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8938 | processing task, is_child = 0
slot update_slots: id  0 | task 8938 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3978
slot update_slots: id  0 | task 8938 | n_tokens = 3627, memory_seq_rm [3627, end)
slot update_slots: id  0 | task 8938 | prompt processing progress, n_tokens = 3914, batch.n_tokens = 287, progress = 0.983912
slot update_slots: id  0 | task 8938 | n_tokens = 3914, memory_seq_rm [3914, end)
slot update_slots: id  0 | task 8938 | prompt processing progress, n_tokens = 3978, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8938 | prompt done, n_tokens = 3978, batch.n_tokens = 64
slot init_sampler: id  0 | task 8938 | init sampler, took 0.79 ms, tokens: text = 3978, total = 3978
slot update_slots: id  0 | task 8938 | created context checkpoint 5 of 8 (pos_min = 3146, pos_max = 3913, size = 18.009 MiB)
slot print_timing: id  0 | task 8938 | 
prompt eval time =     494.36 ms /   351 tokens (    1.41 ms per token,   710.02 tokens per second)
       eval time =    3653.84 ms /   153 tokens (   23.88 ms per token,    41.87 tokens per second)
      total time =    4148.19 ms /   504 tokens
slot      release: id  0 | task 8938 | stop processing: n_tokens = 4130, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9093 | processing task, is_child = 0
slot update_slots: id  0 | task 9093 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4162
slot update_slots: id  0 | task 9093 | n_tokens = 4106, memory_seq_rm [4106, end)
slot update_slots: id  0 | task 9093 | prompt processing progress, n_tokens = 4162, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  0 | task 9093 | prompt done, n_tokens = 4162, batch.n_tokens = 56
slot init_sampler: id  0 | task 9093 | init sampler, took 0.81 ms, tokens: text = 4162, total = 4162
slot update_slots: id  0 | task 9093 | created context checkpoint 6 of 8 (pos_min = 3362, pos_max = 4105, size = 17.446 MiB)
slot print_timing: id  0 | task 9093 | 
prompt eval time =     255.67 ms /    56 tokens (    4.57 ms per token,   219.03 tokens per second)
       eval time =    1177.93 ms /    48 tokens (   24.54 ms per token,    40.75 tokens per second)
      total time =    1433.60 ms /   104 tokens
slot      release: id  0 | task 9093 | stop processing: n_tokens = 4209, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9142 | processing task, is_child = 0
slot update_slots: id  0 | task 9142 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4350
slot update_slots: id  0 | task 9142 | n_tokens = 4182, memory_seq_rm [4182, end)
slot update_slots: id  0 | task 9142 | prompt processing progress, n_tokens = 4286, batch.n_tokens = 104, progress = 0.985287
slot update_slots: id  0 | task 9142 | n_tokens = 4286, memory_seq_rm [4286, end)
slot update_slots: id  0 | task 9142 | prompt processing progress, n_tokens = 4350, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9142 | prompt done, n_tokens = 4350, batch.n_tokens = 64
slot init_sampler: id  0 | task 9142 | init sampler, took 0.82 ms, tokens: text = 4350, total = 4350
slot update_slots: id  0 | task 9142 | created context checkpoint 7 of 8 (pos_min = 3518, pos_max = 4285, size = 18.009 MiB)
slot print_timing: id  0 | task 9142 | 
prompt eval time =     418.19 ms /   168 tokens (    2.49 ms per token,   401.73 tokens per second)
       eval time =    1061.07 ms /    36 tokens (   29.47 ms per token,    33.93 tokens per second)
      total time =    1479.26 ms /   204 tokens
slot      release: id  0 | task 9142 | stop processing: n_tokens = 4385, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9180 | processing task, is_child = 0
slot update_slots: id  0 | task 9180 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4450
slot update_slots: id  0 | task 9180 | n_tokens = 4360, memory_seq_rm [4360, end)
slot update_slots: id  0 | task 9180 | prompt processing progress, n_tokens = 4386, batch.n_tokens = 26, progress = 0.985618
slot update_slots: id  0 | task 9180 | n_tokens = 4386, memory_seq_rm [4386, end)
slot update_slots: id  0 | task 9180 | prompt processing progress, n_tokens = 4450, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9180 | prompt done, n_tokens = 4450, batch.n_tokens = 64
slot init_sampler: id  0 | task 9180 | init sampler, took 0.90 ms, tokens: text = 4450, total = 4450
slot update_slots: id  0 | task 9180 | created context checkpoint 8 of 8 (pos_min = 3618, pos_max = 4385, size = 18.009 MiB)
slot print_timing: id  0 | task 9180 | 
prompt eval time =     348.40 ms /    90 tokens (    3.87 ms per token,   258.32 tokens per second)
       eval time =     971.52 ms /    40 tokens (   24.29 ms per token,    41.17 tokens per second)
      total time =    1319.92 ms /   130 tokens
slot      release: id  0 | task 9180 | stop processing: n_tokens = 4489, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.875 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9222 | processing task, is_child = 0
slot update_slots: id  0 | task 9222 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5100
slot update_slots: id  0 | task 9222 | n_tokens = 4462, memory_seq_rm [4462, end)
slot update_slots: id  0 | task 9222 | prompt processing progress, n_tokens = 5036, batch.n_tokens = 574, progress = 0.987451
slot update_slots: id  0 | task 9222 | n_tokens = 5036, memory_seq_rm [5036, end)
slot update_slots: id  0 | task 9222 | prompt processing progress, n_tokens = 5100, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9222 | prompt done, n_tokens = 5100, batch.n_tokens = 64
slot init_sampler: id  0 | task 9222 | init sampler, took 0.98 ms, tokens: text = 5100, total = 5100
slot update_slots: id  0 | task 9222 | erasing old context checkpoint (pos_min = 0, pos_max = 757, size = 17.775 MiB)
slot update_slots: id  0 | task 9222 | created context checkpoint 8 of 8 (pos_min = 4268, pos_max = 5035, size = 18.009 MiB)
slot print_timing: id  0 | task 9222 | 
prompt eval time =     861.73 ms /   638 tokens (    1.35 ms per token,   740.37 tokens per second)
       eval time =    1460.56 ms /    56 tokens (   26.08 ms per token,    38.34 tokens per second)
      total time =    2322.30 ms /   694 tokens
slot      release: id  0 | task 9222 | stop processing: n_tokens = 5155, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.740 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9280 | processing task, is_child = 0
slot update_slots: id  0 | task 9280 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6929
slot update_slots: id  0 | task 9280 | n_tokens = 5129, memory_seq_rm [5129, end)
slot update_slots: id  0 | task 9280 | prompt processing progress, n_tokens = 6865, batch.n_tokens = 1736, progress = 0.990763
slot update_slots: id  0 | task 9280 | n_tokens = 6865, memory_seq_rm [6865, end)
slot update_slots: id  0 | task 9280 | prompt processing progress, n_tokens = 6929, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9280 | prompt done, n_tokens = 6929, batch.n_tokens = 64
slot init_sampler: id  0 | task 9280 | init sampler, took 4.54 ms, tokens: text = 6929, total = 6929
slot update_slots: id  0 | task 9280 | erasing old context checkpoint (pos_min = 634, pos_max = 1401, size = 18.009 MiB)
slot update_slots: id  0 | task 9280 | created context checkpoint 8 of 8 (pos_min = 6097, pos_max = 6864, size = 18.009 MiB)
slot print_timing: id  0 | task 9280 | 
prompt eval time =    2135.56 ms /  1800 tokens (    1.19 ms per token,   842.87 tokens per second)
       eval time =    1548.15 ms /    55 tokens (   28.15 ms per token,    35.53 tokens per second)
      total time =    3683.71 ms /  1855 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 9280 | stop processing: n_tokens = 6983, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.918 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9337 | processing task, is_child = 0
slot update_slots: id  0 | task 9337 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7577
slot update_slots: id  0 | task 9337 | n_tokens = 6956, memory_seq_rm [6956, end)
slot update_slots: id  0 | task 9337 | prompt processing progress, n_tokens = 7513, batch.n_tokens = 557, progress = 0.991553
slot update_slots: id  0 | task 9337 | n_tokens = 7513, memory_seq_rm [7513, end)
slot update_slots: id  0 | task 9337 | prompt processing progress, n_tokens = 7577, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9337 | prompt done, n_tokens = 7577, batch.n_tokens = 64
slot init_sampler: id  0 | task 9337 | init sampler, took 1.47 ms, tokens: text = 7577, total = 7577
slot update_slots: id  0 | task 9337 | erasing old context checkpoint (pos_min = 736, pos_max = 1494, size = 17.798 MiB)
slot update_slots: id  0 | task 9337 | created context checkpoint 8 of 8 (pos_min = 6745, pos_max = 7512, size = 18.009 MiB)
slot print_timing: id  0 | task 9337 | 
prompt eval time =     879.03 ms /   621 tokens (    1.42 ms per token,   706.46 tokens per second)
       eval time =    1044.89 ms /    39 tokens (   26.79 ms per token,    37.32 tokens per second)
      total time =    1923.92 ms /   660 tokens
slot      release: id  0 | task 9337 | stop processing: n_tokens = 7615, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9378 | processing task, is_child = 0
slot update_slots: id  0 | task 9378 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7659
slot update_slots: id  0 | task 9378 | n_tokens = 7591, memory_seq_rm [7591, end)
slot update_slots: id  0 | task 9378 | prompt processing progress, n_tokens = 7595, batch.n_tokens = 4, progress = 0.991644
slot update_slots: id  0 | task 9378 | n_tokens = 7595, memory_seq_rm [7595, end)
slot update_slots: id  0 | task 9378 | prompt processing progress, n_tokens = 7659, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9378 | prompt done, n_tokens = 7659, batch.n_tokens = 64
slot init_sampler: id  0 | task 9378 | init sampler, took 2.61 ms, tokens: text = 7659, total = 7659
slot update_slots: id  0 | task 9378 | erasing old context checkpoint (pos_min = 2787, pos_max = 3554, size = 18.009 MiB)
slot update_slots: id  0 | task 9378 | created context checkpoint 8 of 8 (pos_min = 6847, pos_max = 7594, size = 17.540 MiB)
slot print_timing: id  0 | task 9378 | 
prompt eval time =     240.73 ms /    68 tokens (    3.54 ms per token,   282.48 tokens per second)
       eval time =    1233.46 ms /    49 tokens (   25.17 ms per token,    39.73 tokens per second)
      total time =    1474.18 ms /   117 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 9378 | stop processing: n_tokens = 7707, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9429 | processing task, is_child = 0
slot update_slots: id  0 | task 9429 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7753
slot update_slots: id  0 | task 9429 | n_tokens = 7682, memory_seq_rm [7682, end)
slot update_slots: id  0 | task 9429 | prompt processing progress, n_tokens = 7689, batch.n_tokens = 7, progress = 0.991745
slot update_slots: id  0 | task 9429 | n_tokens = 7689, memory_seq_rm [7689, end)
slot update_slots: id  0 | task 9429 | prompt processing progress, n_tokens = 7753, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9429 | prompt done, n_tokens = 7753, batch.n_tokens = 64
slot init_sampler: id  0 | task 9429 | init sampler, took 3.15 ms, tokens: text = 7753, total = 7753
slot update_slots: id  0 | task 9429 | erasing old context checkpoint (pos_min = 3146, pos_max = 3913, size = 18.009 MiB)
slot update_slots: id  0 | task 9429 | created context checkpoint 8 of 8 (pos_min = 6939, pos_max = 7688, size = 17.587 MiB)
slot print_timing: id  0 | task 9429 | 
prompt eval time =     271.52 ms /    71 tokens (    3.82 ms per token,   261.49 tokens per second)
       eval time =     893.38 ms /    32 tokens (   27.92 ms per token,    35.82 tokens per second)
      total time =    1164.90 ms /   103 tokens
slot      release: id  0 | task 9429 | stop processing: n_tokens = 7784, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9463 | processing task, is_child = 0
slot update_slots: id  0 | task 9463 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7821
slot update_slots: id  0 | task 9463 | n_tokens = 7769, memory_seq_rm [7769, end)
slot update_slots: id  0 | task 9463 | prompt processing progress, n_tokens = 7821, batch.n_tokens = 52, progress = 1.000000
slot update_slots: id  0 | task 9463 | prompt done, n_tokens = 7821, batch.n_tokens = 52
slot init_sampler: id  0 | task 9463 | init sampler, took 1.48 ms, tokens: text = 7821, total = 7821
slot update_slots: id  0 | task 9463 | erasing old context checkpoint (pos_min = 3362, pos_max = 4105, size = 17.446 MiB)
slot update_slots: id  0 | task 9463 | created context checkpoint 8 of 8 (pos_min = 7016, pos_max = 7768, size = 17.657 MiB)
slot print_timing: id  0 | task 9463 | 
prompt eval time =     185.83 ms /    52 tokens (    3.57 ms per token,   279.83 tokens per second)
       eval time =    1183.59 ms /    45 tokens (   26.30 ms per token,    38.02 tokens per second)
      total time =    1369.41 ms /    97 tokens
slot      release: id  0 | task 9463 | stop processing: n_tokens = 7865, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9509 | processing task, is_child = 0
slot update_slots: id  0 | task 9509 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7913
slot update_slots: id  0 | task 9509 | n_tokens = 7838, memory_seq_rm [7838, end)
slot update_slots: id  0 | task 9509 | prompt processing progress, n_tokens = 7849, batch.n_tokens = 11, progress = 0.991912
slot update_slots: id  0 | task 9509 | n_tokens = 7849, memory_seq_rm [7849, end)
slot update_slots: id  0 | task 9509 | prompt processing progress, n_tokens = 7913, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9509 | prompt done, n_tokens = 7913, batch.n_tokens = 64
slot init_sampler: id  0 | task 9509 | init sampler, took 2.71 ms, tokens: text = 7913, total = 7913
slot update_slots: id  0 | task 9509 | erasing old context checkpoint (pos_min = 3518, pos_max = 4285, size = 18.009 MiB)
slot update_slots: id  0 | task 9509 | created context checkpoint 8 of 8 (pos_min = 7097, pos_max = 7848, size = 17.634 MiB)
slot print_timing: id  0 | task 9509 | 
prompt eval time =     262.36 ms /    75 tokens (    3.50 ms per token,   285.87 tokens per second)
       eval time =    1211.69 ms /    45 tokens (   26.93 ms per token,    37.14 tokens per second)
      total time =    1474.05 ms /   120 tokens
slot      release: id  0 | task 9509 | stop processing: n_tokens = 7957, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9556 | processing task, is_child = 0
slot update_slots: id  0 | task 9556 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8232
slot update_slots: id  0 | task 9556 | n_tokens = 7942, memory_seq_rm [7942, end)
slot update_slots: id  0 | task 9556 | prompt processing progress, n_tokens = 8168, batch.n_tokens = 226, progress = 0.992225
slot update_slots: id  0 | task 9556 | n_tokens = 8168, memory_seq_rm [8168, end)
slot update_slots: id  0 | task 9556 | prompt processing progress, n_tokens = 8232, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9556 | prompt done, n_tokens = 8232, batch.n_tokens = 64
slot init_sampler: id  0 | task 9556 | init sampler, took 3.59 ms, tokens: text = 8232, total = 8232
slot update_slots: id  0 | task 9556 | erasing old context checkpoint (pos_min = 3618, pos_max = 4385, size = 18.009 MiB)
slot update_slots: id  0 | task 9556 | created context checkpoint 8 of 8 (pos_min = 7400, pos_max = 8167, size = 18.009 MiB)
slot print_timing: id  0 | task 9556 | 
prompt eval time =     543.78 ms /   290 tokens (    1.88 ms per token,   533.30 tokens per second)
       eval time =    4874.77 ms /   174 tokens (   28.02 ms per token,    35.69 tokens per second)
      total time =    5418.56 ms /   464 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 9556 | stop processing: n_tokens = 8405, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9732 | processing task, is_child = 0
slot update_slots: id  0 | task 9732 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8559
slot update_slots: id  0 | task 9732 | n_tokens = 8378, memory_seq_rm [8378, end)
slot update_slots: id  0 | task 9732 | prompt processing progress, n_tokens = 8495, batch.n_tokens = 117, progress = 0.992522
slot update_slots: id  0 | task 9732 | n_tokens = 8495, memory_seq_rm [8495, end)
slot update_slots: id  0 | task 9732 | prompt processing progress, n_tokens = 8559, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9732 | prompt done, n_tokens = 8559, batch.n_tokens = 64
slot init_sampler: id  0 | task 9732 | init sampler, took 1.61 ms, tokens: text = 8559, total = 8559
slot update_slots: id  0 | task 9732 | erasing old context checkpoint (pos_min = 4268, pos_max = 5035, size = 18.009 MiB)
slot update_slots: id  0 | task 9732 | created context checkpoint 8 of 8 (pos_min = 7727, pos_max = 8494, size = 18.009 MiB)
slot print_timing: id  0 | task 9732 | 
prompt eval time =     491.43 ms /   181 tokens (    2.72 ms per token,   368.31 tokens per second)
       eval time =    1540.47 ms /    58 tokens (   26.56 ms per token,    37.65 tokens per second)
      total time =    2031.90 ms /   239 tokens
slot      release: id  0 | task 9732 | stop processing: n_tokens = 8616, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9792 | processing task, is_child = 0
slot update_slots: id  0 | task 9792 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8787
slot update_slots: id  0 | task 9792 | n_tokens = 8591, memory_seq_rm [8591, end)
slot update_slots: id  0 | task 9792 | prompt processing progress, n_tokens = 8723, batch.n_tokens = 132, progress = 0.992716
slot update_slots: id  0 | task 9792 | n_tokens = 8723, memory_seq_rm [8723, end)
slot update_slots: id  0 | task 9792 | prompt processing progress, n_tokens = 8787, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9792 | prompt done, n_tokens = 8787, batch.n_tokens = 64
slot init_sampler: id  0 | task 9792 | init sampler, took 3.91 ms, tokens: text = 8787, total = 8787
slot update_slots: id  0 | task 9792 | erasing old context checkpoint (pos_min = 6097, pos_max = 6864, size = 18.009 MiB)
slot update_slots: id  0 | task 9792 | created context checkpoint 8 of 8 (pos_min = 7955, pos_max = 8722, size = 18.009 MiB)
slot print_timing: id  0 | task 9792 | 
prompt eval time =     445.06 ms /   196 tokens (    2.27 ms per token,   440.39 tokens per second)
       eval time =    1162.34 ms /    42 tokens (   27.67 ms per token,    36.13 tokens per second)
      total time =    1607.39 ms /   238 tokens
slot      release: id  0 | task 9792 | stop processing: n_tokens = 8828, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.917 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9836 | processing task, is_child = 0
slot update_slots: id  0 | task 9836 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 9595
slot update_slots: id  0 | task 9836 | n_tokens = 8801, memory_seq_rm [8801, end)
slot update_slots: id  0 | task 9836 | prompt processing progress, n_tokens = 9531, batch.n_tokens = 730, progress = 0.993330
slot update_slots: id  0 | task 9836 | n_tokens = 9531, memory_seq_rm [9531, end)
slot update_slots: id  0 | task 9836 | prompt processing progress, n_tokens = 9595, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9836 | prompt done, n_tokens = 9595, batch.n_tokens = 64
slot init_sampler: id  0 | task 9836 | init sampler, took 1.88 ms, tokens: text = 9595, total = 9595
slot update_slots: id  0 | task 9836 | erasing old context checkpoint (pos_min = 6745, pos_max = 7512, size = 18.009 MiB)
slot update_slots: id  0 | task 9836 | created context checkpoint 8 of 8 (pos_min = 8763, pos_max = 9530, size = 18.009 MiB)
slot print_timing: id  0 | task 9836 | 
prompt eval time =    1127.89 ms /   794 tokens (    1.42 ms per token,   703.97 tokens per second)
       eval time =    1089.53 ms /    41 tokens (   26.57 ms per token,    37.63 tokens per second)
      total time =    2217.42 ms /   835 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 9836 | stop processing: n_tokens = 9635, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9879 | processing task, is_child = 0
slot update_slots: id  0 | task 9879 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 10303
slot update_slots: id  0 | task 9879 | n_tokens = 9608, memory_seq_rm [9608, end)
slot update_slots: id  0 | task 9879 | prompt processing progress, n_tokens = 10239, batch.n_tokens = 631, progress = 0.993788
slot update_slots: id  0 | task 9879 | n_tokens = 10239, memory_seq_rm [10239, end)
slot update_slots: id  0 | task 9879 | prompt processing progress, n_tokens = 10303, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9879 | prompt done, n_tokens = 10303, batch.n_tokens = 64
slot init_sampler: id  0 | task 9879 | init sampler, took 1.98 ms, tokens: text = 10303, total = 10303
slot update_slots: id  0 | task 9879 | erasing old context checkpoint (pos_min = 6847, pos_max = 7594, size = 17.540 MiB)
slot update_slots: id  0 | task 9879 | created context checkpoint 8 of 8 (pos_min = 9471, pos_max = 10238, size = 18.009 MiB)
slot print_timing: id  0 | task 9879 | 
prompt eval time =    1120.85 ms /   695 tokens (    1.61 ms per token,   620.06 tokens per second)
       eval time =   18484.38 ms /   656 tokens (   28.18 ms per token,    35.49 tokens per second)
      total time =   19605.23 ms /  1351 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 9879 | stop processing: n_tokens = 10958, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 1406763825
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 10958, total state size = 274.963 MiB
srv          load:  - looking for better prompt, base f_keep = 0.005, sim = 0.007
srv        update:  - cache state: 3 prompts, 1218.554 MiB (limits: 8192.000 MiB, 40192 tokens, 246609 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv  get_availabl: prompt cache update took 1109.25 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 10537 | processing task, is_child = 0
slot update_slots: id  0 | task 10537 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8501
slot update_slots: id  0 | task 10537 | n_past = 59, slot.prompt.tokens.size() = 10958, seq_id = 0, pos_min = 10190, n_swa = 128
slot update_slots: id  0 | task 10537 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 6939, pos_max = 7688, n_swa = 128, size = 17.587 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 7016, pos_max = 7768, n_swa = 128, size = 17.657 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 7097, pos_max = 7848, n_swa = 128, size = 17.634 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 7400, pos_max = 8167, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 7727, pos_max = 8494, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 7955, pos_max = 8722, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 8763, pos_max = 9530, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 9471, pos_max = 10238, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 10537 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.240913
slot update_slots: id  0 | task 10537 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.481826
slot update_slots: id  0 | task 10537 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.722739
slot update_slots: id  0 | task 10537 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.963651
slot update_slots: id  0 | task 10537 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 8437, batch.n_tokens = 245, progress = 0.992471
slot update_slots: id  0 | task 10537 | n_tokens = 8437, memory_seq_rm [8437, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 8501, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 10537 | prompt done, n_tokens = 8501, batch.n_tokens = 64
slot init_sampler: id  0 | task 10537 | init sampler, took 1.66 ms, tokens: text = 8501, total = 8501
slot update_slots: id  0 | task 10537 | created context checkpoint 1 of 8 (pos_min = 7669, pos_max = 8436, size = 18.009 MiB)
slot print_timing: id  0 | task 10537 | 
prompt eval time =    9812.42 ms /  8501 tokens (    1.15 ms per token,   866.35 tokens per second)
       eval time =   62380.58 ms /  1980 tokens (   31.51 ms per token,    31.74 tokens per second)
      total time =   72193.00 ms / 10481 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 10537 | stop processing: n_tokens = 10480, truncated = 0
srv  update_slots: all slots are idle
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 1481477090
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 10480, total state size = 263.754 MiB
srv          load:  - looking for better prompt, base f_keep = 0.006, sim = 0.081
srv        update:  - cache state: 4 prompts, 1500.318 MiB (limits: 8192.000 MiB, 40192 tokens, 257518 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv  get_availabl: prompt cache update took 403.87 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 12523 | processing task, is_child = 0
slot update_slots: id  0 | task 12523 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 729
slot update_slots: id  0 | task 12523 | n_past = 59, slot.prompt.tokens.size() = 10480, seq_id = 0, pos_min = 9712, n_swa = 128
slot update_slots: id  0 | task 12523 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 12523 | erased invalidated context checkpoint (pos_min = 7669, pos_max = 8436, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 12523 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 12523 | prompt processing progress, n_tokens = 665, batch.n_tokens = 665, progress = 0.912208
slot update_slots: id  0 | task 12523 | n_tokens = 665, memory_seq_rm [665, end)
slot update_slots: id  0 | task 12523 | prompt processing progress, n_tokens = 729, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 12523 | prompt done, n_tokens = 729, batch.n_tokens = 64
slot init_sampler: id  0 | task 12523 | init sampler, took 0.16 ms, tokens: text = 729, total = 729
slot update_slots: id  0 | task 12523 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 664, size = 15.594 MiB)
slot print_timing: id  0 | task 12523 | 
prompt eval time =     870.21 ms /   729 tokens (    1.19 ms per token,   837.73 tokens per second)
       eval time =    1130.69 ms /    49 tokens (   23.08 ms per token,    43.34 tokens per second)
      total time =    2000.89 ms /   778 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 12523 | stop processing: n_tokens = 777, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.592 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 12574 | processing task, is_child = 0
slot update_slots: id  0 | task 12574 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1281
slot update_slots: id  0 | task 12574 | n_tokens = 758, memory_seq_rm [758, end)
slot update_slots: id  0 | task 12574 | prompt processing progress, n_tokens = 1217, batch.n_tokens = 459, progress = 0.950039
slot update_slots: id  0 | task 12574 | n_tokens = 1217, memory_seq_rm [1217, end)
slot update_slots: id  0 | task 12574 | prompt processing progress, n_tokens = 1281, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 12574 | prompt done, n_tokens = 1281, batch.n_tokens = 64
slot init_sampler: id  0 | task 12574 | init sampler, took 0.26 ms, tokens: text = 1281, total = 1281
slot update_slots: id  0 | task 12574 | created context checkpoint 2 of 8 (pos_min = 459, pos_max = 1216, size = 17.775 MiB)
slot print_timing: id  0 | task 12574 | 
prompt eval time =     579.20 ms /   523 tokens (    1.11 ms per token,   902.97 tokens per second)
       eval time =    1302.48 ms /    47 tokens (   27.71 ms per token,    36.08 tokens per second)
      total time =    1881.68 ms /   570 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 12574 | stop processing: n_tokens = 1327, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.938 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 12623 | processing task, is_child = 0
slot update_slots: id  0 | task 12623 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1383
slot update_slots: id  0 | task 12623 | n_tokens = 1297, memory_seq_rm [1297, end)
slot update_slots: id  0 | task 12623 | prompt processing progress, n_tokens = 1319, batch.n_tokens = 22, progress = 0.953724
slot update_slots: id  0 | task 12623 | n_tokens = 1319, memory_seq_rm [1319, end)
slot update_slots: id  0 | task 12623 | prompt processing progress, n_tokens = 1383, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 12623 | prompt done, n_tokens = 1383, batch.n_tokens = 64
slot init_sampler: id  0 | task 12623 | init sampler, took 0.29 ms, tokens: text = 1383, total = 1383
slot update_slots: id  0 | task 12623 | created context checkpoint 3 of 8 (pos_min = 569, pos_max = 1318, size = 17.587 MiB)
slot print_timing: id  0 | task 12623 | 
prompt eval time =     356.50 ms /    86 tokens (    4.15 ms per token,   241.24 tokens per second)
       eval time =    1478.27 ms /    52 tokens (   28.43 ms per token,    35.18 tokens per second)
      total time =    1834.77 ms /   138 tokens
slot      release: id  0 | task 12623 | stop processing: n_tokens = 1434, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 12677 | processing task, is_child = 0
slot update_slots: id  0 | task 12677 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1490
slot update_slots: id  0 | task 12677 | n_tokens = 1406, memory_seq_rm [1406, end)
slot update_slots: id  0 | task 12677 | prompt processing progress, n_tokens = 1426, batch.n_tokens = 20, progress = 0.957047
slot update_slots: id  0 | task 12677 | n_tokens = 1426, memory_seq_rm [1426, end)
slot update_slots: id  0 | task 12677 | prompt processing progress, n_tokens = 1490, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 12677 | prompt done, n_tokens = 1490, batch.n_tokens = 64
slot init_sampler: id  0 | task 12677 | init sampler, took 0.28 ms, tokens: text = 1490, total = 1490
slot update_slots: id  0 | task 12677 | created context checkpoint 4 of 8 (pos_min = 676, pos_max = 1425, size = 17.587 MiB)
slot print_timing: id  0 | task 12677 | 
prompt eval time =     351.12 ms /    84 tokens (    4.18 ms per token,   239.23 tokens per second)
       eval time =    1731.99 ms /    71 tokens (   24.39 ms per token,    40.99 tokens per second)
      total time =    2083.12 ms /   155 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 12677 | stop processing: n_tokens = 1560, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.494 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 12750 | processing task, is_child = 0
slot update_slots: id  0 | task 12750 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3099
slot update_slots: id  0 | task 12750 | n_tokens = 1530, memory_seq_rm [1530, end)
slot update_slots: id  0 | task 12750 | prompt processing progress, n_tokens = 3035, batch.n_tokens = 1505, progress = 0.979348
slot update_slots: id  0 | task 12750 | n_tokens = 3035, memory_seq_rm [3035, end)
slot update_slots: id  0 | task 12750 | prompt processing progress, n_tokens = 3099, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 12750 | prompt done, n_tokens = 3099, batch.n_tokens = 64
slot init_sampler: id  0 | task 12750 | init sampler, took 0.59 ms, tokens: text = 3099, total = 3099
slot update_slots: id  0 | task 12750 | created context checkpoint 5 of 8 (pos_min = 2267, pos_max = 3034, size = 18.009 MiB)
slot print_timing: id  0 | task 12750 | 
prompt eval time =    1596.81 ms /  1569 tokens (    1.02 ms per token,   982.58 tokens per second)
       eval time =   75602.33 ms /  2732 tokens (   27.67 ms per token,    36.14 tokens per second)
      total time =   77199.15 ms /  4301 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 12750 | stop processing: n_tokens = 5830, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.137 (> 0.100 thold), f_keep = 0.125
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5830, total state size = 154.717 MiB
srv          load:  - looking for better prompt, base f_keep = 0.125, sim = 0.137
srv        update:  - cache state: 5 prompts, 1741.586 MiB (limits: 8192.000 MiB, 40192 tokens, 249266 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv  get_availabl: prompt cache update took 394.04 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 15484 | processing task, is_child = 0
slot update_slots: id  0 | task 15484 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5303
slot update_slots: id  0 | task 15484 | n_past = 729, slot.prompt.tokens.size() = 5830, seq_id = 0, pos_min = 5062, n_swa = 128
slot update_slots: id  0 | task 15484 | restored context checkpoint (pos_min = 569, pos_max = 1318, size = 17.587 MiB)
slot update_slots: id  0 | task 15484 | erased invalidated context checkpoint (pos_min = 676, pos_max = 1425, n_swa = 128, size = 17.587 MiB)
slot update_slots: id  0 | task 15484 | erased invalidated context checkpoint (pos_min = 2267, pos_max = 3034, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 15484 | n_tokens = 729, memory_seq_rm [729, end)
slot update_slots: id  0 | task 15484 | prompt processing progress, n_tokens = 2777, batch.n_tokens = 2048, progress = 0.523666
slot update_slots: id  0 | task 15484 | n_tokens = 2777, memory_seq_rm [2777, end)
slot update_slots: id  0 | task 15484 | prompt processing progress, n_tokens = 4825, batch.n_tokens = 2048, progress = 0.909862
slot update_slots: id  0 | task 15484 | n_tokens = 4825, memory_seq_rm [4825, end)
slot update_slots: id  0 | task 15484 | prompt processing progress, n_tokens = 5239, batch.n_tokens = 414, progress = 0.987931
slot update_slots: id  0 | task 15484 | n_tokens = 5239, memory_seq_rm [5239, end)
slot update_slots: id  0 | task 15484 | prompt processing progress, n_tokens = 5303, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 15484 | prompt done, n_tokens = 5303, batch.n_tokens = 64
slot init_sampler: id  0 | task 15484 | init sampler, took 1.02 ms, tokens: text = 5303, total = 5303
slot update_slots: id  0 | task 15484 | created context checkpoint 4 of 8 (pos_min = 4471, pos_max = 5238, size = 18.009 MiB)
slot print_timing: id  0 | task 15484 | 
prompt eval time =    4362.65 ms /  4574 tokens (    0.95 ms per token,  1048.44 tokens per second)
       eval time =    3600.28 ms /   144 tokens (   25.00 ms per token,    40.00 tokens per second)
      total time =    7962.93 ms /  4718 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 15484 | stop processing: n_tokens = 5446, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.775 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 15632 | processing task, is_child = 0
slot update_slots: id  0 | task 15632 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6985
slot update_slots: id  0 | task 15632 | n_tokens = 5416, memory_seq_rm [5416, end)
slot update_slots: id  0 | task 15632 | prompt processing progress, n_tokens = 6921, batch.n_tokens = 1505, progress = 0.990838
slot update_slots: id  0 | task 15632 | n_tokens = 6921, memory_seq_rm [6921, end)
slot update_slots: id  0 | task 15632 | prompt processing progress, n_tokens = 6985, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 15632 | prompt done, n_tokens = 6985, batch.n_tokens = 64
slot init_sampler: id  0 | task 15632 | init sampler, took 1.49 ms, tokens: text = 6985, total = 6985
slot update_slots: id  0 | task 15632 | created context checkpoint 5 of 8 (pos_min = 6153, pos_max = 6920, size = 18.009 MiB)
slot print_timing: id  0 | task 15632 | 
prompt eval time =    1683.84 ms /  1569 tokens (    1.07 ms per token,   931.80 tokens per second)
       eval time =   72952.80 ms /  2806 tokens (   26.00 ms per token,    38.46 tokens per second)
      total time =   74636.64 ms /  4375 tokens
slot      release: id  0 | task 15632 | stop processing: n_tokens = 9790, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 2117561613
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9790, total state size = 247.575 MiB
srv          load:  - looking for better prompt, base f_keep = 0.006, sim = 0.012
srv        update:  - cache state: 6 prompts, 2076.134 MiB (limits: 8192.000 MiB, 40192 tokens, 247728 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv  get_availabl: prompt cache update took 902.36 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18440 | processing task, is_child = 0
slot update_slots: id  0 | task 18440 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4904
slot update_slots: id  0 | task 18440 | n_past = 59, slot.prompt.tokens.size() = 9790, seq_id = 0, pos_min = 9022, n_swa = 128
slot update_slots: id  0 | task 18440 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 18440 | erased invalidated context checkpoint (pos_min = 459, pos_max = 1216, n_swa = 128, size = 17.775 MiB)
slot update_slots: id  0 | task 18440 | erased invalidated context checkpoint (pos_min = 569, pos_max = 1318, n_swa = 128, size = 17.587 MiB)
slot update_slots: id  0 | task 18440 | erased invalidated context checkpoint (pos_min = 4471, pos_max = 5238, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 18440 | erased invalidated context checkpoint (pos_min = 6153, pos_max = 6920, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 18440 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 18440 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.417618
slot update_slots: id  0 | task 18440 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 18440 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.835237
slot update_slots: id  0 | task 18440 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 18440 | prompt processing progress, n_tokens = 4840, batch.n_tokens = 744, progress = 0.986949
slot update_slots: id  0 | task 18440 | n_tokens = 4840, memory_seq_rm [4840, end)
slot update_slots: id  0 | task 18440 | prompt processing progress, n_tokens = 4904, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18440 | prompt done, n_tokens = 4904, batch.n_tokens = 64
slot init_sampler: id  0 | task 18440 | init sampler, took 1.50 ms, tokens: text = 4904, total = 4904
slot update_slots: id  0 | task 18440 | created context checkpoint 2 of 8 (pos_min = 4072, pos_max = 4839, size = 18.009 MiB)
slot print_timing: id  0 | task 18440 | 
prompt eval time =    5180.21 ms /  4904 tokens (    1.06 ms per token,   946.68 tokens per second)
       eval time =   31853.14 ms /  1186 tokens (   26.86 ms per token,    37.23 tokens per second)
      total time =   37033.35 ms /  6090 tokens
slot      release: id  0 | task 18440 | stop processing: n_tokens = 6089, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 2156122084
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6089, total state size = 160.790 MiB
srv          load:  - looking for better prompt, base f_keep = 0.010, sim = 0.012
srv        update:  - cache state: 7 prompts, 2270.527 MiB (limits: 8192.000 MiB, 40192 tokens, 248488 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv  get_availabl: prompt cache update took 295.82 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 19630 | processing task, is_child = 0
slot update_slots: id  0 | task 19630 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4945
slot update_slots: id  0 | task 19630 | n_past = 59, slot.prompt.tokens.size() = 6089, seq_id = 0, pos_min = 5321, n_swa = 128
slot update_slots: id  0 | task 19630 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 19630 | erased invalidated context checkpoint (pos_min = 4072, pos_max = 4839, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 19630 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 19630 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.414156
slot update_slots: id  0 | task 19630 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 19630 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.828311
slot update_slots: id  0 | task 19630 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 19630 | prompt processing progress, n_tokens = 4881, batch.n_tokens = 785, progress = 0.987058
slot update_slots: id  0 | task 19630 | n_tokens = 4881, memory_seq_rm [4881, end)
slot update_slots: id  0 | task 19630 | prompt processing progress, n_tokens = 4945, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 19630 | prompt done, n_tokens = 4945, batch.n_tokens = 64
slot init_sampler: id  0 | task 19630 | init sampler, took 0.95 ms, tokens: text = 4945, total = 4945
slot update_slots: id  0 | task 19630 | created context checkpoint 2 of 8 (pos_min = 4113, pos_max = 4880, size = 18.009 MiB)
slot print_timing: id  0 | task 19630 | 
prompt eval time =    5238.90 ms /  4945 tokens (    1.06 ms per token,   943.90 tokens per second)
       eval time =   19377.70 ms /   698 tokens (   27.76 ms per token,    36.02 tokens per second)
      total time =   24616.60 ms /  5643 tokens
slot      release: id  0 | task 19630 | stop processing: n_tokens = 5642, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.878 (> 0.100 thold), f_keep = 0.884
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20332 | processing task, is_child = 0
slot update_slots: id  0 | task 20332 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5681
slot update_slots: id  0 | task 20332 | n_past = 4986, slot.prompt.tokens.size() = 5642, seq_id = 0, pos_min = 4874, n_swa = 128
slot update_slots: id  0 | task 20332 | restored context checkpoint (pos_min = 4113, pos_max = 4880, size = 18.009 MiB)
slot update_slots: id  0 | task 20332 | n_tokens = 4880, memory_seq_rm [4880, end)
slot update_slots: id  0 | task 20332 | prompt processing progress, n_tokens = 5617, batch.n_tokens = 737, progress = 0.988734
slot update_slots: id  0 | task 20332 | n_tokens = 5617, memory_seq_rm [5617, end)
slot update_slots: id  0 | task 20332 | prompt processing progress, n_tokens = 5681, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20332 | prompt done, n_tokens = 5681, batch.n_tokens = 64
slot init_sampler: id  0 | task 20332 | init sampler, took 1.15 ms, tokens: text = 5681, total = 5681
slot update_slots: id  0 | task 20332 | created context checkpoint 3 of 8 (pos_min = 4849, pos_max = 5616, size = 18.009 MiB)
slot print_timing: id  0 | task 20332 | 
prompt eval time =    1215.23 ms /   801 tokens (    1.52 ms per token,   659.14 tokens per second)
       eval time =    6147.81 ms /   217 tokens (   28.33 ms per token,    35.30 tokens per second)
      total time =    7363.04 ms /  1018 tokens
slot      release: id  0 | task 20332 | stop processing: n_tokens = 5897, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20551 | processing task, is_child = 0
slot update_slots: id  0 | task 20551 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5955
slot update_slots: id  0 | task 20551 | n_tokens = 5874, memory_seq_rm [5874, end)
slot update_slots: id  0 | task 20551 | prompt processing progress, n_tokens = 5891, batch.n_tokens = 17, progress = 0.989253
slot update_slots: id  0 | task 20551 | n_tokens = 5891, memory_seq_rm [5891, end)
slot update_slots: id  0 | task 20551 | prompt processing progress, n_tokens = 5955, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20551 | prompt done, n_tokens = 5955, batch.n_tokens = 64
slot init_sampler: id  0 | task 20551 | init sampler, took 2.54 ms, tokens: text = 5955, total = 5955
slot update_slots: id  0 | task 20551 | created context checkpoint 4 of 8 (pos_min = 5129, pos_max = 5890, size = 17.868 MiB)
slot print_timing: id  0 | task 20551 | 
prompt eval time =     293.31 ms /    81 tokens (    3.62 ms per token,   276.15 tokens per second)
       eval time =    1775.62 ms /    61 tokens (   29.11 ms per token,    34.35 tokens per second)
      total time =    2068.94 ms /   142 tokens
slot      release: id  0 | task 20551 | stop processing: n_tokens = 6015, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20614 | processing task, is_child = 0
slot update_slots: id  0 | task 20614 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6058
slot update_slots: id  0 | task 20614 | n_tokens = 5992, memory_seq_rm [5992, end)
slot update_slots: id  0 | task 20614 | prompt processing progress, n_tokens = 5994, batch.n_tokens = 2, progress = 0.989435
slot update_slots: id  0 | task 20614 | n_tokens = 5994, memory_seq_rm [5994, end)
slot update_slots: id  0 | task 20614 | prompt processing progress, n_tokens = 6058, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20614 | prompt done, n_tokens = 6058, batch.n_tokens = 64
slot init_sampler: id  0 | task 20614 | init sampler, took 3.90 ms, tokens: text = 6058, total = 6058
slot update_slots: id  0 | task 20614 | created context checkpoint 5 of 8 (pos_min = 5247, pos_max = 5993, size = 17.517 MiB)
slot print_timing: id  0 | task 20614 | 
prompt eval time =     261.33 ms /    66 tokens (    3.96 ms per token,   252.55 tokens per second)
       eval time =   16560.42 ms /   575 tokens (   28.80 ms per token,    34.72 tokens per second)
      total time =   16821.75 ms /   641 tokens
slot      release: id  0 | task 20614 | stop processing: n_tokens = 6632, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.836 (> 0.100 thold), f_keep = 0.746
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 21191 | processing task, is_child = 0
slot update_slots: id  0 | task 21191 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5912
slot update_slots: id  0 | task 21191 | n_past = 4945, slot.prompt.tokens.size() = 6632, seq_id = 0, pos_min = 5864, n_swa = 128
slot update_slots: id  0 | task 21191 | restored context checkpoint (pos_min = 4113, pos_max = 4880, size = 18.009 MiB)
slot update_slots: id  0 | task 21191 | erased invalidated context checkpoint (pos_min = 4849, pos_max = 5616, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 21191 | erased invalidated context checkpoint (pos_min = 5129, pos_max = 5890, n_swa = 128, size = 17.868 MiB)
slot update_slots: id  0 | task 21191 | erased invalidated context checkpoint (pos_min = 5247, pos_max = 5993, n_swa = 128, size = 17.517 MiB)
slot update_slots: id  0 | task 21191 | n_tokens = 4880, memory_seq_rm [4880, end)
slot update_slots: id  0 | task 21191 | prompt processing progress, n_tokens = 5848, batch.n_tokens = 968, progress = 0.989175
slot update_slots: id  0 | task 21191 | n_tokens = 5848, memory_seq_rm [5848, end)
slot update_slots: id  0 | task 21191 | prompt processing progress, n_tokens = 5912, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 21191 | prompt done, n_tokens = 5912, batch.n_tokens = 64
slot init_sampler: id  0 | task 21191 | init sampler, took 1.13 ms, tokens: text = 5912, total = 5912
slot update_slots: id  0 | task 21191 | created context checkpoint 3 of 8 (pos_min = 5080, pos_max = 5847, size = 18.009 MiB)
slot print_timing: id  0 | task 21191 | 
prompt eval time =    1259.29 ms /  1032 tokens (    1.22 ms per token,   819.51 tokens per second)
       eval time =   12181.05 ms /   475 tokens (   25.64 ms per token,    39.00 tokens per second)
      total time =   13440.34 ms /  1507 tokens
slot      release: id  0 | task 21191 | stop processing: n_tokens = 6386, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 2495991069
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6386, total state size = 167.754 MiB
srv          load:  - looking for better prompt, base f_keep = 0.009, sim = 0.011
srv        update:  - cache state: 8 prompts, 2489.893 MiB (limits: 8192.000 MiB, 40192 tokens, 247606 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv  get_availabl: prompt cache update took 269.31 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 21668 | processing task, is_child = 0
slot update_slots: id  0 | task 21668 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5490
slot update_slots: id  0 | task 21668 | n_past = 59, slot.prompt.tokens.size() = 6386, seq_id = 0, pos_min = 5618, n_swa = 128
slot update_slots: id  0 | task 21668 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 21668 | erased invalidated context checkpoint (pos_min = 4113, pos_max = 4880, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 21668 | erased invalidated context checkpoint (pos_min = 5080, pos_max = 5847, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 21668 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 21668 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.373042
slot update_slots: id  0 | task 21668 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 21668 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.746084
slot update_slots: id  0 | task 21668 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 21668 | prompt processing progress, n_tokens = 5426, batch.n_tokens = 1330, progress = 0.988342
slot update_slots: id  0 | task 21668 | n_tokens = 5426, memory_seq_rm [5426, end)
slot update_slots: id  0 | task 21668 | prompt processing progress, n_tokens = 5490, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 21668 | prompt done, n_tokens = 5490, batch.n_tokens = 64
slot init_sampler: id  0 | task 21668 | init sampler, took 2.13 ms, tokens: text = 5490, total = 5490
slot update_slots: id  0 | task 21668 | created context checkpoint 2 of 8 (pos_min = 4658, pos_max = 5425, size = 18.009 MiB)
slot print_timing: id  0 | task 21668 | 
prompt eval time =    5372.86 ms /  5490 tokens (    0.98 ms per token,  1021.80 tokens per second)
       eval time =   13171.99 ms /   537 tokens (   24.53 ms per token,    40.77 tokens per second)
      total time =   18544.84 ms /  6027 tokens
slot      release: id  0 | task 21668 | stop processing: n_tokens = 6026, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 2522875346
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6026, total state size = 159.313 MiB
srv          load:  - looking for better prompt, base f_keep = 0.010, sim = 0.065
srv        update:  - cache state: 9 prompts, 2682.809 MiB (limits: 8192.000 MiB, 40192 tokens, 248202 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv        update:    - prompt 0x59729e4b9300:    6026 tokens, checkpoints:  2,   192.916 MiB
srv  get_availabl: prompt cache update took 536.56 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22209 | processing task, is_child = 0
slot update_slots: id  0 | task 22209 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 906
slot update_slots: id  0 | task 22209 | n_past = 59, slot.prompt.tokens.size() = 6026, seq_id = 0, pos_min = 5258, n_swa = 128
slot update_slots: id  0 | task 22209 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 22209 | erased invalidated context checkpoint (pos_min = 4658, pos_max = 5425, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 22209 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 22209 | prompt processing progress, n_tokens = 842, batch.n_tokens = 842, progress = 0.929360
slot update_slots: id  0 | task 22209 | n_tokens = 842, memory_seq_rm [842, end)
slot update_slots: id  0 | task 22209 | prompt processing progress, n_tokens = 906, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22209 | prompt done, n_tokens = 906, batch.n_tokens = 64
slot init_sampler: id  0 | task 22209 | init sampler, took 0.17 ms, tokens: text = 906, total = 906
slot update_slots: id  0 | task 22209 | created context checkpoint 2 of 8 (pos_min = 74, pos_max = 841, size = 18.009 MiB)
slot print_timing: id  0 | task 22209 | 
prompt eval time =     993.22 ms /   906 tokens (    1.10 ms per token,   912.19 tokens per second)
       eval time =    2305.92 ms /    85 tokens (   27.13 ms per token,    36.86 tokens per second)
      total time =    3299.14 ms /   991 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 22209 | stop processing: n_tokens = 990, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.851 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22296 | processing task, is_child = 0
slot update_slots: id  0 | task 22296 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1124
slot update_slots: id  0 | task 22296 | n_tokens = 957, memory_seq_rm [957, end)
slot update_slots: id  0 | task 22296 | prompt processing progress, n_tokens = 1060, batch.n_tokens = 103, progress = 0.943061
slot update_slots: id  0 | task 22296 | n_tokens = 1060, memory_seq_rm [1060, end)
slot update_slots: id  0 | task 22296 | prompt processing progress, n_tokens = 1124, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22296 | prompt done, n_tokens = 1124, batch.n_tokens = 64
slot init_sampler: id  0 | task 22296 | init sampler, took 0.24 ms, tokens: text = 1124, total = 1124
slot update_slots: id  0 | task 22296 | created context checkpoint 3 of 8 (pos_min = 292, pos_max = 1059, size = 18.009 MiB)
slot print_timing: id  0 | task 22296 | 
prompt eval time =     513.20 ms /   167 tokens (    3.07 ms per token,   325.41 tokens per second)
       eval time =    2129.42 ms /    70 tokens (   30.42 ms per token,    32.87 tokens per second)
      total time =    2642.62 ms /   237 tokens
slot      release: id  0 | task 22296 | stop processing: n_tokens = 1193, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22368 | processing task, is_child = 0
slot update_slots: id  0 | task 22368 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1224
slot update_slots: id  0 | task 22368 | n_tokens = 1139, memory_seq_rm [1139, end)
slot update_slots: id  0 | task 22368 | prompt processing progress, n_tokens = 1160, batch.n_tokens = 21, progress = 0.947712
slot update_slots: id  0 | task 22368 | n_tokens = 1160, memory_seq_rm [1160, end)
slot update_slots: id  0 | task 22368 | prompt processing progress, n_tokens = 1224, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22368 | prompt done, n_tokens = 1224, batch.n_tokens = 64
slot init_sampler: id  0 | task 22368 | init sampler, took 0.26 ms, tokens: text = 1224, total = 1224
slot update_slots: id  0 | task 22368 | created context checkpoint 4 of 8 (pos_min = 425, pos_max = 1159, size = 17.235 MiB)
slot print_timing: id  0 | task 22368 | 
prompt eval time =     357.71 ms /    85 tokens (    4.21 ms per token,   237.62 tokens per second)
       eval time =    3870.24 ms /   143 tokens (   27.06 ms per token,    36.95 tokens per second)
      total time =    4227.95 ms /   228 tokens
slot      release: id  0 | task 22368 | stop processing: n_tokens = 1366, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.960
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22513 | processing task, is_child = 0
slot update_slots: id  0 | task 22513 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1397
slot update_slots: id  0 | task 22513 | n_tokens = 1312, memory_seq_rm [1312, end)
slot update_slots: id  0 | task 22513 | prompt processing progress, n_tokens = 1333, batch.n_tokens = 21, progress = 0.954188
slot update_slots: id  0 | task 22513 | n_tokens = 1333, memory_seq_rm [1333, end)
slot update_slots: id  0 | task 22513 | prompt processing progress, n_tokens = 1397, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22513 | prompt done, n_tokens = 1397, batch.n_tokens = 64
slot init_sampler: id  0 | task 22513 | init sampler, took 0.26 ms, tokens: text = 1397, total = 1397
slot update_slots: id  0 | task 22513 | created context checkpoint 5 of 8 (pos_min = 598, pos_max = 1332, size = 17.235 MiB)
slot print_timing: id  0 | task 22513 | 
prompt eval time =     355.54 ms /    85 tokens (    4.18 ms per token,   239.07 tokens per second)
       eval time =    8715.29 ms /   336 tokens (   25.94 ms per token,    38.55 tokens per second)
      total time =    9070.83 ms /   421 tokens
slot      release: id  0 | task 22513 | stop processing: n_tokens = 1732, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.906 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22851 | processing task, is_child = 0
slot update_slots: id  0 | task 22851 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1871
slot update_slots: id  0 | task 22851 | n_tokens = 1696, memory_seq_rm [1696, end)
slot update_slots: id  0 | task 22851 | prompt processing progress, n_tokens = 1807, batch.n_tokens = 111, progress = 0.965794
slot update_slots: id  0 | task 22851 | n_tokens = 1807, memory_seq_rm [1807, end)
slot update_slots: id  0 | task 22851 | prompt processing progress, n_tokens = 1871, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22851 | prompt done, n_tokens = 1871, batch.n_tokens = 64
slot init_sampler: id  0 | task 22851 | init sampler, took 0.35 ms, tokens: text = 1871, total = 1871
slot update_slots: id  0 | task 22851 | created context checkpoint 6 of 8 (pos_min = 1039, pos_max = 1806, size = 18.009 MiB)
slot print_timing: id  0 | task 22851 | 
prompt eval time =     542.71 ms /   175 tokens (    3.10 ms per token,   322.46 tokens per second)
       eval time =    2613.76 ms /   103 tokens (   25.38 ms per token,    39.41 tokens per second)
      total time =    3156.46 ms /   278 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 22851 | stop processing: n_tokens = 1973, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.558 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22956 | processing task, is_child = 0
slot update_slots: id  0 | task 22956 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3443
slot update_slots: id  0 | task 22956 | n_tokens = 1922, memory_seq_rm [1922, end)
slot update_slots: id  0 | task 22956 | prompt processing progress, n_tokens = 3379, batch.n_tokens = 1457, progress = 0.981412
slot update_slots: id  0 | task 22956 | n_tokens = 3379, memory_seq_rm [3379, end)
slot update_slots: id  0 | task 22956 | prompt processing progress, n_tokens = 3443, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22956 | prompt done, n_tokens = 3443, batch.n_tokens = 64
slot init_sampler: id  0 | task 22956 | init sampler, took 2.62 ms, tokens: text = 3443, total = 3443
slot update_slots: id  0 | task 22956 | created context checkpoint 7 of 8 (pos_min = 2611, pos_max = 3378, size = 18.009 MiB)
slot print_timing: id  0 | task 22956 | 
prompt eval time =    1716.39 ms /  1521 tokens (    1.13 ms per token,   886.16 tokens per second)
       eval time =   13678.97 ms /   530 tokens (   25.81 ms per token,    38.75 tokens per second)
      total time =   15395.36 ms /  2051 tokens
slot      release: id  0 | task 22956 | stop processing: n_tokens = 3972, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.698 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 23488 | processing task, is_child = 0
slot update_slots: id  0 | task 23488 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5425
slot update_slots: id  0 | task 23488 | n_tokens = 3788, memory_seq_rm [3788, end)
slot update_slots: id  0 | task 23488 | prompt processing progress, n_tokens = 5361, batch.n_tokens = 1573, progress = 0.988203
slot update_slots: id  0 | task 23488 | n_tokens = 5361, memory_seq_rm [5361, end)
slot update_slots: id  0 | task 23488 | prompt processing progress, n_tokens = 5425, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 23488 | prompt done, n_tokens = 5425, batch.n_tokens = 64
slot init_sampler: id  0 | task 23488 | init sampler, took 1.10 ms, tokens: text = 5425, total = 5425
slot update_slots: id  0 | task 23488 | created context checkpoint 8 of 8 (pos_min = 4593, pos_max = 5360, size = 18.009 MiB)
slot print_timing: id  0 | task 23488 | 
prompt eval time =    1919.56 ms /  1637 tokens (    1.17 ms per token,   852.80 tokens per second)
       eval time =   10067.40 ms /   379 tokens (   26.56 ms per token,    37.65 tokens per second)
      total time =   11986.96 ms /  2016 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 23488 | stop processing: n_tokens = 5803, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 23869 | processing task, is_child = 0
slot update_slots: id  0 | task 23869 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5989
slot update_slots: id  0 | task 23869 | n_tokens = 5752, memory_seq_rm [5752, end)
slot update_slots: id  0 | task 23869 | prompt processing progress, n_tokens = 5925, batch.n_tokens = 173, progress = 0.989314
slot update_slots: id  0 | task 23869 | n_tokens = 5925, memory_seq_rm [5925, end)
slot update_slots: id  0 | task 23869 | prompt processing progress, n_tokens = 5989, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 23869 | prompt done, n_tokens = 5989, batch.n_tokens = 64
slot init_sampler: id  0 | task 23869 | init sampler, took 1.98 ms, tokens: text = 5989, total = 5989
slot update_slots: id  0 | task 23869 | erasing old context checkpoint (pos_min = 0, pos_max = 664, size = 15.594 MiB)
slot update_slots: id  0 | task 23869 | created context checkpoint 8 of 8 (pos_min = 5157, pos_max = 5924, size = 18.009 MiB)
slot print_timing: id  0 | task 23869 | 
prompt eval time =     531.33 ms /   237 tokens (    2.24 ms per token,   446.05 tokens per second)
       eval time =    5497.41 ms /   209 tokens (   26.30 ms per token,    38.02 tokens per second)
      total time =    6028.74 ms /   446 tokens
slot      release: id  0 | task 23869 | stop processing: n_tokens = 6197, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 24080 | processing task, is_child = 0
slot update_slots: id  0 | task 24080 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6299
slot update_slots: id  0 | task 24080 | n_tokens = 6143, memory_seq_rm [6143, end)
slot update_slots: id  0 | task 24080 | prompt processing progress, n_tokens = 6235, batch.n_tokens = 92, progress = 0.989840
slot update_slots: id  0 | task 24080 | n_tokens = 6235, memory_seq_rm [6235, end)
slot update_slots: id  0 | task 24080 | prompt processing progress, n_tokens = 6299, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 24080 | prompt done, n_tokens = 6299, batch.n_tokens = 64
slot init_sampler: id  0 | task 24080 | init sampler, took 3.23 ms, tokens: text = 6299, total = 6299
slot update_slots: id  0 | task 24080 | erasing old context checkpoint (pos_min = 74, pos_max = 841, size = 18.009 MiB)
slot update_slots: id  0 | task 24080 | created context checkpoint 8 of 8 (pos_min = 5467, pos_max = 6234, size = 18.009 MiB)
slot print_timing: id  0 | task 24080 | 
prompt eval time =     475.63 ms /   156 tokens (    3.05 ms per token,   327.98 tokens per second)
       eval time =    9457.60 ms /   353 tokens (   26.79 ms per token,    37.32 tokens per second)
      total time =    9933.24 ms /   509 tokens
slot      release: id  0 | task 24080 | stop processing: n_tokens = 6651, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.884 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 24435 | processing task, is_child = 0
slot update_slots: id  0 | task 24435 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7471
slot update_slots: id  0 | task 24435 | n_tokens = 6602, memory_seq_rm [6602, end)
slot update_slots: id  0 | task 24435 | prompt processing progress, n_tokens = 7407, batch.n_tokens = 805, progress = 0.991434
slot update_slots: id  0 | task 24435 | n_tokens = 7407, memory_seq_rm [7407, end)
slot update_slots: id  0 | task 24435 | prompt processing progress, n_tokens = 7471, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 24435 | prompt done, n_tokens = 7471, batch.n_tokens = 64
slot init_sampler: id  0 | task 24435 | init sampler, took 1.41 ms, tokens: text = 7471, total = 7471
slot update_slots: id  0 | task 24435 | erasing old context checkpoint (pos_min = 292, pos_max = 1059, size = 18.009 MiB)
slot update_slots: id  0 | task 24435 | created context checkpoint 8 of 8 (pos_min = 6639, pos_max = 7406, size = 18.009 MiB)
slot print_timing: id  0 | task 24435 | 
prompt eval time =    1184.53 ms /   869 tokens (    1.36 ms per token,   733.63 tokens per second)
       eval time =    2058.41 ms /    71 tokens (   28.99 ms per token,    34.49 tokens per second)
      total time =    3242.94 ms /   940 tokens
slot      release: id  0 | task 24435 | stop processing: n_tokens = 7541, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.902 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 24508 | processing task, is_child = 0
slot update_slots: id  0 | task 24508 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8322
slot update_slots: id  0 | task 24508 | n_tokens = 7509, memory_seq_rm [7509, end)
slot update_slots: id  0 | task 24508 | prompt processing progress, n_tokens = 8258, batch.n_tokens = 749, progress = 0.992310
slot update_slots: id  0 | task 24508 | n_tokens = 8258, memory_seq_rm [8258, end)
slot update_slots: id  0 | task 24508 | prompt processing progress, n_tokens = 8322, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 24508 | prompt done, n_tokens = 8322, batch.n_tokens = 64
slot init_sampler: id  0 | task 24508 | init sampler, took 1.58 ms, tokens: text = 8322, total = 8322
slot update_slots: id  0 | task 24508 | erasing old context checkpoint (pos_min = 425, pos_max = 1159, size = 17.235 MiB)
slot update_slots: id  0 | task 24508 | created context checkpoint 8 of 8 (pos_min = 7490, pos_max = 8257, size = 18.009 MiB)
slot print_timing: id  0 | task 24508 | 
prompt eval time =    1123.45 ms /   813 tokens (    1.38 ms per token,   723.66 tokens per second)
       eval time =    1223.61 ms /    47 tokens (   26.03 ms per token,    38.41 tokens per second)
      total time =    2347.06 ms /   860 tokens
slot      release: id  0 | task 24508 | stop processing: n_tokens = 8368, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.124 (> 0.100 thold), f_keep = 0.108
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8368, total state size = 214.230 MiB
srv          load:  - looking for better prompt, base f_keep = 0.108, sim = 0.124
srv        update:  - cache state: 10 prompts, 3040.338 MiB (limits: 8192.000 MiB, 40192 tokens, 241561 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv        update:    - prompt 0x59729e4b9300:    6026 tokens, checkpoints:  2,   192.916 MiB
srv        update:    - prompt 0x59729721bfc0:    8368 tokens, checkpoints:  8,   357.529 MiB
srv  get_availabl: prompt cache update took 429.74 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 24557 | processing task, is_child = 0
slot update_slots: id  0 | task 24557 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7336
slot update_slots: id  0 | task 24557 | n_past = 906, slot.prompt.tokens.size() = 8368, seq_id = 0, pos_min = 7600, n_swa = 128
slot update_slots: id  0 | task 24557 | restored context checkpoint (pos_min = 598, pos_max = 1332, size = 17.235 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 1039, pos_max = 1806, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 2611, pos_max = 3378, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 4593, pos_max = 5360, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 5157, pos_max = 5924, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 5467, pos_max = 6234, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 6639, pos_max = 7406, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 7490, pos_max = 8257, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | n_tokens = 906, memory_seq_rm [906, end)
slot update_slots: id  0 | task 24557 | prompt processing progress, n_tokens = 2954, batch.n_tokens = 2048, progress = 0.402672
slot update_slots: id  0 | task 24557 | n_tokens = 2954, memory_seq_rm [2954, end)
slot update_slots: id  0 | task 24557 | prompt processing progress, n_tokens = 5002, batch.n_tokens = 2048, progress = 0.681843
slot update_slots: id  0 | task 24557 | n_tokens = 5002, memory_seq_rm [5002, end)
slot update_slots: id  0 | task 24557 | prompt processing progress, n_tokens = 7050, batch.n_tokens = 2048, progress = 0.961014
slot update_slots: id  0 | task 24557 | n_tokens = 7050, memory_seq_rm [7050, end)
slot update_slots: id  0 | task 24557 | prompt processing progress, n_tokens = 7272, batch.n_tokens = 222, progress = 0.991276
slot update_slots: id  0 | task 24557 | n_tokens = 7272, memory_seq_rm [7272, end)
slot update_slots: id  0 | task 24557 | prompt processing progress, n_tokens = 7336, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 24557 | prompt done, n_tokens = 7336, batch.n_tokens = 64
slot init_sampler: id  0 | task 24557 | init sampler, took 3.11 ms, tokens: text = 7336, total = 7336
slot update_slots: id  0 | task 24557 | created context checkpoint 2 of 8 (pos_min = 6504, pos_max = 7271, size = 18.009 MiB)
slot print_timing: id  0 | task 24557 | 
prompt eval time =    6438.17 ms /  6430 tokens (    1.00 ms per token,   998.73 tokens per second)
       eval time =    3724.89 ms /   138 tokens (   26.99 ms per token,    37.05 tokens per second)
      total time =   10163.06 ms /  6568 tokens
slot      release: id  0 | task 24557 | stop processing: n_tokens = 7473, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.827 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 24700 | processing task, is_child = 0
slot update_slots: id  0 | task 24700 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8874
slot update_slots: id  0 | task 24700 | n_tokens = 7336, memory_seq_rm [7336, end)
slot update_slots: id  0 | task 24700 | prompt processing progress, n_tokens = 8810, batch.n_tokens = 1474, progress = 0.992788
slot update_slots: id  0 | task 24700 | n_tokens = 8810, memory_seq_rm [8810, end)
slot update_slots: id  0 | task 24700 | prompt processing progress, n_tokens = 8874, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 24700 | prompt done, n_tokens = 8874, batch.n_tokens = 64
slot init_sampler: id  0 | task 24700 | init sampler, took 1.82 ms, tokens: text = 8874, total = 8874
slot update_slots: id  0 | task 24700 | created context checkpoint 3 of 8 (pos_min = 8042, pos_max = 8809, size = 18.009 MiB)
slot print_timing: id  0 | task 24700 | 
prompt eval time =    1834.58 ms /  1538 tokens (    1.19 ms per token,   838.34 tokens per second)
       eval time =   10360.82 ms /   390 tokens (   26.57 ms per token,    37.64 tokens per second)
      total time =   12195.40 ms /  1928 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 24700 | stop processing: n_tokens = 9263, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.854 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 25092 | processing task, is_child = 0
slot update_slots: id  0 | task 25092 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 10395
slot update_slots: id  0 | task 25092 | n_tokens = 8874, memory_seq_rm [8874, end)
slot update_slots: id  0 | task 25092 | prompt processing progress, n_tokens = 10331, batch.n_tokens = 1457, progress = 0.993843
slot update_slots: id  0 | task 25092 | n_tokens = 10331, memory_seq_rm [10331, end)
slot update_slots: id  0 | task 25092 | prompt processing progress, n_tokens = 10395, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 25092 | prompt done, n_tokens = 10395, batch.n_tokens = 64
slot init_sampler: id  0 | task 25092 | init sampler, took 2.02 ms, tokens: text = 10395, total = 10395
slot update_slots: id  0 | task 25092 | created context checkpoint 4 of 8 (pos_min = 9563, pos_max = 10330, size = 18.009 MiB)
slot print_timing: id  0 | task 25092 | 
prompt eval time =    1834.96 ms /  1521 tokens (    1.21 ms per token,   828.90 tokens per second)
       eval time =    6486.75 ms /   245 tokens (   26.48 ms per token,    37.77 tokens per second)
      total time =    8321.71 ms /  1766 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 25092 | stop processing: n_tokens = 10639, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.873 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 25339 | processing task, is_child = 0
slot update_slots: id  0 | task 25339 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 11910
slot update_slots: id  0 | task 25339 | n_tokens = 10395, memory_seq_rm [10395, end)
slot update_slots: id  0 | task 25339 | prompt processing progress, n_tokens = 11846, batch.n_tokens = 1451, progress = 0.994626
slot update_slots: id  0 | task 25339 | n_tokens = 11846, memory_seq_rm [11846, end)
slot update_slots: id  0 | task 25339 | prompt processing progress, n_tokens = 11910, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 25339 | prompt done, n_tokens = 11910, batch.n_tokens = 64
slot init_sampler: id  0 | task 25339 | init sampler, took 3.78 ms, tokens: text = 11910, total = 11910
slot update_slots: id  0 | task 25339 | created context checkpoint 5 of 8 (pos_min = 11078, pos_max = 11845, size = 18.009 MiB)
slot print_timing: id  0 | task 25339 | 
prompt eval time =    1882.75 ms /  1515 tokens (    1.24 ms per token,   804.67 tokens per second)
       eval time =    3829.53 ms /   147 tokens (   26.05 ms per token,    38.39 tokens per second)
      total time =    5712.28 ms /  1662 tokens
slot      release: id  0 | task 25339 | stop processing: n_tokens = 12056, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.884 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 25488 | processing task, is_child = 0
slot update_slots: id  0 | task 25488 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 13466
slot update_slots: id  0 | task 25488 | n_tokens = 11910, memory_seq_rm [11910, end)
slot update_slots: id  0 | task 25488 | prompt processing progress, n_tokens = 13402, batch.n_tokens = 1492, progress = 0.995247
slot update_slots: id  0 | task 25488 | n_tokens = 13402, memory_seq_rm [13402, end)
slot update_slots: id  0 | task 25488 | prompt processing progress, n_tokens = 13466, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 25488 | prompt done, n_tokens = 13466, batch.n_tokens = 64
slot init_sampler: id  0 | task 25488 | init sampler, took 2.60 ms, tokens: text = 13466, total = 13466
slot update_slots: id  0 | task 25488 | created context checkpoint 6 of 8 (pos_min = 12634, pos_max = 13401, size = 18.009 MiB)
slot print_timing: id  0 | task 25488 | 
prompt eval time =    1991.63 ms /  1556 tokens (    1.28 ms per token,   781.27 tokens per second)
       eval time =   13441.47 ms /   487 tokens (   27.60 ms per token,    36.23 tokens per second)
      total time =   15433.10 ms /  2043 tokens
slot      release: id  0 | task 25488 | stop processing: n_tokens = 13952, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.899 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 25977 | processing task, is_child = 0
slot update_slots: id  0 | task 25977 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 14984
slot update_slots: id  0 | task 25977 | n_tokens = 13466, memory_seq_rm [13466, end)
slot update_slots: id  0 | task 25977 | prompt processing progress, n_tokens = 14920, batch.n_tokens = 1454, progress = 0.995729
slot update_slots: id  0 | task 25977 | n_tokens = 14920, memory_seq_rm [14920, end)
slot update_slots: id  0 | task 25977 | prompt processing progress, n_tokens = 14984, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 25977 | prompt done, n_tokens = 14984, batch.n_tokens = 64
slot init_sampler: id  0 | task 25977 | init sampler, took 7.36 ms, tokens: text = 14984, total = 14984
slot update_slots: id  0 | task 25977 | created context checkpoint 7 of 8 (pos_min = 14152, pos_max = 14919, size = 18.009 MiB)
slot print_timing: id  0 | task 25977 | 
prompt eval time =    1947.40 ms /  1518 tokens (    1.28 ms per token,   779.50 tokens per second)
       eval time =    7573.50 ms /   279 tokens (   27.15 ms per token,    36.84 tokens per second)
      total time =    9520.90 ms /  1797 tokens
slot      release: id  0 | task 25977 | stop processing: n_tokens = 15262, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.907 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 26258 | processing task, is_child = 0
slot update_slots: id  0 | task 26258 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 16526
slot update_slots: id  0 | task 26258 | n_tokens = 14984, memory_seq_rm [14984, end)
slot update_slots: id  0 | task 26258 | prompt processing progress, n_tokens = 16462, batch.n_tokens = 1478, progress = 0.996127
slot update_slots: id  0 | task 26258 | n_tokens = 16462, memory_seq_rm [16462, end)
slot update_slots: id  0 | task 26258 | prompt processing progress, n_tokens = 16526, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 26258 | prompt done, n_tokens = 16526, batch.n_tokens = 64
slot init_sampler: id  0 | task 26258 | init sampler, took 5.88 ms, tokens: text = 16526, total = 16526
slot update_slots: id  0 | task 26258 | created context checkpoint 8 of 8 (pos_min = 15694, pos_max = 16461, size = 18.009 MiB)
slot print_timing: id  0 | task 26258 | 
prompt eval time =    2032.80 ms /  1542 tokens (    1.32 ms per token,   758.56 tokens per second)
       eval time =    6399.37 ms /   230 tokens (   27.82 ms per token,    35.94 tokens per second)
      total time =    8432.17 ms /  1772 tokens
slot      release: id  0 | task 26258 | stop processing: n_tokens = 16755, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.913 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 26490 | processing task, is_child = 0
slot update_slots: id  0 | task 26490 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 18098
slot update_slots: id  0 | task 26490 | n_tokens = 16526, memory_seq_rm [16526, end)
slot update_slots: id  0 | task 26490 | prompt processing progress, n_tokens = 18034, batch.n_tokens = 1508, progress = 0.996464
slot update_slots: id  0 | task 26490 | n_tokens = 18034, memory_seq_rm [18034, end)
slot update_slots: id  0 | task 26490 | prompt processing progress, n_tokens = 18098, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 26490 | prompt done, n_tokens = 18098, batch.n_tokens = 64
slot init_sampler: id  0 | task 26490 | init sampler, took 7.85 ms, tokens: text = 18098, total = 18098
slot update_slots: id  0 | task 26490 | erasing old context checkpoint (pos_min = 598, pos_max = 1332, size = 17.235 MiB)
slot update_slots: id  0 | task 26490 | created context checkpoint 8 of 8 (pos_min = 17266, pos_max = 18033, size = 18.009 MiB)
slot print_timing: id  0 | task 26490 | 
prompt eval time =    2102.10 ms /  1572 tokens (    1.34 ms per token,   747.82 tokens per second)
       eval time =   12207.10 ms /   438 tokens (   27.87 ms per token,    35.88 tokens per second)
      total time =   14309.21 ms /  2010 tokens
slot      release: id  0 | task 26490 | stop processing: n_tokens = 18535, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 26930 | processing task, is_child = 0
slot update_slots: id  0 | task 26930 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 18899
slot update_slots: id  0 | task 26930 | n_tokens = 18098, memory_seq_rm [18098, end)
slot update_slots: id  0 | task 26930 | prompt processing progress, n_tokens = 18835, batch.n_tokens = 737, progress = 0.996614
slot update_slots: id  0 | task 26930 | n_tokens = 18835, memory_seq_rm [18835, end)
slot update_slots: id  0 | task 26930 | prompt processing progress, n_tokens = 18899, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 26930 | prompt done, n_tokens = 18899, batch.n_tokens = 64
slot init_sampler: id  0 | task 26930 | init sampler, took 3.74 ms, tokens: text = 18899, total = 18899
slot update_slots: id  0 | task 26930 | erasing old context checkpoint (pos_min = 6504, pos_max = 7271, size = 18.009 MiB)
slot update_slots: id  0 | task 26930 | created context checkpoint 8 of 8 (pos_min = 18067, pos_max = 18834, size = 18.009 MiB)
slot print_timing: id  0 | task 26930 | 
prompt eval time =    1319.03 ms /   801 tokens (    1.65 ms per token,   607.26 tokens per second)
       eval time =   12243.29 ms /   430 tokens (   28.47 ms per token,    35.12 tokens per second)
      total time =   13562.33 ms /  1231 tokens
slot      release: id  0 | task 26930 | stop processing: n_tokens = 19328, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 27362 | processing task, is_child = 0
slot update_slots: id  0 | task 27362 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 19136
slot update_slots: id  0 | task 27362 | n_tokens = 18899, memory_seq_rm [18899, end)
slot update_slots: id  0 | task 27362 | prompt processing progress, n_tokens = 19072, batch.n_tokens = 173, progress = 0.996656
slot update_slots: id  0 | task 27362 | n_tokens = 19072, memory_seq_rm [19072, end)
slot update_slots: id  0 | task 27362 | prompt processing progress, n_tokens = 19136, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 27362 | prompt done, n_tokens = 19136, batch.n_tokens = 64
slot init_sampler: id  0 | task 27362 | init sampler, took 5.31 ms, tokens: text = 19136, total = 19136
slot update_slots: id  0 | task 27362 | erasing old context checkpoint (pos_min = 8042, pos_max = 8809, size = 18.009 MiB)
slot update_slots: id  0 | task 27362 | created context checkpoint 8 of 8 (pos_min = 18560, pos_max = 19071, size = 12.006 MiB)
slot print_timing: id  0 | task 27362 | 
prompt eval time =     617.10 ms /   237 tokens (    2.60 ms per token,   384.05 tokens per second)
       eval time =   19949.84 ms /   695 tokens (   28.70 ms per token,    34.84 tokens per second)
      total time =   20566.94 ms /   932 tokens
slot      release: id  0 | task 27362 | stop processing: n_tokens = 19830, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.926 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28059 | processing task, is_child = 0
slot update_slots: id  0 | task 28059 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 20665
slot update_slots: id  0 | task 28059 | n_past = 19136, slot.prompt.tokens.size() = 19830, seq_id = 0, pos_min = 19062, n_swa = 128
slot update_slots: id  0 | task 28059 | restored context checkpoint (pos_min = 18560, pos_max = 19071, size = 12.006 MiB)
slot update_slots: id  0 | task 28059 | n_tokens = 19071, memory_seq_rm [19071, end)
slot update_slots: id  0 | task 28059 | prompt processing progress, n_tokens = 20601, batch.n_tokens = 1530, progress = 0.996903
slot update_slots: id  0 | task 28059 | n_tokens = 20601, memory_seq_rm [20601, end)
slot update_slots: id  0 | task 28059 | prompt processing progress, n_tokens = 20665, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28059 | prompt done, n_tokens = 20665, batch.n_tokens = 64
slot init_sampler: id  0 | task 28059 | init sampler, took 3.91 ms, tokens: text = 20665, total = 20665
slot update_slots: id  0 | task 28059 | erasing old context checkpoint (pos_min = 9563, pos_max = 10330, size = 18.009 MiB)
slot update_slots: id  0 | task 28059 | created context checkpoint 8 of 8 (pos_min = 19833, pos_max = 20600, size = 18.009 MiB)
slot print_timing: id  0 | task 28059 | 
prompt eval time =    2350.34 ms /  1594 tokens (    1.47 ms per token,   678.20 tokens per second)
       eval time =    8304.87 ms /   277 tokens (   29.98 ms per token,    33.35 tokens per second)
      total time =   10655.22 ms /  1871 tokens
slot      release: id  0 | task 28059 | stop processing: n_tokens = 20941, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28338 | processing task, is_child = 0
slot update_slots: id  0 | task 28338 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 22176
slot update_slots: id  0 | task 28338 | n_tokens = 20665, memory_seq_rm [20665, end)
slot update_slots: id  0 | task 28338 | prompt processing progress, n_tokens = 22112, batch.n_tokens = 1447, progress = 0.997114
slot update_slots: id  0 | task 28338 | n_tokens = 22112, memory_seq_rm [22112, end)
slot update_slots: id  0 | task 28338 | prompt processing progress, n_tokens = 22176, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28338 | prompt done, n_tokens = 22176, batch.n_tokens = 64
slot init_sampler: id  0 | task 28338 | init sampler, took 5.26 ms, tokens: text = 22176, total = 22176
slot update_slots: id  0 | task 28338 | erasing old context checkpoint (pos_min = 11078, pos_max = 11845, size = 18.009 MiB)
slot update_slots: id  0 | task 28338 | created context checkpoint 8 of 8 (pos_min = 21344, pos_max = 22111, size = 18.009 MiB)
slot print_timing: id  0 | task 28338 | 
prompt eval time =    2330.87 ms /  1511 tokens (    1.54 ms per token,   648.25 tokens per second)
       eval time =   15010.25 ms /   485 tokens (   30.95 ms per token,    32.31 tokens per second)
      total time =   17341.12 ms /  1996 tokens
slot      release: id  0 | task 28338 | stop processing: n_tokens = 22660, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28825 | processing task, is_child = 0
slot update_slots: id  0 | task 28825 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 23004
slot update_slots: id  0 | task 28825 | n_tokens = 22177, memory_seq_rm [22177, end)
slot update_slots: id  0 | task 28825 | prompt processing progress, n_tokens = 22940, batch.n_tokens = 763, progress = 0.997218
slot update_slots: id  0 | task 28825 | n_tokens = 22940, memory_seq_rm [22940, end)
slot update_slots: id  0 | task 28825 | prompt processing progress, n_tokens = 23004, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28825 | prompt done, n_tokens = 23004, batch.n_tokens = 64
slot init_sampler: id  0 | task 28825 | init sampler, took 13.61 ms, tokens: text = 23004, total = 23004
slot update_slots: id  0 | task 28825 | erasing old context checkpoint (pos_min = 12634, pos_max = 13401, size = 18.009 MiB)
slot update_slots: id  0 | task 28825 | created context checkpoint 8 of 8 (pos_min = 22299, pos_max = 22939, size = 15.031 MiB)
slot print_timing: id  0 | task 28825 | 
prompt eval time =    1573.30 ms /   827 tokens (    1.90 ms per token,   525.65 tokens per second)
       eval time =    1844.65 ms /    55 tokens (   33.54 ms per token,    29.82 tokens per second)
      total time =    3417.95 ms /   882 tokens
slot      release: id  0 | task 28825 | stop processing: n_tokens = 23058, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28882 | processing task, is_child = 0
slot update_slots: id  0 | task 28882 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 23285
slot update_slots: id  0 | task 28882 | n_tokens = 23004, memory_seq_rm [23004, end)
slot update_slots: id  0 | task 28882 | prompt processing progress, n_tokens = 23221, batch.n_tokens = 217, progress = 0.997251
slot update_slots: id  0 | task 28882 | n_tokens = 23221, memory_seq_rm [23221, end)
slot update_slots: id  0 | task 28882 | prompt processing progress, n_tokens = 23285, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28882 | prompt done, n_tokens = 23285, batch.n_tokens = 64
slot init_sampler: id  0 | task 28882 | init sampler, took 8.86 ms, tokens: text = 23285, total = 23285
slot update_slots: id  0 | task 28882 | erasing old context checkpoint (pos_min = 14152, pos_max = 14919, size = 18.009 MiB)
slot update_slots: id  0 | task 28882 | created context checkpoint 8 of 8 (pos_min = 22453, pos_max = 23220, size = 18.009 MiB)
slot print_timing: id  0 | task 28882 | 
prompt eval time =     824.43 ms /   281 tokens (    2.93 ms per token,   340.84 tokens per second)
       eval time =    1806.79 ms /    56 tokens (   32.26 ms per token,    30.99 tokens per second)
      total time =    2631.22 ms /   337 tokens
slot      release: id  0 | task 28882 | stop processing: n_tokens = 23340, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28940 | processing task, is_child = 0
slot update_slots: id  0 | task 28940 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 23603
slot update_slots: id  0 | task 28940 | n_tokens = 23285, memory_seq_rm [23285, end)
slot update_slots: id  0 | task 28940 | prompt processing progress, n_tokens = 23539, batch.n_tokens = 254, progress = 0.997288
slot update_slots: id  0 | task 28940 | n_tokens = 23539, memory_seq_rm [23539, end)
slot update_slots: id  0 | task 28940 | prompt processing progress, n_tokens = 23603, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28940 | prompt done, n_tokens = 23603, batch.n_tokens = 64
slot init_sampler: id  0 | task 28940 | init sampler, took 13.88 ms, tokens: text = 23603, total = 23603
slot update_slots: id  0 | task 28940 | erasing old context checkpoint (pos_min = 15694, pos_max = 16461, size = 18.009 MiB)
slot update_slots: id  0 | task 28940 | created context checkpoint 8 of 8 (pos_min = 22771, pos_max = 23538, size = 18.009 MiB)
slot print_timing: id  0 | task 28940 | 
prompt eval time =     761.51 ms /   318 tokens (    2.39 ms per token,   417.59 tokens per second)
       eval time =    1130.82 ms /    38 tokens (   29.76 ms per token,    33.60 tokens per second)
      total time =    1892.33 ms /   356 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 28940 | stop processing: n_tokens = 23640, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28980 | processing task, is_child = 0
slot update_slots: id  0 | task 28980 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 24026
slot update_slots: id  0 | task 28980 | n_tokens = 23603, memory_seq_rm [23603, end)
slot update_slots: id  0 | task 28980 | prompt processing progress, n_tokens = 23962, batch.n_tokens = 359, progress = 0.997336
slot update_slots: id  0 | task 28980 | n_tokens = 23962, memory_seq_rm [23962, end)
slot update_slots: id  0 | task 28980 | prompt processing progress, n_tokens = 24026, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28980 | prompt done, n_tokens = 24026, batch.n_tokens = 64
slot init_sampler: id  0 | task 28980 | init sampler, took 4.72 ms, tokens: text = 24026, total = 24026
slot update_slots: id  0 | task 28980 | erasing old context checkpoint (pos_min = 17266, pos_max = 18033, size = 18.009 MiB)
slot update_slots: id  0 | task 28980 | created context checkpoint 8 of 8 (pos_min = 23194, pos_max = 23961, size = 18.009 MiB)
slot print_timing: id  0 | task 28980 | 
prompt eval time =     832.13 ms /   423 tokens (    1.97 ms per token,   508.33 tokens per second)
       eval time =    1937.76 ms /    68 tokens (   28.50 ms per token,    35.09 tokens per second)
      total time =    2769.89 ms /   491 tokens
slot      release: id  0 | task 28980 | stop processing: n_tokens = 24093, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29050 | processing task, is_child = 0
slot update_slots: id  0 | task 29050 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 24299
slot update_slots: id  0 | task 29050 | n_tokens = 24026, memory_seq_rm [24026, end)
slot update_slots: id  0 | task 29050 | prompt processing progress, n_tokens = 24235, batch.n_tokens = 209, progress = 0.997366
slot update_slots: id  0 | task 29050 | n_tokens = 24235, memory_seq_rm [24235, end)
slot update_slots: id  0 | task 29050 | prompt processing progress, n_tokens = 24299, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29050 | prompt done, n_tokens = 24299, batch.n_tokens = 64
slot init_sampler: id  0 | task 29050 | init sampler, took 7.08 ms, tokens: text = 24299, total = 24299
slot update_slots: id  0 | task 29050 | erasing old context checkpoint (pos_min = 18067, pos_max = 18834, size = 18.009 MiB)
slot update_slots: id  0 | task 29050 | created context checkpoint 8 of 8 (pos_min = 23467, pos_max = 24234, size = 18.009 MiB)
slot print_timing: id  0 | task 29050 | 
prompt eval time =     687.54 ms /   273 tokens (    2.52 ms per token,   397.07 tokens per second)
       eval time =    1074.87 ms /    38 tokens (   28.29 ms per token,    35.35 tokens per second)
      total time =    1762.41 ms /   311 tokens
slot      release: id  0 | task 29050 | stop processing: n_tokens = 24336, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29090 | processing task, is_child = 0
slot update_slots: id  0 | task 29090 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 24604
slot update_slots: id  0 | task 29090 | n_tokens = 24299, memory_seq_rm [24299, end)
slot update_slots: id  0 | task 29090 | prompt processing progress, n_tokens = 24540, batch.n_tokens = 241, progress = 0.997399
slot update_slots: id  0 | task 29090 | n_tokens = 24540, memory_seq_rm [24540, end)
slot update_slots: id  0 | task 29090 | prompt processing progress, n_tokens = 24604, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29090 | prompt done, n_tokens = 24604, batch.n_tokens = 64
slot init_sampler: id  0 | task 29090 | init sampler, took 9.34 ms, tokens: text = 24604, total = 24604
slot update_slots: id  0 | task 29090 | erasing old context checkpoint (pos_min = 18560, pos_max = 19071, size = 12.006 MiB)
slot update_slots: id  0 | task 29090 | created context checkpoint 8 of 8 (pos_min = 23772, pos_max = 24539, size = 18.009 MiB)
slot print_timing: id  0 | task 29090 | 
prompt eval time =     773.11 ms /   305 tokens (    2.53 ms per token,   394.51 tokens per second)
       eval time =    1781.45 ms /    62 tokens (   28.73 ms per token,    34.80 tokens per second)
      total time =    2554.57 ms /   367 tokens
slot      release: id  0 | task 29090 | stop processing: n_tokens = 24665, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29154 | processing task, is_child = 0
slot update_slots: id  0 | task 29154 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 25063
slot update_slots: id  0 | task 29154 | n_tokens = 24605, memory_seq_rm [24605, end)
slot update_slots: id  0 | task 29154 | prompt processing progress, n_tokens = 24999, batch.n_tokens = 394, progress = 0.997446
slot update_slots: id  0 | task 29154 | n_tokens = 24999, memory_seq_rm [24999, end)
slot update_slots: id  0 | task 29154 | prompt processing progress, n_tokens = 25063, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29154 | prompt done, n_tokens = 25063, batch.n_tokens = 64
slot init_sampler: id  0 | task 29154 | init sampler, took 10.31 ms, tokens: text = 25063, total = 25063
slot update_slots: id  0 | task 29154 | erasing old context checkpoint (pos_min = 19833, pos_max = 20600, size = 18.009 MiB)
slot update_slots: id  0 | task 29154 | created context checkpoint 8 of 8 (pos_min = 24231, pos_max = 24998, size = 18.009 MiB)
