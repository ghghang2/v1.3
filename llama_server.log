ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14807 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1763 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 56064 -> need 1767 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.79 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14807 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
srv  log_server_r: request: GET /health 127.0.0.1 503
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
.srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 56064
llama_context: n_ctx_seq     = 56064
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (56064) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 56064 cells
llama_kv_cache:      CUDA0 KV buffer size =  1314.00 MiB
llama_kv_cache: size = 1314.00 MiB ( 56064 cells,  12 layers,  4/1 seqs), K (f16):  657.00 MiB, V (f16):  657.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   117.15 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 74.79 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  1 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  2 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  3 | task -1 | new slot, n_ctx = 56064
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-18

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 824
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 760, batch.n_tokens = 760, progress = 0.922330
slot update_slots: id  3 | task 0 | n_tokens = 760, memory_seq_rm [760, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 824, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 824, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.13 ms, tokens: text = 824, total = 824
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 759, size = 17.821 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =    1363.14 ms /   824 tokens (    1.65 ms per token,   604.49 tokens per second)
       eval time =     823.43 ms /    31 tokens (   26.56 ms per token,    37.65 tokens per second)
      total time =    2186.57 ms /   855 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 854, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.928 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33 | processing task, is_child = 0
slot update_slots: id  3 | task 33 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 906
slot update_slots: id  3 | task 33 | n_tokens = 841, memory_seq_rm [841, end)
slot update_slots: id  3 | task 33 | prompt processing progress, n_tokens = 842, batch.n_tokens = 1, progress = 0.929360
slot update_slots: id  3 | task 33 | n_tokens = 842, memory_seq_rm [842, end)
slot update_slots: id  3 | task 33 | prompt processing progress, n_tokens = 906, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33 | prompt done, n_tokens = 906, batch.n_tokens = 64
slot init_sampler: id  3 | task 33 | init sampler, took 0.17 ms, tokens: text = 906, total = 906
slot update_slots: id  3 | task 33 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 841, size = 19.744 MiB)
slot print_timing: id  3 | task 33 | 
prompt eval time =     244.05 ms /    65 tokens (    3.75 ms per token,   266.34 tokens per second)
       eval time =     659.66 ms /    26 tokens (   25.37 ms per token,    39.41 tokens per second)
      total time =     903.71 ms /    91 tokens
slot      release: id  3 | task 33 | stop processing: n_tokens = 931, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.822 (> 0.100 thold), f_keep = 0.885
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 61 | processing task, is_child = 0
slot update_slots: id  3 | task 61 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1002
slot update_slots: id  3 | task 61 | n_tokens = 824, memory_seq_rm [824, end)
slot update_slots: id  3 | task 61 | prompt processing progress, n_tokens = 938, batch.n_tokens = 114, progress = 0.936128
slot update_slots: id  3 | task 61 | n_tokens = 938, memory_seq_rm [938, end)
slot update_slots: id  3 | task 61 | prompt processing progress, n_tokens = 1002, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 61 | prompt done, n_tokens = 1002, batch.n_tokens = 64
slot init_sampler: id  3 | task 61 | init sampler, took 0.18 ms, tokens: text = 1002, total = 1002
slot update_slots: id  3 | task 61 | created context checkpoint 3 of 8 (pos_min = 0, pos_max = 937, size = 21.995 MiB)
slot print_timing: id  3 | task 61 | 
prompt eval time =     605.87 ms /   178 tokens (    3.40 ms per token,   293.79 tokens per second)
       eval time =    1269.49 ms /    39 tokens (   32.55 ms per token,    30.72 tokens per second)
      total time =    1875.36 ms /   217 tokens
slot      release: id  3 | task 61 | stop processing: n_tokens = 1040, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.861 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 102 | processing task, is_child = 0
slot update_slots: id  3 | task 102 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1186
slot update_slots: id  3 | task 102 | n_tokens = 1021, memory_seq_rm [1021, end)
slot update_slots: id  3 | task 102 | prompt processing progress, n_tokens = 1122, batch.n_tokens = 101, progress = 0.946037
slot update_slots: id  3 | task 102 | n_tokens = 1122, memory_seq_rm [1122, end)
slot update_slots: id  3 | task 102 | prompt processing progress, n_tokens = 1186, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 102 | prompt done, n_tokens = 1186, batch.n_tokens = 64
slot init_sampler: id  3 | task 102 | init sampler, took 0.18 ms, tokens: text = 1186, total = 1186
slot update_slots: id  3 | task 102 | created context checkpoint 4 of 8 (pos_min = 101, pos_max = 1121, size = 23.942 MiB)
slot print_timing: id  3 | task 102 | 
prompt eval time =     493.42 ms /   165 tokens (    2.99 ms per token,   334.40 tokens per second)
       eval time =    1539.09 ms /    47 tokens (   32.75 ms per token,    30.54 tokens per second)
      total time =    2032.51 ms /   212 tokens
slot      release: id  3 | task 102 | stop processing: n_tokens = 1232, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.385 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 151 | processing task, is_child = 0
slot update_slots: id  3 | task 151 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3122
slot update_slots: id  3 | task 151 | n_tokens = 1201, memory_seq_rm [1201, end)
slot update_slots: id  3 | task 151 | prompt processing progress, n_tokens = 3058, batch.n_tokens = 1857, progress = 0.979500
slot update_slots: id  3 | task 151 | n_tokens = 3058, memory_seq_rm [3058, end)
slot update_slots: id  3 | task 151 | prompt processing progress, n_tokens = 3122, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 151 | prompt done, n_tokens = 3122, batch.n_tokens = 64
slot init_sampler: id  3 | task 151 | init sampler, took 0.49 ms, tokens: text = 3122, total = 3122
slot update_slots: id  3 | task 151 | created context checkpoint 5 of 8 (pos_min = 2034, pos_max = 3057, size = 24.012 MiB)
slot print_timing: id  3 | task 151 | 
prompt eval time =    2758.78 ms /  1921 tokens (    1.44 ms per token,   696.32 tokens per second)
       eval time =    1417.35 ms /    41 tokens (   34.57 ms per token,    28.93 tokens per second)
      total time =    4176.13 ms /  1962 tokens
slot      release: id  3 | task 151 | stop processing: n_tokens = 3162, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.606 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 194 | processing task, is_child = 0
slot update_slots: id  3 | task 194 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5165
slot update_slots: id  3 | task 194 | n_tokens = 3131, memory_seq_rm [3131, end)
slot update_slots: id  3 | task 194 | prompt processing progress, n_tokens = 5101, batch.n_tokens = 1970, progress = 0.987609
slot update_slots: id  3 | task 194 | n_tokens = 5101, memory_seq_rm [5101, end)
slot update_slots: id  3 | task 194 | prompt processing progress, n_tokens = 5165, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 194 | prompt done, n_tokens = 5165, batch.n_tokens = 64
slot init_sampler: id  3 | task 194 | init sampler, took 0.97 ms, tokens: text = 5165, total = 5165
slot update_slots: id  3 | task 194 | created context checkpoint 6 of 8 (pos_min = 4077, pos_max = 5100, size = 24.012 MiB)
slot print_timing: id  3 | task 194 | 
prompt eval time =    3059.55 ms /  2034 tokens (    1.50 ms per token,   664.80 tokens per second)
       eval time =    1447.55 ms /    40 tokens (   36.19 ms per token,    27.63 tokens per second)
      total time =    4507.10 ms /  2074 tokens
slot      release: id  3 | task 194 | stop processing: n_tokens = 5204, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.851 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 236 | processing task, is_child = 0
slot update_slots: id  3 | task 236 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6079
slot update_slots: id  3 | task 236 | n_tokens = 5173, memory_seq_rm [5173, end)
slot update_slots: id  3 | task 236 | prompt processing progress, n_tokens = 6015, batch.n_tokens = 842, progress = 0.989472
slot update_slots: id  3 | task 236 | n_tokens = 6015, memory_seq_rm [6015, end)
slot update_slots: id  3 | task 236 | prompt processing progress, n_tokens = 6079, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 236 | prompt done, n_tokens = 6079, batch.n_tokens = 64
slot init_sampler: id  3 | task 236 | init sampler, took 0.94 ms, tokens: text = 6079, total = 6079
slot update_slots: id  3 | task 236 | created context checkpoint 7 of 8 (pos_min = 4991, pos_max = 6014, size = 24.012 MiB)
slot print_timing: id  3 | task 236 | 
prompt eval time =    1548.56 ms /   906 tokens (    1.71 ms per token,   585.06 tokens per second)
       eval time =    1401.60 ms /    38 tokens (   36.88 ms per token,    27.11 tokens per second)
      total time =    2950.16 ms /   944 tokens
slot      release: id  3 | task 236 | stop processing: n_tokens = 6116, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 276 | processing task, is_child = 0
slot update_slots: id  3 | task 276 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6148
slot update_slots: id  3 | task 276 | n_tokens = 6088, memory_seq_rm [6088, end)
slot update_slots: id  3 | task 276 | prompt processing progress, n_tokens = 6148, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  3 | task 276 | prompt done, n_tokens = 6148, batch.n_tokens = 60
slot init_sampler: id  3 | task 276 | init sampler, took 0.87 ms, tokens: text = 6148, total = 6148
slot update_slots: id  3 | task 276 | created context checkpoint 8 of 8 (pos_min = 5092, pos_max = 6087, size = 23.355 MiB)
slot print_timing: id  3 | task 276 | 
prompt eval time =     232.94 ms /    60 tokens (    3.88 ms per token,   257.58 tokens per second)
       eval time =    3197.39 ms /    86 tokens (   37.18 ms per token,    26.90 tokens per second)
      total time =    3430.33 ms /   146 tokens
slot      release: id  3 | task 276 | stop processing: n_tokens = 6233, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.753 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 363 | processing task, is_child = 0
slot update_slots: id  3 | task 363 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8247
slot update_slots: id  3 | task 363 | n_tokens = 6207, memory_seq_rm [6207, end)
slot update_slots: id  3 | task 363 | prompt processing progress, n_tokens = 8183, batch.n_tokens = 1976, progress = 0.992240
slot update_slots: id  3 | task 363 | n_tokens = 8183, memory_seq_rm [8183, end)
slot update_slots: id  3 | task 363 | prompt processing progress, n_tokens = 8247, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 363 | prompt done, n_tokens = 8247, batch.n_tokens = 64
slot init_sampler: id  3 | task 363 | init sampler, took 1.62 ms, tokens: text = 8247, total = 8247
slot update_slots: id  3 | task 363 | erasing old context checkpoint (pos_min = 0, pos_max = 759, size = 17.821 MiB)
slot update_slots: id  3 | task 363 | created context checkpoint 8 of 8 (pos_min = 7159, pos_max = 8182, size = 24.012 MiB)
slot print_timing: id  3 | task 363 | 
prompt eval time =    3157.06 ms /  2040 tokens (    1.55 ms per token,   646.17 tokens per second)
       eval time =    1346.33 ms /    39 tokens (   34.52 ms per token,    28.97 tokens per second)
      total time =    4503.39 ms /  2079 tokens
slot      release: id  3 | task 363 | stop processing: n_tokens = 8285, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.792 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 404 | processing task, is_child = 0
slot update_slots: id  3 | task 404 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10425
slot update_slots: id  3 | task 404 | n_tokens = 8255, memory_seq_rm [8255, end)
slot update_slots: id  3 | task 404 | prompt processing progress, n_tokens = 10303, batch.n_tokens = 2048, progress = 0.988297
slot update_slots: id  3 | task 404 | n_tokens = 10303, memory_seq_rm [10303, end)
slot update_slots: id  3 | task 404 | prompt processing progress, n_tokens = 10361, batch.n_tokens = 58, progress = 0.993861
slot update_slots: id  3 | task 404 | n_tokens = 10361, memory_seq_rm [10361, end)
slot update_slots: id  3 | task 404 | prompt processing progress, n_tokens = 10425, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 404 | prompt done, n_tokens = 10425, batch.n_tokens = 64
slot init_sampler: id  3 | task 404 | init sampler, took 1.43 ms, tokens: text = 10425, total = 10425
slot update_slots: id  3 | task 404 | erasing old context checkpoint (pos_min = 0, pos_max = 841, size = 19.744 MiB)
slot update_slots: id  3 | task 404 | created context checkpoint 8 of 8 (pos_min = 9337, pos_max = 10360, size = 24.012 MiB)
slot print_timing: id  3 | task 404 | 
prompt eval time =    3542.56 ms /  2170 tokens (    1.63 ms per token,   612.55 tokens per second)
       eval time =   65902.55 ms /  2023 tokens (   32.58 ms per token,    30.70 tokens per second)
      total time =   69445.11 ms /  4193 tokens
slot      release: id  3 | task 404 | stop processing: n_tokens = 12447, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 2430 | processing task, is_child = 0
slot update_slots: id  2 | task 2430 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12398
slot update_slots: id  2 | task 2430 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 2430 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.165188
slot update_slots: id  2 | task 2430 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 2430 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.330376
slot update_slots: id  2 | task 2430 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 2430 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.495564
slot update_slots: id  2 | task 2430 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 2430 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.660752
slot update_slots: id  2 | task 2430 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 2430 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.825940
slot update_slots: id  2 | task 2430 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 2430 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.991128
slot update_slots: id  2 | task 2430 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  2 | task 2430 | prompt processing progress, n_tokens = 12334, batch.n_tokens = 46, progress = 0.994838
slot update_slots: id  2 | task 2430 | n_tokens = 12334, memory_seq_rm [12334, end)
slot update_slots: id  2 | task 2430 | prompt processing progress, n_tokens = 12398, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 2430 | prompt done, n_tokens = 12398, batch.n_tokens = 64
slot init_sampler: id  2 | task 2430 | init sampler, took 3.24 ms, tokens: text = 12398, total = 12398
slot update_slots: id  2 | task 2430 | created context checkpoint 1 of 8 (pos_min = 11437, pos_max = 12333, size = 21.034 MiB)
slot print_timing: id  2 | task 2430 | 
prompt eval time =   21462.74 ms / 12398 tokens (    1.73 ms per token,   577.65 tokens per second)
       eval time =   54553.05 ms /  1575 tokens (   34.64 ms per token,    28.87 tokens per second)
      total time =   76015.78 ms / 13973 tokens
slot      release: id  2 | task 2430 | stop processing: n_tokens = 13972, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 4013 | processing task, is_child = 0
slot update_slots: id  1 | task 4013 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12694
slot update_slots: id  1 | task 4013 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 4013 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.161336
slot update_slots: id  1 | task 4013 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 4013 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.322672
slot update_slots: id  1 | task 4013 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 4013 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.484008
slot update_slots: id  1 | task 4013 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 4013 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.645344
slot update_slots: id  1 | task 4013 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  1 | task 4013 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.806680
slot update_slots: id  1 | task 4013 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  1 | task 4013 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.968016
slot update_slots: id  1 | task 4013 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  1 | task 4013 | prompt processing progress, n_tokens = 12630, batch.n_tokens = 342, progress = 0.994958
slot update_slots: id  1 | task 4013 | n_tokens = 12630, memory_seq_rm [12630, end)
slot update_slots: id  1 | task 4013 | prompt processing progress, n_tokens = 12694, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 4013 | prompt done, n_tokens = 12694, batch.n_tokens = 64
slot init_sampler: id  1 | task 4013 | init sampler, took 1.99 ms, tokens: text = 12694, total = 12694
slot update_slots: id  1 | task 4013 | created context checkpoint 1 of 8 (pos_min = 11860, pos_max = 12629, size = 18.056 MiB)
slot print_timing: id  1 | task 4013 | 
prompt eval time =   25847.81 ms / 12694 tokens (    2.04 ms per token,   491.11 tokens per second)
       eval time =   39338.30 ms /  1080 tokens (   36.42 ms per token,    27.45 tokens per second)
      total time =   65186.11 ms / 13774 tokens
slot      release: id  1 | task 4013 | stop processing: n_tokens = 13773, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.919 (> 0.100 thold), f_keep = 0.925
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5101 | processing task, is_child = 0
slot update_slots: id  1 | task 5101 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13875
slot update_slots: id  1 | task 5101 | n_past = 12746, slot.prompt.tokens.size() = 13773, seq_id = 1, pos_min = 13003, n_swa = 128
slot update_slots: id  1 | task 5101 | restored context checkpoint (pos_min = 11860, pos_max = 12629, size = 18.056 MiB)
slot update_slots: id  1 | task 5101 | n_tokens = 12629, memory_seq_rm [12629, end)
slot update_slots: id  1 | task 5101 | prompt processing progress, n_tokens = 13811, batch.n_tokens = 1182, progress = 0.995387
slot update_slots: id  1 | task 5101 | n_tokens = 13811, memory_seq_rm [13811, end)
slot update_slots: id  1 | task 5101 | prompt processing progress, n_tokens = 13875, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5101 | prompt done, n_tokens = 13875, batch.n_tokens = 64
slot init_sampler: id  1 | task 5101 | init sampler, took 2.89 ms, tokens: text = 13875, total = 13875
slot update_slots: id  1 | task 5101 | created context checkpoint 2 of 8 (pos_min = 13041, pos_max = 13810, size = 18.056 MiB)
slot print_timing: id  1 | task 5101 | 
prompt eval time =    2833.34 ms /  1246 tokens (    2.27 ms per token,   439.76 tokens per second)
       eval time =    1688.69 ms /    48 tokens (   35.18 ms per token,    28.42 tokens per second)
      total time =    4522.03 ms /  1294 tokens
slot      release: id  1 | task 5101 | stop processing: n_tokens = 13922, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.912 (> 0.100 thold), f_keep = 0.912
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5151 | processing task, is_child = 0
slot update_slots: id  1 | task 5151 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13912
slot update_slots: id  1 | task 5151 | n_past = 12694, slot.prompt.tokens.size() = 13922, seq_id = 1, pos_min = 13152, n_swa = 128
slot update_slots: id  1 | task 5151 | restored context checkpoint (pos_min = 11860, pos_max = 12629, size = 18.056 MiB)
slot update_slots: id  1 | task 5151 | erased invalidated context checkpoint (pos_min = 13041, pos_max = 13810, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 5151 | n_tokens = 12629, memory_seq_rm [12629, end)
slot update_slots: id  1 | task 5151 | prompt processing progress, n_tokens = 13848, batch.n_tokens = 1219, progress = 0.995400
slot update_slots: id  1 | task 5151 | n_tokens = 13848, memory_seq_rm [13848, end)
slot update_slots: id  1 | task 5151 | prompt processing progress, n_tokens = 13912, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5151 | prompt done, n_tokens = 13912, batch.n_tokens = 64
slot init_sampler: id  1 | task 5151 | init sampler, took 2.18 ms, tokens: text = 13912, total = 13912
slot update_slots: id  1 | task 5151 | created context checkpoint 2 of 8 (pos_min = 13078, pos_max = 13847, size = 18.056 MiB)
slot print_timing: id  1 | task 5151 | 
prompt eval time =    3160.27 ms /  1283 tokens (    2.46 ms per token,   405.98 tokens per second)
       eval time =   82905.20 ms /  2141 tokens (   38.72 ms per token,    25.82 tokens per second)
      total time =   86065.47 ms /  3424 tokens
slot      release: id  1 | task 5151 | stop processing: n_tokens = 16052, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.899 (> 0.100 thold), f_keep = 0.901
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 7294 | processing task, is_child = 0
slot update_slots: id  1 | task 7294 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16083
slot update_slots: id  1 | task 7294 | n_past = 14464, slot.prompt.tokens.size() = 16052, seq_id = 1, pos_min = 15282, n_swa = 128
slot update_slots: id  1 | task 7294 | restored context checkpoint (pos_min = 13078, pos_max = 13847, size = 18.056 MiB)
slot update_slots: id  1 | task 7294 | n_tokens = 13847, memory_seq_rm [13847, end)
slot update_slots: id  1 | task 7294 | prompt processing progress, n_tokens = 15895, batch.n_tokens = 2048, progress = 0.988311
slot update_slots: id  1 | task 7294 | n_tokens = 15895, memory_seq_rm [15895, end)
slot update_slots: id  1 | task 7294 | prompt processing progress, n_tokens = 16019, batch.n_tokens = 124, progress = 0.996021
slot update_slots: id  1 | task 7294 | n_tokens = 16019, memory_seq_rm [16019, end)
slot update_slots: id  1 | task 7294 | prompt processing progress, n_tokens = 16083, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 7294 | prompt done, n_tokens = 16083, batch.n_tokens = 64
slot init_sampler: id  1 | task 7294 | init sampler, took 2.28 ms, tokens: text = 16083, total = 16083
slot update_slots: id  1 | task 7294 | created context checkpoint 3 of 8 (pos_min = 15249, pos_max = 16018, size = 18.056 MiB)
slot print_timing: id  1 | task 7294 | 
prompt eval time =    5279.53 ms /  2236 tokens (    2.36 ms per token,   423.52 tokens per second)
       eval time =    5798.54 ms /   145 tokens (   39.99 ms per token,    25.01 tokens per second)
      total time =   11078.07 ms /  2381 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 7294 | stop processing: n_tokens = 16227, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.891 (> 0.100 thold), f_keep = 0.857
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 7442 | processing task, is_child = 0
slot update_slots: id  1 | task 7442 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15618
slot update_slots: id  1 | task 7442 | n_past = 13912, slot.prompt.tokens.size() = 16227, seq_id = 1, pos_min = 15457, n_swa = 128
slot update_slots: id  1 | task 7442 | restored context checkpoint (pos_min = 13078, pos_max = 13847, size = 18.056 MiB)
slot update_slots: id  1 | task 7442 | erased invalidated context checkpoint (pos_min = 15249, pos_max = 16018, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 7442 | n_tokens = 13847, memory_seq_rm [13847, end)
slot update_slots: id  1 | task 7442 | prompt processing progress, n_tokens = 15554, batch.n_tokens = 1707, progress = 0.995902
slot update_slots: id  1 | task 7442 | n_tokens = 15554, memory_seq_rm [15554, end)
slot update_slots: id  1 | task 7442 | prompt processing progress, n_tokens = 15618, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 7442 | prompt done, n_tokens = 15618, batch.n_tokens = 64
slot init_sampler: id  1 | task 7442 | init sampler, took 2.37 ms, tokens: text = 15618, total = 15618
slot update_slots: id  1 | task 7442 | created context checkpoint 3 of 8 (pos_min = 14784, pos_max = 15553, size = 18.056 MiB)
slot print_timing: id  1 | task 7442 | 
prompt eval time =    4057.17 ms /  1771 tokens (    2.29 ms per token,   436.51 tokens per second)
       eval time =    7447.47 ms /   185 tokens (   40.26 ms per token,    24.84 tokens per second)
      total time =   11504.64 ms /  1956 tokens
slot      release: id  1 | task 7442 | stop processing: n_tokens = 15802, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 7629 | processing task, is_child = 0
slot update_slots: id  1 | task 7629 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15864
slot update_slots: id  1 | task 7629 | n_tokens = 15644, memory_seq_rm [15644, end)
slot update_slots: id  1 | task 7629 | prompt processing progress, n_tokens = 15800, batch.n_tokens = 156, progress = 0.995966
slot update_slots: id  1 | task 7629 | n_tokens = 15800, memory_seq_rm [15800, end)
slot update_slots: id  1 | task 7629 | prompt processing progress, n_tokens = 15864, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 7629 | prompt done, n_tokens = 15864, batch.n_tokens = 64
slot init_sampler: id  1 | task 7629 | init sampler, took 2.35 ms, tokens: text = 15864, total = 15864
slot update_slots: id  1 | task 7629 | created context checkpoint 4 of 8 (pos_min = 15131, pos_max = 15799, size = 15.688 MiB)
slot print_timing: id  1 | task 7629 | 
prompt eval time =     816.32 ms /   220 tokens (    3.71 ms per token,   269.50 tokens per second)
       eval time =    1875.82 ms /    47 tokens (   39.91 ms per token,    25.06 tokens per second)
      total time =    2692.14 ms /   267 tokens
slot      release: id  1 | task 7629 | stop processing: n_tokens = 15910, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 7678 | processing task, is_child = 0
slot update_slots: id  1 | task 7678 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17023
slot update_slots: id  1 | task 7678 | n_tokens = 15885, memory_seq_rm [15885, end)
slot update_slots: id  1 | task 7678 | prompt processing progress, n_tokens = 16959, batch.n_tokens = 1074, progress = 0.996240
slot update_slots: id  1 | task 7678 | n_tokens = 16959, memory_seq_rm [16959, end)
slot update_slots: id  1 | task 7678 | prompt processing progress, n_tokens = 17023, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 7678 | prompt done, n_tokens = 17023, batch.n_tokens = 64
slot init_sampler: id  1 | task 7678 | init sampler, took 2.49 ms, tokens: text = 17023, total = 17023
slot update_slots: id  1 | task 7678 | created context checkpoint 5 of 8 (pos_min = 16189, pos_max = 16958, size = 18.056 MiB)
slot print_timing: id  1 | task 7678 | 
prompt eval time =    2948.41 ms /  1138 tokens (    2.59 ms per token,   385.97 tokens per second)
       eval time =    5863.50 ms /   137 tokens (   42.80 ms per token,    23.36 tokens per second)
      total time =    8811.91 ms /  1275 tokens
slot      release: id  1 | task 7678 | stop processing: n_tokens = 17159, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 7817 | processing task, is_child = 0
slot update_slots: id  1 | task 7817 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17197
slot update_slots: id  1 | task 7817 | n_tokens = 17044, memory_seq_rm [17044, end)
slot update_slots: id  1 | task 7817 | prompt processing progress, n_tokens = 17133, batch.n_tokens = 89, progress = 0.996278
slot update_slots: id  1 | task 7817 | n_tokens = 17133, memory_seq_rm [17133, end)
slot update_slots: id  1 | task 7817 | prompt processing progress, n_tokens = 17197, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 7817 | prompt done, n_tokens = 17197, batch.n_tokens = 64
slot init_sampler: id  1 | task 7817 | init sampler, took 2.50 ms, tokens: text = 17197, total = 17197
slot update_slots: id  1 | task 7817 | created context checkpoint 6 of 8 (pos_min = 16389, pos_max = 17132, size = 17.446 MiB)
slot print_timing: id  1 | task 7817 | 
prompt eval time =     772.06 ms /   153 tokens (    5.05 ms per token,   198.17 tokens per second)
       eval time =    1674.47 ms /    40 tokens (   41.86 ms per token,    23.89 tokens per second)
      total time =    2446.54 ms /   193 tokens
slot      release: id  1 | task 7817 | stop processing: n_tokens = 17236, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.909 (> 0.100 thold), f_keep = 0.906
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 7859 | processing task, is_child = 0
slot update_slots: id  1 | task 7859 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17184
slot update_slots: id  1 | task 7859 | n_past = 15618, slot.prompt.tokens.size() = 17236, seq_id = 1, pos_min = 16466, n_swa = 128
slot update_slots: id  1 | task 7859 | restored context checkpoint (pos_min = 15131, pos_max = 15799, size = 15.688 MiB)
slot update_slots: id  1 | task 7859 | erased invalidated context checkpoint (pos_min = 16189, pos_max = 16958, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 7859 | erased invalidated context checkpoint (pos_min = 16389, pos_max = 17132, n_swa = 128, size = 17.446 MiB)
slot update_slots: id  1 | task 7859 | n_tokens = 15618, memory_seq_rm [15618, end)
slot update_slots: id  1 | task 7859 | prompt processing progress, n_tokens = 17120, batch.n_tokens = 1502, progress = 0.996276
slot update_slots: id  1 | task 7859 | n_tokens = 17120, memory_seq_rm [17120, end)
slot update_slots: id  1 | task 7859 | prompt processing progress, n_tokens = 17184, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 7859 | prompt done, n_tokens = 17184, batch.n_tokens = 64
slot init_sampler: id  1 | task 7859 | init sampler, took 2.61 ms, tokens: text = 17184, total = 17184
slot update_slots: id  1 | task 7859 | created context checkpoint 5 of 8 (pos_min = 16350, pos_max = 17119, size = 18.056 MiB)
slot print_timing: id  1 | task 7859 | 
prompt eval time =    3551.08 ms /  1566 tokens (    2.27 ms per token,   440.99 tokens per second)
       eval time =   12006.97 ms /   296 tokens (   40.56 ms per token,    24.65 tokens per second)
      total time =   15558.05 ms /  1862 tokens
slot      release: id  1 | task 7859 | stop processing: n_tokens = 17479, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 8157 | processing task, is_child = 0
slot update_slots: id  1 | task 8157 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17423
slot update_slots: id  1 | task 8157 | n_tokens = 17185, memory_seq_rm [17185, end)
slot update_slots: id  1 | task 8157 | prompt processing progress, n_tokens = 17359, batch.n_tokens = 174, progress = 0.996327
slot update_slots: id  1 | task 8157 | n_tokens = 17359, memory_seq_rm [17359, end)
slot update_slots: id  1 | task 8157 | prompt processing progress, n_tokens = 17423, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 8157 | prompt done, n_tokens = 17423, batch.n_tokens = 64
slot init_sampler: id  1 | task 8157 | init sampler, took 2.42 ms, tokens: text = 17423, total = 17423
slot update_slots: id  1 | task 8157 | created context checkpoint 6 of 8 (pos_min = 16709, pos_max = 17358, size = 15.242 MiB)
slot print_timing: id  1 | task 8157 | 
prompt eval time =     846.69 ms /   238 tokens (    3.56 ms per token,   281.09 tokens per second)
       eval time =   93771.23 ms /  2360 tokens (   39.73 ms per token,    25.17 tokens per second)
      total time =   94617.93 ms /  2598 tokens
slot      release: id  1 | task 8157 | stop processing: n_tokens = 19782, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.952
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 10519 | processing task, is_child = 0
slot update_slots: id  1 | task 10519 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19819
slot update_slots: id  1 | task 10519 | n_past = 18836, slot.prompt.tokens.size() = 19782, seq_id = 1, pos_min = 19012, n_swa = 128
slot update_slots: id  1 | task 10519 | restored context checkpoint (pos_min = 16709, pos_max = 17358, size = 15.242 MiB)
slot update_slots: id  1 | task 10519 | n_tokens = 17358, memory_seq_rm [17358, end)
slot update_slots: id  1 | task 10519 | prompt processing progress, n_tokens = 19406, batch.n_tokens = 2048, progress = 0.979161
slot update_slots: id  1 | task 10519 | n_tokens = 19406, memory_seq_rm [19406, end)
slot update_slots: id  1 | task 10519 | prompt processing progress, n_tokens = 19755, batch.n_tokens = 349, progress = 0.996771
slot update_slots: id  1 | task 10519 | n_tokens = 19755, memory_seq_rm [19755, end)
slot update_slots: id  1 | task 10519 | prompt processing progress, n_tokens = 19819, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 10519 | prompt done, n_tokens = 19819, batch.n_tokens = 64
slot init_sampler: id  1 | task 10519 | init sampler, took 4.15 ms, tokens: text = 19819, total = 19819
slot update_slots: id  1 | task 10519 | created context checkpoint 7 of 8 (pos_min = 18985, pos_max = 19754, size = 18.056 MiB)
slot print_timing: id  1 | task 10519 | 
prompt eval time =    5641.72 ms /  2461 tokens (    2.29 ms per token,   436.21 tokens per second)
       eval time =    2817.18 ms /    71 tokens (   39.68 ms per token,    25.20 tokens per second)
      total time =    8458.89 ms /  2532 tokens
slot      release: id  1 | task 10519 | stop processing: n_tokens = 19889, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.941 (> 0.100 thold), f_keep = 0.876
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 10593 | processing task, is_child = 0
slot update_slots: id  1 | task 10593 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18513
slot update_slots: id  1 | task 10593 | n_past = 17423, slot.prompt.tokens.size() = 19889, seq_id = 1, pos_min = 19119, n_swa = 128
slot update_slots: id  1 | task 10593 | restored context checkpoint (pos_min = 16709, pos_max = 17358, size = 15.242 MiB)
slot update_slots: id  1 | task 10593 | erased invalidated context checkpoint (pos_min = 18985, pos_max = 19754, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 10593 | n_tokens = 17358, memory_seq_rm [17358, end)
slot update_slots: id  1 | task 10593 | prompt processing progress, n_tokens = 18449, batch.n_tokens = 1091, progress = 0.996543
slot update_slots: id  1 | task 10593 | n_tokens = 18449, memory_seq_rm [18449, end)
slot update_slots: id  1 | task 10593 | prompt processing progress, n_tokens = 18513, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 10593 | prompt done, n_tokens = 18513, batch.n_tokens = 64
slot init_sampler: id  1 | task 10593 | init sampler, took 2.74 ms, tokens: text = 18513, total = 18513
slot update_slots: id  1 | task 10593 | created context checkpoint 7 of 8 (pos_min = 17679, pos_max = 18448, size = 18.056 MiB)
slot print_timing: id  1 | task 10593 | 
prompt eval time =    3117.82 ms /  1155 tokens (    2.70 ms per token,   370.45 tokens per second)
       eval time =   12478.52 ms /   299 tokens (   41.73 ms per token,    23.96 tokens per second)
      total time =   15596.34 ms /  1454 tokens
slot      release: id  1 | task 10593 | stop processing: n_tokens = 18811, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 10894 | processing task, is_child = 0
slot update_slots: id  1 | task 10894 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18875
slot update_slots: id  1 | task 10894 | n_tokens = 18692, memory_seq_rm [18692, end)
slot update_slots: id  1 | task 10894 | prompt processing progress, n_tokens = 18811, batch.n_tokens = 119, progress = 0.996609
slot update_slots: id  1 | task 10894 | n_tokens = 18811, memory_seq_rm [18811, end)
slot update_slots: id  1 | task 10894 | prompt processing progress, n_tokens = 18875, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 10894 | prompt done, n_tokens = 18875, batch.n_tokens = 64
slot init_sampler: id  1 | task 10894 | init sampler, took 2.83 ms, tokens: text = 18875, total = 18875
slot update_slots: id  1 | task 10894 | created context checkpoint 8 of 8 (pos_min = 18047, pos_max = 18810, size = 17.915 MiB)
slot print_timing: id  1 | task 10894 | 
prompt eval time =     890.26 ms /   183 tokens (    4.86 ms per token,   205.56 tokens per second)
       eval time =    2214.31 ms /    51 tokens (   43.42 ms per token,    23.03 tokens per second)
      total time =    3104.57 ms /   234 tokens
slot      release: id  1 | task 10894 | stop processing: n_tokens = 18925, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 10947 | processing task, is_child = 0
slot update_slots: id  1 | task 10947 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19036
slot update_slots: id  1 | task 10947 | n_tokens = 18899, memory_seq_rm [18899, end)
slot update_slots: id  1 | task 10947 | prompt processing progress, n_tokens = 18972, batch.n_tokens = 73, progress = 0.996638
slot update_slots: id  1 | task 10947 | n_tokens = 18972, memory_seq_rm [18972, end)
slot update_slots: id  1 | task 10947 | prompt processing progress, n_tokens = 19036, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 10947 | prompt done, n_tokens = 19036, batch.n_tokens = 64
slot init_sampler: id  1 | task 10947 | init sampler, took 2.75 ms, tokens: text = 19036, total = 19036
slot update_slots: id  1 | task 10947 | erasing old context checkpoint (pos_min = 11860, pos_max = 12629, size = 18.056 MiB)
slot update_slots: id  1 | task 10947 | created context checkpoint 8 of 8 (pos_min = 18208, pos_max = 18971, size = 17.915 MiB)
slot print_timing: id  1 | task 10947 | 
prompt eval time =     823.68 ms /   137 tokens (    6.01 ms per token,   166.33 tokens per second)
       eval time =    5922.67 ms /   142 tokens (   41.71 ms per token,    23.98 tokens per second)
      total time =    6746.35 ms /   279 tokens
slot      release: id  1 | task 10947 | stop processing: n_tokens = 19177, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 11091 | processing task, is_child = 0
slot update_slots: id  1 | task 11091 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19241
slot update_slots: id  1 | task 11091 | n_tokens = 19057, memory_seq_rm [19057, end)
slot update_slots: id  1 | task 11091 | prompt processing progress, n_tokens = 19177, batch.n_tokens = 120, progress = 0.996674
slot update_slots: id  1 | task 11091 | n_tokens = 19177, memory_seq_rm [19177, end)
slot update_slots: id  1 | task 11091 | prompt processing progress, n_tokens = 19241, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 11091 | prompt done, n_tokens = 19241, batch.n_tokens = 64
slot init_sampler: id  1 | task 11091 | init sampler, took 4.18 ms, tokens: text = 19241, total = 19241
slot update_slots: id  1 | task 11091 | erasing old context checkpoint (pos_min = 13078, pos_max = 13847, size = 18.056 MiB)
slot update_slots: id  1 | task 11091 | created context checkpoint 8 of 8 (pos_min = 18413, pos_max = 19176, size = 17.915 MiB)
slot print_timing: id  1 | task 11091 | 
prompt eval time =     833.23 ms /   184 tokens (    4.53 ms per token,   220.83 tokens per second)
       eval time =    1935.88 ms /    49 tokens (   39.51 ms per token,    25.31 tokens per second)
      total time =    2769.11 ms /   233 tokens
slot      release: id  1 | task 11091 | stop processing: n_tokens = 19289, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 11142 | processing task, is_child = 0
slot update_slots: id  1 | task 11142 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19681
slot update_slots: id  1 | task 11142 | n_tokens = 19264, memory_seq_rm [19264, end)
slot update_slots: id  1 | task 11142 | prompt processing progress, n_tokens = 19617, batch.n_tokens = 353, progress = 0.996748
slot update_slots: id  1 | task 11142 | n_tokens = 19617, memory_seq_rm [19617, end)
slot update_slots: id  1 | task 11142 | prompt processing progress, n_tokens = 19681, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 11142 | prompt done, n_tokens = 19681, batch.n_tokens = 64
slot init_sampler: id  1 | task 11142 | init sampler, took 2.91 ms, tokens: text = 19681, total = 19681
slot update_slots: id  1 | task 11142 | erasing old context checkpoint (pos_min = 14784, pos_max = 15553, size = 18.056 MiB)
slot update_slots: id  1 | task 11142 | created context checkpoint 8 of 8 (pos_min = 18847, pos_max = 19616, size = 18.056 MiB)
slot print_timing: id  1 | task 11142 | 
prompt eval time =    1201.97 ms /   417 tokens (    2.88 ms per token,   346.93 tokens per second)
       eval time =    6341.91 ms /   158 tokens (   40.14 ms per token,    24.91 tokens per second)
      total time =    7543.89 ms /   575 tokens
slot      release: id  1 | task 11142 | stop processing: n_tokens = 19838, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 11302 | processing task, is_child = 0
slot update_slots: id  1 | task 11302 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19876
slot update_slots: id  1 | task 11302 | n_tokens = 19716, memory_seq_rm [19716, end)
slot update_slots: id  1 | task 11302 | prompt processing progress, n_tokens = 19812, batch.n_tokens = 96, progress = 0.996780
slot update_slots: id  1 | task 11302 | n_tokens = 19812, memory_seq_rm [19812, end)
slot update_slots: id  1 | task 11302 | prompt processing progress, n_tokens = 19876, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 11302 | prompt done, n_tokens = 19876, batch.n_tokens = 64
slot init_sampler: id  1 | task 11302 | init sampler, took 4.07 ms, tokens: text = 19876, total = 19876
slot update_slots: id  1 | task 11302 | erasing old context checkpoint (pos_min = 15131, pos_max = 15799, size = 15.688 MiB)
slot update_slots: id  1 | task 11302 | created context checkpoint 8 of 8 (pos_min = 19068, pos_max = 19811, size = 17.446 MiB)
slot print_timing: id  1 | task 11302 | 
prompt eval time =     694.23 ms /   160 tokens (    4.34 ms per token,   230.47 tokens per second)
       eval time =    2333.77 ms /    60 tokens (   38.90 ms per token,    25.71 tokens per second)
      total time =    3027.99 ms /   220 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 11302 | stop processing: n_tokens = 19935, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.942 (> 0.100 thold), f_keep = 0.929
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 11364 | processing task, is_child = 0
slot update_slots: id  1 | task 11364 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19657
slot update_slots: id  1 | task 11364 | n_past = 18513, slot.prompt.tokens.size() = 19935, seq_id = 1, pos_min = 19165, n_swa = 128
slot update_slots: id  1 | task 11364 | restored context checkpoint (pos_min = 18208, pos_max = 18971, size = 17.915 MiB)
slot update_slots: id  1 | task 11364 | erased invalidated context checkpoint (pos_min = 18413, pos_max = 19176, n_swa = 128, size = 17.915 MiB)
slot update_slots: id  1 | task 11364 | erased invalidated context checkpoint (pos_min = 18847, pos_max = 19616, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 11364 | erased invalidated context checkpoint (pos_min = 19068, pos_max = 19811, n_swa = 128, size = 17.446 MiB)
slot update_slots: id  1 | task 11364 | n_tokens = 18513, memory_seq_rm [18513, end)
slot update_slots: id  1 | task 11364 | prompt processing progress, n_tokens = 19593, batch.n_tokens = 1080, progress = 0.996744
slot update_slots: id  1 | task 11364 | n_tokens = 19593, memory_seq_rm [19593, end)
slot update_slots: id  1 | task 11364 | prompt processing progress, n_tokens = 19657, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 11364 | prompt done, n_tokens = 19657, batch.n_tokens = 64
slot init_sampler: id  1 | task 11364 | init sampler, took 2.71 ms, tokens: text = 19657, total = 19657
slot update_slots: id  1 | task 11364 | created context checkpoint 6 of 8 (pos_min = 18950, pos_max = 19592, size = 15.078 MiB)
slot print_timing: id  1 | task 11364 | 
prompt eval time =    2799.55 ms /  1144 tokens (    2.45 ms per token,   408.64 tokens per second)
       eval time =   61479.13 ms /  1525 tokens (   40.31 ms per token,    24.81 tokens per second)
      total time =   64278.67 ms /  2669 tokens
slot      release: id  1 | task 11364 | stop processing: n_tokens = 21181, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12891 | processing task, is_child = 0
slot update_slots: id  1 | task 12891 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21227
slot update_slots: id  1 | task 12891 | n_tokens = 21014, memory_seq_rm [21014, end)
slot update_slots: id  1 | task 12891 | prompt processing progress, n_tokens = 21163, batch.n_tokens = 149, progress = 0.996985
slot update_slots: id  1 | task 12891 | n_tokens = 21163, memory_seq_rm [21163, end)
slot update_slots: id  1 | task 12891 | prompt processing progress, n_tokens = 21227, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12891 | prompt done, n_tokens = 21227, batch.n_tokens = 64
slot init_sampler: id  1 | task 12891 | init sampler, took 4.09 ms, tokens: text = 21227, total = 21227
slot update_slots: id  1 | task 12891 | created context checkpoint 7 of 8 (pos_min = 20411, pos_max = 21162, size = 17.634 MiB)
slot print_timing: id  1 | task 12891 | 
prompt eval time =     733.07 ms /   213 tokens (    3.44 ms per token,   290.56 tokens per second)
       eval time =    5819.49 ms /   158 tokens (   36.83 ms per token,    27.15 tokens per second)
      total time =    6552.56 ms /   371 tokens
slot      release: id  1 | task 12891 | stop processing: n_tokens = 21384, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13051 | processing task, is_child = 0
slot update_slots: id  1 | task 13051 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21426
slot update_slots: id  1 | task 13051 | n_tokens = 21304, memory_seq_rm [21304, end)
slot update_slots: id  1 | task 13051 | prompt processing progress, n_tokens = 21362, batch.n_tokens = 58, progress = 0.997013
slot update_slots: id  1 | task 13051 | n_tokens = 21362, memory_seq_rm [21362, end)
slot update_slots: id  1 | task 13051 | prompt processing progress, n_tokens = 21426, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13051 | prompt done, n_tokens = 21426, batch.n_tokens = 64
slot init_sampler: id  1 | task 13051 | init sampler, took 3.06 ms, tokens: text = 21426, total = 21426
slot update_slots: id  1 | task 13051 | created context checkpoint 8 of 8 (pos_min = 20614, pos_max = 21361, size = 17.540 MiB)
slot print_timing: id  1 | task 13051 | 
prompt eval time =     535.07 ms /   122 tokens (    4.39 ms per token,   228.01 tokens per second)
       eval time =    1790.36 ms /    49 tokens (   36.54 ms per token,    27.37 tokens per second)
      total time =    2325.44 ms /   171 tokens
slot      release: id  1 | task 13051 | stop processing: n_tokens = 21474, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13102 | processing task, is_child = 0
slot update_slots: id  1 | task 13102 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21684
slot update_slots: id  1 | task 13102 | n_tokens = 21447, memory_seq_rm [21447, end)
slot update_slots: id  1 | task 13102 | prompt processing progress, n_tokens = 21620, batch.n_tokens = 173, progress = 0.997048
slot update_slots: id  1 | task 13102 | n_tokens = 21620, memory_seq_rm [21620, end)
slot update_slots: id  1 | task 13102 | prompt processing progress, n_tokens = 21684, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13102 | prompt done, n_tokens = 21684, batch.n_tokens = 64
slot init_sampler: id  1 | task 13102 | init sampler, took 3.10 ms, tokens: text = 21684, total = 21684
slot update_slots: id  1 | task 13102 | erasing old context checkpoint (pos_min = 16350, pos_max = 17119, size = 18.056 MiB)
slot update_slots: id  1 | task 13102 | created context checkpoint 8 of 8 (pos_min = 20869, pos_max = 21619, size = 17.610 MiB)
slot print_timing: id  1 | task 13102 | 
prompt eval time =     788.69 ms /   237 tokens (    3.33 ms per token,   300.50 tokens per second)
       eval time =    7489.18 ms /   200 tokens (   37.45 ms per token,    26.71 tokens per second)
      total time =    8277.86 ms /   437 tokens
slot      release: id  1 | task 13102 | stop processing: n_tokens = 21883, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13304 | processing task, is_child = 0
slot update_slots: id  1 | task 13304 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21929
slot update_slots: id  1 | task 13304 | n_tokens = 21740, memory_seq_rm [21740, end)
slot update_slots: id  1 | task 13304 | prompt processing progress, n_tokens = 21865, batch.n_tokens = 125, progress = 0.997082
slot update_slots: id  1 | task 13304 | n_tokens = 21865, memory_seq_rm [21865, end)
slot update_slots: id  1 | task 13304 | prompt processing progress, n_tokens = 21929, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13304 | prompt done, n_tokens = 21929, batch.n_tokens = 64
slot init_sampler: id  1 | task 13304 | init sampler, took 3.05 ms, tokens: text = 21929, total = 21929
slot update_slots: id  1 | task 13304 | erasing old context checkpoint (pos_min = 16709, pos_max = 17358, size = 15.242 MiB)
slot update_slots: id  1 | task 13304 | created context checkpoint 8 of 8 (pos_min = 21132, pos_max = 21864, size = 17.188 MiB)
slot print_timing: id  1 | task 13304 | 
prompt eval time =     906.42 ms /   189 tokens (    4.80 ms per token,   208.51 tokens per second)
       eval time =    4203.14 ms /   107 tokens (   39.28 ms per token,    25.46 tokens per second)
      total time =    5109.57 ms /   296 tokens
slot      release: id  1 | task 13304 | stop processing: n_tokens = 22035, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13413 | processing task, is_child = 0
slot update_slots: id  1 | task 13413 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 22079
slot update_slots: id  1 | task 13413 | n_tokens = 22006, memory_seq_rm [22006, end)
slot update_slots: id  1 | task 13413 | prompt processing progress, n_tokens = 22015, batch.n_tokens = 9, progress = 0.997101
slot update_slots: id  1 | task 13413 | n_tokens = 22015, memory_seq_rm [22015, end)
slot update_slots: id  1 | task 13413 | prompt processing progress, n_tokens = 22079, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13413 | prompt done, n_tokens = 22079, batch.n_tokens = 64
slot init_sampler: id  1 | task 13413 | init sampler, took 3.64 ms, tokens: text = 22079, total = 22079
slot update_slots: id  1 | task 13413 | erasing old context checkpoint (pos_min = 17679, pos_max = 18448, size = 18.056 MiB)
slot update_slots: id  1 | task 13413 | created context checkpoint 8 of 8 (pos_min = 21284, pos_max = 22014, size = 17.141 MiB)
slot print_timing: id  1 | task 13413 | 
prompt eval time =     432.89 ms /    73 tokens (    5.93 ms per token,   168.63 tokens per second)
       eval time =    1092.89 ms /    28 tokens (   39.03 ms per token,    25.62 tokens per second)
      total time =    1525.78 ms /   101 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 13413 | stop processing: n_tokens = 22106, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13443 | processing task, is_child = 0
slot update_slots: id  1 | task 13443 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 22697
slot update_slots: id  1 | task 13443 | n_tokens = 22079, memory_seq_rm [22079, end)
slot update_slots: id  1 | task 13443 | prompt processing progress, n_tokens = 22633, batch.n_tokens = 554, progress = 0.997180
slot update_slots: id  1 | task 13443 | n_tokens = 22633, memory_seq_rm [22633, end)
slot update_slots: id  1 | task 13443 | prompt processing progress, n_tokens = 22697, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13443 | prompt done, n_tokens = 22697, batch.n_tokens = 64
slot init_sampler: id  1 | task 13443 | init sampler, took 3.98 ms, tokens: text = 22697, total = 22697
slot update_slots: id  1 | task 13443 | erasing old context checkpoint (pos_min = 18047, pos_max = 18810, size = 17.915 MiB)
slot update_slots: id  1 | task 13443 | created context checkpoint 8 of 8 (pos_min = 21863, pos_max = 22632, size = 18.056 MiB)
slot print_timing: id  1 | task 13443 | 
prompt eval time =    1688.45 ms /   618 tokens (    2.73 ms per token,   366.02 tokens per second)
       eval time =   32090.11 ms /   794 tokens (   40.42 ms per token,    24.74 tokens per second)
      total time =   33778.56 ms /  1412 tokens
slot      release: id  1 | task 13443 | stop processing: n_tokens = 23490, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 14239 | processing task, is_child = 0
slot update_slots: id  1 | task 14239 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 23536
slot update_slots: id  1 | task 14239 | n_tokens = 22858, memory_seq_rm [22858, end)
slot update_slots: id  1 | task 14239 | prompt processing progress, n_tokens = 23472, batch.n_tokens = 614, progress = 0.997281
slot update_slots: id  1 | task 14239 | n_tokens = 23472, memory_seq_rm [23472, end)
slot update_slots: id  1 | task 14239 | prompt processing progress, n_tokens = 23536, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 14239 | prompt done, n_tokens = 23536, batch.n_tokens = 64
slot init_sampler: id  1 | task 14239 | init sampler, took 3.28 ms, tokens: text = 23536, total = 23536
slot update_slots: id  1 | task 14239 | erasing old context checkpoint (pos_min = 18208, pos_max = 18971, size = 17.915 MiB)
slot update_slots: id  1 | task 14239 | created context checkpoint 8 of 8 (pos_min = 22831, pos_max = 23471, size = 15.031 MiB)
slot print_timing: id  1 | task 14239 | 
prompt eval time =    1888.97 ms /   678 tokens (    2.79 ms per token,   358.93 tokens per second)
       eval time =    2840.49 ms /    71 tokens (   40.01 ms per token,    25.00 tokens per second)
      total time =    4729.46 ms /   749 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 14239 | stop processing: n_tokens = 23606, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 14312 | processing task, is_child = 0
slot update_slots: id  1 | task 14312 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 23658
slot update_slots: id  1 | task 14312 | n_tokens = 23576, memory_seq_rm [23576, end)
slot update_slots: id  1 | task 14312 | prompt processing progress, n_tokens = 23594, batch.n_tokens = 18, progress = 0.997295
slot update_slots: id  1 | task 14312 | n_tokens = 23594, memory_seq_rm [23594, end)
slot update_slots: id  1 | task 14312 | prompt processing progress, n_tokens = 23658, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 14312 | prompt done, n_tokens = 23658, batch.n_tokens = 64
slot init_sampler: id  1 | task 14312 | init sampler, took 6.55 ms, tokens: text = 23658, total = 23658
slot update_slots: id  1 | task 14312 | erasing old context checkpoint (pos_min = 18950, pos_max = 19592, size = 15.078 MiB)
slot update_slots: id  1 | task 14312 | created context checkpoint 8 of 8 (pos_min = 22858, pos_max = 23593, size = 17.259 MiB)
slot print_timing: id  1 | task 14312 | 
prompt eval time =     480.47 ms /    82 tokens (    5.86 ms per token,   170.67 tokens per second)
       eval time =   14988.23 ms /   371 tokens (   40.40 ms per token,    24.75 tokens per second)
      total time =   15468.70 ms /   453 tokens
slot      release: id  1 | task 14312 | stop processing: n_tokens = 24028, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 14685 | processing task, is_child = 0
slot update_slots: id  1 | task 14685 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 24059
slot update_slots: id  1 | task 14685 | n_tokens = 23673, memory_seq_rm [23673, end)
slot update_slots: id  1 | task 14685 | prompt processing progress, n_tokens = 23995, batch.n_tokens = 322, progress = 0.997340
slot update_slots: id  1 | task 14685 | n_tokens = 23995, memory_seq_rm [23995, end)
slot update_slots: id  1 | task 14685 | prompt processing progress, n_tokens = 24059, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 14685 | prompt done, n_tokens = 24059, batch.n_tokens = 64
slot init_sampler: id  1 | task 14685 | init sampler, took 3.37 ms, tokens: text = 24059, total = 24059
slot update_slots: id  1 | task 14685 | erasing old context checkpoint (pos_min = 20411, pos_max = 21162, size = 17.634 MiB)
slot update_slots: id  1 | task 14685 | created context checkpoint 8 of 8 (pos_min = 23258, pos_max = 23994, size = 17.282 MiB)
slot print_timing: id  1 | task 14685 | 
prompt eval time =    1120.03 ms /   386 tokens (    2.90 ms per token,   344.63 tokens per second)
       eval time =  165801.92 ms /  4096 tokens (   40.48 ms per token,    24.70 tokens per second)
      total time =  166921.95 ms /  4482 tokens
slot      release: id  1 | task 14685 | stop processing: n_tokens = 28154, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 14685
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.138 (> 0.100 thold), f_keep = 0.004
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 28154, total state size = 678.238 MiB
srv          load:  - looking for better prompt, base f_keep = 0.004, sim = 0.138
srv        update:  - cache state: 1 prompts, 815.346 MiB (limits: 8192.000 MiB, 56064 tokens, 282870 est)
srv        update:    - prompt 0x56346d802600:   28154 tokens, checkpoints:  8,   815.346 MiB
srv  get_availabl: prompt cache update took 637.09 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18784 | processing task, is_child = 0
slot update_slots: id  1 | task 18784 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 877
slot update_slots: id  1 | task 18784 | n_past = 121, slot.prompt.tokens.size() = 28154, seq_id = 1, pos_min = 27384, n_swa = 128
slot update_slots: id  1 | task 18784 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 18784 | erased invalidated context checkpoint (pos_min = 20614, pos_max = 21361, n_swa = 128, size = 17.540 MiB)
slot update_slots: id  1 | task 18784 | erased invalidated context checkpoint (pos_min = 20869, pos_max = 21619, n_swa = 128, size = 17.610 MiB)
slot update_slots: id  1 | task 18784 | erased invalidated context checkpoint (pos_min = 21132, pos_max = 21864, n_swa = 128, size = 17.188 MiB)
slot update_slots: id  1 | task 18784 | erased invalidated context checkpoint (pos_min = 21284, pos_max = 22014, n_swa = 128, size = 17.141 MiB)
slot update_slots: id  1 | task 18784 | erased invalidated context checkpoint (pos_min = 21863, pos_max = 22632, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 18784 | erased invalidated context checkpoint (pos_min = 22831, pos_max = 23471, n_swa = 128, size = 15.031 MiB)
slot update_slots: id  1 | task 18784 | erased invalidated context checkpoint (pos_min = 22858, pos_max = 23593, n_swa = 128, size = 17.259 MiB)
slot update_slots: id  1 | task 18784 | erased invalidated context checkpoint (pos_min = 23258, pos_max = 23994, n_swa = 128, size = 17.282 MiB)
slot update_slots: id  1 | task 18784 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 18784 | prompt processing progress, n_tokens = 813, batch.n_tokens = 813, progress = 0.927024
slot update_slots: id  1 | task 18784 | n_tokens = 813, memory_seq_rm [813, end)
slot update_slots: id  1 | task 18784 | prompt processing progress, n_tokens = 877, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 18784 | prompt done, n_tokens = 877, batch.n_tokens = 64
slot init_sampler: id  1 | task 18784 | init sampler, took 0.15 ms, tokens: text = 877, total = 877
slot update_slots: id  1 | task 18784 | created context checkpoint 1 of 8 (pos_min = 43, pos_max = 812, size = 18.056 MiB)
slot print_timing: id  1 | task 18784 | 
prompt eval time =    1812.48 ms /   877 tokens (    2.07 ms per token,   483.87 tokens per second)
       eval time =    2122.26 ms /    56 tokens (   37.90 ms per token,    26.39 tokens per second)
      total time =    3934.74 ms /   933 tokens
slot      release: id  1 | task 18784 | stop processing: n_tokens = 932, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.440 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18842 | processing task, is_child = 0
slot update_slots: id  1 | task 18842 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2045
slot update_slots: id  1 | task 18842 | n_tokens = 899, memory_seq_rm [899, end)
slot update_slots: id  1 | task 18842 | prompt processing progress, n_tokens = 1981, batch.n_tokens = 1082, progress = 0.968704
slot update_slots: id  1 | task 18842 | n_tokens = 1981, memory_seq_rm [1981, end)
slot update_slots: id  1 | task 18842 | prompt processing progress, n_tokens = 2045, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 18842 | prompt done, n_tokens = 2045, batch.n_tokens = 64
slot init_sampler: id  1 | task 18842 | init sampler, took 0.35 ms, tokens: text = 2045, total = 2045
slot update_slots: id  1 | task 18842 | created context checkpoint 2 of 8 (pos_min = 1211, pos_max = 1980, size = 18.056 MiB)
slot print_timing: id  1 | task 18842 | 
prompt eval time =    2495.72 ms /  1146 tokens (    2.18 ms per token,   459.19 tokens per second)
       eval time =    2255.27 ms /    57 tokens (   39.57 ms per token,    25.27 tokens per second)
      total time =    4750.99 ms /  1203 tokens
slot      release: id  1 | task 18842 | stop processing: n_tokens = 2101, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18901 | processing task, is_child = 0
slot update_slots: id  1 | task 18901 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2145
slot update_slots: id  1 | task 18901 | n_tokens = 2070, memory_seq_rm [2070, end)
slot update_slots: id  1 | task 18901 | prompt processing progress, n_tokens = 2081, batch.n_tokens = 11, progress = 0.970163
slot update_slots: id  1 | task 18901 | n_tokens = 2081, memory_seq_rm [2081, end)
slot update_slots: id  1 | task 18901 | prompt processing progress, n_tokens = 2145, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 18901 | prompt done, n_tokens = 2145, batch.n_tokens = 64
slot init_sampler: id  1 | task 18901 | init sampler, took 0.33 ms, tokens: text = 2145, total = 2145
slot update_slots: id  1 | task 18901 | created context checkpoint 3 of 8 (pos_min = 1331, pos_max = 2080, size = 17.587 MiB)
slot print_timing: id  1 | task 18901 | 
prompt eval time =     388.89 ms /    75 tokens (    5.19 ms per token,   192.86 tokens per second)
       eval time =    1651.79 ms /    42 tokens (   39.33 ms per token,    25.43 tokens per second)
      total time =    2040.68 ms /   117 tokens
slot      release: id  1 | task 18901 | stop processing: n_tokens = 2186, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.605 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18945 | processing task, is_child = 0
slot update_slots: id  1 | task 18945 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3560
slot update_slots: id  1 | task 18945 | n_tokens = 2154, memory_seq_rm [2154, end)
slot update_slots: id  1 | task 18945 | prompt processing progress, n_tokens = 3496, batch.n_tokens = 1342, progress = 0.982022
slot update_slots: id  1 | task 18945 | n_tokens = 3496, memory_seq_rm [3496, end)
slot update_slots: id  1 | task 18945 | prompt processing progress, n_tokens = 3560, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 18945 | prompt done, n_tokens = 3560, batch.n_tokens = 64
slot init_sampler: id  1 | task 18945 | init sampler, took 0.61 ms, tokens: text = 3560, total = 3560
slot update_slots: id  1 | task 18945 | created context checkpoint 4 of 8 (pos_min = 2726, pos_max = 3495, size = 18.056 MiB)
slot print_timing: id  1 | task 18945 | 
prompt eval time =    2885.73 ms /  1406 tokens (    2.05 ms per token,   487.22 tokens per second)
       eval time =    1985.57 ms /    50 tokens (   39.71 ms per token,    25.18 tokens per second)
      total time =    4871.30 ms /  1456 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 18945 | stop processing: n_tokens = 3609, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.650 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18997 | processing task, is_child = 0
slot update_slots: id  1 | task 18997 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5499
slot update_slots: id  1 | task 18997 | n_tokens = 3574, memory_seq_rm [3574, end)
slot update_slots: id  1 | task 18997 | prompt processing progress, n_tokens = 5435, batch.n_tokens = 1861, progress = 0.988362
slot update_slots: id  1 | task 18997 | n_tokens = 5435, memory_seq_rm [5435, end)
slot update_slots: id  1 | task 18997 | prompt processing progress, n_tokens = 5499, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 18997 | prompt done, n_tokens = 5499, batch.n_tokens = 64
slot init_sampler: id  1 | task 18997 | init sampler, took 1.06 ms, tokens: text = 5499, total = 5499
slot update_slots: id  1 | task 18997 | created context checkpoint 5 of 8 (pos_min = 4665, pos_max = 5434, size = 18.056 MiB)
slot print_timing: id  1 | task 18997 | 
prompt eval time =    4177.85 ms /  1925 tokens (    2.17 ms per token,   460.76 tokens per second)
       eval time =    1741.98 ms /    44 tokens (   39.59 ms per token,    25.26 tokens per second)
      total time =    5919.83 ms /  1969 tokens
slot      release: id  1 | task 18997 | stop processing: n_tokens = 5542, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.730 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19043 | processing task, is_child = 0
slot update_slots: id  1 | task 19043 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7545
slot update_slots: id  1 | task 19043 | n_tokens = 5507, memory_seq_rm [5507, end)
slot update_slots: id  1 | task 19043 | prompt processing progress, n_tokens = 7481, batch.n_tokens = 1974, progress = 0.991518
slot update_slots: id  1 | task 19043 | n_tokens = 7481, memory_seq_rm [7481, end)
slot update_slots: id  1 | task 19043 | prompt processing progress, n_tokens = 7545, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 19043 | prompt done, n_tokens = 7545, batch.n_tokens = 64
slot init_sampler: id  1 | task 19043 | init sampler, took 1.04 ms, tokens: text = 7545, total = 7545
slot update_slots: id  1 | task 19043 | created context checkpoint 6 of 8 (pos_min = 6711, pos_max = 7480, size = 18.056 MiB)
slot print_timing: id  1 | task 19043 | 
prompt eval time =    4221.24 ms /  2038 tokens (    2.07 ms per token,   482.80 tokens per second)
       eval time =    1742.58 ms /    44 tokens (   39.60 ms per token,    25.25 tokens per second)
      total time =    5963.82 ms /  2082 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 19043 | stop processing: n_tokens = 7588, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.893 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19089 | processing task, is_child = 0
slot update_slots: id  1 | task 19089 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8461
slot update_slots: id  1 | task 19089 | n_tokens = 7553, memory_seq_rm [7553, end)
slot update_slots: id  1 | task 19089 | prompt processing progress, n_tokens = 8397, batch.n_tokens = 844, progress = 0.992436
slot update_slots: id  1 | task 19089 | n_tokens = 8397, memory_seq_rm [8397, end)
slot update_slots: id  1 | task 19089 | prompt processing progress, n_tokens = 8461, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 19089 | prompt done, n_tokens = 8461, batch.n_tokens = 64
slot init_sampler: id  1 | task 19089 | init sampler, took 1.82 ms, tokens: text = 8461, total = 8461
slot update_slots: id  1 | task 19089 | created context checkpoint 7 of 8 (pos_min = 7627, pos_max = 8396, size = 18.056 MiB)
slot print_timing: id  1 | task 19089 | 
prompt eval time =    2028.15 ms /   908 tokens (    2.23 ms per token,   447.70 tokens per second)
       eval time =    1560.52 ms /    41 tokens (   38.06 ms per token,    26.27 tokens per second)
      total time =    3588.67 ms /   949 tokens
slot      release: id  1 | task 19089 | stop processing: n_tokens = 8501, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19132 | processing task, is_child = 0
slot update_slots: id  1 | task 19132 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8533
slot update_slots: id  1 | task 19132 | n_tokens = 8469, memory_seq_rm [8469, end)
slot update_slots: id  1 | task 19132 | prompt processing progress, n_tokens = 8533, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 19132 | prompt done, n_tokens = 8533, batch.n_tokens = 64
slot init_sampler: id  1 | task 19132 | init sampler, took 1.17 ms, tokens: text = 8533, total = 8533
slot update_slots: id  1 | task 19132 | created context checkpoint 8 of 8 (pos_min = 7731, pos_max = 8468, size = 17.306 MiB)
slot print_timing: id  1 | task 19132 | 
prompt eval time =     303.60 ms /    64 tokens (    4.74 ms per token,   210.81 tokens per second)
       eval time =    5366.11 ms /   146 tokens (   36.75 ms per token,    27.21 tokens per second)
      total time =    5669.70 ms /   210 tokens
slot      release: id  1 | task 19132 | stop processing: n_tokens = 8678, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.857 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19279 | processing task, is_child = 0
slot update_slots: id  1 | task 19279 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10088
slot update_slots: id  1 | task 19279 | n_tokens = 8647, memory_seq_rm [8647, end)
slot update_slots: id  1 | task 19279 | prompt processing progress, n_tokens = 10024, batch.n_tokens = 1377, progress = 0.993656
slot update_slots: id  1 | task 19279 | n_tokens = 10024, memory_seq_rm [10024, end)
slot update_slots: id  1 | task 19279 | prompt processing progress, n_tokens = 10088, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 19279 | prompt done, n_tokens = 10088, batch.n_tokens = 64
slot init_sampler: id  1 | task 19279 | init sampler, took 1.40 ms, tokens: text = 10088, total = 10088
slot update_slots: id  1 | task 19279 | erasing old context checkpoint (pos_min = 43, pos_max = 812, size = 18.056 MiB)
slot update_slots: id  1 | task 19279 | created context checkpoint 8 of 8 (pos_min = 9254, pos_max = 10023, size = 18.056 MiB)
slot print_timing: id  1 | task 19279 | 
prompt eval time =    2990.24 ms /  1441 tokens (    2.08 ms per token,   481.90 tokens per second)
       eval time =    1790.07 ms /    50 tokens (   35.80 ms per token,    27.93 tokens per second)
      total time =    4780.32 ms /  1491 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 19279 | stop processing: n_tokens = 10137, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.832 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19331 | processing task, is_child = 0
slot update_slots: id  1 | task 19331 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12148
slot update_slots: id  1 | task 19331 | n_tokens = 10104, memory_seq_rm [10104, end)
slot update_slots: id  1 | task 19331 | prompt processing progress, n_tokens = 12084, batch.n_tokens = 1980, progress = 0.994732
slot update_slots: id  1 | task 19331 | n_tokens = 12084, memory_seq_rm [12084, end)
slot update_slots: id  1 | task 19331 | prompt processing progress, n_tokens = 12148, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 19331 | prompt done, n_tokens = 12148, batch.n_tokens = 64
slot init_sampler: id  1 | task 19331 | init sampler, took 1.80 ms, tokens: text = 12148, total = 12148
slot update_slots: id  1 | task 19331 | erasing old context checkpoint (pos_min = 1211, pos_max = 1980, size = 18.056 MiB)
slot update_slots: id  1 | task 19331 | created context checkpoint 8 of 8 (pos_min = 11314, pos_max = 12083, size = 18.056 MiB)
slot print_timing: id  1 | task 19331 | 
prompt eval time =    4093.65 ms /  2044 tokens (    2.00 ms per token,   499.31 tokens per second)
       eval time =    1509.79 ms /    43 tokens (   35.11 ms per token,    28.48 tokens per second)
      total time =    5603.44 ms /  2087 tokens
slot      release: id  1 | task 19331 | stop processing: n_tokens = 12190, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.848 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19376 | processing task, is_child = 0
slot update_slots: id  1 | task 19376 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14330
slot update_slots: id  1 | task 19376 | n_tokens = 12156, memory_seq_rm [12156, end)
slot update_slots: id  1 | task 19376 | prompt processing progress, n_tokens = 14204, batch.n_tokens = 2048, progress = 0.991207
slot update_slots: id  1 | task 19376 | n_tokens = 14204, memory_seq_rm [14204, end)
slot update_slots: id  1 | task 19376 | prompt processing progress, n_tokens = 14266, batch.n_tokens = 62, progress = 0.995534
slot update_slots: id  1 | task 19376 | n_tokens = 14266, memory_seq_rm [14266, end)
slot update_slots: id  1 | task 19376 | prompt processing progress, n_tokens = 14330, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 19376 | prompt done, n_tokens = 14330, batch.n_tokens = 64
slot init_sampler: id  1 | task 19376 | init sampler, took 2.27 ms, tokens: text = 14330, total = 14330
slot update_slots: id  1 | task 19376 | erasing old context checkpoint (pos_min = 1331, pos_max = 2080, size = 17.587 MiB)
slot update_slots: id  1 | task 19376 | created context checkpoint 8 of 8 (pos_min = 13496, pos_max = 14265, size = 18.056 MiB)
slot print_timing: id  1 | task 19376 | 
prompt eval time =    4639.31 ms /  2174 tokens (    2.13 ms per token,   468.60 tokens per second)
       eval time =    9011.53 ms /   253 tokens (   35.62 ms per token,    28.08 tokens per second)
      total time =   13650.84 ms /  2427 tokens
slot      release: id  1 | task 19376 | stop processing: n_tokens = 14582, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19632 | processing task, is_child = 0
slot update_slots: id  1 | task 19632 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14821
slot update_slots: id  1 | task 19632 | n_tokens = 14548, memory_seq_rm [14548, end)
slot update_slots: id  1 | task 19632 | prompt processing progress, n_tokens = 14757, batch.n_tokens = 209, progress = 0.995682
slot update_slots: id  1 | task 19632 | n_tokens = 14757, memory_seq_rm [14757, end)
slot update_slots: id  1 | task 19632 | prompt processing progress, n_tokens = 14821, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 19632 | prompt done, n_tokens = 14821, batch.n_tokens = 64
slot init_sampler: id  1 | task 19632 | init sampler, took 2.12 ms, tokens: text = 14821, total = 14821
slot update_slots: id  1 | task 19632 | erasing old context checkpoint (pos_min = 2726, pos_max = 3495, size = 18.056 MiB)
slot update_slots: id  1 | task 19632 | created context checkpoint 8 of 8 (pos_min = 13987, pos_max = 14756, size = 18.056 MiB)
slot print_timing: id  1 | task 19632 | 
prompt eval time =     860.09 ms /   273 tokens (    3.15 ms per token,   317.41 tokens per second)
       eval time =   44741.77 ms /  1150 tokens (   38.91 ms per token,    25.70 tokens per second)
      total time =   45601.86 ms /  1423 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 19632 | stop processing: n_tokens = 15970, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.961
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 20784 | processing task, is_child = 0
slot update_slots: id  1 | task 20784 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15998
slot update_slots: id  1 | task 20784 | n_tokens = 15353, memory_seq_rm [15353, end)
slot update_slots: id  1 | task 20784 | prompt processing progress, n_tokens = 15934, batch.n_tokens = 581, progress = 0.996000
slot update_slots: id  1 | task 20784 | n_tokens = 15934, memory_seq_rm [15934, end)
slot update_slots: id  1 | task 20784 | prompt processing progress, n_tokens = 15998, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 20784 | prompt done, n_tokens = 15998, batch.n_tokens = 64
slot init_sampler: id  1 | task 20784 | init sampler, took 2.42 ms, tokens: text = 15998, total = 15998
slot update_slots: id  1 | task 20784 | erasing old context checkpoint (pos_min = 4665, pos_max = 5434, size = 18.056 MiB)
slot update_slots: id  1 | task 20784 | created context checkpoint 8 of 8 (pos_min = 15210, pos_max = 15933, size = 16.977 MiB)
slot print_timing: id  1 | task 20784 | 
prompt eval time =    1709.25 ms /   645 tokens (    2.65 ms per token,   377.36 tokens per second)
       eval time =    4041.41 ms /   101 tokens (   40.01 ms per token,    24.99 tokens per second)
      total time =    5750.65 ms /   746 tokens
slot      release: id  1 | task 20784 | stop processing: n_tokens = 16098, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 20887 | processing task, is_child = 0
slot update_slots: id  1 | task 20887 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16144
slot update_slots: id  1 | task 20887 | n_tokens = 16014, memory_seq_rm [16014, end)
slot update_slots: id  1 | task 20887 | prompt processing progress, n_tokens = 16080, batch.n_tokens = 66, progress = 0.996036
slot update_slots: id  1 | task 20887 | n_tokens = 16080, memory_seq_rm [16080, end)
slot update_slots: id  1 | task 20887 | prompt processing progress, n_tokens = 16144, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 20887 | prompt done, n_tokens = 16144, batch.n_tokens = 64
slot init_sampler: id  1 | task 20887 | init sampler, took 2.29 ms, tokens: text = 16144, total = 16144
slot update_slots: id  1 | task 20887 | erasing old context checkpoint (pos_min = 6711, pos_max = 7480, size = 18.056 MiB)
slot update_slots: id  1 | task 20887 | created context checkpoint 8 of 8 (pos_min = 15353, pos_max = 16079, size = 17.048 MiB)
slot print_timing: id  1 | task 20887 | 
prompt eval time =     662.52 ms /   130 tokens (    5.10 ms per token,   196.22 tokens per second)
       eval time =   23084.40 ms /   572 tokens (   40.36 ms per token,    24.78 tokens per second)
      total time =   23746.93 ms /   702 tokens
slot      release: id  1 | task 20887 | stop processing: n_tokens = 16715, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 21461 | processing task, is_child = 0
slot update_slots: id  1 | task 21461 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16753
slot update_slots: id  1 | task 21461 | n_tokens = 16155, memory_seq_rm [16155, end)
slot update_slots: id  1 | task 21461 | prompt processing progress, n_tokens = 16689, batch.n_tokens = 534, progress = 0.996180
slot update_slots: id  1 | task 21461 | n_tokens = 16689, memory_seq_rm [16689, end)
slot update_slots: id  1 | task 21461 | prompt processing progress, n_tokens = 16753, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 21461 | prompt done, n_tokens = 16753, batch.n_tokens = 64
slot init_sampler: id  1 | task 21461 | init sampler, took 2.32 ms, tokens: text = 16753, total = 16753
slot update_slots: id  1 | task 21461 | erasing old context checkpoint (pos_min = 7627, pos_max = 8396, size = 18.056 MiB)
slot update_slots: id  1 | task 21461 | created context checkpoint 8 of 8 (pos_min = 15945, pos_max = 16688, size = 17.446 MiB)
slot print_timing: id  1 | task 21461 | 
prompt eval time =    1546.94 ms /   598 tokens (    2.59 ms per token,   386.57 tokens per second)
       eval time =   63729.13 ms /  1585 tokens (   40.21 ms per token,    24.87 tokens per second)
      total time =   65276.07 ms /  2183 tokens
slot      release: id  1 | task 21461 | stop processing: n_tokens = 18337, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.928 (> 0.100 thold), f_keep = 0.930
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23048 | processing task, is_child = 0
slot update_slots: id  1 | task 23048 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18377
slot update_slots: id  1 | task 23048 | n_past = 17062, slot.prompt.tokens.size() = 18337, seq_id = 1, pos_min = 17567, n_swa = 128
slot update_slots: id  1 | task 23048 | restored context checkpoint (pos_min = 15945, pos_max = 16688, size = 17.446 MiB)
slot update_slots: id  1 | task 23048 | n_tokens = 16688, memory_seq_rm [16688, end)
slot update_slots: id  1 | task 23048 | prompt processing progress, n_tokens = 18313, batch.n_tokens = 1625, progress = 0.996517
slot update_slots: id  1 | task 23048 | n_tokens = 18313, memory_seq_rm [18313, end)
slot update_slots: id  1 | task 23048 | prompt processing progress, n_tokens = 18377, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23048 | prompt done, n_tokens = 18377, batch.n_tokens = 64
slot init_sampler: id  1 | task 23048 | init sampler, took 2.65 ms, tokens: text = 18377, total = 18377
slot update_slots: id  1 | task 23048 | erasing old context checkpoint (pos_min = 7731, pos_max = 8468, size = 17.306 MiB)
slot update_slots: id  1 | task 23048 | created context checkpoint 8 of 8 (pos_min = 17543, pos_max = 18312, size = 18.056 MiB)
slot print_timing: id  1 | task 23048 | 
prompt eval time =    4022.70 ms /  1689 tokens (    2.38 ms per token,   419.87 tokens per second)
       eval time =    2063.62 ms /    52 tokens (   39.68 ms per token,    25.20 tokens per second)
      total time =    6086.31 ms /  1741 tokens
slot      release: id  1 | task 23048 | stop processing: n_tokens = 18428, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23102 | processing task, is_child = 0
slot update_slots: id  1 | task 23102 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18501
slot update_slots: id  1 | task 23102 | n_tokens = 18400, memory_seq_rm [18400, end)
slot update_slots: id  1 | task 23102 | prompt processing progress, n_tokens = 18437, batch.n_tokens = 37, progress = 0.996541
slot update_slots: id  1 | task 23102 | n_tokens = 18437, memory_seq_rm [18437, end)
slot update_slots: id  1 | task 23102 | prompt processing progress, n_tokens = 18501, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23102 | prompt done, n_tokens = 18501, batch.n_tokens = 64
slot init_sampler: id  1 | task 23102 | init sampler, took 2.75 ms, tokens: text = 18501, total = 18501
slot update_slots: id  1 | task 23102 | erasing old context checkpoint (pos_min = 9254, pos_max = 10023, size = 18.056 MiB)
slot update_slots: id  1 | task 23102 | created context checkpoint 8 of 8 (pos_min = 17667, pos_max = 18436, size = 18.056 MiB)
slot print_timing: id  1 | task 23102 | 
prompt eval time =     504.74 ms /   101 tokens (    5.00 ms per token,   200.10 tokens per second)
       eval time =    1883.55 ms /    47 tokens (   40.08 ms per token,    24.95 tokens per second)
      total time =    2388.29 ms /   148 tokens
slot      release: id  1 | task 23102 | stop processing: n_tokens = 18547, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23151 | processing task, is_child = 0
slot update_slots: id  1 | task 23151 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19005
slot update_slots: id  1 | task 23151 | n_tokens = 18516, memory_seq_rm [18516, end)
slot update_slots: id  1 | task 23151 | prompt processing progress, n_tokens = 18941, batch.n_tokens = 425, progress = 0.996632
slot update_slots: id  1 | task 23151 | n_tokens = 18941, memory_seq_rm [18941, end)
slot update_slots: id  1 | task 23151 | prompt processing progress, n_tokens = 19005, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23151 | prompt done, n_tokens = 19005, batch.n_tokens = 64
slot init_sampler: id  1 | task 23151 | init sampler, took 4.12 ms, tokens: text = 19005, total = 19005
slot update_slots: id  1 | task 23151 | erasing old context checkpoint (pos_min = 11314, pos_max = 12083, size = 18.056 MiB)
slot update_slots: id  1 | task 23151 | created context checkpoint 8 of 8 (pos_min = 18171, pos_max = 18940, size = 18.056 MiB)
slot print_timing: id  1 | task 23151 | 
prompt eval time =    1257.68 ms /   489 tokens (    2.57 ms per token,   388.81 tokens per second)
       eval time =    4279.14 ms /   107 tokens (   39.99 ms per token,    25.01 tokens per second)
      total time =    5536.82 ms /   596 tokens
slot      release: id  1 | task 23151 | stop processing: n_tokens = 19111, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23260 | processing task, is_child = 0
slot update_slots: id  1 | task 23260 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19157
slot update_slots: id  1 | task 23260 | n_tokens = 19038, memory_seq_rm [19038, end)
slot update_slots: id  1 | task 23260 | prompt processing progress, n_tokens = 19093, batch.n_tokens = 55, progress = 0.996659
slot update_slots: id  1 | task 23260 | n_tokens = 19093, memory_seq_rm [19093, end)
slot update_slots: id  1 | task 23260 | prompt processing progress, n_tokens = 19157, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23260 | prompt done, n_tokens = 19157, batch.n_tokens = 64
slot init_sampler: id  1 | task 23260 | init sampler, took 3.08 ms, tokens: text = 19157, total = 19157
slot update_slots: id  1 | task 23260 | erasing old context checkpoint (pos_min = 13496, pos_max = 14265, size = 18.056 MiB)
slot update_slots: id  1 | task 23260 | created context checkpoint 8 of 8 (pos_min = 18341, pos_max = 19092, size = 17.634 MiB)
slot print_timing: id  1 | task 23260 | 
prompt eval time =     559.84 ms /   119 tokens (    4.70 ms per token,   212.56 tokens per second)
       eval time =    1972.17 ms /    50 tokens (   39.44 ms per token,    25.35 tokens per second)
      total time =    2532.01 ms /   169 tokens
slot      release: id  1 | task 23260 | stop processing: n_tokens = 19206, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23312 | processing task, is_child = 0
slot update_slots: id  1 | task 23312 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19597
slot update_slots: id  1 | task 23312 | n_tokens = 19175, memory_seq_rm [19175, end)
slot update_slots: id  1 | task 23312 | prompt processing progress, n_tokens = 19533, batch.n_tokens = 358, progress = 0.996734
slot update_slots: id  1 | task 23312 | n_tokens = 19533, memory_seq_rm [19533, end)
slot update_slots: id  1 | task 23312 | prompt processing progress, n_tokens = 19597, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23312 | prompt done, n_tokens = 19597, batch.n_tokens = 64
slot init_sampler: id  1 | task 23312 | init sampler, took 2.68 ms, tokens: text = 19597, total = 19597
slot update_slots: id  1 | task 23312 | erasing old context checkpoint (pos_min = 13987, pos_max = 14756, size = 18.056 MiB)
slot update_slots: id  1 | task 23312 | created context checkpoint 8 of 8 (pos_min = 18791, pos_max = 19532, size = 17.399 MiB)
slot print_timing: id  1 | task 23312 | 
prompt eval time =    1144.52 ms /   422 tokens (    2.71 ms per token,   368.71 tokens per second)
       eval time =   15935.46 ms /   394 tokens (   40.45 ms per token,    24.72 tokens per second)
      total time =   17079.98 ms /   816 tokens
slot      release: id  1 | task 23312 | stop processing: n_tokens = 19990, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.968 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23708 | processing task, is_child = 0
slot update_slots: id  1 | task 23708 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 20276
slot update_slots: id  1 | task 23708 | n_tokens = 19626, memory_seq_rm [19626, end)
slot update_slots: id  1 | task 23708 | prompt processing progress, n_tokens = 20212, batch.n_tokens = 586, progress = 0.996844
slot update_slots: id  1 | task 23708 | n_tokens = 20212, memory_seq_rm [20212, end)
slot update_slots: id  1 | task 23708 | prompt processing progress, n_tokens = 20276, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23708 | prompt done, n_tokens = 20276, batch.n_tokens = 64
slot init_sampler: id  1 | task 23708 | init sampler, took 3.14 ms, tokens: text = 20276, total = 20276
slot update_slots: id  1 | task 23708 | erasing old context checkpoint (pos_min = 15210, pos_max = 15933, size = 16.977 MiB)
slot update_slots: id  1 | task 23708 | created context checkpoint 8 of 8 (pos_min = 19499, pos_max = 20211, size = 16.719 MiB)
slot print_timing: id  1 | task 23708 | 
prompt eval time =    1820.81 ms /   650 tokens (    2.80 ms per token,   356.98 tokens per second)
       eval time =    2274.20 ms /    57 tokens (   39.90 ms per token,    25.06 tokens per second)
      total time =    4095.00 ms /   707 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 23708 | stop processing: n_tokens = 20332, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23767 | processing task, is_child = 0
slot update_slots: id  1 | task 23767 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 20723
slot update_slots: id  1 | task 23767 | n_tokens = 20301, memory_seq_rm [20301, end)
slot update_slots: id  1 | task 23767 | prompt processing progress, n_tokens = 20659, batch.n_tokens = 358, progress = 0.996912
slot update_slots: id  1 | task 23767 | n_tokens = 20659, memory_seq_rm [20659, end)
slot update_slots: id  1 | task 23767 | prompt processing progress, n_tokens = 20723, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23767 | prompt done, n_tokens = 20723, batch.n_tokens = 64
slot init_sampler: id  1 | task 23767 | init sampler, took 3.63 ms, tokens: text = 20723, total = 20723
slot update_slots: id  1 | task 23767 | erasing old context checkpoint (pos_min = 15353, pos_max = 16079, size = 17.048 MiB)
slot update_slots: id  1 | task 23767 | created context checkpoint 8 of 8 (pos_min = 19950, pos_max = 20658, size = 16.626 MiB)
slot print_timing: id  1 | task 23767 | 
prompt eval time =    1148.38 ms /   422 tokens (    2.72 ms per token,   367.47 tokens per second)
       eval time =   15707.02 ms /   389 tokens (   40.38 ms per token,    24.77 tokens per second)
      total time =   16855.41 ms /   811 tokens
slot      release: id  1 | task 23767 | stop processing: n_tokens = 21111, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24158 | processing task, is_child = 0
slot update_slots: id  1 | task 24158 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21400
slot update_slots: id  1 | task 24158 | n_tokens = 20750, memory_seq_rm [20750, end)
slot update_slots: id  1 | task 24158 | prompt processing progress, n_tokens = 21336, batch.n_tokens = 586, progress = 0.997009
slot update_slots: id  1 | task 24158 | n_tokens = 21336, memory_seq_rm [21336, end)
slot update_slots: id  1 | task 24158 | prompt processing progress, n_tokens = 21400, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24158 | prompt done, n_tokens = 21400, batch.n_tokens = 64
slot init_sampler: id  1 | task 24158 | init sampler, took 3.32 ms, tokens: text = 21400, total = 21400
slot update_slots: id  1 | task 24158 | erasing old context checkpoint (pos_min = 15945, pos_max = 16688, size = 17.446 MiB)
slot update_slots: id  1 | task 24158 | created context checkpoint 8 of 8 (pos_min = 20623, pos_max = 21335, size = 16.719 MiB)
slot print_timing: id  1 | task 24158 | 
prompt eval time =    1845.75 ms /   650 tokens (    2.84 ms per token,   352.16 tokens per second)
       eval time =    2525.68 ms /    63 tokens (   40.09 ms per token,    24.94 tokens per second)
      total time =    4371.43 ms /   713 tokens
slot      release: id  1 | task 24158 | stop processing: n_tokens = 21462, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24223 | processing task, is_child = 0
slot update_slots: id  1 | task 24223 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21999
slot update_slots: id  1 | task 24223 | n_tokens = 21427, memory_seq_rm [21427, end)
slot update_slots: id  1 | task 24223 | prompt processing progress, n_tokens = 21935, batch.n_tokens = 508, progress = 0.997091
slot update_slots: id  1 | task 24223 | n_tokens = 21935, memory_seq_rm [21935, end)
slot update_slots: id  1 | task 24223 | prompt processing progress, n_tokens = 21999, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24223 | prompt done, n_tokens = 21999, batch.n_tokens = 64
slot init_sampler: id  1 | task 24223 | init sampler, took 3.18 ms, tokens: text = 21999, total = 21999
slot update_slots: id  1 | task 24223 | erasing old context checkpoint (pos_min = 17543, pos_max = 18312, size = 18.056 MiB)
slot update_slots: id  1 | task 24223 | created context checkpoint 8 of 8 (pos_min = 21165, pos_max = 21934, size = 18.056 MiB)
slot print_timing: id  1 | task 24223 | 
prompt eval time =    1409.21 ms /   572 tokens (    2.46 ms per token,   405.90 tokens per second)
       eval time =   15661.79 ms /   388 tokens (   40.37 ms per token,    24.77 tokens per second)
      total time =   17071.00 ms /   960 tokens
slot      release: id  1 | task 24223 | stop processing: n_tokens = 22386, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24613 | processing task, is_child = 0
slot update_slots: id  1 | task 24613 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 22675
slot update_slots: id  1 | task 24613 | n_tokens = 22025, memory_seq_rm [22025, end)
slot update_slots: id  1 | task 24613 | prompt processing progress, n_tokens = 22611, batch.n_tokens = 586, progress = 0.997177
slot update_slots: id  1 | task 24613 | n_tokens = 22611, memory_seq_rm [22611, end)
slot update_slots: id  1 | task 24613 | prompt processing progress, n_tokens = 22675, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24613 | prompt done, n_tokens = 22675, batch.n_tokens = 64
slot init_sampler: id  1 | task 24613 | init sampler, took 3.43 ms, tokens: text = 22675, total = 22675
slot update_slots: id  1 | task 24613 | erasing old context checkpoint (pos_min = 17667, pos_max = 18436, size = 18.056 MiB)
slot update_slots: id  1 | task 24613 | created context checkpoint 8 of 8 (pos_min = 21898, pos_max = 22610, size = 16.719 MiB)
slot print_timing: id  1 | task 24613 | 
prompt eval time =    1853.23 ms /   650 tokens (    2.85 ms per token,   350.74 tokens per second)
       eval time =    2714.63 ms /    68 tokens (   39.92 ms per token,    25.05 tokens per second)
      total time =    4567.87 ms /   718 tokens
slot      release: id  1 | task 24613 | stop processing: n_tokens = 22742, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24683 | processing task, is_child = 0
slot update_slots: id  1 | task 24683 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 23036
slot update_slots: id  1 | task 24683 | n_tokens = 22707, memory_seq_rm [22707, end)
slot update_slots: id  1 | task 24683 | prompt processing progress, n_tokens = 22972, batch.n_tokens = 265, progress = 0.997222
slot update_slots: id  1 | task 24683 | n_tokens = 22972, memory_seq_rm [22972, end)
slot update_slots: id  1 | task 24683 | prompt processing progress, n_tokens = 23036, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24683 | prompt done, n_tokens = 23036, batch.n_tokens = 64
slot init_sampler: id  1 | task 24683 | init sampler, took 3.46 ms, tokens: text = 23036, total = 23036
slot update_slots: id  1 | task 24683 | erasing old context checkpoint (pos_min = 18171, pos_max = 18940, size = 18.056 MiB)
slot update_slots: id  1 | task 24683 | created context checkpoint 8 of 8 (pos_min = 22329, pos_max = 22971, size = 15.078 MiB)
slot print_timing: id  1 | task 24683 | 
prompt eval time =     991.94 ms /   329 tokens (    3.02 ms per token,   331.67 tokens per second)
       eval time =   30455.03 ms /   753 tokens (   40.44 ms per token,    24.72 tokens per second)
      total time =   31446.97 ms /  1082 tokens
slot      release: id  1 | task 24683 | stop processing: n_tokens = 23788, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25438 | processing task, is_child = 0
slot update_slots: id  1 | task 25438 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 24061
slot update_slots: id  1 | task 25438 | n_tokens = 23460, memory_seq_rm [23460, end)
slot update_slots: id  1 | task 25438 | prompt processing progress, n_tokens = 23997, batch.n_tokens = 537, progress = 0.997340
slot update_slots: id  1 | task 25438 | n_tokens = 23997, memory_seq_rm [23997, end)
slot update_slots: id  1 | task 25438 | prompt processing progress, n_tokens = 24061, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25438 | prompt done, n_tokens = 24061, batch.n_tokens = 64
slot init_sampler: id  1 | task 25438 | init sampler, took 4.79 ms, tokens: text = 24061, total = 24061
slot update_slots: id  1 | task 25438 | erasing old context checkpoint (pos_min = 18341, pos_max = 19092, size = 17.634 MiB)
slot update_slots: id  1 | task 25438 | created context checkpoint 8 of 8 (pos_min = 23227, pos_max = 23996, size = 18.056 MiB)
slot print_timing: id  1 | task 25438 | 
prompt eval time =    1667.00 ms /   601 tokens (    2.77 ms per token,   360.53 tokens per second)
       eval time =    4857.47 ms /   121 tokens (   40.14 ms per token,    24.91 tokens per second)
      total time =    6524.47 ms /   722 tokens
slot      release: id  1 | task 25438 | stop processing: n_tokens = 24181, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25561 | processing task, is_child = 0
slot update_slots: id  1 | task 25561 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 24326
slot update_slots: id  1 | task 25561 | n_tokens = 24146, memory_seq_rm [24146, end)
slot update_slots: id  1 | task 25561 | prompt processing progress, n_tokens = 24262, batch.n_tokens = 116, progress = 0.997369
slot update_slots: id  1 | task 25561 | n_tokens = 24262, memory_seq_rm [24262, end)
slot update_slots: id  1 | task 25561 | prompt processing progress, n_tokens = 24326, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25561 | prompt done, n_tokens = 24326, batch.n_tokens = 64
slot init_sampler: id  1 | task 25561 | init sampler, took 3.79 ms, tokens: text = 24326, total = 24326
slot update_slots: id  1 | task 25561 | erasing old context checkpoint (pos_min = 18791, pos_max = 19532, size = 17.399 MiB)
slot update_slots: id  1 | task 25561 | created context checkpoint 8 of 8 (pos_min = 23492, pos_max = 24261, size = 18.056 MiB)
slot print_timing: id  1 | task 25561 | 
prompt eval time =     797.51 ms /   180 tokens (    4.43 ms per token,   225.70 tokens per second)
       eval time =    2435.89 ms /    61 tokens (   39.93 ms per token,    25.04 tokens per second)
      total time =    3233.40 ms /   241 tokens
slot      release: id  1 | task 25561 | stop processing: n_tokens = 24386, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25624 | processing task, is_child = 0
slot update_slots: id  1 | task 25624 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 24747
slot update_slots: id  1 | task 25624 | n_tokens = 24351, memory_seq_rm [24351, end)
slot update_slots: id  1 | task 25624 | prompt processing progress, n_tokens = 24683, batch.n_tokens = 332, progress = 0.997414
slot update_slots: id  1 | task 25624 | n_tokens = 24683, memory_seq_rm [24683, end)
slot update_slots: id  1 | task 25624 | prompt processing progress, n_tokens = 24747, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25624 | prompt done, n_tokens = 24747, batch.n_tokens = 64
slot init_sampler: id  1 | task 25624 | init sampler, took 4.93 ms, tokens: text = 24747, total = 24747
slot update_slots: id  1 | task 25624 | erasing old context checkpoint (pos_min = 19499, pos_max = 20211, size = 16.719 MiB)
slot update_slots: id  1 | task 25624 | created context checkpoint 8 of 8 (pos_min = 23913, pos_max = 24682, size = 18.056 MiB)
slot print_timing: id  1 | task 25624 | 
prompt eval time =    1141.25 ms /   396 tokens (    2.88 ms per token,   346.99 tokens per second)
       eval time =   18486.86 ms /   458 tokens (   40.36 ms per token,    24.77 tokens per second)
      total time =   19628.11 ms /   854 tokens
slot      release: id  1 | task 25624 | stop processing: n_tokens = 25204, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26084 | processing task, is_child = 0
slot update_slots: id  1 | task 26084 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 25558
slot update_slots: id  1 | task 26084 | n_tokens = 24780, memory_seq_rm [24780, end)
slot update_slots: id  1 | task 26084 | prompt processing progress, n_tokens = 25494, batch.n_tokens = 714, progress = 0.997496
slot update_slots: id  1 | task 26084 | n_tokens = 25494, memory_seq_rm [25494, end)
slot update_slots: id  1 | task 26084 | prompt processing progress, n_tokens = 25558, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26084 | prompt done, n_tokens = 25558, batch.n_tokens = 64
slot init_sampler: id  1 | task 26084 | init sampler, took 3.71 ms, tokens: text = 25558, total = 25558
slot update_slots: id  1 | task 26084 | erasing old context checkpoint (pos_min = 19950, pos_max = 20658, size = 16.626 MiB)
slot update_slots: id  1 | task 26084 | created context checkpoint 8 of 8 (pos_min = 24851, pos_max = 25493, size = 15.078 MiB)
slot print_timing: id  1 | task 26084 | 
prompt eval time =    2072.63 ms /   778 tokens (    2.66 ms per token,   375.37 tokens per second)
       eval time =    3042.26 ms /    76 tokens (   40.03 ms per token,    24.98 tokens per second)
      total time =    5114.89 ms /   854 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 26084 | stop processing: n_tokens = 25633, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26162 | processing task, is_child = 0
slot update_slots: id  1 | task 26162 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 25862
slot update_slots: id  1 | task 26162 | n_tokens = 25598, memory_seq_rm [25598, end)
slot update_slots: id  1 | task 26162 | prompt processing progress, n_tokens = 25798, batch.n_tokens = 200, progress = 0.997525
slot update_slots: id  1 | task 26162 | n_tokens = 25798, memory_seq_rm [25798, end)
slot update_slots: id  1 | task 26162 | prompt processing progress, n_tokens = 25862, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26162 | prompt done, n_tokens = 25862, batch.n_tokens = 64
slot init_sampler: id  1 | task 26162 | init sampler, took 5.88 ms, tokens: text = 25862, total = 25862
slot update_slots: id  1 | task 26162 | erasing old context checkpoint (pos_min = 20623, pos_max = 21335, size = 16.719 MiB)
slot update_slots: id  1 | task 26162 | created context checkpoint 8 of 8 (pos_min = 25155, pos_max = 25797, size = 15.078 MiB)
slot print_timing: id  1 | task 26162 | 
prompt eval time =     906.63 ms /   264 tokens (    3.43 ms per token,   291.19 tokens per second)
       eval time =    6830.22 ms /   170 tokens (   40.18 ms per token,    24.89 tokens per second)
      total time =    7736.85 ms /   434 tokens
slot      release: id  1 | task 26162 | stop processing: n_tokens = 26031, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26334 | processing task, is_child = 0
slot update_slots: id  1 | task 26334 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 26081
slot update_slots: id  1 | task 26334 | n_tokens = 25996, memory_seq_rm [25996, end)
slot update_slots: id  1 | task 26334 | prompt processing progress, n_tokens = 26017, batch.n_tokens = 21, progress = 0.997546
slot update_slots: id  1 | task 26334 | n_tokens = 26017, memory_seq_rm [26017, end)
slot update_slots: id  1 | task 26334 | prompt processing progress, n_tokens = 26081, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26334 | prompt done, n_tokens = 26081, batch.n_tokens = 64
slot init_sampler: id  1 | task 26334 | init sampler, took 4.16 ms, tokens: text = 26081, total = 26081
slot update_slots: id  1 | task 26334 | erasing old context checkpoint (pos_min = 21165, pos_max = 21934, size = 18.056 MiB)
slot update_slots: id  1 | task 26334 | created context checkpoint 8 of 8 (pos_min = 25261, pos_max = 26016, size = 17.728 MiB)
slot print_timing: id  1 | task 26334 | 
prompt eval time =     485.26 ms /    85 tokens (    5.71 ms per token,   175.17 tokens per second)
       eval time =   30735.24 ms /   760 tokens (   40.44 ms per token,    24.73 tokens per second)
      total time =   31220.50 ms /   845 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 26334 | stop processing: n_tokens = 26840, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27096 | processing task, is_child = 0
slot update_slots: id  1 | task 27096 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 26954
slot update_slots: id  1 | task 27096 | n_tokens = 26807, memory_seq_rm [26807, end)
slot update_slots: id  1 | task 27096 | prompt processing progress, n_tokens = 26890, batch.n_tokens = 83, progress = 0.997626
slot update_slots: id  1 | task 27096 | n_tokens = 26890, memory_seq_rm [26890, end)
slot update_slots: id  1 | task 27096 | prompt processing progress, n_tokens = 26954, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27096 | prompt done, n_tokens = 26954, batch.n_tokens = 64
slot init_sampler: id  1 | task 27096 | init sampler, took 5.96 ms, tokens: text = 26954, total = 26954
slot update_slots: id  1 | task 27096 | erasing old context checkpoint (pos_min = 21898, pos_max = 22610, size = 16.719 MiB)
slot update_slots: id  1 | task 27096 | created context checkpoint 8 of 8 (pos_min = 26120, pos_max = 26889, size = 18.056 MiB)
slot print_timing: id  1 | task 27096 | 
prompt eval time =     731.54 ms /   147 tokens (    4.98 ms per token,   200.95 tokens per second)
       eval time =    9535.21 ms /   236 tokens (   40.40 ms per token,    24.75 tokens per second)
      total time =   10266.75 ms /   383 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 27096 | stop processing: n_tokens = 27189, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27334 | processing task, is_child = 0
slot update_slots: id  1 | task 27334 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 27235
slot update_slots: id  1 | task 27334 | n_tokens = 27099, memory_seq_rm [27099, end)
slot update_slots: id  1 | task 27334 | prompt processing progress, n_tokens = 27171, batch.n_tokens = 72, progress = 0.997650
slot update_slots: id  1 | task 27334 | n_tokens = 27171, memory_seq_rm [27171, end)
slot update_slots: id  1 | task 27334 | prompt processing progress, n_tokens = 27235, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27334 | prompt done, n_tokens = 27235, batch.n_tokens = 64
slot init_sampler: id  1 | task 27334 | init sampler, took 5.38 ms, tokens: text = 27235, total = 27235
slot update_slots: id  1 | task 27334 | erasing old context checkpoint (pos_min = 22329, pos_max = 22971, size = 15.078 MiB)
slot update_slots: id  1 | task 27334 | created context checkpoint 8 of 8 (pos_min = 26419, pos_max = 27170, size = 17.634 MiB)
slot print_timing: id  1 | task 27334 | 
prompt eval time =     711.07 ms /   136 tokens (    5.23 ms per token,   191.26 tokens per second)
       eval time =   11424.27 ms /   283 tokens (   40.37 ms per token,    24.77 tokens per second)
      total time =   12135.35 ms /   419 tokens
slot      release: id  1 | task 27334 | stop processing: n_tokens = 27517, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 1.000
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27619 | processing task, is_child = 0
slot update_slots: id  1 | task 27619 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 27571
slot update_slots: id  1 | task 27619 | n_tokens = 27506, memory_seq_rm [27506, end)
slot update_slots: id  1 | task 27619 | prompt processing progress, n_tokens = 27507, batch.n_tokens = 1, progress = 0.997679
slot update_slots: id  1 | task 27619 | n_tokens = 27507, memory_seq_rm [27507, end)
slot update_slots: id  1 | task 27619 | prompt processing progress, n_tokens = 27571, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27619 | prompt done, n_tokens = 27571, batch.n_tokens = 64
slot init_sampler: id  1 | task 27619 | init sampler, took 4.00 ms, tokens: text = 27571, total = 27571
slot update_slots: id  1 | task 27619 | erasing old context checkpoint (pos_min = 23227, pos_max = 23996, size = 18.056 MiB)
slot update_slots: id  1 | task 27619 | created context checkpoint 8 of 8 (pos_min = 26747, pos_max = 27506, size = 17.821 MiB)
slot print_timing: id  1 | task 27619 | 
prompt eval time =     392.31 ms /    65 tokens (    6.04 ms per token,   165.68 tokens per second)
       eval time =    5155.60 ms /   128 tokens (   40.28 ms per token,    24.83 tokens per second)
      total time =    5547.92 ms /   193 tokens
slot      release: id  1 | task 27619 | stop processing: n_tokens = 27698, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.189 (> 0.100 thold), f_keep = 0.006
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 27698, total state size = 667.545 MiB
srv          load:  - looking for better prompt, base f_keep = 0.006, sim = 0.189
srv        update:  - cache state: 2 prompts, 1620.398 MiB (limits: 8192.000 MiB, 56064 tokens, 282362 est)
srv        update:    - prompt 0x56346d802600:   28154 tokens, checkpoints:  8,   815.346 MiB
srv        update:    - prompt 0x5634781e0370:   27698 tokens, checkpoints:  8,   805.052 MiB
srv  get_availabl: prompt cache update took 692.29 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27749 | processing task, is_child = 0
slot update_slots: id  1 | task 27749 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 859
slot update_slots: id  1 | task 27749 | n_past = 162, slot.prompt.tokens.size() = 27698, seq_id = 1, pos_min = 26928, n_swa = 128
slot update_slots: id  1 | task 27749 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 27749 | erased invalidated context checkpoint (pos_min = 23492, pos_max = 24261, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 27749 | erased invalidated context checkpoint (pos_min = 23913, pos_max = 24682, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 27749 | erased invalidated context checkpoint (pos_min = 24851, pos_max = 25493, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 27749 | erased invalidated context checkpoint (pos_min = 25155, pos_max = 25797, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 27749 | erased invalidated context checkpoint (pos_min = 25261, pos_max = 26016, n_swa = 128, size = 17.728 MiB)
slot update_slots: id  1 | task 27749 | erased invalidated context checkpoint (pos_min = 26120, pos_max = 26889, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 27749 | erased invalidated context checkpoint (pos_min = 26419, pos_max = 27170, n_swa = 128, size = 17.634 MiB)
slot update_slots: id  1 | task 27749 | erased invalidated context checkpoint (pos_min = 26747, pos_max = 27506, n_swa = 128, size = 17.821 MiB)
slot update_slots: id  1 | task 27749 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 27749 | prompt processing progress, n_tokens = 795, batch.n_tokens = 795, progress = 0.925495
slot update_slots: id  1 | task 27749 | n_tokens = 795, memory_seq_rm [795, end)
slot update_slots: id  1 | task 27749 | prompt processing progress, n_tokens = 859, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27749 | prompt done, n_tokens = 859, batch.n_tokens = 64
slot init_sampler: id  1 | task 27749 | init sampler, took 0.16 ms, tokens: text = 859, total = 859
slot update_slots: id  1 | task 27749 | created context checkpoint 1 of 8 (pos_min = 25, pos_max = 794, size = 18.056 MiB)
slot print_timing: id  1 | task 27749 | 
prompt eval time =    1769.94 ms /   859 tokens (    2.06 ms per token,   485.33 tokens per second)
       eval time =    1349.44 ms /    33 tokens (   40.89 ms per token,    24.45 tokens per second)
      total time =    3119.38 ms /   892 tokens
slot      release: id  1 | task 27749 | stop processing: n_tokens = 891, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27784 | processing task, is_child = 0
slot update_slots: id  1 | task 27784 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 860
slot update_slots: id  1 | task 27784 | n_tokens = 854, memory_seq_rm [854, end)
slot update_slots: id  1 | task 27784 | prompt processing progress, n_tokens = 860, batch.n_tokens = 6, progress = 1.000000
slot update_slots: id  1 | task 27784 | prompt done, n_tokens = 860, batch.n_tokens = 6
slot init_sampler: id  1 | task 27784 | init sampler, took 0.16 ms, tokens: text = 860, total = 860
slot print_timing: id  1 | task 27784 | 
prompt eval time =      96.66 ms /     6 tokens (   16.11 ms per token,    62.08 tokens per second)
       eval time =    1208.28 ms /    32 tokens (   37.76 ms per token,    26.48 tokens per second)
      total time =    1304.94 ms /    38 tokens
slot      release: id  1 | task 27784 | stop processing: n_tokens = 891, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27817 | processing task, is_child = 0
slot update_slots: id  1 | task 27817 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 941
slot update_slots: id  1 | task 27817 | n_tokens = 876, memory_seq_rm [876, end)
slot update_slots: id  1 | task 27817 | prompt processing progress, n_tokens = 877, batch.n_tokens = 1, progress = 0.931987
slot update_slots: id  1 | task 27817 | n_tokens = 877, memory_seq_rm [877, end)
slot update_slots: id  1 | task 27817 | prompt processing progress, n_tokens = 941, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27817 | prompt done, n_tokens = 941, batch.n_tokens = 64
slot init_sampler: id  1 | task 27817 | init sampler, took 0.20 ms, tokens: text = 941, total = 941
slot update_slots: id  1 | task 27817 | created context checkpoint 2 of 8 (pos_min = 121, pos_max = 876, size = 17.728 MiB)
slot print_timing: id  1 | task 27817 | 
prompt eval time =     337.92 ms /    65 tokens (    5.20 ms per token,   192.35 tokens per second)
       eval time =     528.15 ms /    15 tokens (   35.21 ms per token,    28.40 tokens per second)
      total time =     866.07 ms /    80 tokens
slot      release: id  1 | task 27817 | stop processing: n_tokens = 955, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.903 (> 0.100 thold), f_keep = 0.901
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27834 | processing task, is_child = 0
slot update_slots: id  1 | task 27834 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 952
slot update_slots: id  1 | task 27834 | n_tokens = 860, memory_seq_rm [860, end)
slot update_slots: id  1 | task 27834 | prompt processing progress, n_tokens = 888, batch.n_tokens = 28, progress = 0.932773
slot update_slots: id  1 | task 27834 | n_tokens = 888, memory_seq_rm [888, end)
slot update_slots: id  1 | task 27834 | prompt processing progress, n_tokens = 952, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27834 | prompt done, n_tokens = 952, batch.n_tokens = 64
slot init_sampler: id  1 | task 27834 | init sampler, took 0.18 ms, tokens: text = 952, total = 952
slot print_timing: id  1 | task 27834 | 
prompt eval time =     492.46 ms /    92 tokens (    5.35 ms per token,   186.82 tokens per second)
       eval time =    1155.39 ms /    33 tokens (   35.01 ms per token,    28.56 tokens per second)
      total time =    1647.86 ms /   125 tokens
slot      release: id  1 | task 27834 | stop processing: n_tokens = 984, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.852 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27869 | processing task, is_child = 0
slot update_slots: id  1 | task 27869 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1132
slot update_slots: id  1 | task 27869 | n_tokens = 965, memory_seq_rm [965, end)
slot update_slots: id  1 | task 27869 | prompt processing progress, n_tokens = 1068, batch.n_tokens = 103, progress = 0.943463
slot update_slots: id  1 | task 27869 | n_tokens = 1068, memory_seq_rm [1068, end)
slot update_slots: id  1 | task 27869 | prompt processing progress, n_tokens = 1132, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27869 | prompt done, n_tokens = 1132, batch.n_tokens = 64
slot init_sampler: id  1 | task 27869 | init sampler, took 0.17 ms, tokens: text = 1132, total = 1132
slot update_slots: id  1 | task 27869 | created context checkpoint 3 of 8 (pos_min = 298, pos_max = 1067, size = 18.056 MiB)
slot print_timing: id  1 | task 27869 | 
prompt eval time =     605.25 ms /   167 tokens (    3.62 ms per token,   275.92 tokens per second)
       eval time =    2908.26 ms /    78 tokens (   37.29 ms per token,    26.82 tokens per second)
      total time =    3513.51 ms /   245 tokens
slot      release: id  1 | task 27869 | stop processing: n_tokens = 1209, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.922 (> 0.100 thold), f_keep = 0.706
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27949 | processing task, is_child = 0
slot update_slots: id  1 | task 27949 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 926
slot update_slots: id  1 | task 27949 | n_tokens = 854, memory_seq_rm [854, end)
slot update_slots: id  1 | task 27949 | prompt processing progress, n_tokens = 862, batch.n_tokens = 8, progress = 0.930886
slot update_slots: id  1 | task 27949 | n_tokens = 862, memory_seq_rm [862, end)
slot update_slots: id  1 | task 27949 | prompt processing progress, n_tokens = 926, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27949 | prompt done, n_tokens = 926, batch.n_tokens = 64
slot init_sampler: id  1 | task 27949 | init sampler, took 0.15 ms, tokens: text = 926, total = 926
slot print_timing: id  1 | task 27949 | 
prompt eval time =     386.26 ms /    72 tokens (    5.36 ms per token,   186.40 tokens per second)
       eval time =    2891.96 ms /    79 tokens (   36.61 ms per token,    27.32 tokens per second)
      total time =    3278.22 ms /   151 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 27949 | stop processing: n_tokens = 1004, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.922 (> 0.100 thold), f_keep = 0.947
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28030 | processing task, is_child = 0
slot update_slots: id  1 | task 28030 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1031
slot update_slots: id  1 | task 28030 | n_tokens = 951, memory_seq_rm [951, end)
slot update_slots: id  1 | task 28030 | prompt processing progress, n_tokens = 967, batch.n_tokens = 16, progress = 0.937924
slot update_slots: id  1 | task 28030 | n_tokens = 967, memory_seq_rm [967, end)
slot update_slots: id  1 | task 28030 | prompt processing progress, n_tokens = 1031, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28030 | prompt done, n_tokens = 1031, batch.n_tokens = 64
slot init_sampler: id  1 | task 28030 | init sampler, took 0.18 ms, tokens: text = 1031, total = 1031
slot print_timing: id  1 | task 28030 | 
prompt eval time =     393.87 ms /    80 tokens (    4.92 ms per token,   203.11 tokens per second)
       eval time =    1805.08 ms /    47 tokens (   38.41 ms per token,    26.04 tokens per second)
      total time =    2198.95 ms /   127 tokens
slot      release: id  1 | task 28030 | stop processing: n_tokens = 1077, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.479 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28079 | processing task, is_child = 0
slot update_slots: id  1 | task 28079 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2190
slot update_slots: id  1 | task 28079 | n_tokens = 1048, memory_seq_rm [1048, end)
slot update_slots: id  1 | task 28079 | prompt processing progress, n_tokens = 2126, batch.n_tokens = 1078, progress = 0.970776
slot update_slots: id  1 | task 28079 | n_tokens = 2126, memory_seq_rm [2126, end)
slot update_slots: id  1 | task 28079 | prompt processing progress, n_tokens = 2190, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28079 | prompt done, n_tokens = 2190, batch.n_tokens = 64
slot init_sampler: id  1 | task 28079 | init sampler, took 0.34 ms, tokens: text = 2190, total = 2190
slot update_slots: id  1 | task 28079 | created context checkpoint 4 of 8 (pos_min = 1356, pos_max = 2125, size = 18.056 MiB)
slot print_timing: id  1 | task 28079 | 
prompt eval time =    2479.74 ms /  1142 tokens (    2.17 ms per token,   460.53 tokens per second)
       eval time =    1858.91 ms /    47 tokens (   39.55 ms per token,    25.28 tokens per second)
      total time =    4338.65 ms /  1189 tokens
slot      release: id  1 | task 28079 | stop processing: n_tokens = 2236, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28128 | processing task, is_child = 0
slot update_slots: id  1 | task 28128 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2268
slot update_slots: id  1 | task 28128 | n_tokens = 2213, memory_seq_rm [2213, end)
slot update_slots: id  1 | task 28128 | prompt processing progress, n_tokens = 2268, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  1 | task 28128 | prompt done, n_tokens = 2268, batch.n_tokens = 55
slot init_sampler: id  1 | task 28128 | init sampler, took 0.35 ms, tokens: text = 2268, total = 2268
slot update_slots: id  1 | task 28128 | created context checkpoint 5 of 8 (pos_min = 1466, pos_max = 2212, size = 17.517 MiB)
slot print_timing: id  1 | task 28128 | 
prompt eval time =     285.94 ms /    55 tokens (    5.20 ms per token,   192.35 tokens per second)
       eval time =    1564.39 ms /    47 tokens (   33.28 ms per token,    30.04 tokens per second)
      total time =    1850.33 ms /   102 tokens
slot      release: id  1 | task 28128 | stop processing: n_tokens = 2314, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.737 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28176 | processing task, is_child = 0
slot update_slots: id  1 | task 28176 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3110
slot update_slots: id  1 | task 28176 | n_tokens = 2292, memory_seq_rm [2292, end)
slot update_slots: id  1 | task 28176 | prompt processing progress, n_tokens = 3046, batch.n_tokens = 754, progress = 0.979421
slot update_slots: id  1 | task 28176 | n_tokens = 3046, memory_seq_rm [3046, end)
slot update_slots: id  1 | task 28176 | prompt processing progress, n_tokens = 3110, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28176 | prompt done, n_tokens = 3110, batch.n_tokens = 64
slot init_sampler: id  1 | task 28176 | init sampler, took 0.45 ms, tokens: text = 3110, total = 3110
slot update_slots: id  1 | task 28176 | created context checkpoint 6 of 8 (pos_min = 2292, pos_max = 3045, size = 17.681 MiB)
slot print_timing: id  1 | task 28176 | 
prompt eval time =    1703.19 ms /   818 tokens (    2.08 ms per token,   480.28 tokens per second)
       eval time =    1475.85 ms /    42 tokens (   35.14 ms per token,    28.46 tokens per second)
      total time =    3179.04 ms /   860 tokens
slot      release: id  1 | task 28176 | stop processing: n_tokens = 3151, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.619 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28220 | processing task, is_child = 0
slot update_slots: id  1 | task 28220 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5044
slot update_slots: id  1 | task 28220 | n_tokens = 3124, memory_seq_rm [3124, end)
slot update_slots: id  1 | task 28220 | prompt processing progress, n_tokens = 4980, batch.n_tokens = 1856, progress = 0.987312
slot update_slots: id  1 | task 28220 | n_tokens = 4980, memory_seq_rm [4980, end)
slot update_slots: id  1 | task 28220 | prompt processing progress, n_tokens = 5044, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28220 | prompt done, n_tokens = 5044, batch.n_tokens = 64
slot init_sampler: id  1 | task 28220 | init sampler, took 0.76 ms, tokens: text = 5044, total = 5044
slot update_slots: id  1 | task 28220 | created context checkpoint 7 of 8 (pos_min = 4210, pos_max = 4979, size = 18.056 MiB)
slot print_timing: id  1 | task 28220 | 
prompt eval time =    3824.48 ms /  1920 tokens (    1.99 ms per token,   502.03 tokens per second)
       eval time =    1459.93 ms /    37 tokens (   39.46 ms per token,    25.34 tokens per second)
      total time =    5284.42 ms /  1957 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 28220 | stop processing: n_tokens = 5080, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.712 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28259 | processing task, is_child = 0
slot update_slots: id  1 | task 28259 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7097
slot update_slots: id  1 | task 28259 | n_tokens = 5053, memory_seq_rm [5053, end)
slot update_slots: id  1 | task 28259 | prompt processing progress, n_tokens = 7033, batch.n_tokens = 1980, progress = 0.990982
slot update_slots: id  1 | task 28259 | n_tokens = 7033, memory_seq_rm [7033, end)
slot update_slots: id  1 | task 28259 | prompt processing progress, n_tokens = 7097, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28259 | prompt done, n_tokens = 7097, batch.n_tokens = 64
slot init_sampler: id  1 | task 28259 | init sampler, took 1.05 ms, tokens: text = 7097, total = 7097
slot update_slots: id  1 | task 28259 | created context checkpoint 8 of 8 (pos_min = 6263, pos_max = 7032, size = 18.056 MiB)
slot print_timing: id  1 | task 28259 | 
prompt eval time =    4251.46 ms /  2044 tokens (    2.08 ms per token,   480.78 tokens per second)
       eval time =    1577.53 ms /    40 tokens (   39.44 ms per token,    25.36 tokens per second)
      total time =    5828.99 ms /  2084 tokens
slot      release: id  1 | task 28259 | stop processing: n_tokens = 7136, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.911 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28301 | processing task, is_child = 0
slot update_slots: id  1 | task 28301 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7798
slot update_slots: id  1 | task 28301 | n_tokens = 7105, memory_seq_rm [7105, end)
slot update_slots: id  1 | task 28301 | prompt processing progress, n_tokens = 7734, batch.n_tokens = 629, progress = 0.991793
slot update_slots: id  1 | task 28301 | n_tokens = 7734, memory_seq_rm [7734, end)
slot update_slots: id  1 | task 28301 | prompt processing progress, n_tokens = 7798, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28301 | prompt done, n_tokens = 7798, batch.n_tokens = 64
slot init_sampler: id  1 | task 28301 | init sampler, took 1.31 ms, tokens: text = 7798, total = 7798
slot update_slots: id  1 | task 28301 | erasing old context checkpoint (pos_min = 25, pos_max = 794, size = 18.056 MiB)
slot update_slots: id  1 | task 28301 | created context checkpoint 8 of 8 (pos_min = 6964, pos_max = 7733, size = 18.056 MiB)
slot print_timing: id  1 | task 28301 | 
prompt eval time =    1798.51 ms /   693 tokens (    2.60 ms per token,   385.32 tokens per second)
       eval time =    1611.27 ms /    40 tokens (   40.28 ms per token,    24.83 tokens per second)
      total time =    3409.79 ms /   733 tokens
slot      release: id  1 | task 28301 | stop processing: n_tokens = 7837, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28343 | processing task, is_child = 0
slot update_slots: id  1 | task 28343 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7865
slot update_slots: id  1 | task 28343 | n_tokens = 7806, memory_seq_rm [7806, end)
slot update_slots: id  1 | task 28343 | prompt processing progress, n_tokens = 7865, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  1 | task 28343 | prompt done, n_tokens = 7865, batch.n_tokens = 59
slot init_sampler: id  1 | task 28343 | init sampler, took 1.19 ms, tokens: text = 7865, total = 7865
slot update_slots: id  1 | task 28343 | erasing old context checkpoint (pos_min = 121, pos_max = 876, size = 17.728 MiB)
slot update_slots: id  1 | task 28343 | created context checkpoint 8 of 8 (pos_min = 7067, pos_max = 7805, size = 17.329 MiB)
slot print_timing: id  1 | task 28343 | 
prompt eval time =     279.33 ms /    59 tokens (    4.73 ms per token,   211.22 tokens per second)
       eval time =    1701.66 ms /    42 tokens (   40.52 ms per token,    24.68 tokens per second)
      total time =    1980.99 ms /   101 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 28343 | stop processing: n_tokens = 7906, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28386 | processing task, is_child = 0
slot update_slots: id  1 | task 28386 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7947
slot update_slots: id  1 | task 28386 | n_tokens = 7885, memory_seq_rm [7885, end)
slot update_slots: id  1 | task 28386 | prompt processing progress, n_tokens = 7947, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  1 | task 28386 | prompt done, n_tokens = 7947, batch.n_tokens = 62
slot init_sampler: id  1 | task 28386 | init sampler, took 1.63 ms, tokens: text = 7947, total = 7947
slot update_slots: id  1 | task 28386 | erasing old context checkpoint (pos_min = 298, pos_max = 1067, size = 18.056 MiB)
slot update_slots: id  1 | task 28386 | created context checkpoint 8 of 8 (pos_min = 7136, pos_max = 7884, size = 17.564 MiB)
slot print_timing: id  1 | task 28386 | 
prompt eval time =     324.21 ms /    62 tokens (    5.23 ms per token,   191.23 tokens per second)
       eval time =    1668.97 ms /    42 tokens (   39.74 ms per token,    25.17 tokens per second)
      total time =    1993.18 ms /   104 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 28386 | stop processing: n_tokens = 7988, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28429 | processing task, is_child = 0
slot update_slots: id  1 | task 28429 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8529
slot update_slots: id  1 | task 28429 | n_tokens = 7961, memory_seq_rm [7961, end)
slot update_slots: id  1 | task 28429 | prompt processing progress, n_tokens = 8465, batch.n_tokens = 504, progress = 0.992496
slot update_slots: id  1 | task 28429 | n_tokens = 8465, memory_seq_rm [8465, end)
slot update_slots: id  1 | task 28429 | prompt processing progress, n_tokens = 8529, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28429 | prompt done, n_tokens = 8529, batch.n_tokens = 64
slot init_sampler: id  1 | task 28429 | init sampler, took 1.69 ms, tokens: text = 8529, total = 8529
slot update_slots: id  1 | task 28429 | erasing old context checkpoint (pos_min = 1356, pos_max = 2125, size = 18.056 MiB)
slot update_slots: id  1 | task 28429 | created context checkpoint 8 of 8 (pos_min = 7695, pos_max = 8464, size = 18.056 MiB)
slot print_timing: id  1 | task 28429 | 
prompt eval time =    1397.13 ms /   568 tokens (    2.46 ms per token,   406.55 tokens per second)
       eval time =    1602.22 ms /    41 tokens (   39.08 ms per token,    25.59 tokens per second)
      total time =    2999.35 ms /   609 tokens
slot      release: id  1 | task 28429 | stop processing: n_tokens = 8569, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.942 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28472 | processing task, is_child = 0
slot update_slots: id  1 | task 28472 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9063
slot update_slots: id  1 | task 28472 | n_tokens = 8539, memory_seq_rm [8539, end)
slot update_slots: id  1 | task 28472 | prompt processing progress, n_tokens = 8999, batch.n_tokens = 460, progress = 0.992938
slot update_slots: id  1 | task 28472 | n_tokens = 8999, memory_seq_rm [8999, end)
slot update_slots: id  1 | task 28472 | prompt processing progress, n_tokens = 9063, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28472 | prompt done, n_tokens = 9063, batch.n_tokens = 64
slot init_sampler: id  1 | task 28472 | init sampler, took 1.50 ms, tokens: text = 9063, total = 9063
slot update_slots: id  1 | task 28472 | erasing old context checkpoint (pos_min = 1466, pos_max = 2212, size = 17.517 MiB)
slot update_slots: id  1 | task 28472 | created context checkpoint 8 of 8 (pos_min = 8229, pos_max = 8998, size = 18.056 MiB)
slot print_timing: id  1 | task 28472 | 
prompt eval time =    1320.90 ms /   524 tokens (    2.52 ms per token,   396.70 tokens per second)
       eval time =    2018.74 ms /    51 tokens (   39.58 ms per token,    25.26 tokens per second)
      total time =    3339.64 ms /   575 tokens
slot      release: id  1 | task 28472 | stop processing: n_tokens = 9113, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28525 | processing task, is_child = 0
slot update_slots: id  1 | task 28525 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9352
slot update_slots: id  1 | task 28525 | n_tokens = 9083, memory_seq_rm [9083, end)
slot update_slots: id  1 | task 28525 | prompt processing progress, n_tokens = 9288, batch.n_tokens = 205, progress = 0.993157
slot update_slots: id  1 | task 28525 | n_tokens = 9288, memory_seq_rm [9288, end)
slot update_slots: id  1 | task 28525 | prompt processing progress, n_tokens = 9352, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28525 | prompt done, n_tokens = 9352, batch.n_tokens = 64
slot init_sampler: id  1 | task 28525 | init sampler, took 1.31 ms, tokens: text = 9352, total = 9352
slot update_slots: id  1 | task 28525 | erasing old context checkpoint (pos_min = 2292, pos_max = 3045, size = 17.681 MiB)
slot update_slots: id  1 | task 28525 | created context checkpoint 8 of 8 (pos_min = 8518, pos_max = 9287, size = 18.056 MiB)
slot print_timing: id  1 | task 28525 | 
prompt eval time =     895.73 ms /   269 tokens (    3.33 ms per token,   300.31 tokens per second)
       eval time =    1490.21 ms /    38 tokens (   39.22 ms per token,    25.50 tokens per second)
      total time =    2385.94 ms /   307 tokens
slot      release: id  1 | task 28525 | stop processing: n_tokens = 9389, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28565 | processing task, is_child = 0
slot update_slots: id  1 | task 28565 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9517
slot update_slots: id  1 | task 28565 | n_tokens = 9363, memory_seq_rm [9363, end)
slot update_slots: id  1 | task 28565 | prompt processing progress, n_tokens = 9453, batch.n_tokens = 90, progress = 0.993275
slot update_slots: id  1 | task 28565 | n_tokens = 9453, memory_seq_rm [9453, end)
slot update_slots: id  1 | task 28565 | prompt processing progress, n_tokens = 9517, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28565 | prompt done, n_tokens = 9517, batch.n_tokens = 64
slot init_sampler: id  1 | task 28565 | init sampler, took 1.92 ms, tokens: text = 9517, total = 9517
slot update_slots: id  1 | task 28565 | erasing old context checkpoint (pos_min = 4210, pos_max = 4979, size = 18.056 MiB)
slot update_slots: id  1 | task 28565 | created context checkpoint 8 of 8 (pos_min = 8683, pos_max = 9452, size = 18.056 MiB)
slot print_timing: id  1 | task 28565 | 
prompt eval time =     700.59 ms /   154 tokens (    4.55 ms per token,   219.81 tokens per second)
       eval time =    1471.34 ms /    37 tokens (   39.77 ms per token,    25.15 tokens per second)
      total time =    2171.93 ms /   191 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 28565 | stop processing: n_tokens = 9553, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.824 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28604 | processing task, is_child = 0
slot update_slots: id  1 | task 28604 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11567
slot update_slots: id  1 | task 28604 | n_tokens = 9527, memory_seq_rm [9527, end)
slot update_slots: id  1 | task 28604 | prompt processing progress, n_tokens = 11503, batch.n_tokens = 1976, progress = 0.994467
slot update_slots: id  1 | task 28604 | n_tokens = 11503, memory_seq_rm [11503, end)
slot update_slots: id  1 | task 28604 | prompt processing progress, n_tokens = 11567, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28604 | prompt done, n_tokens = 11567, batch.n_tokens = 64
slot init_sampler: id  1 | task 28604 | init sampler, took 1.58 ms, tokens: text = 11567, total = 11567
slot update_slots: id  1 | task 28604 | erasing old context checkpoint (pos_min = 6263, pos_max = 7032, size = 18.056 MiB)
slot update_slots: id  1 | task 28604 | created context checkpoint 8 of 8 (pos_min = 10733, pos_max = 11502, size = 18.056 MiB)
slot print_timing: id  1 | task 28604 | 
prompt eval time =    4252.39 ms /  2040 tokens (    2.08 ms per token,   479.73 tokens per second)
       eval time =    1497.26 ms /    39 tokens (   38.39 ms per token,    26.05 tokens per second)
      total time =    5749.65 ms /  2079 tokens
slot      release: id  1 | task 28604 | stop processing: n_tokens = 11605, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.842 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28645 | processing task, is_child = 0
slot update_slots: id  1 | task 28645 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13745
slot update_slots: id  1 | task 28645 | n_tokens = 11575, memory_seq_rm [11575, end)
slot update_slots: id  1 | task 28645 | prompt processing progress, n_tokens = 13623, batch.n_tokens = 2048, progress = 0.991124
slot update_slots: id  1 | task 28645 | n_tokens = 13623, memory_seq_rm [13623, end)
slot update_slots: id  1 | task 28645 | prompt processing progress, n_tokens = 13681, batch.n_tokens = 58, progress = 0.995344
slot update_slots: id  1 | task 28645 | n_tokens = 13681, memory_seq_rm [13681, end)
slot update_slots: id  1 | task 28645 | prompt processing progress, n_tokens = 13745, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28645 | prompt done, n_tokens = 13745, batch.n_tokens = 64
slot init_sampler: id  1 | task 28645 | init sampler, took 1.90 ms, tokens: text = 13745, total = 13745
slot update_slots: id  1 | task 28645 | erasing old context checkpoint (pos_min = 6964, pos_max = 7733, size = 18.056 MiB)
slot update_slots: id  1 | task 28645 | created context checkpoint 8 of 8 (pos_min = 12911, pos_max = 13680, size = 18.056 MiB)
slot print_timing: id  1 | task 28645 | 
prompt eval time =    4685.86 ms /  2170 tokens (    2.16 ms per token,   463.10 tokens per second)
       eval time =   16208.57 ms /   448 tokens (   36.18 ms per token,    27.64 tokens per second)
      total time =   20894.43 ms /  2618 tokens
slot      release: id  1 | task 28645 | stop processing: n_tokens = 14192, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29096 | processing task, is_child = 0
slot update_slots: id  1 | task 29096 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14386
slot update_slots: id  1 | task 29096 | n_tokens = 13773, memory_seq_rm [13773, end)
slot update_slots: id  1 | task 29096 | prompt processing progress, n_tokens = 14322, batch.n_tokens = 549, progress = 0.995551
slot update_slots: id  1 | task 29096 | n_tokens = 14322, memory_seq_rm [14322, end)
slot update_slots: id  1 | task 29096 | prompt processing progress, n_tokens = 14386, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29096 | prompt done, n_tokens = 14386, batch.n_tokens = 64
slot init_sampler: id  1 | task 29096 | init sampler, took 2.09 ms, tokens: text = 14386, total = 14386
slot update_slots: id  1 | task 29096 | erasing old context checkpoint (pos_min = 7067, pos_max = 7805, size = 17.329 MiB)
slot update_slots: id  1 | task 29096 | created context checkpoint 8 of 8 (pos_min = 13621, pos_max = 14321, size = 16.438 MiB)
slot print_timing: id  1 | task 29096 | 
prompt eval time =    1487.34 ms /   613 tokens (    2.43 ms per token,   412.15 tokens per second)
       eval time =    1481.32 ms /    42 tokens (   35.27 ms per token,    28.35 tokens per second)
      total time =    2968.66 ms /   655 tokens
slot      release: id  1 | task 29096 | stop processing: n_tokens = 14427, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.909 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29140 | processing task, is_child = 0
slot update_slots: id  1 | task 29140 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15837
slot update_slots: id  1 | task 29140 | n_tokens = 14400, memory_seq_rm [14400, end)
slot update_slots: id  1 | task 29140 | prompt processing progress, n_tokens = 15773, batch.n_tokens = 1373, progress = 0.995959
slot update_slots: id  1 | task 29140 | n_tokens = 15773, memory_seq_rm [15773, end)
slot update_slots: id  1 | task 29140 | prompt processing progress, n_tokens = 15837, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29140 | prompt done, n_tokens = 15837, batch.n_tokens = 64
slot init_sampler: id  1 | task 29140 | init sampler, took 2.23 ms, tokens: text = 15837, total = 15837
slot update_slots: id  1 | task 29140 | erasing old context checkpoint (pos_min = 7136, pos_max = 7884, size = 17.564 MiB)
slot update_slots: id  1 | task 29140 | created context checkpoint 8 of 8 (pos_min = 15003, pos_max = 15772, size = 18.056 MiB)
slot print_timing: id  1 | task 29140 | 
prompt eval time =    3122.85 ms /  1437 tokens (    2.17 ms per token,   460.16 tokens per second)
       eval time =   29730.63 ms /   775 tokens (   38.36 ms per token,    26.07 tokens per second)
      total time =   32853.48 ms /  2212 tokens
slot      release: id  1 | task 29140 | stop processing: n_tokens = 16611, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29917 | processing task, is_child = 0
slot update_slots: id  1 | task 29917 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16654
slot update_slots: id  1 | task 29917 | n_tokens = 16587, memory_seq_rm [16587, end)
slot update_slots: id  1 | task 29917 | prompt processing progress, n_tokens = 16590, batch.n_tokens = 3, progress = 0.996157
slot update_slots: id  1 | task 29917 | n_tokens = 16590, memory_seq_rm [16590, end)
slot update_slots: id  1 | task 29917 | prompt processing progress, n_tokens = 16654, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29917 | prompt done, n_tokens = 16654, batch.n_tokens = 64
slot init_sampler: id  1 | task 29917 | init sampler, took 2.31 ms, tokens: text = 16654, total = 16654
slot update_slots: id  1 | task 29917 | erasing old context checkpoint (pos_min = 7695, pos_max = 8464, size = 18.056 MiB)
slot update_slots: id  1 | task 29917 | created context checkpoint 8 of 8 (pos_min = 15841, pos_max = 16589, size = 17.564 MiB)
slot print_timing: id  1 | task 29917 | 
prompt eval time =     373.09 ms /    67 tokens (    5.57 ms per token,   179.58 tokens per second)
       eval time =    2245.15 ms /    56 tokens (   40.09 ms per token,    24.94 tokens per second)
      total time =    2618.23 ms /   123 tokens
slot      release: id  1 | task 29917 | stop processing: n_tokens = 16709, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29975 | processing task, is_child = 0
slot update_slots: id  1 | task 29975 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17020
slot update_slots: id  1 | task 29975 | n_tokens = 16680, memory_seq_rm [16680, end)
slot update_slots: id  1 | task 29975 | prompt processing progress, n_tokens = 16956, batch.n_tokens = 276, progress = 0.996240
slot update_slots: id  1 | task 29975 | n_tokens = 16956, memory_seq_rm [16956, end)
slot update_slots: id  1 | task 29975 | prompt processing progress, n_tokens = 17020, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29975 | prompt done, n_tokens = 17020, batch.n_tokens = 64
slot init_sampler: id  1 | task 29975 | init sampler, took 3.49 ms, tokens: text = 17020, total = 17020
slot update_slots: id  1 | task 29975 | erasing old context checkpoint (pos_min = 8229, pos_max = 8998, size = 18.056 MiB)
slot update_slots: id  1 | task 29975 | created context checkpoint 8 of 8 (pos_min = 16186, pos_max = 16955, size = 18.056 MiB)
slot print_timing: id  1 | task 29975 | 
prompt eval time =     969.10 ms /   340 tokens (    2.85 ms per token,   350.84 tokens per second)
       eval time =   64501.74 ms /  1594 tokens (   40.47 ms per token,    24.71 tokens per second)
      total time =   65470.84 ms /  1934 tokens
slot      release: id  1 | task 29975 | stop processing: n_tokens = 18613, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31571 | processing task, is_child = 0
slot update_slots: id  0 | task 31571 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17155
slot update_slots: id  0 | task 31571 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 31571 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.119382
slot update_slots: id  0 | task 31571 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 31571 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.238764
slot update_slots: id  0 | task 31571 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 31571 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.358146
slot update_slots: id  0 | task 31571 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 31571 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.477528
slot update_slots: id  0 | task 31571 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 31571 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.596911
slot update_slots: id  0 | task 31571 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  0 | task 31571 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.716293
decode: failed to find a memory slot for batch of size 2048
srv  try_clear_id: purging slot 1 with 18613 tokens
slot prompt_clear: id  1 | task -1 | clearing prompt with 18613 tokens
srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 2048, ret = 1
slot update_slots: id  0 | task 31571 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  0 | task 31571 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.835675
slot update_slots: id  0 | task 31571 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  0 | task 31571 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.955057
slot update_slots: id  0 | task 31571 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  0 | task 31571 | prompt processing progress, n_tokens = 17091, batch.n_tokens = 707, progress = 0.996269
slot update_slots: id  0 | task 31571 | n_tokens = 17091, memory_seq_rm [17091, end)
slot update_slots: id  0 | task 31571 | prompt processing progress, n_tokens = 17155, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31571 | prompt done, n_tokens = 17155, batch.n_tokens = 64
slot init_sampler: id  0 | task 31571 | init sampler, took 2.72 ms, tokens: text = 17155, total = 17155
slot update_slots: id  0 | task 31571 | created context checkpoint 1 of 8 (pos_min = 16321, pos_max = 17090, size = 18.056 MiB)
slot print_timing: id  0 | task 31571 | 
prompt eval time =   40345.47 ms / 17155 tokens (    2.35 ms per token,   425.20 tokens per second)
       eval time =   16333.89 ms /   416 tokens (   39.26 ms per token,    25.47 tokens per second)
      total time =   56679.36 ms / 17571 tokens
slot      release: id  0 | task 31571 | stop processing: n_tokens = 17570, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31997 | processing task, is_child = 0
slot update_slots: id  0 | task 31997 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17637
slot update_slots: id  0 | task 31997 | n_tokens = 17540, memory_seq_rm [17540, end)
slot update_slots: id  0 | task 31997 | prompt processing progress, n_tokens = 17573, batch.n_tokens = 33, progress = 0.996371
slot update_slots: id  0 | task 31997 | n_tokens = 17573, memory_seq_rm [17573, end)
slot update_slots: id  0 | task 31997 | prompt processing progress, n_tokens = 17637, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31997 | prompt done, n_tokens = 17637, batch.n_tokens = 64
slot init_sampler: id  0 | task 31997 | init sampler, took 2.69 ms, tokens: text = 17637, total = 17637
slot update_slots: id  0 | task 31997 | created context checkpoint 2 of 8 (pos_min = 16803, pos_max = 17572, size = 18.056 MiB)
slot print_timing: id  0 | task 31997 | 
prompt eval time =     524.71 ms /    97 tokens (    5.41 ms per token,   184.86 tokens per second)
       eval time =    1171.97 ms /    31 tokens (   37.81 ms per token,    26.45 tokens per second)
      total time =    1696.68 ms /   128 tokens
slot      release: id  0 | task 31997 | stop processing: n_tokens = 17667, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.968 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 32030 | processing task, is_child = 0
slot update_slots: id  0 | task 32030 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18213
slot update_slots: id  0 | task 32030 | n_tokens = 17637, memory_seq_rm [17637, end)
slot update_slots: id  0 | task 32030 | prompt processing progress, n_tokens = 18149, batch.n_tokens = 512, progress = 0.996486
slot update_slots: id  0 | task 32030 | n_tokens = 18149, memory_seq_rm [18149, end)
slot update_slots: id  0 | task 32030 | prompt processing progress, n_tokens = 18213, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 32030 | prompt done, n_tokens = 18213, batch.n_tokens = 64
slot init_sampler: id  0 | task 32030 | init sampler, took 2.54 ms, tokens: text = 18213, total = 18213
slot update_slots: id  0 | task 32030 | created context checkpoint 3 of 8 (pos_min = 17379, pos_max = 18148, size = 18.056 MiB)
slot print_timing: id  0 | task 32030 | 
prompt eval time =    1457.36 ms /   576 tokens (    2.53 ms per token,   395.23 tokens per second)
       eval time =   22732.08 ms /   575 tokens (   39.53 ms per token,    25.29 tokens per second)
      total time =   24189.44 ms /  1151 tokens
slot      release: id  0 | task 32030 | stop processing: n_tokens = 18787, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 32607 | processing task, is_child = 0
slot update_slots: id  0 | task 32607 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18839
slot update_slots: id  0 | task 32607 | n_tokens = 18758, memory_seq_rm [18758, end)
slot update_slots: id  0 | task 32607 | prompt processing progress, n_tokens = 18775, batch.n_tokens = 17, progress = 0.996603
slot update_slots: id  0 | task 32607 | n_tokens = 18775, memory_seq_rm [18775, end)
slot update_slots: id  0 | task 32607 | prompt processing progress, n_tokens = 18839, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 32607 | prompt done, n_tokens = 18839, batch.n_tokens = 64
slot init_sampler: id  0 | task 32607 | init sampler, took 2.76 ms, tokens: text = 18839, total = 18839
slot update_slots: id  0 | task 32607 | created context checkpoint 4 of 8 (pos_min = 18017, pos_max = 18774, size = 17.775 MiB)
slot print_timing: id  0 | task 32607 | 
prompt eval time =     488.07 ms /    81 tokens (    6.03 ms per token,   165.96 tokens per second)
       eval time =   30430.22 ms /   753 tokens (   40.41 ms per token,    24.75 tokens per second)
      total time =   30918.29 ms /   834 tokens
slot      release: id  0 | task 32607 | stop processing: n_tokens = 19591, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 33362 | processing task, is_child = 0
slot update_slots: id  0 | task 33362 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19637
slot update_slots: id  0 | task 33362 | n_tokens = 19468, memory_seq_rm [19468, end)
slot update_slots: id  0 | task 33362 | prompt processing progress, n_tokens = 19573, batch.n_tokens = 105, progress = 0.996741
slot update_slots: id  0 | task 33362 | n_tokens = 19573, memory_seq_rm [19573, end)
slot update_slots: id  0 | task 33362 | prompt processing progress, n_tokens = 19637, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 33362 | prompt done, n_tokens = 19637, batch.n_tokens = 64
slot init_sampler: id  0 | task 33362 | init sampler, took 4.02 ms, tokens: text = 19637, total = 19637
slot update_slots: id  0 | task 33362 | created context checkpoint 5 of 8 (pos_min = 18821, pos_max = 19572, size = 17.634 MiB)
slot print_timing: id  0 | task 33362 | 
prompt eval time =     756.37 ms /   169 tokens (    4.48 ms per token,   223.44 tokens per second)
       eval time =    3841.39 ms /    96 tokens (   40.01 ms per token,    24.99 tokens per second)
      total time =    4597.76 ms /   265 tokens
slot      release: id  0 | task 33362 | stop processing: n_tokens = 19732, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 33460 | processing task, is_child = 0
slot update_slots: id  0 | task 33460 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19784
slot update_slots: id  0 | task 33460 | n_tokens = 19703, memory_seq_rm [19703, end)
slot update_slots: id  0 | task 33460 | prompt processing progress, n_tokens = 19720, batch.n_tokens = 17, progress = 0.996765
slot update_slots: id  0 | task 33460 | n_tokens = 19720, memory_seq_rm [19720, end)
slot update_slots: id  0 | task 33460 | prompt processing progress, n_tokens = 19784, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 33460 | prompt done, n_tokens = 19784, batch.n_tokens = 64
slot init_sampler: id  0 | task 33460 | init sampler, took 3.14 ms, tokens: text = 19784, total = 19784
slot update_slots: id  0 | task 33460 | created context checkpoint 6 of 8 (pos_min = 18962, pos_max = 19719, size = 17.775 MiB)
slot print_timing: id  0 | task 33460 | 
prompt eval time =     471.40 ms /    81 tokens (    5.82 ms per token,   171.83 tokens per second)
       eval time =   24554.69 ms /   607 tokens (   40.45 ms per token,    24.72 tokens per second)
      total time =   25026.09 ms /   688 tokens
slot      release: id  0 | task 33460 | stop processing: n_tokens = 20390, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34069 | processing task, is_child = 0
slot update_slots: id  0 | task 34069 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 20436
slot update_slots: id  0 | task 34069 | n_tokens = 19843, memory_seq_rm [19843, end)
slot update_slots: id  0 | task 34069 | prompt processing progress, n_tokens = 20372, batch.n_tokens = 529, progress = 0.996868
slot update_slots: id  0 | task 34069 | n_tokens = 20372, memory_seq_rm [20372, end)
slot update_slots: id  0 | task 34069 | prompt processing progress, n_tokens = 20436, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34069 | prompt done, n_tokens = 20436, batch.n_tokens = 64
slot init_sampler: id  0 | task 34069 | init sampler, took 3.09 ms, tokens: text = 20436, total = 20436
slot update_slots: id  0 | task 34069 | created context checkpoint 7 of 8 (pos_min = 19716, pos_max = 20371, size = 15.383 MiB)
slot print_timing: id  0 | task 34069 | 
prompt eval time =    1628.42 ms /   593 tokens (    2.75 ms per token,   364.16 tokens per second)
       eval time =    2597.49 ms /    65 tokens (   39.96 ms per token,    25.02 tokens per second)
      total time =    4225.90 ms /   658 tokens
slot      release: id  0 | task 34069 | stop processing: n_tokens = 20500, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34136 | processing task, is_child = 0
slot update_slots: id  0 | task 34136 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 20537
slot update_slots: id  0 | task 34136 | n_tokens = 20453, memory_seq_rm [20453, end)
slot update_slots: id  0 | task 34136 | prompt processing progress, n_tokens = 20473, batch.n_tokens = 20, progress = 0.996884
slot update_slots: id  0 | task 34136 | n_tokens = 20473, memory_seq_rm [20473, end)
slot update_slots: id  0 | task 34136 | prompt processing progress, n_tokens = 20537, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34136 | prompt done, n_tokens = 20537, batch.n_tokens = 64
slot init_sampler: id  0 | task 34136 | init sampler, took 3.17 ms, tokens: text = 20537, total = 20537
slot update_slots: id  0 | task 34136 | created context checkpoint 8 of 8 (pos_min = 19863, pos_max = 20472, size = 14.304 MiB)
slot print_timing: id  0 | task 34136 | 
prompt eval time =     502.60 ms /    84 tokens (    5.98 ms per token,   167.13 tokens per second)
       eval time =   25148.83 ms /   623 tokens (   40.37 ms per token,    24.77 tokens per second)
      total time =   25651.43 ms /   707 tokens
slot      release: id  0 | task 34136 | stop processing: n_tokens = 21159, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34761 | processing task, is_child = 0
slot update_slots: id  0 | task 34761 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21205
slot update_slots: id  0 | task 34761 | n_tokens = 20555, memory_seq_rm [20555, end)
slot update_slots: id  0 | task 34761 | prompt processing progress, n_tokens = 21141, batch.n_tokens = 586, progress = 0.996982
slot update_slots: id  0 | task 34761 | n_tokens = 21141, memory_seq_rm [21141, end)
slot update_slots: id  0 | task 34761 | prompt processing progress, n_tokens = 21205, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34761 | prompt done, n_tokens = 21205, batch.n_tokens = 64
slot init_sampler: id  0 | task 34761 | init sampler, took 2.97 ms, tokens: text = 21205, total = 21205
slot update_slots: id  0 | task 34761 | erasing old context checkpoint (pos_min = 16321, pos_max = 17090, size = 18.056 MiB)
slot update_slots: id  0 | task 34761 | created context checkpoint 8 of 8 (pos_min = 20422, pos_max = 21140, size = 16.860 MiB)
slot print_timing: id  0 | task 34761 | 
prompt eval time =    1955.60 ms /   650 tokens (    3.01 ms per token,   332.38 tokens per second)
       eval time =    7056.44 ms /   176 tokens (   40.09 ms per token,    24.94 tokens per second)
      total time =    9012.04 ms /   826 tokens
slot      release: id  0 | task 34761 | stop processing: n_tokens = 21380, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34939 | processing task, is_child = 0
slot update_slots: id  0 | task 34939 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21426
slot update_slots: id  0 | task 34939 | n_tokens = 21222, memory_seq_rm [21222, end)
slot update_slots: id  0 | task 34939 | prompt processing progress, n_tokens = 21362, batch.n_tokens = 140, progress = 0.997013
slot update_slots: id  0 | task 34939 | n_tokens = 21362, memory_seq_rm [21362, end)
slot update_slots: id  0 | task 34939 | prompt processing progress, n_tokens = 21426, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34939 | prompt done, n_tokens = 21426, batch.n_tokens = 64
slot init_sampler: id  0 | task 34939 | init sampler, took 3.15 ms, tokens: text = 21426, total = 21426
slot update_slots: id  0 | task 34939 | erasing old context checkpoint (pos_min = 16803, pos_max = 17572, size = 18.056 MiB)
slot update_slots: id  0 | task 34939 | created context checkpoint 8 of 8 (pos_min = 20610, pos_max = 21361, size = 17.634 MiB)
slot print_timing: id  0 | task 34939 | 
prompt eval time =     825.49 ms /   204 tokens (    4.05 ms per token,   247.13 tokens per second)
       eval time =    9127.83 ms /   227 tokens (   40.21 ms per token,    24.87 tokens per second)
      total time =    9953.32 ms /   431 tokens
slot      release: id  0 | task 34939 | stop processing: n_tokens = 21652, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35168 | processing task, is_child = 0
slot update_slots: id  0 | task 35168 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21698
slot update_slots: id  0 | task 35168 | n_tokens = 21450, memory_seq_rm [21450, end)
slot update_slots: id  0 | task 35168 | prompt processing progress, n_tokens = 21634, batch.n_tokens = 184, progress = 0.997050
slot update_slots: id  0 | task 35168 | n_tokens = 21634, memory_seq_rm [21634, end)
slot update_slots: id  0 | task 35168 | prompt processing progress, n_tokens = 21698, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35168 | prompt done, n_tokens = 21698, batch.n_tokens = 64
slot init_sampler: id  0 | task 35168 | init sampler, took 4.16 ms, tokens: text = 21698, total = 21698
slot update_slots: id  0 | task 35168 | erasing old context checkpoint (pos_min = 17379, pos_max = 18148, size = 18.056 MiB)
slot update_slots: id  0 | task 35168 | created context checkpoint 8 of 8 (pos_min = 20883, pos_max = 21633, size = 17.610 MiB)
slot print_timing: id  0 | task 35168 | 
prompt eval time =     922.01 ms /   248 tokens (    3.72 ms per token,   268.98 tokens per second)
       eval time =    3235.00 ms /    81 tokens (   39.94 ms per token,    25.04 tokens per second)
      total time =    4157.01 ms /   329 tokens
slot      release: id  0 | task 35168 | stop processing: n_tokens = 21778, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35251 | processing task, is_child = 0
slot update_slots: id  0 | task 35251 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21874
slot update_slots: id  0 | task 35251 | n_tokens = 21752, memory_seq_rm [21752, end)
slot update_slots: id  0 | task 35251 | prompt processing progress, n_tokens = 21810, batch.n_tokens = 58, progress = 0.997074
slot update_slots: id  0 | task 35251 | n_tokens = 21810, memory_seq_rm [21810, end)
slot update_slots: id  0 | task 35251 | prompt processing progress, n_tokens = 21874, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35251 | prompt done, n_tokens = 21874, batch.n_tokens = 64
slot init_sampler: id  0 | task 35251 | init sampler, took 3.29 ms, tokens: text = 21874, total = 21874
slot update_slots: id  0 | task 35251 | erasing old context checkpoint (pos_min = 18017, pos_max = 18774, size = 17.775 MiB)
slot update_slots: id  0 | task 35251 | created context checkpoint 8 of 8 (pos_min = 21041, pos_max = 21809, size = 18.033 MiB)
slot print_timing: id  0 | task 35251 | 
prompt eval time =     615.87 ms /   122 tokens (    5.05 ms per token,   198.09 tokens per second)
       eval time =   50953.23 ms /  1260 tokens (   40.44 ms per token,    24.73 tokens per second)
      total time =   51569.10 ms /  1382 tokens
slot      release: id  0 | task 35251 | stop processing: n_tokens = 23133, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36513 | processing task, is_child = 0
slot update_slots: id  0 | task 36513 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 23179
slot update_slots: id  0 | task 36513 | n_past = 22464, slot.prompt.tokens.size() = 23133, seq_id = 0, pos_min = 22363, n_swa = 128
slot update_slots: id  0 | task 36513 | restored context checkpoint (pos_min = 21041, pos_max = 21809, size = 18.033 MiB)
slot update_slots: id  0 | task 36513 | n_tokens = 21809, memory_seq_rm [21809, end)
slot update_slots: id  0 | task 36513 | prompt processing progress, n_tokens = 23115, batch.n_tokens = 1306, progress = 0.997239
slot update_slots: id  0 | task 36513 | n_tokens = 23115, memory_seq_rm [23115, end)
slot update_slots: id  0 | task 36513 | prompt processing progress, n_tokens = 23179, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36513 | prompt done, n_tokens = 23179, batch.n_tokens = 64
slot init_sampler: id  0 | task 36513 | init sampler, took 3.24 ms, tokens: text = 23179, total = 23179
slot update_slots: id  0 | task 36513 | erasing old context checkpoint (pos_min = 18821, pos_max = 19572, size = 17.634 MiB)
slot update_slots: id  0 | task 36513 | created context checkpoint 8 of 8 (pos_min = 22345, pos_max = 23114, size = 18.056 MiB)
slot print_timing: id  0 | task 36513 | 
prompt eval time =    3621.03 ms /  1370 tokens (    2.64 ms per token,   378.35 tokens per second)
       eval time =    3839.17 ms /    96 tokens (   39.99 ms per token,    25.01 tokens per second)
      total time =    7460.20 ms /  1466 tokens
slot      release: id  0 | task 36513 | stop processing: n_tokens = 23274, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36611 | processing task, is_child = 0
slot update_slots: id  0 | task 36611 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 23306
slot update_slots: id  0 | task 36611 | n_tokens = 23248, memory_seq_rm [23248, end)
slot update_slots: id  0 | task 36611 | prompt processing progress, n_tokens = 23306, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 36611 | prompt done, n_tokens = 23306, batch.n_tokens = 58
slot init_sampler: id  0 | task 36611 | init sampler, took 3.31 ms, tokens: text = 23306, total = 23306
slot update_slots: id  0 | task 36611 | erasing old context checkpoint (pos_min = 18962, pos_max = 19719, size = 17.775 MiB)
slot update_slots: id  0 | task 36611 | created context checkpoint 8 of 8 (pos_min = 22504, pos_max = 23247, size = 17.446 MiB)
slot print_timing: id  0 | task 36611 | 
prompt eval time =     273.86 ms /    58 tokens (    4.72 ms per token,   211.79 tokens per second)
       eval time =   22157.15 ms /   548 tokens (   40.43 ms per token,    24.73 tokens per second)
      total time =   22431.01 ms /   606 tokens
slot      release: id  0 | task 36611 | stop processing: n_tokens = 23853, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37160 | processing task, is_child = 0
slot update_slots: id  0 | task 37160 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 23970
slot update_slots: id  0 | task 37160 | n_tokens = 23829, memory_seq_rm [23829, end)
slot update_slots: id  0 | task 37160 | prompt processing progress, n_tokens = 23906, batch.n_tokens = 77, progress = 0.997330
slot update_slots: id  0 | task 37160 | n_tokens = 23906, memory_seq_rm [23906, end)
slot update_slots: id  0 | task 37160 | prompt processing progress, n_tokens = 23970, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37160 | prompt done, n_tokens = 23970, batch.n_tokens = 64
slot init_sampler: id  0 | task 37160 | init sampler, took 3.72 ms, tokens: text = 23970, total = 23970
slot update_slots: id  0 | task 37160 | erasing old context checkpoint (pos_min = 19716, pos_max = 20371, size = 15.383 MiB)
slot update_slots: id  0 | task 37160 | created context checkpoint 8 of 8 (pos_min = 23136, pos_max = 23905, size = 18.056 MiB)
slot print_timing: id  0 | task 37160 | 
prompt eval time =     731.51 ms /   141 tokens (    5.19 ms per token,   192.75 tokens per second)
       eval time =    9786.88 ms /   243 tokens (   40.28 ms per token,    24.83 tokens per second)
      total time =   10518.39 ms /   384 tokens
slot      release: id  0 | task 37160 | stop processing: n_tokens = 24212, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37405 | processing task, is_child = 0
slot update_slots: id  0 | task 37405 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 24244
slot update_slots: id  0 | task 37405 | n_tokens = 24131, memory_seq_rm [24131, end)
slot update_slots: id  0 | task 37405 | prompt processing progress, n_tokens = 24180, batch.n_tokens = 49, progress = 0.997360
slot update_slots: id  0 | task 37405 | n_tokens = 24180, memory_seq_rm [24180, end)
slot update_slots: id  0 | task 37405 | prompt processing progress, n_tokens = 24244, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37405 | prompt done, n_tokens = 24244, batch.n_tokens = 64
slot init_sampler: id  0 | task 37405 | init sampler, took 3.59 ms, tokens: text = 24244, total = 24244
slot update_slots: id  0 | task 37405 | erasing old context checkpoint (pos_min = 19863, pos_max = 20472, size = 14.304 MiB)
slot update_slots: id  0 | task 37405 | created context checkpoint 8 of 8 (pos_min = 23442, pos_max = 24179, size = 17.306 MiB)
slot print_timing: id  0 | task 37405 | 
prompt eval time =     643.85 ms /   113 tokens (    5.70 ms per token,   175.51 tokens per second)
       eval time =    1335.69 ms /    34 tokens (   39.29 ms per token,    25.45 tokens per second)
      total time =    1979.54 ms /   147 tokens
slot      release: id  0 | task 37405 | stop processing: n_tokens = 24277, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37441 | processing task, is_child = 0
slot update_slots: id  0 | task 37441 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 24607
slot update_slots: id  0 | task 37441 | n_tokens = 24254, memory_seq_rm [24254, end)
slot update_slots: id  0 | task 37441 | prompt processing progress, n_tokens = 24543, batch.n_tokens = 289, progress = 0.997399
slot update_slots: id  0 | task 37441 | n_tokens = 24543, memory_seq_rm [24543, end)
slot update_slots: id  0 | task 37441 | prompt processing progress, n_tokens = 24607, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37441 | prompt done, n_tokens = 24607, batch.n_tokens = 64
slot init_sampler: id  0 | task 37441 | init sampler, took 3.57 ms, tokens: text = 24607, total = 24607
slot update_slots: id  0 | task 37441 | erasing old context checkpoint (pos_min = 20422, pos_max = 21140, size = 16.860 MiB)
slot update_slots: id  0 | task 37441 | created context checkpoint 8 of 8 (pos_min = 23773, pos_max = 24542, size = 18.056 MiB)
slot print_timing: id  0 | task 37441 | 
prompt eval time =    1090.54 ms /   353 tokens (    3.09 ms per token,   323.69 tokens per second)
       eval time =    2185.14 ms /    55 tokens (   39.73 ms per token,    25.17 tokens per second)
      total time =    3275.68 ms /   408 tokens
slot      release: id  0 | task 37441 | stop processing: n_tokens = 24661, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37498 | processing task, is_child = 0
slot update_slots: id  0 | task 37498 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 25176
slot update_slots: id  0 | task 37498 | n_tokens = 24635, memory_seq_rm [24635, end)
slot update_slots: id  0 | task 37498 | prompt processing progress, n_tokens = 25112, batch.n_tokens = 477, progress = 0.997458
slot update_slots: id  0 | task 37498 | n_tokens = 25112, memory_seq_rm [25112, end)
slot update_slots: id  0 | task 37498 | prompt processing progress, n_tokens = 25176, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37498 | prompt done, n_tokens = 25176, batch.n_tokens = 64
slot init_sampler: id  0 | task 37498 | init sampler, took 3.77 ms, tokens: text = 25176, total = 25176
slot update_slots: id  0 | task 37498 | erasing old context checkpoint (pos_min = 20610, pos_max = 21361, size = 17.634 MiB)
slot update_slots: id  0 | task 37498 | created context checkpoint 8 of 8 (pos_min = 24342, pos_max = 25111, size = 18.056 MiB)
slot print_timing: id  0 | task 37498 | 
prompt eval time =    1507.46 ms /   541 tokens (    2.79 ms per token,   358.88 tokens per second)
       eval time =    4695.16 ms /   117 tokens (   40.13 ms per token,    24.92 tokens per second)
      total time =    6202.62 ms /   658 tokens
slot      release: id  0 | task 37498 | stop processing: n_tokens = 25292, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37617 | processing task, is_child = 0
slot update_slots: id  0 | task 37617 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 25324
slot update_slots: id  0 | task 37617 | n_tokens = 25266, memory_seq_rm [25266, end)
slot update_slots: id  0 | task 37617 | prompt processing progress, n_tokens = 25324, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 37617 | prompt done, n_tokens = 25324, batch.n_tokens = 58
slot init_sampler: id  0 | task 37617 | init sampler, took 5.21 ms, tokens: text = 25324, total = 25324
slot update_slots: id  0 | task 37617 | erasing old context checkpoint (pos_min = 20883, pos_max = 21633, size = 17.610 MiB)
slot update_slots: id  0 | task 37617 | created context checkpoint 8 of 8 (pos_min = 24522, pos_max = 25265, size = 17.446 MiB)
slot print_timing: id  0 | task 37617 | 
prompt eval time =     288.18 ms /    58 tokens (    4.97 ms per token,   201.26 tokens per second)
       eval time =    6290.50 ms /   149 tokens (   42.22 ms per token,    23.69 tokens per second)
      total time =    6578.68 ms /   207 tokens
slot      release: id  0 | task 37617 | stop processing: n_tokens = 25472, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37767 | processing task, is_child = 0
slot update_slots: id  0 | task 37767 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 25742
slot update_slots: id  0 | task 37767 | n_tokens = 25448, memory_seq_rm [25448, end)
slot update_slots: id  0 | task 37767 | prompt processing progress, n_tokens = 25678, batch.n_tokens = 230, progress = 0.997514
slot update_slots: id  0 | task 37767 | n_tokens = 25678, memory_seq_rm [25678, end)
slot update_slots: id  0 | task 37767 | prompt processing progress, n_tokens = 25742, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37767 | prompt done, n_tokens = 25742, batch.n_tokens = 64
slot init_sampler: id  0 | task 37767 | init sampler, took 4.07 ms, tokens: text = 25742, total = 25742
slot update_slots: id  0 | task 37767 | erasing old context checkpoint (pos_min = 21041, pos_max = 21809, size = 18.033 MiB)
slot update_slots: id  0 | task 37767 | created context checkpoint 8 of 8 (pos_min = 24908, pos_max = 25677, size = 18.056 MiB)
slot print_timing: id  0 | task 37767 | 
prompt eval time =    1109.88 ms /   294 tokens (    3.78 ms per token,   264.89 tokens per second)
       eval time =   10913.52 ms /   245 tokens (   44.54 ms per token,    22.45 tokens per second)
      total time =   12023.40 ms /   539 tokens
slot      release: id  0 | task 37767 | stop processing: n_tokens = 25986, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 38014 | processing task, is_child = 0
slot update_slots: id  0 | task 38014 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 26018
slot update_slots: id  0 | task 38014 | n_tokens = 25962, memory_seq_rm [25962, end)
slot update_slots: id  0 | task 38014 | prompt processing progress, n_tokens = 26018, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  0 | task 38014 | prompt done, n_tokens = 26018, batch.n_tokens = 56
slot init_sampler: id  0 | task 38014 | init sampler, took 3.87 ms, tokens: text = 26018, total = 26018
slot update_slots: id  0 | task 38014 | erasing old context checkpoint (pos_min = 22345, pos_max = 23114, size = 18.056 MiB)
slot update_slots: id  0 | task 38014 | created context checkpoint 8 of 8 (pos_min = 25216, pos_max = 25961, size = 17.493 MiB)
slot print_timing: id  0 | task 38014 | 
prompt eval time =     338.90 ms /    56 tokens (    6.05 ms per token,   165.24 tokens per second)
       eval time =    1628.06 ms /    44 tokens (   37.00 ms per token,    27.03 tokens per second)
      total time =    1966.96 ms /   100 tokens
slot      release: id  0 | task 38014 | stop processing: n_tokens = 26061, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 38059 | processing task, is_child = 0
slot update_slots: id  0 | task 38059 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 26373
slot update_slots: id  0 | task 38059 | n_tokens = 26031, memory_seq_rm [26031, end)
slot update_slots: id  0 | task 38059 | prompt processing progress, n_tokens = 26309, batch.n_tokens = 278, progress = 0.997573
slot update_slots: id  0 | task 38059 | n_tokens = 26309, memory_seq_rm [26309, end)
slot update_slots: id  0 | task 38059 | prompt processing progress, n_tokens = 26373, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 38059 | prompt done, n_tokens = 26373, batch.n_tokens = 64
slot init_sampler: id  0 | task 38059 | init sampler, took 5.32 ms, tokens: text = 26373, total = 26373
slot update_slots: id  0 | task 38059 | erasing old context checkpoint (pos_min = 22504, pos_max = 23247, size = 17.446 MiB)
slot update_slots: id  0 | task 38059 | created context checkpoint 8 of 8 (pos_min = 25539, pos_max = 26308, size = 18.056 MiB)
slot print_timing: id  0 | task 38059 | 
prompt eval time =    1063.47 ms /   342 tokens (    3.11 ms per token,   321.59 tokens per second)
       eval time =    2421.87 ms /    61 tokens (   39.70 ms per token,    25.19 tokens per second)
      total time =    3485.34 ms /   403 tokens
slot      release: id  0 | task 38059 | stop processing: n_tokens = 26433, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 38122 | processing task, is_child = 0
slot update_slots: id  0 | task 38122 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 26550
slot update_slots: id  0 | task 38122 | n_tokens = 26409, memory_seq_rm [26409, end)
slot update_slots: id  0 | task 38122 | prompt processing progress, n_tokens = 26486, batch.n_tokens = 77, progress = 0.997589
slot update_slots: id  0 | task 38122 | n_tokens = 26486, memory_seq_rm [26486, end)
slot update_slots: id  0 | task 38122 | prompt processing progress, n_tokens = 26550, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 38122 | prompt done, n_tokens = 26550, batch.n_tokens = 64
slot init_sampler: id  0 | task 38122 | init sampler, took 3.93 ms, tokens: text = 26550, total = 26550
slot update_slots: id  0 | task 38122 | erasing old context checkpoint (pos_min = 23136, pos_max = 23905, size = 18.056 MiB)
slot update_slots: id  0 | task 38122 | created context checkpoint 8 of 8 (pos_min = 25716, pos_max = 26485, size = 18.056 MiB)
slot print_timing: id  0 | task 38122 | 
prompt eval time =     715.25 ms /   141 tokens (    5.07 ms per token,   197.13 tokens per second)
       eval time =   57251.57 ms /  1373 tokens (   41.70 ms per token,    23.98 tokens per second)
      total time =   57966.82 ms /  1514 tokens
slot      release: id  0 | task 38122 | stop processing: n_tokens = 27922, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 39497 | processing task, is_child = 0
slot update_slots: id  0 | task 39497 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 28349
slot update_slots: id  0 | task 39497 | n_tokens = 27895, memory_seq_rm [27895, end)
slot update_slots: id  0 | task 39497 | prompt processing progress, n_tokens = 28285, batch.n_tokens = 390, progress = 0.997742
slot update_slots: id  0 | task 39497 | n_tokens = 28285, memory_seq_rm [28285, end)
slot update_slots: id  0 | task 39497 | prompt processing progress, n_tokens = 28349, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 39497 | prompt done, n_tokens = 28349, batch.n_tokens = 64
slot init_sampler: id  0 | task 39497 | init sampler, took 4.67 ms, tokens: text = 28349, total = 28349
slot update_slots: id  0 | task 39497 | erasing old context checkpoint (pos_min = 23442, pos_max = 24179, size = 17.306 MiB)
slot update_slots: id  0 | task 39497 | created context checkpoint 8 of 8 (pos_min = 27515, pos_max = 28284, size = 18.056 MiB)
slot print_timing: id  0 | task 39497 | 
prompt eval time =    1252.25 ms /   454 tokens (    2.76 ms per token,   362.55 tokens per second)
       eval time =   25496.27 ms /   631 tokens (   40.41 ms per token,    24.75 tokens per second)
      total time =   26748.52 ms /  1085 tokens
slot      release: id  0 | task 39497 | stop processing: n_tokens = 28979, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 40130 | processing task, is_child = 0
slot update_slots: id  0 | task 40130 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 29011
slot update_slots: id  0 | task 40130 | n_tokens = 28953, memory_seq_rm [28953, end)
slot update_slots: id  0 | task 40130 | prompt processing progress, n_tokens = 29011, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 40130 | prompt done, n_tokens = 29011, batch.n_tokens = 58
slot init_sampler: id  0 | task 40130 | init sampler, took 4.34 ms, tokens: text = 29011, total = 29011
slot update_slots: id  0 | task 40130 | erasing old context checkpoint (pos_min = 23773, pos_max = 24542, size = 18.056 MiB)
slot update_slots: id  0 | task 40130 | created context checkpoint 8 of 8 (pos_min = 28209, pos_max = 28952, size = 17.446 MiB)
decode: failed to find a memory slot for batch of size 1
srv  try_clear_id: purging slot 2 with 13972 tokens
slot prompt_clear: id  2 | task -1 | clearing prompt with 13972 tokens
srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 2048, ret = 1
slot print_timing: id  0 | task 40130 | 
prompt eval time =     278.73 ms /    58 tokens (    4.81 ms per token,   208.08 tokens per second)
       eval time =   57304.31 ms /  1419 tokens (   40.38 ms per token,    24.76 tokens per second)
      total time =   57583.05 ms /  1477 tokens
slot      release: id  0 | task 40130 | stop processing: n_tokens = 30429, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.967 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41550 | processing task, is_child = 0
slot update_slots: id  0 | task 41550 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 31280
slot update_slots: id  0 | task 41550 | n_tokens = 30257, memory_seq_rm [30257, end)
slot update_slots: id  0 | task 41550 | prompt processing progress, n_tokens = 31216, batch.n_tokens = 959, progress = 0.997954
slot update_slots: id  0 | task 41550 | n_tokens = 31216, memory_seq_rm [31216, end)
slot update_slots: id  0 | task 41550 | prompt processing progress, n_tokens = 31280, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41550 | prompt done, n_tokens = 31280, batch.n_tokens = 64
slot init_sampler: id  0 | task 41550 | init sampler, took 4.51 ms, tokens: text = 31280, total = 31280
slot update_slots: id  0 | task 41550 | erasing old context checkpoint (pos_min = 24342, pos_max = 25111, size = 18.056 MiB)
slot update_slots: id  0 | task 41550 | created context checkpoint 8 of 8 (pos_min = 30319, pos_max = 31215, size = 21.034 MiB)
slot print_timing: id  0 | task 41550 | 
prompt eval time =    2586.02 ms /  1023 tokens (    2.53 ms per token,   395.59 tokens per second)
       eval time =    9164.79 ms /   227 tokens (   40.37 ms per token,    24.77 tokens per second)
      total time =   11750.81 ms /  1250 tokens
slot      release: id  0 | task 41550 | stop processing: n_tokens = 31506, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41779 | processing task, is_child = 0
slot update_slots: id  0 | task 41779 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 32686
slot update_slots: id  0 | task 41779 | n_tokens = 31480, memory_seq_rm [31480, end)
slot update_slots: id  0 | task 41779 | prompt processing progress, n_tokens = 32622, batch.n_tokens = 1142, progress = 0.998042
slot update_slots: id  0 | task 41779 | n_tokens = 32622, memory_seq_rm [32622, end)
slot update_slots: id  0 | task 41779 | prompt processing progress, n_tokens = 32686, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41779 | prompt done, n_tokens = 32686, batch.n_tokens = 64
slot init_sampler: id  0 | task 41779 | init sampler, took 4.60 ms, tokens: text = 32686, total = 32686
slot update_slots: id  0 | task 41779 | erasing old context checkpoint (pos_min = 24522, pos_max = 25265, size = 17.446 MiB)
slot update_slots: id  0 | task 41779 | created context checkpoint 8 of 8 (pos_min = 31725, pos_max = 32621, size = 21.034 MiB)
slot print_timing: id  0 | task 41779 | 
prompt eval time =    3206.99 ms /  1206 tokens (    2.66 ms per token,   376.05 tokens per second)
       eval time =   72933.64 ms /  1804 tokens (   40.43 ms per token,    24.73 tokens per second)
      total time =   76140.64 ms /  3010 tokens
slot      release: id  0 | task 41779 | stop processing: n_tokens = 34489, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 43585 | processing task, is_child = 0
slot update_slots: id  0 | task 43585 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 34546
slot update_slots: id  0 | task 43585 | n_tokens = 34458, memory_seq_rm [34458, end)
slot update_slots: id  0 | task 43585 | prompt processing progress, n_tokens = 34482, batch.n_tokens = 24, progress = 0.998147
slot update_slots: id  0 | task 43585 | n_tokens = 34482, memory_seq_rm [34482, end)
slot update_slots: id  0 | task 43585 | prompt processing progress, n_tokens = 34546, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 43585 | prompt done, n_tokens = 34546, batch.n_tokens = 64
slot init_sampler: id  0 | task 43585 | init sampler, took 5.36 ms, tokens: text = 34546, total = 34546
slot update_slots: id  0 | task 43585 | erasing old context checkpoint (pos_min = 24908, pos_max = 25677, size = 18.056 MiB)
slot update_slots: id  0 | task 43585 | created context checkpoint 8 of 8 (pos_min = 33592, pos_max = 34481, size = 20.870 MiB)
slot print_timing: id  0 | task 43585 | 
prompt eval time =     500.77 ms /    88 tokens (    5.69 ms per token,   175.73 tokens per second)
       eval time =    4789.93 ms /   119 tokens (   40.25 ms per token,    24.84 tokens per second)
      total time =    5290.70 ms /   207 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 43585 | stop processing: n_tokens = 34664, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 43706 | processing task, is_child = 0
slot update_slots: id  0 | task 43706 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 34867
slot update_slots: id  0 | task 43706 | n_tokens = 34638, memory_seq_rm [34638, end)
slot update_slots: id  0 | task 43706 | prompt processing progress, n_tokens = 34803, batch.n_tokens = 165, progress = 0.998164
slot update_slots: id  0 | task 43706 | n_tokens = 34803, memory_seq_rm [34803, end)
slot update_slots: id  0 | task 43706 | prompt processing progress, n_tokens = 34867, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 43706 | prompt done, n_tokens = 34867, batch.n_tokens = 64
slot init_sampler: id  0 | task 43706 | init sampler, took 4.95 ms, tokens: text = 34867, total = 34867
slot update_slots: id  0 | task 43706 | erasing old context checkpoint (pos_min = 25216, pos_max = 25961, size = 17.493 MiB)
slot update_slots: id  0 | task 43706 | created context checkpoint 8 of 8 (pos_min = 33906, pos_max = 34802, size = 21.034 MiB)
slot print_timing: id  0 | task 43706 | 
prompt eval time =     836.29 ms /   229 tokens (    3.65 ms per token,   273.83 tokens per second)
       eval time =    3809.39 ms /    95 tokens (   40.10 ms per token,    24.94 tokens per second)
      total time =    4645.68 ms /   324 tokens
slot      release: id  0 | task 43706 | stop processing: n_tokens = 34961, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 43803 | processing task, is_child = 0
slot update_slots: id  0 | task 43803 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 35005
slot update_slots: id  0 | task 43803 | n_tokens = 34933, memory_seq_rm [34933, end)
slot update_slots: id  0 | task 43803 | prompt processing progress, n_tokens = 34941, batch.n_tokens = 8, progress = 0.998172
slot update_slots: id  0 | task 43803 | n_tokens = 34941, memory_seq_rm [34941, end)
slot update_slots: id  0 | task 43803 | prompt processing progress, n_tokens = 35005, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 43803 | prompt done, n_tokens = 35005, batch.n_tokens = 64
slot init_sampler: id  0 | task 43803 | init sampler, took 4.82 ms, tokens: text = 35005, total = 35005
slot update_slots: id  0 | task 43803 | erasing old context checkpoint (pos_min = 25539, pos_max = 26308, size = 18.056 MiB)
slot update_slots: id  0 | task 43803 | created context checkpoint 8 of 8 (pos_min = 34064, pos_max = 34940, size = 20.565 MiB)
slot print_timing: id  0 | task 43803 | 
prompt eval time =     458.83 ms /    72 tokens (    6.37 ms per token,   156.92 tokens per second)
       eval time =  165764.35 ms /  4096 tokens (   40.47 ms per token,    24.71 tokens per second)
      total time =  166223.18 ms /  4168 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 43803 | stop processing: n_tokens = 39100, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.022
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 39100, total state size = 937.888 MiB
srv          load:  - looking for better prompt, base f_keep = 0.022, sim = 0.993
srv        update:  - cache state: 3 prompts, 2716.381 MiB (limits: 8192.000 MiB, 56064 tokens, 286354 est)
srv        update:    - prompt 0x56346d802600:   28154 tokens, checkpoints:  8,   815.346 MiB
srv        update:    - prompt 0x5634781e0370:   27698 tokens, checkpoints:  8,   805.052 MiB
srv        update:    - prompt 0x5634794c3a90:   39100 tokens, checkpoints:  8,  1095.983 MiB
srv  get_availabl: prompt cache update took 867.04 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 47901 | processing task, is_child = 0
slot update_slots: id  0 | task 47901 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 860
slot update_slots: id  0 | task 47901 | n_past = 854, slot.prompt.tokens.size() = 39100, seq_id = 0, pos_min = 38203, n_swa = 128
slot update_slots: id  0 | task 47901 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 47901 | erased invalidated context checkpoint (pos_min = 25716, pos_max = 26485, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 47901 | erased invalidated context checkpoint (pos_min = 27515, pos_max = 28284, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  0 | task 47901 | erased invalidated context checkpoint (pos_min = 28209, pos_max = 28952, n_swa = 128, size = 17.446 MiB)
slot update_slots: id  0 | task 47901 | erased invalidated context checkpoint (pos_min = 30319, pos_max = 31215, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  0 | task 47901 | erased invalidated context checkpoint (pos_min = 31725, pos_max = 32621, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  0 | task 47901 | erased invalidated context checkpoint (pos_min = 33592, pos_max = 34481, n_swa = 128, size = 20.870 MiB)
slot update_slots: id  0 | task 47901 | erased invalidated context checkpoint (pos_min = 33906, pos_max = 34802, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  0 | task 47901 | erased invalidated context checkpoint (pos_min = 34064, pos_max = 34940, n_swa = 128, size = 20.565 MiB)
slot update_slots: id  0 | task 47901 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 47901 | prompt processing progress, n_tokens = 796, batch.n_tokens = 796, progress = 0.925581
slot update_slots: id  0 | task 47901 | n_tokens = 796, memory_seq_rm [796, end)
slot update_slots: id  0 | task 47901 | prompt processing progress, n_tokens = 860, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 47901 | prompt done, n_tokens = 860, batch.n_tokens = 64
slot init_sampler: id  0 | task 47901 | init sampler, took 0.12 ms, tokens: text = 860, total = 860
slot update_slots: id  0 | task 47901 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 795, size = 18.666 MiB)
slot print_timing: id  0 | task 47901 | 
prompt eval time =    1458.33 ms /   860 tokens (    1.70 ms per token,   589.72 tokens per second)
       eval time =    1237.79 ms /    37 tokens (   33.45 ms per token,    29.89 tokens per second)
      total time =    2696.12 ms /   897 tokens
slot      release: id  0 | task 47901 | stop processing: n_tokens = 896, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 47940 | processing task, is_child = 0
slot update_slots: id  0 | task 47940 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 948
slot update_slots: id  0 | task 47940 | n_tokens = 883, memory_seq_rm [883, end)
slot update_slots: id  0 | task 47940 | prompt processing progress, n_tokens = 884, batch.n_tokens = 1, progress = 0.932489
slot update_slots: id  0 | task 47940 | n_tokens = 884, memory_seq_rm [884, end)
slot update_slots: id  0 | task 47940 | prompt processing progress, n_tokens = 948, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 47940 | prompt done, n_tokens = 948, batch.n_tokens = 64
slot init_sampler: id  0 | task 47940 | init sampler, took 0.17 ms, tokens: text = 948, total = 948
slot update_slots: id  0 | task 47940 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 883, size = 20.729 MiB)
slot print_timing: id  0 | task 47940 | 
prompt eval time =     289.76 ms /    65 tokens (    4.46 ms per token,   224.33 tokens per second)
       eval time =     398.48 ms /    13 tokens (   30.65 ms per token,    32.62 tokens per second)
      total time =     688.24 ms /    78 tokens
slot      release: id  0 | task 47940 | stop processing: n_tokens = 960, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.906 (> 0.100 thold), f_keep = 0.896
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 47955 | processing task, is_child = 0
slot update_slots: id  0 | task 47955 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 949
slot update_slots: id  0 | task 47955 | n_tokens = 860, memory_seq_rm [860, end)
slot update_slots: id  0 | task 47955 | prompt processing progress, n_tokens = 885, batch.n_tokens = 25, progress = 0.932561
slot update_slots: id  0 | task 47955 | n_tokens = 885, memory_seq_rm [885, end)
slot update_slots: id  0 | task 47955 | prompt processing progress, n_tokens = 949, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 47955 | prompt done, n_tokens = 949, batch.n_tokens = 64
slot init_sampler: id  0 | task 47955 | init sampler, took 0.16 ms, tokens: text = 949, total = 949
slot print_timing: id  0 | task 47955 | 
prompt eval time =     437.44 ms /    89 tokens (    4.92 ms per token,   203.46 tokens per second)
       eval time =     572.13 ms /    18 tokens (   31.79 ms per token,    31.46 tokens per second)
      total time =    1009.57 ms /   107 tokens
slot      release: id  0 | task 47955 | stop processing: n_tokens = 966, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.890
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 47975 | processing task, is_child = 0
slot update_slots: id  0 | task 47975 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 860
slot update_slots: id  0 | task 47975 | need to evaluate at least 1 token for each active slot (n_past = 860, task.n_tokens() = 860)
slot update_slots: id  0 | task 47975 | n_past was set to 859
slot update_slots: id  0 | task 47975 | n_tokens = 859, memory_seq_rm [859, end)
slot update_slots: id  0 | task 47975 | prompt processing progress, n_tokens = 860, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  0 | task 47975 | prompt done, n_tokens = 860, batch.n_tokens = 1
slot init_sampler: id  0 | task 47975 | init sampler, took 0.13 ms, tokens: text = 860, total = 860
slot print_timing: id  0 | task 47975 | 
prompt eval time =      36.50 ms /     1 tokens (   36.50 ms per token,    27.40 tokens per second)
       eval time =    1047.73 ms /    31 tokens (   33.80 ms per token,    29.59 tokens per second)
      total time =    1084.23 ms /    32 tokens
slot      release: id  0 | task 47975 | stop processing: n_tokens = 890, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 48007 | processing task, is_child = 0
slot update_slots: id  0 | task 48007 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 942
slot update_slots: id  0 | task 48007 | n_tokens = 877, memory_seq_rm [877, end)
slot update_slots: id  0 | task 48007 | prompt processing progress, n_tokens = 878, batch.n_tokens = 1, progress = 0.932059
slot update_slots: id  0 | task 48007 | n_tokens = 878, memory_seq_rm [878, end)
slot update_slots: id  0 | task 48007 | prompt processing progress, n_tokens = 942, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 48007 | prompt done, n_tokens = 942, batch.n_tokens = 64
slot init_sampler: id  0 | task 48007 | init sampler, took 0.20 ms, tokens: text = 942, total = 942
slot print_timing: id  0 | task 48007 | 
prompt eval time =     291.44 ms /    65 tokens (    4.48 ms per token,   223.03 tokens per second)
       eval time =     545.56 ms /    17 tokens (   32.09 ms per token,    31.16 tokens per second)
      total time =     836.99 ms /    82 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 48007 | stop processing: n_tokens = 958, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.904 (> 0.100 thold), f_keep = 0.898
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 48026 | processing task, is_child = 0
slot update_slots: id  0 | task 48026 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 951
slot update_slots: id  0 | task 48026 | n_tokens = 860, memory_seq_rm [860, end)
slot update_slots: id  0 | task 48026 | prompt processing progress, n_tokens = 887, batch.n_tokens = 27, progress = 0.932702
slot update_slots: id  0 | task 48026 | n_tokens = 887, memory_seq_rm [887, end)
slot update_slots: id  0 | task 48026 | prompt processing progress, n_tokens = 951, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 48026 | prompt done, n_tokens = 951, batch.n_tokens = 64
slot init_sampler: id  0 | task 48026 | init sampler, took 0.17 ms, tokens: text = 951, total = 951
slot print_timing: id  0 | task 48026 | 
prompt eval time =     402.28 ms /    91 tokens (    4.42 ms per token,   226.21 tokens per second)
       eval time =    1027.63 ms /    30 tokens (   34.25 ms per token,    29.19 tokens per second)
      total time =    1429.91 ms /   121 tokens
slot      release: id  0 | task 48026 | stop processing: n_tokens = 980, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.855 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 48058 | processing task, is_child = 0
slot update_slots: id  0 | task 48058 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1126
slot update_slots: id  0 | task 48058 | n_tokens = 963, memory_seq_rm [963, end)
slot update_slots: id  0 | task 48058 | prompt processing progress, n_tokens = 1062, batch.n_tokens = 99, progress = 0.943162
slot update_slots: id  0 | task 48058 | n_tokens = 1062, memory_seq_rm [1062, end)
slot update_slots: id  0 | task 48058 | prompt processing progress, n_tokens = 1126, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 48058 | prompt done, n_tokens = 1126, batch.n_tokens = 64
slot init_sampler: id  0 | task 48058 | init sampler, took 0.20 ms, tokens: text = 1126, total = 1126
slot update_slots: id  0 | task 48058 | created context checkpoint 3 of 8 (pos_min = 203, pos_max = 1061, size = 20.143 MiB)
slot print_timing: id  0 | task 48058 | 
prompt eval time =     549.01 ms /   163 tokens (    3.37 ms per token,   296.90 tokens per second)
       eval time =    4681.78 ms /   130 tokens (   36.01 ms per token,    27.77 tokens per second)
      total time =    5230.79 ms /   293 tokens
slot      release: id  0 | task 48058 | stop processing: n_tokens = 1255, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
