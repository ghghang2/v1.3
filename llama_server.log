ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14807 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1763 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 56064 -> need 1767 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 2.40 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14807 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.....srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 56064
llama_context: n_ctx_seq     = 56064
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (56064) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 56064 cells
llama_kv_cache:      CUDA0 KV buffer size =  1314.00 MiB
llama_kv_cache: size = 1314.00 MiB ( 56064 cells,  12 layers,  4/1 seqs), K (f16):  657.00 MiB, V (f16):  657.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   117.15 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 56.24 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  1 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  2 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  3 | task -1 | new slot, n_ctx = 56064
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-18

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 682
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 618, batch.n_tokens = 618, progress = 0.906158
slot update_slots: id  3 | task 0 | n_tokens = 618, memory_seq_rm [618, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 682, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 682, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.11 ms, tokens: text = 682, total = 682
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 617, size = 14.492 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =    1160.64 ms /   682 tokens (    1.70 ms per token,   587.61 tokens per second)
       eval time =    1030.32 ms /    45 tokens (   22.90 ms per token,    43.68 tokens per second)
      total time =    2190.96 ms /   727 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 726, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.916 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 47 | processing task, is_child = 0
slot update_slots: id  3 | task 47 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 778
slot update_slots: id  3 | task 47 | n_tokens = 713, memory_seq_rm [713, end)
slot update_slots: id  3 | task 47 | prompt processing progress, n_tokens = 714, batch.n_tokens = 1, progress = 0.917738
slot update_slots: id  3 | task 47 | n_tokens = 714, memory_seq_rm [714, end)
slot update_slots: id  3 | task 47 | prompt processing progress, n_tokens = 778, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 47 | prompt done, n_tokens = 778, batch.n_tokens = 64
slot init_sampler: id  3 | task 47 | init sampler, took 0.12 ms, tokens: text = 778, total = 778
slot update_slots: id  3 | task 47 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 713, size = 16.743 MiB)
slot print_timing: id  3 | task 47 | 
prompt eval time =     290.29 ms /    65 tokens (    4.47 ms per token,   223.91 tokens per second)
       eval time =     591.87 ms /    27 tokens (   21.92 ms per token,    45.62 tokens per second)
      total time =     882.16 ms /    92 tokens
slot      release: id  3 | task 47 | stop processing: n_tokens = 804, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.863 (> 0.100 thold), f_keep = 0.848
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 76 | processing task, is_child = 0
slot update_slots: id  3 | task 76 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 790
slot update_slots: id  3 | task 76 | n_tokens = 682, memory_seq_rm [682, end)
slot update_slots: id  3 | task 76 | prompt processing progress, n_tokens = 726, batch.n_tokens = 44, progress = 0.918987
slot update_slots: id  3 | task 76 | n_tokens = 726, memory_seq_rm [726, end)
slot update_slots: id  3 | task 76 | prompt processing progress, n_tokens = 790, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 76 | prompt done, n_tokens = 790, batch.n_tokens = 64
slot init_sampler: id  3 | task 76 | init sampler, took 0.14 ms, tokens: text = 790, total = 790
slot print_timing: id  3 | task 76 | 
prompt eval time =     404.15 ms /   108 tokens (    3.74 ms per token,   267.22 tokens per second)
       eval time =     938.69 ms /    42 tokens (   22.35 ms per token,    44.74 tokens per second)
      total time =    1342.84 ms /   150 tokens
slot      release: id  3 | task 76 | stop processing: n_tokens = 831, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.831 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 120 | processing task, is_child = 0
slot update_slots: id  3 | task 120 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 978
slot update_slots: id  3 | task 120 | n_tokens = 813, memory_seq_rm [813, end)
slot update_slots: id  3 | task 120 | prompt processing progress, n_tokens = 914, batch.n_tokens = 101, progress = 0.934560
slot update_slots: id  3 | task 120 | n_tokens = 914, memory_seq_rm [914, end)
slot update_slots: id  3 | task 120 | prompt processing progress, n_tokens = 978, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 120 | prompt done, n_tokens = 978, batch.n_tokens = 64
slot init_sampler: id  3 | task 120 | init sampler, took 0.15 ms, tokens: text = 978, total = 978
slot update_slots: id  3 | task 120 | created context checkpoint 3 of 8 (pos_min = 0, pos_max = 913, size = 21.433 MiB)
slot print_timing: id  3 | task 120 | 
prompt eval time =     376.92 ms /   165 tokens (    2.28 ms per token,   437.76 tokens per second)
       eval time =    2273.24 ms /    99 tokens (   22.96 ms per token,    43.55 tokens per second)
      total time =    2650.16 ms /   264 tokens
slot      release: id  3 | task 120 | stop processing: n_tokens = 1076, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.948 (> 0.100 thold), f_keep = 0.629
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 221 | processing task, is_child = 0
slot update_slots: id  3 | task 221 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 714
slot update_slots: id  3 | task 221 | n_tokens = 677, memory_seq_rm [677, end)
slot update_slots: id  3 | task 221 | prompt processing progress, n_tokens = 714, batch.n_tokens = 37, progress = 1.000000
slot update_slots: id  3 | task 221 | prompt done, n_tokens = 714, batch.n_tokens = 37
slot init_sampler: id  3 | task 221 | init sampler, took 0.10 ms, tokens: text = 714, total = 714
slot print_timing: id  3 | task 221 | 
prompt eval time =     212.57 ms /    37 tokens (    5.75 ms per token,   174.06 tokens per second)
       eval time =    1573.85 ms /    65 tokens (   24.21 ms per token,    41.30 tokens per second)
      total time =    1786.42 ms /   102 tokens
slot      release: id  3 | task 221 | stop processing: n_tokens = 778, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.516 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 287 | processing task, is_child = 0
slot update_slots: id  3 | task 287 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1462
slot update_slots: id  3 | task 287 | n_tokens = 754, memory_seq_rm [754, end)
slot update_slots: id  3 | task 287 | prompt processing progress, n_tokens = 1398, batch.n_tokens = 644, progress = 0.956224
slot update_slots: id  3 | task 287 | n_tokens = 1398, memory_seq_rm [1398, end)
slot update_slots: id  3 | task 287 | prompt processing progress, n_tokens = 1462, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 287 | prompt done, n_tokens = 1462, batch.n_tokens = 64
slot init_sampler: id  3 | task 287 | init sampler, took 0.22 ms, tokens: text = 1462, total = 1462
slot update_slots: id  3 | task 287 | created context checkpoint 4 of 8 (pos_min = 677, pos_max = 1397, size = 16.907 MiB)
slot print_timing: id  3 | task 287 | 
prompt eval time =     863.57 ms /   708 tokens (    1.22 ms per token,   819.86 tokens per second)
       eval time =    1088.13 ms /    44 tokens (   24.73 ms per token,    40.44 tokens per second)
      total time =    1951.69 ms /   752 tokens
slot      release: id  3 | task 287 | stop processing: n_tokens = 1505, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.457 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 333 | processing task, is_child = 0
slot update_slots: id  3 | task 333 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3228
slot update_slots: id  3 | task 333 | n_tokens = 1475, memory_seq_rm [1475, end)
slot update_slots: id  3 | task 333 | prompt processing progress, n_tokens = 3164, batch.n_tokens = 1689, progress = 0.980173
slot update_slots: id  3 | task 333 | n_tokens = 3164, memory_seq_rm [3164, end)
slot update_slots: id  3 | task 333 | prompt processing progress, n_tokens = 3228, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 333 | prompt done, n_tokens = 3228, batch.n_tokens = 64
slot init_sampler: id  3 | task 333 | init sampler, took 0.49 ms, tokens: text = 3228, total = 3228
slot update_slots: id  3 | task 333 | created context checkpoint 5 of 8 (pos_min = 2140, pos_max = 3163, size = 24.012 MiB)
slot print_timing: id  3 | task 333 | 
prompt eval time =    1968.33 ms /  1753 tokens (    1.12 ms per token,   890.60 tokens per second)
       eval time =    1005.93 ms /    39 tokens (   25.79 ms per token,    38.77 tokens per second)
      total time =    2974.26 ms /  1792 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 333 | stop processing: n_tokens = 3266, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.563 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 374 | processing task, is_child = 0
slot update_slots: id  3 | task 374 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5752
slot update_slots: id  3 | task 374 | n_tokens = 3236, memory_seq_rm [3236, end)
slot update_slots: id  3 | task 374 | prompt processing progress, n_tokens = 5284, batch.n_tokens = 2048, progress = 0.918637
slot update_slots: id  3 | task 374 | n_tokens = 5284, memory_seq_rm [5284, end)
slot update_slots: id  3 | task 374 | prompt processing progress, n_tokens = 5688, batch.n_tokens = 404, progress = 0.988873
slot update_slots: id  3 | task 374 | n_tokens = 5688, memory_seq_rm [5688, end)
slot update_slots: id  3 | task 374 | prompt processing progress, n_tokens = 5752, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 374 | prompt done, n_tokens = 5752, batch.n_tokens = 64
slot init_sampler: id  3 | task 374 | init sampler, took 0.82 ms, tokens: text = 5752, total = 5752
slot update_slots: id  3 | task 374 | created context checkpoint 6 of 8 (pos_min = 4664, pos_max = 5687, size = 24.012 MiB)
slot print_timing: id  3 | task 374 | 
prompt eval time =    2795.81 ms /  2516 tokens (    1.11 ms per token,   899.92 tokens per second)
       eval time =    1015.26 ms /    39 tokens (   26.03 ms per token,    38.41 tokens per second)
      total time =    3811.07 ms /  2555 tokens
slot      release: id  3 | task 374 | stop processing: n_tokens = 5790, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.771 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 416 | processing task, is_child = 0
slot update_slots: id  3 | task 416 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7466
slot update_slots: id  3 | task 416 | n_tokens = 5760, memory_seq_rm [5760, end)
slot update_slots: id  3 | task 416 | prompt processing progress, n_tokens = 7402, batch.n_tokens = 1642, progress = 0.991428
slot update_slots: id  3 | task 416 | n_tokens = 7402, memory_seq_rm [7402, end)
slot update_slots: id  3 | task 416 | prompt processing progress, n_tokens = 7466, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 416 | prompt done, n_tokens = 7466, batch.n_tokens = 64
slot init_sampler: id  3 | task 416 | init sampler, took 1.42 ms, tokens: text = 7466, total = 7466
slot update_slots: id  3 | task 416 | created context checkpoint 7 of 8 (pos_min = 6378, pos_max = 7401, size = 24.012 MiB)
slot print_timing: id  3 | task 416 | 
prompt eval time =    2171.56 ms /  1706 tokens (    1.27 ms per token,   785.61 tokens per second)
       eval time =   66424.41 ms /  2458 tokens (   27.02 ms per token,    37.00 tokens per second)
      total time =   68595.97 ms /  4164 tokens
slot      release: id  3 | task 416 | stop processing: n_tokens = 9923, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 2876 | processing task, is_child = 0
slot update_slots: id  2 | task 2876 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9502
slot update_slots: id  2 | task 2876 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.215534
slot update_slots: id  2 | task 2876 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.431067
slot update_slots: id  2 | task 2876 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.646601
slot update_slots: id  2 | task 2876 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.862134
slot update_slots: id  2 | task 2876 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 9438, batch.n_tokens = 1246, progress = 0.993265
slot update_slots: id  2 | task 2876 | n_tokens = 9438, memory_seq_rm [9438, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 9502, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 2876 | prompt done, n_tokens = 9502, batch.n_tokens = 64
slot init_sampler: id  2 | task 2876 | init sampler, took 1.40 ms, tokens: text = 9502, total = 9502
slot update_slots: id  2 | task 2876 | created context checkpoint 1 of 8 (pos_min = 8541, pos_max = 9437, size = 21.034 MiB)
slot print_timing: id  2 | task 2876 | 
prompt eval time =   11765.15 ms /  9502 tokens (    1.24 ms per token,   807.64 tokens per second)
       eval time =    5365.91 ms /   189 tokens (   28.39 ms per token,    35.22 tokens per second)
      total time =   17131.06 ms /  9691 tokens
slot      release: id  2 | task 2876 | stop processing: n_tokens = 9690, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3071 | processing task, is_child = 0
slot update_slots: id  2 | task 3071 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9789
slot update_slots: id  2 | task 3071 | n_tokens = 9660, memory_seq_rm [9660, end)
slot update_slots: id  2 | task 3071 | prompt processing progress, n_tokens = 9725, batch.n_tokens = 65, progress = 0.993462
slot update_slots: id  2 | task 3071 | n_tokens = 9725, memory_seq_rm [9725, end)
slot update_slots: id  2 | task 3071 | prompt processing progress, n_tokens = 9789, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3071 | prompt done, n_tokens = 9789, batch.n_tokens = 64
slot init_sampler: id  2 | task 3071 | init sampler, took 1.42 ms, tokens: text = 9789, total = 9789
slot update_slots: id  2 | task 3071 | created context checkpoint 2 of 8 (pos_min = 8828, pos_max = 9724, size = 21.034 MiB)
slot print_timing: id  2 | task 3071 | 
prompt eval time =     457.56 ms /   129 tokens (    3.55 ms per token,   281.93 tokens per second)
       eval time =    7526.95 ms /   253 tokens (   29.75 ms per token,    33.61 tokens per second)
      total time =    7984.51 ms /   382 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 3071 | stop processing: n_tokens = 10041, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3326 | processing task, is_child = 0
slot update_slots: id  2 | task 3326 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10083
slot update_slots: id  2 | task 3326 | n_tokens = 9872, memory_seq_rm [9872, end)
slot update_slots: id  2 | task 3326 | prompt processing progress, n_tokens = 10019, batch.n_tokens = 147, progress = 0.993653
slot update_slots: id  2 | task 3326 | n_tokens = 10019, memory_seq_rm [10019, end)
slot update_slots: id  2 | task 3326 | prompt processing progress, n_tokens = 10083, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3326 | prompt done, n_tokens = 10083, batch.n_tokens = 64
slot init_sampler: id  2 | task 3326 | init sampler, took 2.02 ms, tokens: text = 10083, total = 10083
slot update_slots: id  2 | task 3326 | created context checkpoint 3 of 8 (pos_min = 9144, pos_max = 10018, size = 20.518 MiB)
slot print_timing: id  2 | task 3326 | 
prompt eval time =     551.65 ms /   211 tokens (    2.61 ms per token,   382.49 tokens per second)
       eval time =    3696.32 ms /   127 tokens (   29.10 ms per token,    34.36 tokens per second)
      total time =    4247.96 ms /   338 tokens
slot      release: id  2 | task 3326 | stop processing: n_tokens = 10209, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.931
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3455 | processing task, is_child = 0
slot update_slots: id  2 | task 3455 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9988
slot update_slots: id  2 | task 3455 | n_tokens = 9502, memory_seq_rm [9502, end)
slot update_slots: id  2 | task 3455 | prompt processing progress, n_tokens = 9924, batch.n_tokens = 422, progress = 0.993592
slot update_slots: id  2 | task 3455 | n_tokens = 9924, memory_seq_rm [9924, end)
slot update_slots: id  2 | task 3455 | prompt processing progress, n_tokens = 9988, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3455 | prompt done, n_tokens = 9988, batch.n_tokens = 64
slot init_sampler: id  2 | task 3455 | init sampler, took 2.19 ms, tokens: text = 9988, total = 9988
slot print_timing: id  2 | task 3455 | 
prompt eval time =     871.06 ms /   486 tokens (    1.79 ms per token,   557.94 tokens per second)
       eval time =    4755.41 ms /   177 tokens (   26.87 ms per token,    37.22 tokens per second)
      total time =    5626.47 ms /   663 tokens
slot      release: id  2 | task 3455 | stop processing: n_tokens = 10164, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3634 | processing task, is_child = 0
slot update_slots: id  2 | task 3634 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10113
slot update_slots: id  2 | task 3634 | n_tokens = 9989, memory_seq_rm [9989, end)
slot update_slots: id  2 | task 3634 | prompt processing progress, n_tokens = 10049, batch.n_tokens = 60, progress = 0.993672
slot update_slots: id  2 | task 3634 | n_tokens = 10049, memory_seq_rm [10049, end)
slot update_slots: id  2 | task 3634 | prompt processing progress, n_tokens = 10113, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3634 | prompt done, n_tokens = 10113, batch.n_tokens = 64
slot init_sampler: id  2 | task 3634 | init sampler, took 3.51 ms, tokens: text = 10113, total = 10113
slot print_timing: id  2 | task 3634 | 
prompt eval time =     467.94 ms /   124 tokens (    3.77 ms per token,   264.99 tokens per second)
       eval time =    2451.58 ms /    90 tokens (   27.24 ms per token,    36.71 tokens per second)
      total time =    2919.51 ms /   214 tokens
slot      release: id  2 | task 3634 | stop processing: n_tokens = 10202, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3726 | processing task, is_child = 0
slot update_slots: id  2 | task 3726 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10259
slot update_slots: id  2 | task 3726 | n_tokens = 10143, memory_seq_rm [10143, end)
slot update_slots: id  2 | task 3726 | prompt processing progress, n_tokens = 10195, batch.n_tokens = 52, progress = 0.993762
slot update_slots: id  2 | task 3726 | n_tokens = 10195, memory_seq_rm [10195, end)
slot update_slots: id  2 | task 3726 | prompt processing progress, n_tokens = 10259, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3726 | prompt done, n_tokens = 10259, batch.n_tokens = 64
slot init_sampler: id  2 | task 3726 | init sampler, took 1.46 ms, tokens: text = 10259, total = 10259
slot update_slots: id  2 | task 3726 | created context checkpoint 4 of 8 (pos_min = 9375, pos_max = 10194, size = 19.228 MiB)
slot print_timing: id  2 | task 3726 | 
prompt eval time =     388.11 ms /   116 tokens (    3.35 ms per token,   298.89 tokens per second)
       eval time =    1885.63 ms /    69 tokens (   27.33 ms per token,    36.59 tokens per second)
      total time =    2273.74 ms /   185 tokens
slot      release: id  2 | task 3726 | stop processing: n_tokens = 10327, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3797 | processing task, is_child = 0
slot update_slots: id  2 | task 3797 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10309
slot update_slots: id  2 | task 3797 | n_tokens = 10113, memory_seq_rm [10113, end)
slot update_slots: id  2 | task 3797 | prompt processing progress, n_tokens = 10245, batch.n_tokens = 132, progress = 0.993792
slot update_slots: id  2 | task 3797 | n_tokens = 10245, memory_seq_rm [10245, end)
slot update_slots: id  2 | task 3797 | prompt processing progress, n_tokens = 10309, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3797 | prompt done, n_tokens = 10309, batch.n_tokens = 64
slot init_sampler: id  2 | task 3797 | init sampler, took 1.45 ms, tokens: text = 10309, total = 10309
slot print_timing: id  2 | task 3797 | 
prompt eval time =     603.26 ms /   196 tokens (    3.08 ms per token,   324.90 tokens per second)
       eval time =    2849.66 ms /   103 tokens (   27.67 ms per token,    36.14 tokens per second)
      total time =    3452.92 ms /   299 tokens
slot      release: id  2 | task 3797 | stop processing: n_tokens = 10411, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3902 | processing task, is_child = 0
slot update_slots: id  2 | task 3902 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10449
slot update_slots: id  2 | task 3902 | n_tokens = 10338, memory_seq_rm [10338, end)
slot update_slots: id  2 | task 3902 | prompt processing progress, n_tokens = 10385, batch.n_tokens = 47, progress = 0.993875
slot update_slots: id  2 | task 3902 | n_tokens = 10385, memory_seq_rm [10385, end)
slot update_slots: id  2 | task 3902 | prompt processing progress, n_tokens = 10449, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3902 | prompt done, n_tokens = 10449, batch.n_tokens = 64
slot init_sampler: id  2 | task 3902 | init sampler, took 1.47 ms, tokens: text = 10449, total = 10449
slot update_slots: id  2 | task 3902 | created context checkpoint 5 of 8 (pos_min = 9800, pos_max = 10384, size = 13.718 MiB)
slot print_timing: id  2 | task 3902 | 
prompt eval time =     373.87 ms /   111 tokens (    3.37 ms per token,   296.90 tokens per second)
       eval time =    1041.64 ms /    38 tokens (   27.41 ms per token,    36.48 tokens per second)
      total time =    1415.51 ms /   149 tokens
slot      release: id  2 | task 3902 | stop processing: n_tokens = 10486, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3942 | processing task, is_child = 0
slot update_slots: id  2 | task 3942 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10474
slot update_slots: id  2 | task 3942 | n_tokens = 10309, memory_seq_rm [10309, end)
slot update_slots: id  2 | task 3942 | prompt processing progress, n_tokens = 10410, batch.n_tokens = 101, progress = 0.993890
slot update_slots: id  2 | task 3942 | n_tokens = 10410, memory_seq_rm [10410, end)
slot update_slots: id  2 | task 3942 | prompt processing progress, n_tokens = 10474, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3942 | prompt done, n_tokens = 10474, batch.n_tokens = 64
slot init_sampler: id  2 | task 3942 | init sampler, took 1.51 ms, tokens: text = 10474, total = 10474
slot print_timing: id  2 | task 3942 | 
prompt eval time =     595.58 ms /   165 tokens (    3.61 ms per token,   277.04 tokens per second)
       eval time =    2556.71 ms /    93 tokens (   27.49 ms per token,    36.37 tokens per second)
      total time =    3152.29 ms /   258 tokens
slot      release: id  2 | task 3942 | stop processing: n_tokens = 10566, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4037 | processing task, is_child = 0
slot update_slots: id  2 | task 4037 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10605
slot update_slots: id  2 | task 4037 | n_tokens = 10497, memory_seq_rm [10497, end)
slot update_slots: id  2 | task 4037 | prompt processing progress, n_tokens = 10541, batch.n_tokens = 44, progress = 0.993965
slot update_slots: id  2 | task 4037 | n_tokens = 10541, memory_seq_rm [10541, end)
slot update_slots: id  2 | task 4037 | prompt processing progress, n_tokens = 10605, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4037 | prompt done, n_tokens = 10605, batch.n_tokens = 64
slot init_sampler: id  2 | task 4037 | init sampler, took 1.50 ms, tokens: text = 10605, total = 10605
slot update_slots: id  2 | task 4037 | created context checkpoint 6 of 8 (pos_min = 9907, pos_max = 10540, size = 14.867 MiB)
slot print_timing: id  2 | task 4037 | 
prompt eval time =     377.60 ms /   108 tokens (    3.50 ms per token,   286.02 tokens per second)
       eval time =    1326.69 ms /    49 tokens (   27.08 ms per token,    36.93 tokens per second)
      total time =    1704.30 ms /   157 tokens
slot      release: id  2 | task 4037 | stop processing: n_tokens = 10653, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4088 | processing task, is_child = 0
slot update_slots: id  2 | task 4088 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10642
slot update_slots: id  2 | task 4088 | n_tokens = 10474, memory_seq_rm [10474, end)
slot update_slots: id  2 | task 4088 | prompt processing progress, n_tokens = 10578, batch.n_tokens = 104, progress = 0.993986
slot update_slots: id  2 | task 4088 | n_tokens = 10578, memory_seq_rm [10578, end)
slot update_slots: id  2 | task 4088 | prompt processing progress, n_tokens = 10642, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4088 | prompt done, n_tokens = 10642, batch.n_tokens = 64
slot init_sampler: id  2 | task 4088 | init sampler, took 1.52 ms, tokens: text = 10642, total = 10642
slot print_timing: id  2 | task 4088 | 
prompt eval time =     603.21 ms /   168 tokens (    3.59 ms per token,   278.51 tokens per second)
       eval time =    2511.39 ms /    91 tokens (   27.60 ms per token,    36.23 tokens per second)
      total time =    3114.60 ms /   259 tokens
slot      release: id  2 | task 4088 | stop processing: n_tokens = 10732, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4181 | processing task, is_child = 0
slot update_slots: id  2 | task 4181 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10769
slot update_slots: id  2 | task 4181 | n_tokens = 10659, memory_seq_rm [10659, end)
slot update_slots: id  2 | task 4181 | prompt processing progress, n_tokens = 10705, batch.n_tokens = 46, progress = 0.994057
slot update_slots: id  2 | task 4181 | n_tokens = 10705, memory_seq_rm [10705, end)
slot update_slots: id  2 | task 4181 | prompt processing progress, n_tokens = 10769, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4181 | prompt done, n_tokens = 10769, batch.n_tokens = 64
slot init_sampler: id  2 | task 4181 | init sampler, took 1.62 ms, tokens: text = 10769, total = 10769
slot update_slots: id  2 | task 4181 | created context checkpoint 7 of 8 (pos_min = 9994, pos_max = 10704, size = 16.672 MiB)
slot print_timing: id  2 | task 4181 | 
prompt eval time =     382.21 ms /   110 tokens (    3.47 ms per token,   287.80 tokens per second)
       eval time =    1245.15 ms /    45 tokens (   27.67 ms per token,    36.14 tokens per second)
      total time =    1627.37 ms /   155 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 4181 | stop processing: n_tokens = 10813, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4228 | processing task, is_child = 0
slot update_slots: id  2 | task 4228 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10810
slot update_slots: id  2 | task 4228 | n_tokens = 10642, memory_seq_rm [10642, end)
slot update_slots: id  2 | task 4228 | prompt processing progress, n_tokens = 10746, batch.n_tokens = 104, progress = 0.994080
slot update_slots: id  2 | task 4228 | n_tokens = 10746, memory_seq_rm [10746, end)
slot update_slots: id  2 | task 4228 | prompt processing progress, n_tokens = 10810, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4228 | prompt done, n_tokens = 10810, batch.n_tokens = 64
slot init_sampler: id  2 | task 4228 | init sampler, took 1.53 ms, tokens: text = 10810, total = 10810
slot print_timing: id  2 | task 4228 | 
prompt eval time =     594.11 ms /   168 tokens (    3.54 ms per token,   282.77 tokens per second)
       eval time =    5740.17 ms /   206 tokens (   27.86 ms per token,    35.89 tokens per second)
      total time =    6334.28 ms /   374 tokens
slot      release: id  2 | task 4228 | stop processing: n_tokens = 11015, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4436 | processing task, is_child = 0
slot update_slots: id  2 | task 4436 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10869
slot update_slots: id  2 | task 4436 | n_tokens = 10811, memory_seq_rm [10811, end)
slot update_slots: id  2 | task 4436 | prompt processing progress, n_tokens = 10869, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  2 | task 4436 | prompt done, n_tokens = 10869, batch.n_tokens = 58
slot init_sampler: id  2 | task 4436 | init sampler, took 2.19 ms, tokens: text = 10869, total = 10869
slot update_slots: id  2 | task 4436 | created context checkpoint 8 of 8 (pos_min = 10118, pos_max = 10810, size = 16.250 MiB)
slot print_timing: id  2 | task 4436 | 
prompt eval time =     312.95 ms /    58 tokens (    5.40 ms per token,   185.34 tokens per second)
       eval time =    2595.34 ms /    94 tokens (   27.61 ms per token,    36.22 tokens per second)
      total time =    2908.29 ms /   152 tokens
slot      release: id  2 | task 4436 | stop processing: n_tokens = 10962, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4531 | processing task, is_child = 0
slot update_slots: id  2 | task 4531 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10999
slot update_slots: id  2 | task 4531 | n_tokens = 10889, memory_seq_rm [10889, end)
slot update_slots: id  2 | task 4531 | prompt processing progress, n_tokens = 10935, batch.n_tokens = 46, progress = 0.994181
slot update_slots: id  2 | task 4531 | n_tokens = 10935, memory_seq_rm [10935, end)
slot update_slots: id  2 | task 4531 | prompt processing progress, n_tokens = 10999, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4531 | prompt done, n_tokens = 10999, batch.n_tokens = 64
slot init_sampler: id  2 | task 4531 | init sampler, took 1.58 ms, tokens: text = 10999, total = 10999
slot update_slots: id  2 | task 4531 | erasing old context checkpoint (pos_min = 8541, pos_max = 9437, size = 21.034 MiB)
slot update_slots: id  2 | task 4531 | created context checkpoint 8 of 8 (pos_min = 10264, pos_max = 10934, size = 15.735 MiB)
slot print_timing: id  2 | task 4531 | 
prompt eval time =     374.88 ms /   110 tokens (    3.41 ms per token,   293.43 tokens per second)
       eval time =    1216.40 ms /    45 tokens (   27.03 ms per token,    36.99 tokens per second)
      total time =    1591.27 ms /   155 tokens
slot      release: id  2 | task 4531 | stop processing: n_tokens = 11043, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.061
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 11043, total state size = 275.315 MiB
srv          load:  - looking for better prompt, base f_keep = 0.061, sim = 0.962
srv        update:  - cache state: 1 prompts, 413.337 MiB (limits: 8192.000 MiB, 56064 tokens, 218862 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv  get_availabl: prompt cache update took 300.13 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4578 | processing task, is_child = 0
slot update_slots: id  2 | task 4578 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 704
slot update_slots: id  2 | task 4578 | n_past = 677, slot.prompt.tokens.size() = 11043, seq_id = 2, pos_min = 10345, n_swa = 128
slot update_slots: id  2 | task 4578 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 8828, pos_max = 9724, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 9144, pos_max = 10018, n_swa = 128, size = 20.518 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 9375, pos_max = 10194, n_swa = 128, size = 19.228 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 9800, pos_max = 10384, n_swa = 128, size = 13.718 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 9907, pos_max = 10540, n_swa = 128, size = 14.867 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 9994, pos_max = 10704, n_swa = 128, size = 16.672 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 10118, pos_max = 10810, n_swa = 128, size = 16.250 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 10264, pos_max = 10934, n_swa = 128, size = 15.735 MiB)
slot update_slots: id  2 | task 4578 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 4578 | prompt processing progress, n_tokens = 640, batch.n_tokens = 640, progress = 0.909091
slot update_slots: id  2 | task 4578 | n_tokens = 640, memory_seq_rm [640, end)
slot update_slots: id  2 | task 4578 | prompt processing progress, n_tokens = 704, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4578 | prompt done, n_tokens = 704, batch.n_tokens = 64
slot init_sampler: id  2 | task 4578 | init sampler, took 0.11 ms, tokens: text = 704, total = 704
slot update_slots: id  2 | task 4578 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot print_timing: id  2 | task 4578 | 
prompt eval time =    1161.70 ms /   704 tokens (    1.65 ms per token,   606.01 tokens per second)
       eval time =    1720.09 ms /    66 tokens (   26.06 ms per token,    38.37 tokens per second)
      total time =    2881.78 ms /   770 tokens
slot      release: id  2 | task 4578 | stop processing: n_tokens = 769, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.513 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4646 | processing task, is_child = 0
slot update_slots: id  2 | task 4646 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1453
slot update_slots: id  2 | task 4646 | n_tokens = 745, memory_seq_rm [745, end)
slot update_slots: id  2 | task 4646 | prompt processing progress, n_tokens = 1389, batch.n_tokens = 644, progress = 0.955953
slot update_slots: id  2 | task 4646 | n_tokens = 1389, memory_seq_rm [1389, end)
slot update_slots: id  2 | task 4646 | prompt processing progress, n_tokens = 1453, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4646 | prompt done, n_tokens = 1453, batch.n_tokens = 64
slot init_sampler: id  2 | task 4646 | init sampler, took 0.21 ms, tokens: text = 1453, total = 1453
slot update_slots: id  2 | task 4646 | created context checkpoint 2 of 8 (pos_min = 492, pos_max = 1388, size = 21.034 MiB)
slot print_timing: id  2 | task 4646 | 
prompt eval time =    1042.84 ms /   708 tokens (    1.47 ms per token,   678.91 tokens per second)
       eval time =    1285.68 ms /    48 tokens (   26.78 ms per token,    37.33 tokens per second)
      total time =    2328.52 ms /   756 tokens
slot      release: id  2 | task 4646 | stop processing: n_tokens = 1500, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.457 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4696 | processing task, is_child = 0
slot update_slots: id  2 | task 4696 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3208
slot update_slots: id  2 | task 4696 | n_tokens = 1466, memory_seq_rm [1466, end)
slot update_slots: id  2 | task 4696 | prompt processing progress, n_tokens = 3144, batch.n_tokens = 1678, progress = 0.980050
slot update_slots: id  2 | task 4696 | n_tokens = 3144, memory_seq_rm [3144, end)
slot update_slots: id  2 | task 4696 | prompt processing progress, n_tokens = 3208, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4696 | prompt done, n_tokens = 3208, batch.n_tokens = 64
slot init_sampler: id  2 | task 4696 | init sampler, took 0.59 ms, tokens: text = 3208, total = 3208
slot update_slots: id  2 | task 4696 | created context checkpoint 3 of 8 (pos_min = 2247, pos_max = 3143, size = 21.034 MiB)
slot print_timing: id  2 | task 4696 | 
prompt eval time =    2344.20 ms /  1742 tokens (    1.35 ms per token,   743.11 tokens per second)
       eval time =    1210.53 ms /    43 tokens (   28.15 ms per token,    35.52 tokens per second)
      total time =    3554.73 ms /  1785 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 4696 | stop processing: n_tokens = 3250, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.564 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4741 | processing task, is_child = 0
slot update_slots: id  2 | task 4741 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5699
slot update_slots: id  2 | task 4741 | n_tokens = 3216, memory_seq_rm [3216, end)
slot update_slots: id  2 | task 4741 | prompt processing progress, n_tokens = 5264, batch.n_tokens = 2048, progress = 0.923671
slot update_slots: id  2 | task 4741 | n_tokens = 5264, memory_seq_rm [5264, end)
slot update_slots: id  2 | task 4741 | prompt processing progress, n_tokens = 5635, batch.n_tokens = 371, progress = 0.988770
slot update_slots: id  2 | task 4741 | n_tokens = 5635, memory_seq_rm [5635, end)
slot update_slots: id  2 | task 4741 | prompt processing progress, n_tokens = 5699, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4741 | prompt done, n_tokens = 5699, batch.n_tokens = 64
slot init_sampler: id  2 | task 4741 | init sampler, took 0.81 ms, tokens: text = 5699, total = 5699
slot update_slots: id  2 | task 4741 | created context checkpoint 4 of 8 (pos_min = 4738, pos_max = 5634, size = 21.034 MiB)
slot print_timing: id  2 | task 4741 | 
prompt eval time =    3361.15 ms /  2483 tokens (    1.35 ms per token,   738.74 tokens per second)
       eval time =    1117.34 ms /    39 tokens (   28.65 ms per token,    34.90 tokens per second)
      total time =    4478.49 ms /  2522 tokens
slot      release: id  2 | task 4741 | stop processing: n_tokens = 5737, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.771 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4783 | processing task, is_child = 0
slot update_slots: id  2 | task 4783 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7399
slot update_slots: id  2 | task 4783 | n_tokens = 5707, memory_seq_rm [5707, end)
slot update_slots: id  2 | task 4783 | prompt processing progress, n_tokens = 7335, batch.n_tokens = 1628, progress = 0.991350
slot update_slots: id  2 | task 4783 | n_tokens = 7335, memory_seq_rm [7335, end)
slot update_slots: id  2 | task 4783 | prompt processing progress, n_tokens = 7399, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4783 | prompt done, n_tokens = 7399, batch.n_tokens = 64
slot init_sampler: id  2 | task 4783 | init sampler, took 1.21 ms, tokens: text = 7399, total = 7399
slot update_slots: id  2 | task 4783 | created context checkpoint 5 of 8 (pos_min = 6438, pos_max = 7334, size = 21.034 MiB)
slot print_timing: id  2 | task 4783 | 
prompt eval time =    2537.78 ms /  1692 tokens (    1.50 ms per token,   666.72 tokens per second)
       eval time =   10007.33 ms /   338 tokens (   29.61 ms per token,    33.78 tokens per second)
      total time =   12545.12 ms /  2030 tokens
slot      release: id  2 | task 4783 | stop processing: n_tokens = 7736, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 5123 | processing task, is_child = 0
slot update_slots: id  2 | task 5123 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7776
slot update_slots: id  2 | task 5123 | n_tokens = 7586, memory_seq_rm [7586, end)
slot update_slots: id  2 | task 5123 | prompt processing progress, n_tokens = 7712, batch.n_tokens = 126, progress = 0.991770
slot update_slots: id  2 | task 5123 | n_tokens = 7712, memory_seq_rm [7712, end)
slot update_slots: id  2 | task 5123 | prompt processing progress, n_tokens = 7776, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 5123 | prompt done, n_tokens = 7776, batch.n_tokens = 64
slot init_sampler: id  2 | task 5123 | init sampler, took 1.70 ms, tokens: text = 7776, total = 7776
slot update_slots: id  2 | task 5123 | created context checkpoint 6 of 8 (pos_min = 6839, pos_max = 7711, size = 20.471 MiB)
slot print_timing: id  2 | task 5123 | 
prompt eval time =     635.88 ms /   190 tokens (    3.35 ms per token,   298.80 tokens per second)
       eval time =    6565.73 ms /   236 tokens (   27.82 ms per token,    35.94 tokens per second)
      total time =    7201.61 ms /   426 tokens
slot      release: id  2 | task 5123 | stop processing: n_tokens = 8011, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5361 | processing task, is_child = 0
slot update_slots: id  1 | task 5361 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7719
slot update_slots: id  1 | task 5361 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 5361 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.265319
slot update_slots: id  1 | task 5361 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 5361 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.530639
slot update_slots: id  1 | task 5361 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 5361 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.795958
slot update_slots: id  1 | task 5361 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 5361 | prompt processing progress, n_tokens = 7655, batch.n_tokens = 1511, progress = 0.991709
slot update_slots: id  1 | task 5361 | n_tokens = 7655, memory_seq_rm [7655, end)
slot update_slots: id  1 | task 5361 | prompt processing progress, n_tokens = 7719, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5361 | prompt done, n_tokens = 7719, batch.n_tokens = 64
slot init_sampler: id  1 | task 5361 | init sampler, took 1.10 ms, tokens: text = 7719, total = 7719
slot update_slots: id  1 | task 5361 | created context checkpoint 1 of 8 (pos_min = 6885, pos_max = 7654, size = 18.056 MiB)
slot print_timing: id  1 | task 5361 | 
prompt eval time =   10161.75 ms /  7719 tokens (    1.32 ms per token,   759.61 tokens per second)
       eval time =    8704.34 ms /   309 tokens (   28.17 ms per token,    35.50 tokens per second)
      total time =   18866.09 ms /  8028 tokens
slot      release: id  1 | task 5361 | stop processing: n_tokens = 8027, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5675 | processing task, is_child = 0
slot update_slots: id  1 | task 5675 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8067
slot update_slots: id  1 | task 5675 | n_tokens = 7935, memory_seq_rm [7935, end)
slot update_slots: id  1 | task 5675 | prompt processing progress, n_tokens = 8003, batch.n_tokens = 68, progress = 0.992066
slot update_slots: id  1 | task 5675 | n_tokens = 8003, memory_seq_rm [8003, end)
slot update_slots: id  1 | task 5675 | prompt processing progress, n_tokens = 8067, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5675 | prompt done, n_tokens = 8067, batch.n_tokens = 64
slot init_sampler: id  1 | task 5675 | init sampler, took 1.52 ms, tokens: text = 8067, total = 8067
slot update_slots: id  1 | task 5675 | created context checkpoint 2 of 8 (pos_min = 7257, pos_max = 8002, size = 17.493 MiB)
slot print_timing: id  1 | task 5675 | 
prompt eval time =     511.47 ms /   132 tokens (    3.87 ms per token,   258.08 tokens per second)
       eval time =    3206.26 ms /   110 tokens (   29.15 ms per token,    34.31 tokens per second)
      total time =    3717.73 ms /   242 tokens
slot      release: id  1 | task 5675 | stop processing: n_tokens = 8176, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.084
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8176, total state size = 209.775 MiB
srv          load:  - looking for better prompt, base f_keep = 0.084, sim = 0.973
srv        update:  - cache state: 2 prompts, 658.661 MiB (limits: 8192.000 MiB, 56064 tokens, 239033 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv  get_availabl: prompt cache update took 188.16 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5787 | processing task, is_child = 0
slot update_slots: id  1 | task 5787 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 706
slot update_slots: id  1 | task 5787 | n_past = 687, slot.prompt.tokens.size() = 8176, seq_id = 1, pos_min = 7406, n_swa = 128
slot update_slots: id  1 | task 5787 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 5787 | erased invalidated context checkpoint (pos_min = 6885, pos_max = 7654, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 5787 | erased invalidated context checkpoint (pos_min = 7257, pos_max = 8002, n_swa = 128, size = 17.493 MiB)
slot update_slots: id  1 | task 5787 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 5787 | prompt processing progress, n_tokens = 642, batch.n_tokens = 642, progress = 0.909348
slot update_slots: id  1 | task 5787 | n_tokens = 642, memory_seq_rm [642, end)
slot update_slots: id  1 | task 5787 | prompt processing progress, n_tokens = 706, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5787 | prompt done, n_tokens = 706, batch.n_tokens = 64
slot init_sampler: id  1 | task 5787 | init sampler, took 0.11 ms, tokens: text = 706, total = 706
slot update_slots: id  1 | task 5787 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 641, size = 15.055 MiB)
slot print_timing: id  1 | task 5787 | 
prompt eval time =    1202.42 ms /   706 tokens (    1.70 ms per token,   587.15 tokens per second)
       eval time =    1455.90 ms /    54 tokens (   26.96 ms per token,    37.09 tokens per second)
      total time =    2658.32 ms /   760 tokens
slot      release: id  1 | task 5787 | stop processing: n_tokens = 759, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.897 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5843 | processing task, is_child = 0
slot update_slots: id  1 | task 5843 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 814
slot update_slots: id  1 | task 5843 | n_tokens = 730, memory_seq_rm [730, end)
slot update_slots: id  1 | task 5843 | prompt processing progress, n_tokens = 750, batch.n_tokens = 20, progress = 0.921376
slot update_slots: id  1 | task 5843 | n_tokens = 750, memory_seq_rm [750, end)
slot update_slots: id  1 | task 5843 | prompt processing progress, n_tokens = 814, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5843 | prompt done, n_tokens = 814, batch.n_tokens = 64
slot init_sampler: id  1 | task 5843 | init sampler, took 0.12 ms, tokens: text = 814, total = 814
slot update_slots: id  1 | task 5843 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 749, size = 17.587 MiB)
slot print_timing: id  1 | task 5843 | 
prompt eval time =     384.52 ms /    84 tokens (    4.58 ms per token,   218.45 tokens per second)
       eval time =    1348.96 ms /    50 tokens (   26.98 ms per token,    37.07 tokens per second)
      total time =    1733.48 ms /   134 tokens
slot      release: id  1 | task 5843 | stop processing: n_tokens = 863, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.127 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5895 | processing task, is_child = 0
slot update_slots: id  1 | task 5895 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6657
slot update_slots: id  1 | task 5895 | n_tokens = 845, memory_seq_rm [845, end)
slot update_slots: id  1 | task 5895 | prompt processing progress, n_tokens = 2893, batch.n_tokens = 2048, progress = 0.434580
slot update_slots: id  1 | task 5895 | n_tokens = 2893, memory_seq_rm [2893, end)
slot update_slots: id  1 | task 5895 | prompt processing progress, n_tokens = 4941, batch.n_tokens = 2048, progress = 0.742226
slot update_slots: id  1 | task 5895 | n_tokens = 4941, memory_seq_rm [4941, end)
slot update_slots: id  1 | task 5895 | prompt processing progress, n_tokens = 6593, batch.n_tokens = 1652, progress = 0.990386
slot update_slots: id  1 | task 5895 | n_tokens = 6593, memory_seq_rm [6593, end)
slot update_slots: id  1 | task 5895 | prompt processing progress, n_tokens = 6657, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5895 | prompt done, n_tokens = 6657, batch.n_tokens = 64
slot init_sampler: id  1 | task 5895 | init sampler, took 0.95 ms, tokens: text = 6657, total = 6657
slot update_slots: id  1 | task 5895 | created context checkpoint 3 of 8 (pos_min = 5823, pos_max = 6592, size = 18.056 MiB)
slot print_timing: id  1 | task 5895 | 
prompt eval time =    8325.30 ms /  5812 tokens (    1.43 ms per token,   698.11 tokens per second)
       eval time =   52465.08 ms /  1805 tokens (   29.07 ms per token,    34.40 tokens per second)
      total time =   60790.38 ms /  7617 tokens
slot      release: id  1 | task 5895 | stop processing: n_tokens = 8461, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 7704 | processing task, is_child = 0
slot update_slots: id  0 | task 7704 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8379
slot update_slots: id  0 | task 7704 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.244421
slot update_slots: id  0 | task 7704 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.488841
slot update_slots: id  0 | task 7704 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.733262
slot update_slots: id  0 | task 7704 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.977682
slot update_slots: id  0 | task 7704 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 8315, batch.n_tokens = 123, progress = 0.992362
slot update_slots: id  0 | task 7704 | n_tokens = 8315, memory_seq_rm [8315, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 8379, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 7704 | prompt done, n_tokens = 8379, batch.n_tokens = 64
slot init_sampler: id  0 | task 7704 | init sampler, took 1.59 ms, tokens: text = 8379, total = 8379
slot update_slots: id  0 | task 7704 | created context checkpoint 1 of 8 (pos_min = 7672, pos_max = 8314, size = 15.078 MiB)
slot print_timing: id  0 | task 7704 | 
prompt eval time =   12563.23 ms /  8379 tokens (    1.50 ms per token,   666.95 tokens per second)
       eval time =   32340.88 ms /  1065 tokens (   30.37 ms per token,    32.93 tokens per second)
      total time =   44904.11 ms /  9444 tokens
slot      release: id  0 | task 7704 | stop processing: n_tokens = 9443, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8775 | processing task, is_child = 0
slot update_slots: id  0 | task 8775 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9470
slot update_slots: id  0 | task 8775 | n_tokens = 9377, memory_seq_rm [9377, end)
slot update_slots: id  0 | task 8775 | prompt processing progress, n_tokens = 9406, batch.n_tokens = 29, progress = 0.993242
slot update_slots: id  0 | task 8775 | n_tokens = 9406, memory_seq_rm [9406, end)
slot update_slots: id  0 | task 8775 | prompt processing progress, n_tokens = 9470, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8775 | prompt done, n_tokens = 9470, batch.n_tokens = 64
slot init_sampler: id  0 | task 8775 | init sampler, took 1.78 ms, tokens: text = 9470, total = 9470
slot update_slots: id  0 | task 8775 | created context checkpoint 2 of 8 (pos_min = 8800, pos_max = 9405, size = 14.210 MiB)
slot print_timing: id  0 | task 8775 | 
prompt eval time =     400.21 ms /    93 tokens (    4.30 ms per token,   232.38 tokens per second)
       eval time =    2300.63 ms /    77 tokens (   29.88 ms per token,    33.47 tokens per second)
      total time =    2700.84 ms /   170 tokens
slot      release: id  0 | task 8775 | stop processing: n_tokens = 9546, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8854 | processing task, is_child = 0
slot update_slots: id  0 | task 8854 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9574
slot update_slots: id  0 | task 8854 | n_tokens = 9480, memory_seq_rm [9480, end)
slot update_slots: id  0 | task 8854 | prompt processing progress, n_tokens = 9510, batch.n_tokens = 30, progress = 0.993315
slot update_slots: id  0 | task 8854 | n_tokens = 9510, memory_seq_rm [9510, end)
slot update_slots: id  0 | task 8854 | prompt processing progress, n_tokens = 9574, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8854 | prompt done, n_tokens = 9574, batch.n_tokens = 64
slot init_sampler: id  0 | task 8854 | init sampler, took 1.37 ms, tokens: text = 9574, total = 9574
slot update_slots: id  0 | task 8854 | created context checkpoint 3 of 8 (pos_min = 8903, pos_max = 9509, size = 14.234 MiB)
slot print_timing: id  0 | task 8854 | 
prompt eval time =     401.60 ms /    94 tokens (    4.27 ms per token,   234.07 tokens per second)
       eval time =    6322.26 ms /   214 tokens (   29.54 ms per token,    33.85 tokens per second)
      total time =    6723.85 ms /   308 tokens
slot      release: id  0 | task 8854 | stop processing: n_tokens = 9787, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9070 | processing task, is_child = 0
slot update_slots: id  0 | task 9070 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9815
slot update_slots: id  0 | task 9070 | n_tokens = 9585, memory_seq_rm [9585, end)
slot update_slots: id  0 | task 9070 | prompt processing progress, n_tokens = 9751, batch.n_tokens = 166, progress = 0.993479
slot update_slots: id  0 | task 9070 | n_tokens = 9751, memory_seq_rm [9751, end)
slot update_slots: id  0 | task 9070 | prompt processing progress, n_tokens = 9815, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9070 | prompt done, n_tokens = 9815, batch.n_tokens = 64
slot init_sampler: id  0 | task 9070 | init sampler, took 1.41 ms, tokens: text = 9815, total = 9815
slot update_slots: id  0 | task 9070 | created context checkpoint 4 of 8 (pos_min = 9177, pos_max = 9750, size = 13.460 MiB)
slot print_timing: id  0 | task 9070 | 
prompt eval time =     637.59 ms /   230 tokens (    2.77 ms per token,   360.73 tokens per second)
       eval time =    8652.06 ms /   298 tokens (   29.03 ms per token,    34.44 tokens per second)
      total time =    9289.66 ms /   528 tokens
slot      release: id  0 | task 9070 | stop processing: n_tokens = 10112, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9370 | processing task, is_child = 0
slot update_slots: id  0 | task 9370 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10139
slot update_slots: id  0 | task 9370 | n_tokens = 9829, memory_seq_rm [9829, end)
slot update_slots: id  0 | task 9370 | prompt processing progress, n_tokens = 10075, batch.n_tokens = 246, progress = 0.993688
slot update_slots: id  0 | task 9370 | n_tokens = 10075, memory_seq_rm [10075, end)
slot update_slots: id  0 | task 9370 | prompt processing progress, n_tokens = 10139, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9370 | prompt done, n_tokens = 10139, batch.n_tokens = 64
slot init_sampler: id  0 | task 9370 | init sampler, took 1.88 ms, tokens: text = 10139, total = 10139
slot update_slots: id  0 | task 9370 | created context checkpoint 5 of 8 (pos_min = 9538, pos_max = 10074, size = 12.592 MiB)
slot print_timing: id  0 | task 9370 | 
prompt eval time =     756.89 ms /   310 tokens (    2.44 ms per token,   409.57 tokens per second)
       eval time =   14984.57 ms /   515 tokens (   29.10 ms per token,    34.37 tokens per second)
      total time =   15741.47 ms /   825 tokens
slot      release: id  0 | task 9370 | stop processing: n_tokens = 10653, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9887 | processing task, is_child = 0
slot update_slots: id  0 | task 9887 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10685
slot update_slots: id  0 | task 9887 | n_tokens = 10148, memory_seq_rm [10148, end)
slot update_slots: id  0 | task 9887 | prompt processing progress, n_tokens = 10621, batch.n_tokens = 473, progress = 0.994010
slot update_slots: id  0 | task 9887 | n_tokens = 10621, memory_seq_rm [10621, end)
slot update_slots: id  0 | task 9887 | prompt processing progress, n_tokens = 10685, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9887 | prompt done, n_tokens = 10685, batch.n_tokens = 64
slot init_sampler: id  0 | task 9887 | init sampler, took 1.51 ms, tokens: text = 10685, total = 10685
slot update_slots: id  0 | task 9887 | created context checkpoint 6 of 8 (pos_min = 10021, pos_max = 10620, size = 14.070 MiB)
slot print_timing: id  0 | task 9887 | 
prompt eval time =    1025.54 ms /   537 tokens (    1.91 ms per token,   523.63 tokens per second)
       eval time =    5661.80 ms /   194 tokens (   29.18 ms per token,    34.26 tokens per second)
      total time =    6687.34 ms /   731 tokens
slot      release: id  0 | task 9887 | stop processing: n_tokens = 10878, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.866 (> 0.100 thold), f_keep = 0.770
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 10083 | processing task, is_child = 0
slot update_slots: id  0 | task 10083 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9673
slot update_slots: id  0 | task 10083 | n_past = 8379, slot.prompt.tokens.size() = 10878, seq_id = 0, pos_min = 10362, n_swa = 128
slot update_slots: id  0 | task 10083 | restored context checkpoint (pos_min = 7672, pos_max = 8314, size = 15.078 MiB)
slot update_slots: id  0 | task 10083 | erased invalidated context checkpoint (pos_min = 8800, pos_max = 9405, n_swa = 128, size = 14.210 MiB)
slot update_slots: id  0 | task 10083 | erased invalidated context checkpoint (pos_min = 8903, pos_max = 9509, n_swa = 128, size = 14.234 MiB)
slot update_slots: id  0 | task 10083 | erased invalidated context checkpoint (pos_min = 9177, pos_max = 9750, n_swa = 128, size = 13.460 MiB)
slot update_slots: id  0 | task 10083 | erased invalidated context checkpoint (pos_min = 9538, pos_max = 10074, n_swa = 128, size = 12.592 MiB)
slot update_slots: id  0 | task 10083 | erased invalidated context checkpoint (pos_min = 10021, pos_max = 10620, n_swa = 128, size = 14.070 MiB)
slot update_slots: id  0 | task 10083 | n_tokens = 8314, memory_seq_rm [8314, end)
slot update_slots: id  0 | task 10083 | prompt processing progress, n_tokens = 9609, batch.n_tokens = 1295, progress = 0.993384
slot update_slots: id  0 | task 10083 | n_tokens = 9609, memory_seq_rm [9609, end)
slot update_slots: id  0 | task 10083 | prompt processing progress, n_tokens = 9673, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 10083 | prompt done, n_tokens = 9673, batch.n_tokens = 64
slot init_sampler: id  0 | task 10083 | init sampler, took 1.88 ms, tokens: text = 9673, total = 9673
slot update_slots: id  0 | task 10083 | created context checkpoint 2 of 8 (pos_min = 8966, pos_max = 9608, size = 15.078 MiB)
slot print_timing: id  0 | task 10083 | 
prompt eval time =    2476.77 ms /  1359 tokens (    1.82 ms per token,   548.70 tokens per second)
       eval time =   34754.25 ms /  1148 tokens (   30.27 ms per token,    33.03 tokens per second)
      total time =   37231.02 ms /  2507 tokens
slot      release: id  0 | task 10083 | stop processing: n_tokens = 10820, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 11233 | processing task, is_child = 0
slot update_slots: id  0 | task 11233 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10848
slot update_slots: id  0 | task 11233 | n_tokens = 10750, memory_seq_rm [10750, end)
slot update_slots: id  0 | task 11233 | prompt processing progress, n_tokens = 10784, batch.n_tokens = 34, progress = 0.994100
slot update_slots: id  0 | task 11233 | n_tokens = 10784, memory_seq_rm [10784, end)
slot update_slots: id  0 | task 11233 | prompt processing progress, n_tokens = 10848, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 11233 | prompt done, n_tokens = 10848, batch.n_tokens = 64
slot init_sampler: id  0 | task 11233 | init sampler, took 1.54 ms, tokens: text = 10848, total = 10848
slot update_slots: id  0 | task 11233 | created context checkpoint 3 of 8 (pos_min = 10177, pos_max = 10783, size = 14.234 MiB)
slot print_timing: id  0 | task 11233 | 
prompt eval time =     425.72 ms /    98 tokens (    4.34 ms per token,   230.20 tokens per second)
       eval time =   19120.95 ms /   635 tokens (   30.11 ms per token,    33.21 tokens per second)
      total time =   19546.68 ms /   733 tokens
slot      release: id  0 | task 11233 | stop processing: n_tokens = 11482, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.956
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 11870 | processing task, is_child = 0
slot update_slots: id  0 | task 11870 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11505
slot update_slots: id  0 | task 11870 | n_tokens = 10980, memory_seq_rm [10980, end)
slot update_slots: id  0 | task 11870 | prompt processing progress, n_tokens = 11441, batch.n_tokens = 461, progress = 0.994437
slot update_slots: id  0 | task 11870 | n_tokens = 11441, memory_seq_rm [11441, end)
slot update_slots: id  0 | task 11870 | prompt processing progress, n_tokens = 11505, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 11870 | prompt done, n_tokens = 11505, batch.n_tokens = 64
slot init_sampler: id  0 | task 11870 | init sampler, took 1.62 ms, tokens: text = 11505, total = 11505
slot update_slots: id  0 | task 11870 | created context checkpoint 4 of 8 (pos_min = 10839, pos_max = 11440, size = 14.117 MiB)
slot print_timing: id  0 | task 11870 | 
prompt eval time =    1026.57 ms /   525 tokens (    1.96 ms per token,   511.41 tokens per second)
       eval time =  123332.81 ms /  4096 tokens (   30.11 ms per token,    33.21 tokens per second)
      total time =  124359.38 ms /  4621 tokens
slot      release: id  0 | task 11870 | stop processing: n_tokens = 15600, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 11870
srv  update_slots: all slots are idle
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.318 (> 0.100 thold), f_keep = 0.043
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 15600, total state size = 380.882 MiB
srv          load:  - looking for better prompt, base f_keep = 0.043, sim = 0.318
srv        update:  - cache state: 3 prompts, 1098.050 MiB (limits: 8192.000 MiB, 56064 tokens, 259767 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv  get_availabl: prompt cache update took 336.12 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 15969 | processing task, is_child = 0
slot update_slots: id  0 | task 15969 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2131
slot update_slots: id  0 | task 15969 | n_past = 677, slot.prompt.tokens.size() = 15600, seq_id = 0, pos_min = 14957, n_swa = 128
slot update_slots: id  0 | task 15969 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 15969 | erased invalidated context checkpoint (pos_min = 7672, pos_max = 8314, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 15969 | erased invalidated context checkpoint (pos_min = 8966, pos_max = 9608, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 15969 | erased invalidated context checkpoint (pos_min = 10177, pos_max = 10783, n_swa = 128, size = 14.234 MiB)
slot update_slots: id  0 | task 15969 | erased invalidated context checkpoint (pos_min = 10839, pos_max = 11440, n_swa = 128, size = 14.117 MiB)
slot update_slots: id  0 | task 15969 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 15969 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.961051
slot update_slots: id  0 | task 15969 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 15969 | prompt processing progress, n_tokens = 2067, batch.n_tokens = 19, progress = 0.969967
slot update_slots: id  0 | task 15969 | n_tokens = 2067, memory_seq_rm [2067, end)
slot update_slots: id  0 | task 15969 | prompt processing progress, n_tokens = 2131, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 15969 | prompt done, n_tokens = 2131, batch.n_tokens = 64
slot init_sampler: id  0 | task 15969 | init sampler, took 0.31 ms, tokens: text = 2131, total = 2131
slot update_slots: id  0 | task 15969 | created context checkpoint 1 of 8 (pos_min = 1424, pos_max = 2066, size = 15.078 MiB)
slot print_timing: id  0 | task 15969 | 
prompt eval time =    3244.73 ms /  2131 tokens (    1.52 ms per token,   656.76 tokens per second)
       eval time =     901.93 ms /    33 tokens (   27.33 ms per token,    36.59 tokens per second)
      total time =    4146.66 ms /  2164 tokens
slot      release: id  0 | task 15969 | stop processing: n_tokens = 2163, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.739 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16005 | processing task, is_child = 0
slot update_slots: id  0 | task 16005 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2901
slot update_slots: id  0 | task 16005 | n_tokens = 2143, memory_seq_rm [2143, end)
slot update_slots: id  0 | task 16005 | prompt processing progress, n_tokens = 2837, batch.n_tokens = 694, progress = 0.977939
slot update_slots: id  0 | task 16005 | n_tokens = 2837, memory_seq_rm [2837, end)
slot update_slots: id  0 | task 16005 | prompt processing progress, n_tokens = 2901, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16005 | prompt done, n_tokens = 2901, batch.n_tokens = 64
slot init_sampler: id  0 | task 16005 | init sampler, took 0.42 ms, tokens: text = 2901, total = 2901
slot update_slots: id  0 | task 16005 | created context checkpoint 2 of 8 (pos_min = 2194, pos_max = 2836, size = 15.078 MiB)
slot print_timing: id  0 | task 16005 | 
prompt eval time =    1323.40 ms /   758 tokens (    1.75 ms per token,   572.77 tokens per second)
       eval time =    1565.17 ms /    55 tokens (   28.46 ms per token,    35.14 tokens per second)
      total time =    2888.57 ms /   813 tokens
slot      release: id  0 | task 16005 | stop processing: n_tokens = 2955, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.865 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16062 | processing task, is_child = 0
slot update_slots: id  0 | task 16062 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3384
slot update_slots: id  0 | task 16062 | n_tokens = 2926, memory_seq_rm [2926, end)
slot update_slots: id  0 | task 16062 | prompt processing progress, n_tokens = 3320, batch.n_tokens = 394, progress = 0.981087
slot update_slots: id  0 | task 16062 | n_tokens = 3320, memory_seq_rm [3320, end)
slot update_slots: id  0 | task 16062 | prompt processing progress, n_tokens = 3384, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16062 | prompt done, n_tokens = 3384, batch.n_tokens = 64
slot init_sampler: id  0 | task 16062 | init sampler, took 0.48 ms, tokens: text = 3384, total = 3384
slot update_slots: id  0 | task 16062 | created context checkpoint 3 of 8 (pos_min = 2677, pos_max = 3319, size = 15.078 MiB)
slot print_timing: id  0 | task 16062 | 
prompt eval time =     856.47 ms /   458 tokens (    1.87 ms per token,   534.75 tokens per second)
       eval time =    1146.69 ms /    40 tokens (   28.67 ms per token,    34.88 tokens per second)
      total time =    2003.16 ms /   498 tokens
slot      release: id  0 | task 16062 | stop processing: n_tokens = 3423, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16104 | processing task, is_child = 0
slot update_slots: id  0 | task 16104 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3451
slot update_slots: id  0 | task 16104 | n_tokens = 3393, memory_seq_rm [3393, end)
slot update_slots: id  0 | task 16104 | prompt processing progress, n_tokens = 3451, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 16104 | prompt done, n_tokens = 3451, batch.n_tokens = 58
slot init_sampler: id  0 | task 16104 | init sampler, took 0.53 ms, tokens: text = 3451, total = 3451
slot update_slots: id  0 | task 16104 | created context checkpoint 4 of 8 (pos_min = 2780, pos_max = 3392, size = 14.374 MiB)
slot print_timing: id  0 | task 16104 | 
prompt eval time =     211.39 ms /    58 tokens (    3.64 ms per token,   274.37 tokens per second)
       eval time =    1529.51 ms /    52 tokens (   29.41 ms per token,    34.00 tokens per second)
      total time =    1740.90 ms /   110 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 16104 | stop processing: n_tokens = 3502, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.882 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16157 | processing task, is_child = 0
slot update_slots: id  0 | task 16157 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3941
slot update_slots: id  0 | task 16157 | n_tokens = 3476, memory_seq_rm [3476, end)
slot update_slots: id  0 | task 16157 | prompt processing progress, n_tokens = 3877, batch.n_tokens = 401, progress = 0.983760
slot update_slots: id  0 | task 16157 | n_tokens = 3877, memory_seq_rm [3877, end)
slot update_slots: id  0 | task 16157 | prompt processing progress, n_tokens = 3941, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16157 | prompt done, n_tokens = 3941, batch.n_tokens = 64
slot init_sampler: id  0 | task 16157 | init sampler, took 0.75 ms, tokens: text = 3941, total = 3941
slot update_slots: id  0 | task 16157 | created context checkpoint 5 of 8 (pos_min = 3234, pos_max = 3876, size = 15.078 MiB)
slot print_timing: id  0 | task 16157 | 
prompt eval time =     887.94 ms /   465 tokens (    1.91 ms per token,   523.69 tokens per second)
       eval time =    1677.65 ms /    57 tokens (   29.43 ms per token,    33.98 tokens per second)
      total time =    2565.59 ms /   522 tokens
slot      release: id  0 | task 16157 | stop processing: n_tokens = 3997, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16216 | processing task, is_child = 0
slot update_slots: id  0 | task 16216 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4167
slot update_slots: id  0 | task 16216 | n_tokens = 3971, memory_seq_rm [3971, end)
slot update_slots: id  0 | task 16216 | prompt processing progress, n_tokens = 4103, batch.n_tokens = 132, progress = 0.984641
slot update_slots: id  0 | task 16216 | n_tokens = 4103, memory_seq_rm [4103, end)
slot update_slots: id  0 | task 16216 | prompt processing progress, n_tokens = 4167, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16216 | prompt done, n_tokens = 4167, batch.n_tokens = 64
slot init_sampler: id  0 | task 16216 | init sampler, took 0.65 ms, tokens: text = 4167, total = 4167
slot update_slots: id  0 | task 16216 | created context checkpoint 6 of 8 (pos_min = 3460, pos_max = 4102, size = 15.078 MiB)
slot print_timing: id  0 | task 16216 | 
prompt eval time =     632.72 ms /   196 tokens (    3.23 ms per token,   309.78 tokens per second)
       eval time =    2235.85 ms /    74 tokens (   30.21 ms per token,    33.10 tokens per second)
      total time =    2868.57 ms /   270 tokens
slot      release: id  0 | task 16216 | stop processing: n_tokens = 4240, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.940 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16292 | processing task, is_child = 0
slot update_slots: id  0 | task 16292 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4478
slot update_slots: id  0 | task 16292 | n_tokens = 4210, memory_seq_rm [4210, end)
slot update_slots: id  0 | task 16292 | prompt processing progress, n_tokens = 4414, batch.n_tokens = 204, progress = 0.985708
slot update_slots: id  0 | task 16292 | n_tokens = 4414, memory_seq_rm [4414, end)
slot update_slots: id  0 | task 16292 | prompt processing progress, n_tokens = 4478, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16292 | prompt done, n_tokens = 4478, batch.n_tokens = 64
slot init_sampler: id  0 | task 16292 | init sampler, took 0.65 ms, tokens: text = 4478, total = 4478
slot update_slots: id  0 | task 16292 | created context checkpoint 7 of 8 (pos_min = 3771, pos_max = 4413, size = 15.078 MiB)
slot print_timing: id  0 | task 16292 | 
prompt eval time =     699.75 ms /   268 tokens (    2.61 ms per token,   382.99 tokens per second)
       eval time =    1655.95 ms /    55 tokens (   30.11 ms per token,    33.21 tokens per second)
      total time =    2355.70 ms /   323 tokens
slot      release: id  0 | task 16292 | stop processing: n_tokens = 4532, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.722 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16349 | processing task, is_child = 0
slot update_slots: id  0 | task 16349 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6238
slot update_slots: id  0 | task 16349 | n_tokens = 4506, memory_seq_rm [4506, end)
slot update_slots: id  0 | task 16349 | prompt processing progress, n_tokens = 6174, batch.n_tokens = 1668, progress = 0.989740
slot update_slots: id  0 | task 16349 | n_tokens = 6174, memory_seq_rm [6174, end)
slot update_slots: id  0 | task 16349 | prompt processing progress, n_tokens = 6238, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16349 | prompt done, n_tokens = 6238, batch.n_tokens = 64
slot init_sampler: id  0 | task 16349 | init sampler, took 1.09 ms, tokens: text = 6238, total = 6238
slot update_slots: id  0 | task 16349 | created context checkpoint 8 of 8 (pos_min = 5531, pos_max = 6173, size = 15.078 MiB)
slot print_timing: id  0 | task 16349 | 
prompt eval time =    3154.54 ms /  1732 tokens (    1.82 ms per token,   549.05 tokens per second)
       eval time =    3068.57 ms /   100 tokens (   30.69 ms per token,    32.59 tokens per second)
      total time =    6223.10 ms /  1832 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 16349 | stop processing: n_tokens = 6337, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16451 | processing task, is_child = 0
slot update_slots: id  0 | task 16451 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6369
slot update_slots: id  0 | task 16451 | n_tokens = 6311, memory_seq_rm [6311, end)
slot update_slots: id  0 | task 16451 | prompt processing progress, n_tokens = 6369, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 16451 | prompt done, n_tokens = 6369, batch.n_tokens = 58
slot init_sampler: id  0 | task 16451 | init sampler, took 0.92 ms, tokens: text = 6369, total = 6369
slot update_slots: id  0 | task 16451 | erasing old context checkpoint (pos_min = 1424, pos_max = 2066, size = 15.078 MiB)
slot update_slots: id  0 | task 16451 | created context checkpoint 8 of 8 (pos_min = 5694, pos_max = 6310, size = 14.468 MiB)
slot print_timing: id  0 | task 16451 | 
prompt eval time =     217.67 ms /    58 tokens (    3.75 ms per token,   266.46 tokens per second)
       eval time =    2316.81 ms /    76 tokens (   30.48 ms per token,    32.80 tokens per second)
      total time =    2534.47 ms /   134 tokens
slot      release: id  0 | task 16451 | stop processing: n_tokens = 6444, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16528 | processing task, is_child = 0
slot update_slots: id  0 | task 16528 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6876
slot update_slots: id  0 | task 16528 | n_tokens = 6418, memory_seq_rm [6418, end)
slot update_slots: id  0 | task 16528 | prompt processing progress, n_tokens = 6812, batch.n_tokens = 394, progress = 0.990692
slot update_slots: id  0 | task 16528 | n_tokens = 6812, memory_seq_rm [6812, end)
slot update_slots: id  0 | task 16528 | prompt processing progress, n_tokens = 6876, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16528 | prompt done, n_tokens = 6876, batch.n_tokens = 64
slot init_sampler: id  0 | task 16528 | init sampler, took 0.98 ms, tokens: text = 6876, total = 6876
slot update_slots: id  0 | task 16528 | erasing old context checkpoint (pos_min = 2194, pos_max = 2836, size = 15.078 MiB)
slot update_slots: id  0 | task 16528 | created context checkpoint 8 of 8 (pos_min = 6181, pos_max = 6811, size = 14.797 MiB)
slot print_timing: id  0 | task 16528 | 
prompt eval time =     941.96 ms /   458 tokens (    2.06 ms per token,   486.22 tokens per second)
       eval time =    5141.94 ms /   170 tokens (   30.25 ms per token,    33.06 tokens per second)
      total time =    6083.90 ms /   628 tokens
slot      release: id  0 | task 16528 | stop processing: n_tokens = 7045, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.849 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16700 | processing task, is_child = 0
slot update_slots: id  0 | task 16700 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8270
slot update_slots: id  0 | task 16700 | n_tokens = 7020, memory_seq_rm [7020, end)
slot update_slots: id  0 | task 16700 | prompt processing progress, n_tokens = 8206, batch.n_tokens = 1186, progress = 0.992261
slot update_slots: id  0 | task 16700 | n_tokens = 8206, memory_seq_rm [8206, end)
slot update_slots: id  0 | task 16700 | prompt processing progress, n_tokens = 8270, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16700 | prompt done, n_tokens = 8270, batch.n_tokens = 64
slot init_sampler: id  0 | task 16700 | init sampler, took 1.54 ms, tokens: text = 8270, total = 8270
slot update_slots: id  0 | task 16700 | erasing old context checkpoint (pos_min = 2677, pos_max = 3319, size = 15.078 MiB)
slot update_slots: id  0 | task 16700 | created context checkpoint 8 of 8 (pos_min = 7563, pos_max = 8205, size = 15.078 MiB)
slot print_timing: id  0 | task 16700 | 
prompt eval time =    2312.64 ms /  1250 tokens (    1.85 ms per token,   540.51 tokens per second)
       eval time =    6206.96 ms /   208 tokens (   29.84 ms per token,    33.51 tokens per second)
      total time =    8519.60 ms /  1458 tokens
slot      release: id  0 | task 16700 | stop processing: n_tokens = 8477, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.356 (> 0.100 thold), f_keep = 0.087
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8477, total state size = 213.855 MiB
srv          load:  - looking for better prompt, base f_keep = 0.087, sim = 0.356
srv        update:  - cache state: 4 prompts, 1430.934 MiB (limits: 8192.000 MiB, 56064 tokens, 247866 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv  get_availabl: prompt cache update took 249.15 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16910 | processing task, is_child = 0
slot update_slots: id  0 | task 16910 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2065
slot update_slots: id  0 | task 16910 | n_past = 736, slot.prompt.tokens.size() = 8477, seq_id = 0, pos_min = 7834, n_swa = 128
slot update_slots: id  0 | task 16910 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 2780, pos_max = 3392, n_swa = 128, size = 14.374 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 3234, pos_max = 3876, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 3460, pos_max = 4102, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 3771, pos_max = 4413, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 5531, pos_max = 6173, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 5694, pos_max = 6310, n_swa = 128, size = 14.468 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 6181, pos_max = 6811, n_swa = 128, size = 14.797 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 7563, pos_max = 8205, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 16910 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 16910 | prompt processing progress, n_tokens = 2001, batch.n_tokens = 2001, progress = 0.969007
slot update_slots: id  0 | task 16910 | n_tokens = 2001, memory_seq_rm [2001, end)
slot update_slots: id  0 | task 16910 | prompt processing progress, n_tokens = 2065, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16910 | prompt done, n_tokens = 2065, batch.n_tokens = 64
slot init_sampler: id  0 | task 16910 | init sampler, took 0.31 ms, tokens: text = 2065, total = 2065
slot update_slots: id  0 | task 16910 | created context checkpoint 1 of 8 (pos_min = 1358, pos_max = 2000, size = 15.078 MiB)
slot print_timing: id  0 | task 16910 | 
prompt eval time =    3078.35 ms /  2065 tokens (    1.49 ms per token,   670.81 tokens per second)
       eval time =    1198.15 ms /    44 tokens (   27.23 ms per token,    36.72 tokens per second)
      total time =    4276.50 ms /  2109 tokens
slot      release: id  0 | task 16910 | stop processing: n_tokens = 2108, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.734 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16956 | processing task, is_child = 0
slot update_slots: id  0 | task 16956 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2846
slot update_slots: id  0 | task 16956 | n_tokens = 2089, memory_seq_rm [2089, end)
slot update_slots: id  0 | task 16956 | prompt processing progress, n_tokens = 2782, batch.n_tokens = 693, progress = 0.977512
slot update_slots: id  0 | task 16956 | n_tokens = 2782, memory_seq_rm [2782, end)
slot update_slots: id  0 | task 16956 | prompt processing progress, n_tokens = 2846, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16956 | prompt done, n_tokens = 2846, batch.n_tokens = 64
slot init_sampler: id  0 | task 16956 | init sampler, took 0.46 ms, tokens: text = 2846, total = 2846
slot update_slots: id  0 | task 16956 | created context checkpoint 2 of 8 (pos_min = 2139, pos_max = 2781, size = 15.078 MiB)
slot print_timing: id  0 | task 16956 | 
prompt eval time =    1302.17 ms /   757 tokens (    1.72 ms per token,   581.34 tokens per second)
       eval time =    2002.73 ms /    71 tokens (   28.21 ms per token,    35.45 tokens per second)
      total time =    3304.90 ms /   828 tokens
slot      release: id  0 | task 16956 | stop processing: n_tokens = 2916, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.625 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17029 | processing task, is_child = 0
slot update_slots: id  0 | task 17029 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4622
slot update_slots: id  0 | task 17029 | n_tokens = 2890, memory_seq_rm [2890, end)
slot update_slots: id  0 | task 17029 | prompt processing progress, n_tokens = 4558, batch.n_tokens = 1668, progress = 0.986153
slot update_slots: id  0 | task 17029 | n_tokens = 4558, memory_seq_rm [4558, end)
slot update_slots: id  0 | task 17029 | prompt processing progress, n_tokens = 4622, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17029 | prompt done, n_tokens = 4622, batch.n_tokens = 64
slot init_sampler: id  0 | task 17029 | init sampler, took 0.89 ms, tokens: text = 4622, total = 4622
slot update_slots: id  0 | task 17029 | created context checkpoint 3 of 8 (pos_min = 3915, pos_max = 4557, size = 15.078 MiB)
slot print_timing: id  0 | task 17029 | 
prompt eval time =    2854.08 ms /  1732 tokens (    1.65 ms per token,   606.85 tokens per second)
       eval time =    1113.78 ms /    38 tokens (   29.31 ms per token,    34.12 tokens per second)
      total time =    3967.85 ms /  1770 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 17029 | stop processing: n_tokens = 4659, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.648 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17069 | processing task, is_child = 0
slot update_slots: id  0 | task 17069 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7145
slot update_slots: id  0 | task 17069 | n_tokens = 4633, memory_seq_rm [4633, end)
slot update_slots: id  0 | task 17069 | prompt processing progress, n_tokens = 6681, batch.n_tokens = 2048, progress = 0.935059
slot update_slots: id  0 | task 17069 | n_tokens = 6681, memory_seq_rm [6681, end)
slot update_slots: id  0 | task 17069 | prompt processing progress, n_tokens = 7081, batch.n_tokens = 400, progress = 0.991043
slot update_slots: id  0 | task 17069 | n_tokens = 7081, memory_seq_rm [7081, end)
slot update_slots: id  0 | task 17069 | prompt processing progress, n_tokens = 7145, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17069 | prompt done, n_tokens = 7145, batch.n_tokens = 64
slot init_sampler: id  0 | task 17069 | init sampler, took 1.01 ms, tokens: text = 7145, total = 7145
slot update_slots: id  0 | task 17069 | created context checkpoint 4 of 8 (pos_min = 6438, pos_max = 7080, size = 15.078 MiB)
slot print_timing: id  0 | task 17069 | 
prompt eval time =    4049.09 ms /  2512 tokens (    1.61 ms per token,   620.39 tokens per second)
       eval time =    1044.24 ms /    35 tokens (   29.84 ms per token,    33.52 tokens per second)
      total time =    5093.34 ms /  2547 tokens
slot      release: id  0 | task 17069 | stop processing: n_tokens = 7179, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.807 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17107 | processing task, is_child = 0
slot update_slots: id  0 | task 17107 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8864
slot update_slots: id  0 | task 17107 | n_tokens = 7153, memory_seq_rm [7153, end)
slot update_slots: id  0 | task 17107 | prompt processing progress, n_tokens = 8800, batch.n_tokens = 1647, progress = 0.992780
slot update_slots: id  0 | task 17107 | n_tokens = 8800, memory_seq_rm [8800, end)
slot update_slots: id  0 | task 17107 | prompt processing progress, n_tokens = 8864, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17107 | prompt done, n_tokens = 8864, batch.n_tokens = 64
slot init_sampler: id  0 | task 17107 | init sampler, took 1.25 ms, tokens: text = 8864, total = 8864
slot update_slots: id  0 | task 17107 | created context checkpoint 5 of 8 (pos_min = 8157, pos_max = 8799, size = 15.078 MiB)
slot print_timing: id  0 | task 17107 | 
prompt eval time =    3047.08 ms /  1711 tokens (    1.78 ms per token,   561.52 tokens per second)
       eval time =    2293.14 ms /    76 tokens (   30.17 ms per token,    33.14 tokens per second)
      total time =    5340.21 ms /  1787 tokens
slot      release: id  0 | task 17107 | stop processing: n_tokens = 8939, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17185 | processing task, is_child = 0
slot update_slots: id  0 | task 17185 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9368
slot update_slots: id  0 | task 17185 | n_tokens = 8910, memory_seq_rm [8910, end)
slot update_slots: id  0 | task 17185 | prompt processing progress, n_tokens = 9304, batch.n_tokens = 394, progress = 0.993168
slot update_slots: id  0 | task 17185 | n_tokens = 9304, memory_seq_rm [9304, end)
slot update_slots: id  0 | task 17185 | prompt processing progress, n_tokens = 9368, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17185 | prompt done, n_tokens = 9368, batch.n_tokens = 64
slot init_sampler: id  0 | task 17185 | init sampler, took 1.34 ms, tokens: text = 9368, total = 9368
slot update_slots: id  0 | task 17185 | created context checkpoint 6 of 8 (pos_min = 8661, pos_max = 9303, size = 15.078 MiB)
slot print_timing: id  0 | task 17185 | 
prompt eval time =     964.33 ms /   458 tokens (    2.11 ms per token,   474.94 tokens per second)
       eval time =    1209.14 ms /    39 tokens (   31.00 ms per token,    32.25 tokens per second)
      total time =    2173.47 ms /   497 tokens
slot      release: id  0 | task 17185 | stop processing: n_tokens = 9406, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17226 | processing task, is_child = 0
slot update_slots: id  0 | task 17226 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9435
slot update_slots: id  0 | task 17226 | n_tokens = 9377, memory_seq_rm [9377, end)
slot update_slots: id  0 | task 17226 | prompt processing progress, n_tokens = 9435, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 17226 | prompt done, n_tokens = 9435, batch.n_tokens = 58
slot init_sampler: id  0 | task 17226 | init sampler, took 1.85 ms, tokens: text = 9435, total = 9435
slot update_slots: id  0 | task 17226 | created context checkpoint 7 of 8 (pos_min = 8763, pos_max = 9376, size = 14.398 MiB)
slot print_timing: id  0 | task 17226 | 
prompt eval time =     223.73 ms /    58 tokens (    3.86 ms per token,   259.24 tokens per second)
       eval time =    1526.32 ms /    49 tokens (   31.15 ms per token,    32.10 tokens per second)
      total time =    1750.05 ms /   107 tokens
slot      release: id  0 | task 17226 | stop processing: n_tokens = 9483, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17276 | processing task, is_child = 0
slot update_slots: id  0 | task 17276 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9915
slot update_slots: id  0 | task 17276 | n_tokens = 9457, memory_seq_rm [9457, end)
slot update_slots: id  0 | task 17276 | prompt processing progress, n_tokens = 9851, batch.n_tokens = 394, progress = 0.993545
slot update_slots: id  0 | task 17276 | n_tokens = 9851, memory_seq_rm [9851, end)
slot update_slots: id  0 | task 17276 | prompt processing progress, n_tokens = 9915, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17276 | prompt done, n_tokens = 9915, batch.n_tokens = 64
slot init_sampler: id  0 | task 17276 | init sampler, took 1.41 ms, tokens: text = 9915, total = 9915
slot update_slots: id  0 | task 17276 | created context checkpoint 8 of 8 (pos_min = 9208, pos_max = 9850, size = 15.078 MiB)
slot print_timing: id  0 | task 17276 | 
prompt eval time =     973.70 ms /   458 tokens (    2.13 ms per token,   470.37 tokens per second)
       eval time =    1547.79 ms /    50 tokens (   30.96 ms per token,    32.30 tokens per second)
      total time =    2521.49 ms /   508 tokens
slot      release: id  0 | task 17276 | stop processing: n_tokens = 9964, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17328 | processing task, is_child = 0
slot update_slots: id  0 | task 17328 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9996
slot update_slots: id  0 | task 17328 | n_tokens = 9938, memory_seq_rm [9938, end)
slot update_slots: id  0 | task 17328 | prompt processing progress, n_tokens = 9996, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 17328 | prompt done, n_tokens = 9996, batch.n_tokens = 58
slot init_sampler: id  0 | task 17328 | init sampler, took 1.71 ms, tokens: text = 9996, total = 9996
slot update_slots: id  0 | task 17328 | erasing old context checkpoint (pos_min = 1358, pos_max = 2000, size = 15.078 MiB)
slot update_slots: id  0 | task 17328 | created context checkpoint 8 of 8 (pos_min = 9321, pos_max = 9937, size = 14.468 MiB)
slot print_timing: id  0 | task 17328 | 
prompt eval time =     223.49 ms /    58 tokens (    3.85 ms per token,   259.52 tokens per second)
       eval time =    1632.82 ms /    53 tokens (   30.81 ms per token,    32.46 tokens per second)
      total time =    1856.30 ms /   111 tokens
slot      release: id  0 | task 17328 | stop processing: n_tokens = 10048, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17382 | processing task, is_child = 0
slot update_slots: id  0 | task 17382 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10218
slot update_slots: id  0 | task 17382 | n_tokens = 10022, memory_seq_rm [10022, end)
slot update_slots: id  0 | task 17382 | prompt processing progress, n_tokens = 10154, batch.n_tokens = 132, progress = 0.993737
slot update_slots: id  0 | task 17382 | n_tokens = 10154, memory_seq_rm [10154, end)
slot update_slots: id  0 | task 17382 | prompt processing progress, n_tokens = 10218, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17382 | prompt done, n_tokens = 10218, batch.n_tokens = 64
slot init_sampler: id  0 | task 17382 | init sampler, took 1.45 ms, tokens: text = 10218, total = 10218
slot update_slots: id  0 | task 17382 | erasing old context checkpoint (pos_min = 2139, pos_max = 2781, size = 15.078 MiB)
slot update_slots: id  0 | task 17382 | created context checkpoint 8 of 8 (pos_min = 9511, pos_max = 10153, size = 15.078 MiB)
slot print_timing: id  0 | task 17382 | 
prompt eval time =     689.79 ms /   196 tokens (    3.52 ms per token,   284.14 tokens per second)
       eval time =    5684.45 ms /   186 tokens (   30.56 ms per token,    32.72 tokens per second)
      total time =    6374.24 ms /   382 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 17382 | stop processing: n_tokens = 10403, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17570 | processing task, is_child = 0
slot update_slots: id  0 | task 17570 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10842
slot update_slots: id  0 | task 17570 | n_tokens = 10377, memory_seq_rm [10377, end)
slot update_slots: id  0 | task 17570 | prompt processing progress, n_tokens = 10778, batch.n_tokens = 401, progress = 0.994097
slot update_slots: id  0 | task 17570 | n_tokens = 10778, memory_seq_rm [10778, end)
slot update_slots: id  0 | task 17570 | prompt processing progress, n_tokens = 10842, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17570 | prompt done, n_tokens = 10842, batch.n_tokens = 64
slot init_sampler: id  0 | task 17570 | init sampler, took 2.07 ms, tokens: text = 10842, total = 10842
slot update_slots: id  0 | task 17570 | erasing old context checkpoint (pos_min = 3915, pos_max = 4557, size = 15.078 MiB)
slot update_slots: id  0 | task 17570 | created context checkpoint 8 of 8 (pos_min = 10135, pos_max = 10777, size = 15.078 MiB)
slot print_timing: id  0 | task 17570 | 
prompt eval time =     961.58 ms /   465 tokens (    2.07 ms per token,   483.58 tokens per second)
       eval time =   12093.50 ms /   401 tokens (   30.16 ms per token,    33.16 tokens per second)
      total time =   13055.08 ms /   866 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 17570 | stop processing: n_tokens = 11242, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17973 | processing task, is_child = 0
slot update_slots: id  0 | task 17973 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11291
slot update_slots: id  0 | task 17973 | n_tokens = 11216, memory_seq_rm [11216, end)
slot update_slots: id  0 | task 17973 | prompt processing progress, n_tokens = 11227, batch.n_tokens = 11, progress = 0.994332
slot update_slots: id  0 | task 17973 | n_tokens = 11227, memory_seq_rm [11227, end)
slot update_slots: id  0 | task 17973 | prompt processing progress, n_tokens = 11291, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17973 | prompt done, n_tokens = 11291, batch.n_tokens = 64
slot init_sampler: id  0 | task 17973 | init sampler, took 1.62 ms, tokens: text = 11291, total = 11291
slot update_slots: id  0 | task 17973 | erasing old context checkpoint (pos_min = 6438, pos_max = 7080, size = 15.078 MiB)
slot update_slots: id  0 | task 17973 | created context checkpoint 8 of 8 (pos_min = 10599, pos_max = 11226, size = 14.726 MiB)
slot print_timing: id  0 | task 17973 | 
prompt eval time =     354.82 ms /    75 tokens (    4.73 ms per token,   211.37 tokens per second)
       eval time =    1960.70 ms /    67 tokens (   29.26 ms per token,    34.17 tokens per second)
      total time =    2315.53 ms /   142 tokens
slot      release: id  0 | task 17973 | stop processing: n_tokens = 11357, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18042 | processing task, is_child = 0
slot update_slots: id  0 | task 18042 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11598
slot update_slots: id  0 | task 18042 | n_tokens = 11330, memory_seq_rm [11330, end)
slot update_slots: id  0 | task 18042 | prompt processing progress, n_tokens = 11534, batch.n_tokens = 204, progress = 0.994482
slot update_slots: id  0 | task 18042 | n_tokens = 11534, memory_seq_rm [11534, end)
slot update_slots: id  0 | task 18042 | prompt processing progress, n_tokens = 11598, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18042 | prompt done, n_tokens = 11598, batch.n_tokens = 64
slot init_sampler: id  0 | task 18042 | init sampler, took 1.64 ms, tokens: text = 11598, total = 11598
slot update_slots: id  0 | task 18042 | erasing old context checkpoint (pos_min = 8157, pos_max = 8799, size = 15.078 MiB)
slot update_slots: id  0 | task 18042 | created context checkpoint 8 of 8 (pos_min = 10891, pos_max = 11533, size = 15.078 MiB)
slot print_timing: id  0 | task 18042 | 
prompt eval time =     709.70 ms /   268 tokens (    2.65 ms per token,   377.62 tokens per second)
       eval time =    4995.09 ms /   171 tokens (   29.21 ms per token,    34.23 tokens per second)
      total time =    5704.79 ms /   439 tokens
slot      release: id  0 | task 18042 | stop processing: n_tokens = 11768, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.904 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18215 | processing task, is_child = 0
slot update_slots: id  0 | task 18215 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12993
slot update_slots: id  0 | task 18215 | n_tokens = 11743, memory_seq_rm [11743, end)
slot update_slots: id  0 | task 18215 | prompt processing progress, n_tokens = 12929, batch.n_tokens = 1186, progress = 0.995074
slot update_slots: id  0 | task 18215 | n_tokens = 12929, memory_seq_rm [12929, end)
slot update_slots: id  0 | task 18215 | prompt processing progress, n_tokens = 12993, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18215 | prompt done, n_tokens = 12993, batch.n_tokens = 64
slot init_sampler: id  0 | task 18215 | init sampler, took 2.67 ms, tokens: text = 12993, total = 12993
slot update_slots: id  0 | task 18215 | erasing old context checkpoint (pos_min = 8661, pos_max = 9303, size = 15.078 MiB)
slot update_slots: id  0 | task 18215 | created context checkpoint 8 of 8 (pos_min = 12286, pos_max = 12928, size = 15.078 MiB)
slot print_timing: id  0 | task 18215 | 
prompt eval time =    2314.46 ms /  1250 tokens (    1.85 ms per token,   540.08 tokens per second)
       eval time =   10312.92 ms /   350 tokens (   29.47 ms per token,    33.94 tokens per second)
      total time =   12627.38 ms /  1600 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 18215 | stop processing: n_tokens = 13342, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18567 | processing task, is_child = 0
slot update_slots: id  0 | task 18567 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13536
slot update_slots: id  0 | task 18567 | n_tokens = 13316, memory_seq_rm [13316, end)
slot update_slots: id  0 | task 18567 | prompt processing progress, n_tokens = 13472, batch.n_tokens = 156, progress = 0.995272
slot update_slots: id  0 | task 18567 | n_tokens = 13472, memory_seq_rm [13472, end)
slot update_slots: id  0 | task 18567 | prompt processing progress, n_tokens = 13536, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18567 | prompt done, n_tokens = 13536, batch.n_tokens = 64
slot init_sampler: id  0 | task 18567 | init sampler, took 1.92 ms, tokens: text = 13536, total = 13536
slot update_slots: id  0 | task 18567 | erasing old context checkpoint (pos_min = 8763, pos_max = 9376, size = 14.398 MiB)
slot update_slots: id  0 | task 18567 | created context checkpoint 8 of 8 (pos_min = 12829, pos_max = 13471, size = 15.078 MiB)
slot print_timing: id  0 | task 18567 | 
prompt eval time =     603.72 ms /   220 tokens (    2.74 ms per token,   364.41 tokens per second)
       eval time =    3760.05 ms /   125 tokens (   30.08 ms per token,    33.24 tokens per second)
      total time =    4363.76 ms /   345 tokens
slot      release: id  0 | task 18567 | stop processing: n_tokens = 13660, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18694 | processing task, is_child = 0
slot update_slots: id  0 | task 18694 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13923
slot update_slots: id  0 | task 18694 | n_tokens = 13636, memory_seq_rm [13636, end)
slot update_slots: id  0 | task 18694 | prompt processing progress, n_tokens = 13859, batch.n_tokens = 223, progress = 0.995403
slot update_slots: id  0 | task 18694 | n_tokens = 13859, memory_seq_rm [13859, end)
slot update_slots: id  0 | task 18694 | prompt processing progress, n_tokens = 13923, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18694 | prompt done, n_tokens = 13923, batch.n_tokens = 64
slot init_sampler: id  0 | task 18694 | init sampler, took 1.97 ms, tokens: text = 13923, total = 13923
slot update_slots: id  0 | task 18694 | erasing old context checkpoint (pos_min = 9208, pos_max = 9850, size = 15.078 MiB)
slot update_slots: id  0 | task 18694 | created context checkpoint 8 of 8 (pos_min = 13216, pos_max = 13858, size = 15.078 MiB)
slot print_timing: id  0 | task 18694 | 
prompt eval time =     709.68 ms /   287 tokens (    2.47 ms per token,   404.40 tokens per second)
       eval time =    4518.94 ms /   152 tokens (   29.73 ms per token,    33.64 tokens per second)
      total time =    5228.63 ms /   439 tokens
slot      release: id  0 | task 18694 | stop processing: n_tokens = 14074, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18848 | processing task, is_child = 0
slot update_slots: id  0 | task 18848 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14120
slot update_slots: id  0 | task 18848 | n_tokens = 14047, memory_seq_rm [14047, end)
slot update_slots: id  0 | task 18848 | prompt processing progress, n_tokens = 14056, batch.n_tokens = 9, progress = 0.995467
slot update_slots: id  0 | task 18848 | n_tokens = 14056, memory_seq_rm [14056, end)
slot update_slots: id  0 | task 18848 | prompt processing progress, n_tokens = 14120, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18848 | prompt done, n_tokens = 14120, batch.n_tokens = 64
slot init_sampler: id  0 | task 18848 | init sampler, took 2.06 ms, tokens: text = 14120, total = 14120
slot update_slots: id  0 | task 18848 | erasing old context checkpoint (pos_min = 9321, pos_max = 9937, size = 14.468 MiB)
slot update_slots: id  0 | task 18848 | created context checkpoint 8 of 8 (pos_min = 13431, pos_max = 14055, size = 14.656 MiB)
slot print_timing: id  0 | task 18848 | 
prompt eval time =     349.73 ms /    73 tokens (    4.79 ms per token,   208.73 tokens per second)
       eval time =   18739.35 ms /   616 tokens (   30.42 ms per token,    32.87 tokens per second)
      total time =   19089.09 ms /   689 tokens
slot      release: id  0 | task 18848 | stop processing: n_tokens = 14735, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 19466 | processing task, is_child = 0
slot update_slots: id  0 | task 19466 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14779
slot update_slots: id  0 | task 19466 | n_tokens = 14711, memory_seq_rm [14711, end)
slot update_slots: id  0 | task 19466 | prompt processing progress, n_tokens = 14715, batch.n_tokens = 4, progress = 0.995670
slot update_slots: id  0 | task 19466 | n_tokens = 14715, memory_seq_rm [14715, end)
slot update_slots: id  0 | task 19466 | prompt processing progress, n_tokens = 14779, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 19466 | prompt done, n_tokens = 14779, batch.n_tokens = 64
slot init_sampler: id  0 | task 19466 | init sampler, took 2.28 ms, tokens: text = 14779, total = 14779
slot update_slots: id  0 | task 19466 | erasing old context checkpoint (pos_min = 9511, pos_max = 10153, size = 15.078 MiB)
slot update_slots: id  0 | task 19466 | created context checkpoint 8 of 8 (pos_min = 14092, pos_max = 14714, size = 14.609 MiB)
slot print_timing: id  0 | task 19466 | 
prompt eval time =     318.53 ms /    68 tokens (    4.68 ms per token,   213.48 tokens per second)
       eval time =    6917.40 ms /   228 tokens (   30.34 ms per token,    32.96 tokens per second)
      total time =    7235.93 ms /   296 tokens
slot      release: id  0 | task 19466 | stop processing: n_tokens = 15006, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 19696 | processing task, is_child = 0
slot update_slots: id  0 | task 19696 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15131
slot update_slots: id  0 | task 19696 | n_tokens = 14982, memory_seq_rm [14982, end)
slot update_slots: id  0 | task 19696 | prompt processing progress, n_tokens = 15067, batch.n_tokens = 85, progress = 0.995770
slot update_slots: id  0 | task 19696 | n_tokens = 15067, memory_seq_rm [15067, end)
slot update_slots: id  0 | task 19696 | prompt processing progress, n_tokens = 15131, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 19696 | prompt done, n_tokens = 15131, batch.n_tokens = 64
slot init_sampler: id  0 | task 19696 | init sampler, took 2.14 ms, tokens: text = 15131, total = 15131
slot update_slots: id  0 | task 19696 | erasing old context checkpoint (pos_min = 10135, pos_max = 10777, size = 15.078 MiB)
slot update_slots: id  0 | task 19696 | created context checkpoint 8 of 8 (pos_min = 14424, pos_max = 15066, size = 15.078 MiB)
slot print_timing: id  0 | task 19696 | 
prompt eval time =     636.31 ms /   149 tokens (    4.27 ms per token,   234.16 tokens per second)
       eval time =    1630.28 ms /    54 tokens (   30.19 ms per token,    33.12 tokens per second)
      total time =    2266.59 ms /   203 tokens
slot      release: id  0 | task 19696 | stop processing: n_tokens = 15184, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.945 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 19752 | processing task, is_child = 0
slot update_slots: id  0 | task 19752 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16035
slot update_slots: id  0 | task 19752 | n_tokens = 15157, memory_seq_rm [15157, end)
slot update_slots: id  0 | task 19752 | prompt processing progress, n_tokens = 15971, batch.n_tokens = 814, progress = 0.996009
slot update_slots: id  0 | task 19752 | n_tokens = 15971, memory_seq_rm [15971, end)
slot update_slots: id  0 | task 19752 | prompt processing progress, n_tokens = 16035, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 19752 | prompt done, n_tokens = 16035, batch.n_tokens = 64
slot init_sampler: id  0 | task 19752 | init sampler, took 3.03 ms, tokens: text = 16035, total = 16035
slot update_slots: id  0 | task 19752 | erasing old context checkpoint (pos_min = 10599, pos_max = 11226, size = 14.726 MiB)
slot update_slots: id  0 | task 19752 | created context checkpoint 8 of 8 (pos_min = 15328, pos_max = 15970, size = 15.078 MiB)
slot print_timing: id  0 | task 19752 | 
prompt eval time =    1779.50 ms /   878 tokens (    2.03 ms per token,   493.40 tokens per second)
       eval time =   15019.74 ms /   494 tokens (   30.40 ms per token,    32.89 tokens per second)
      total time =   16799.23 ms /  1372 tokens
slot      release: id  0 | task 19752 | stop processing: n_tokens = 16528, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20248 | processing task, is_child = 0
slot update_slots: id  0 | task 20248 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16574
slot update_slots: id  0 | task 20248 | n_tokens = 16505, memory_seq_rm [16505, end)
slot update_slots: id  0 | task 20248 | prompt processing progress, n_tokens = 16510, batch.n_tokens = 5, progress = 0.996139
slot update_slots: id  0 | task 20248 | n_tokens = 16510, memory_seq_rm [16510, end)
slot update_slots: id  0 | task 20248 | prompt processing progress, n_tokens = 16574, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20248 | prompt done, n_tokens = 16574, batch.n_tokens = 64
slot init_sampler: id  0 | task 20248 | init sampler, took 2.37 ms, tokens: text = 16574, total = 16574
slot update_slots: id  0 | task 20248 | erasing old context checkpoint (pos_min = 10891, pos_max = 11533, size = 15.078 MiB)
slot update_slots: id  0 | task 20248 | created context checkpoint 8 of 8 (pos_min = 15885, pos_max = 16509, size = 14.656 MiB)
slot print_timing: id  0 | task 20248 | 
prompt eval time =     323.13 ms /    69 tokens (    4.68 ms per token,   213.53 tokens per second)
       eval time =   16473.38 ms /   543 tokens (   30.34 ms per token,    32.96 tokens per second)
      total time =   16796.51 ms /   612 tokens
slot      release: id  0 | task 20248 | stop processing: n_tokens = 17116, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20793 | processing task, is_child = 0
slot update_slots: id  0 | task 20793 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17156
slot update_slots: id  0 | task 20793 | n_tokens = 16651, memory_seq_rm [16651, end)
slot update_slots: id  0 | task 20793 | prompt processing progress, n_tokens = 17092, batch.n_tokens = 441, progress = 0.996270
slot update_slots: id  0 | task 20793 | n_tokens = 17092, memory_seq_rm [17092, end)
slot update_slots: id  0 | task 20793 | prompt processing progress, n_tokens = 17156, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20793 | prompt done, n_tokens = 17156, batch.n_tokens = 64
slot init_sampler: id  0 | task 20793 | init sampler, took 2.65 ms, tokens: text = 17156, total = 17156
slot update_slots: id  0 | task 20793 | erasing old context checkpoint (pos_min = 12286, pos_max = 12928, size = 15.078 MiB)
slot update_slots: id  0 | task 20793 | created context checkpoint 8 of 8 (pos_min = 16516, pos_max = 17091, size = 13.507 MiB)
slot print_timing: id  0 | task 20793 | 
prompt eval time =    1048.67 ms /   505 tokens (    2.08 ms per token,   481.56 tokens per second)
       eval time =    5551.76 ms /   183 tokens (   30.34 ms per token,    32.96 tokens per second)
      total time =    6600.43 ms /   688 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 20793 | stop processing: n_tokens = 17338, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20978 | processing task, is_child = 0
slot update_slots: id  0 | task 20978 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17368
slot update_slots: id  0 | task 20978 | n_tokens = 17167, memory_seq_rm [17167, end)
slot update_slots: id  0 | task 20978 | prompt processing progress, n_tokens = 17304, batch.n_tokens = 137, progress = 0.996315
slot update_slots: id  0 | task 20978 | n_tokens = 17304, memory_seq_rm [17304, end)
slot update_slots: id  0 | task 20978 | prompt processing progress, n_tokens = 17368, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20978 | prompt done, n_tokens = 17368, batch.n_tokens = 64
slot init_sampler: id  0 | task 20978 | init sampler, took 2.46 ms, tokens: text = 17368, total = 17368
slot update_slots: id  0 | task 20978 | erasing old context checkpoint (pos_min = 12829, pos_max = 13471, size = 15.078 MiB)
slot update_slots: id  0 | task 20978 | created context checkpoint 8 of 8 (pos_min = 16788, pos_max = 17303, size = 12.100 MiB)
slot print_timing: id  0 | task 20978 | 
prompt eval time =     631.90 ms /   201 tokens (    3.14 ms per token,   318.09 tokens per second)
       eval time =   12537.47 ms /   413 tokens (   30.36 ms per token,    32.94 tokens per second)
      total time =   13169.37 ms /   614 tokens
slot      release: id  0 | task 20978 | stop processing: n_tokens = 17780, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 21393 | processing task, is_child = 0
slot update_slots: id  0 | task 21393 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17810
slot update_slots: id  0 | task 21393 | n_tokens = 17379, memory_seq_rm [17379, end)
slot update_slots: id  0 | task 21393 | prompt processing progress, n_tokens = 17746, batch.n_tokens = 367, progress = 0.996406
slot update_slots: id  0 | task 21393 | n_tokens = 17746, memory_seq_rm [17746, end)
slot update_slots: id  0 | task 21393 | prompt processing progress, n_tokens = 17810, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 21393 | prompt done, n_tokens = 17810, batch.n_tokens = 64
slot init_sampler: id  0 | task 21393 | init sampler, took 2.53 ms, tokens: text = 17810, total = 17810
slot update_slots: id  0 | task 21393 | erasing old context checkpoint (pos_min = 13216, pos_max = 13858, size = 15.078 MiB)
slot update_slots: id  0 | task 21393 | created context checkpoint 8 of 8 (pos_min = 17167, pos_max = 17745, size = 13.577 MiB)
slot print_timing: id  0 | task 21393 | 
prompt eval time =     995.64 ms /   431 tokens (    2.31 ms per token,   432.89 tokens per second)
       eval time =    4277.96 ms /   141 tokens (   30.34 ms per token,    32.96 tokens per second)
      total time =    5273.60 ms /   572 tokens
slot      release: id  0 | task 21393 | stop processing: n_tokens = 17950, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 21536 | processing task, is_child = 0
slot update_slots: id  0 | task 21536 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17980
slot update_slots: id  0 | task 21536 | n_tokens = 17821, memory_seq_rm [17821, end)
slot update_slots: id  0 | task 21536 | prompt processing progress, n_tokens = 17916, batch.n_tokens = 95, progress = 0.996440
slot update_slots: id  0 | task 21536 | n_tokens = 17916, memory_seq_rm [17916, end)
slot update_slots: id  0 | task 21536 | prompt processing progress, n_tokens = 17980, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 21536 | prompt done, n_tokens = 17980, batch.n_tokens = 64
slot init_sampler: id  0 | task 21536 | init sampler, took 2.58 ms, tokens: text = 17980, total = 17980
slot update_slots: id  0 | task 21536 | erasing old context checkpoint (pos_min = 13431, pos_max = 14055, size = 14.656 MiB)
slot update_slots: id  0 | task 21536 | created context checkpoint 8 of 8 (pos_min = 17307, pos_max = 17915, size = 14.281 MiB)
slot print_timing: id  0 | task 21536 | 
prompt eval time =     576.89 ms /   159 tokens (    3.63 ms per token,   275.62 tokens per second)
       eval time =   26585.86 ms /   874 tokens (   30.42 ms per token,    32.87 tokens per second)
      total time =   27162.75 ms /  1033 tokens
slot      release: id  0 | task 21536 | stop processing: n_tokens = 18853, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22412 | processing task, is_child = 0
slot update_slots: id  0 | task 22412 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18884
slot update_slots: id  0 | task 22412 | n_past = 18146, slot.prompt.tokens.size() = 18853, seq_id = 0, pos_min = 18210, n_swa = 128
slot update_slots: id  0 | task 22412 | restored context checkpoint (pos_min = 17307, pos_max = 17915, size = 14.281 MiB)
slot update_slots: id  0 | task 22412 | n_tokens = 17915, memory_seq_rm [17915, end)
slot update_slots: id  0 | task 22412 | prompt processing progress, n_tokens = 18820, batch.n_tokens = 905, progress = 0.996611
slot update_slots: id  0 | task 22412 | n_tokens = 18820, memory_seq_rm [18820, end)
slot update_slots: id  0 | task 22412 | prompt processing progress, n_tokens = 18884, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22412 | prompt done, n_tokens = 18884, batch.n_tokens = 64
slot init_sampler: id  0 | task 22412 | init sampler, took 2.65 ms, tokens: text = 18884, total = 18884
slot update_slots: id  0 | task 22412 | erasing old context checkpoint (pos_min = 14092, pos_max = 14714, size = 14.609 MiB)
slot update_slots: id  0 | task 22412 | created context checkpoint 8 of 8 (pos_min = 18199, pos_max = 18819, size = 14.562 MiB)
slot print_timing: id  0 | task 22412 | 
prompt eval time =    1950.30 ms /   969 tokens (    2.01 ms per token,   496.85 tokens per second)
       eval time =    6849.10 ms /   224 tokens (   30.58 ms per token,    32.71 tokens per second)
      total time =    8799.40 ms /  1193 tokens
slot      release: id  0 | task 22412 | stop processing: n_tokens = 19107, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22638 | processing task, is_child = 0
slot update_slots: id  0 | task 22638 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19155
slot update_slots: id  0 | task 22638 | n_tokens = 19087, memory_seq_rm [19087, end)
slot update_slots: id  0 | task 22638 | prompt processing progress, n_tokens = 19091, batch.n_tokens = 4, progress = 0.996659
slot update_slots: id  0 | task 22638 | n_tokens = 19091, memory_seq_rm [19091, end)
slot update_slots: id  0 | task 22638 | prompt processing progress, n_tokens = 19155, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22638 | prompt done, n_tokens = 19155, batch.n_tokens = 64
slot init_sampler: id  0 | task 22638 | init sampler, took 2.78 ms, tokens: text = 19155, total = 19155
slot update_slots: id  0 | task 22638 | erasing old context checkpoint (pos_min = 14424, pos_max = 15066, size = 15.078 MiB)
slot update_slots: id  0 | task 22638 | created context checkpoint 8 of 8 (pos_min = 18464, pos_max = 19090, size = 14.703 MiB)
slot print_timing: id  0 | task 22638 | 
prompt eval time =     387.50 ms /    68 tokens (    5.70 ms per token,   175.48 tokens per second)
       eval time =   27636.32 ms /   908 tokens (   30.44 ms per token,    32.86 tokens per second)
      total time =   28023.82 ms /   976 tokens
slot      release: id  0 | task 22638 | stop processing: n_tokens = 20062, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 23548 | processing task, is_child = 0
slot update_slots: id  0 | task 23548 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 20094
slot update_slots: id  0 | task 23548 | n_tokens = 19717, memory_seq_rm [19717, end)
slot update_slots: id  0 | task 23548 | prompt processing progress, n_tokens = 20030, batch.n_tokens = 313, progress = 0.996815
slot update_slots: id  0 | task 23548 | n_tokens = 20030, memory_seq_rm [20030, end)
slot update_slots: id  0 | task 23548 | prompt processing progress, n_tokens = 20094, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 23548 | prompt done, n_tokens = 20094, batch.n_tokens = 64
slot init_sampler: id  0 | task 23548 | init sampler, took 3.81 ms, tokens: text = 20094, total = 20094
slot update_slots: id  0 | task 23548 | erasing old context checkpoint (pos_min = 15328, pos_max = 15970, size = 15.078 MiB)
slot update_slots: id  0 | task 23548 | created context checkpoint 8 of 8 (pos_min = 19419, pos_max = 20029, size = 14.328 MiB)
slot print_timing: id  0 | task 23548 | 
prompt eval time =     928.45 ms /   377 tokens (    2.46 ms per token,   406.05 tokens per second)
       eval time =    5692.38 ms /   187 tokens (   30.44 ms per token,    32.85 tokens per second)
      total time =    6620.83 ms /   564 tokens
slot      release: id  0 | task 23548 | stop processing: n_tokens = 20280, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.126 (> 0.100 thold), f_keep = 0.102
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 20280, total state size = 490.623 MiB
srv          load:  - looking for better prompt, base f_keep = 0.102, sim = 0.126
srv        update:  - cache state: 5 prompts, 2033.270 MiB (limits: 8192.000 MiB, 56064 tokens, 256146 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv  get_availabl: prompt cache update took 612.88 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 23737 | processing task, is_child = 0
slot update_slots: id  0 | task 23737 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16404
slot update_slots: id  0 | task 23737 | n_past = 2065, slot.prompt.tokens.size() = 20280, seq_id = 0, pos_min = 19637, n_swa = 128
slot update_slots: id  0 | task 23737 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 15885, pos_max = 16509, n_swa = 128, size = 14.656 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 16516, pos_max = 17091, n_swa = 128, size = 13.507 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 16788, pos_max = 17303, n_swa = 128, size = 12.100 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 17167, pos_max = 17745, n_swa = 128, size = 13.577 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 17307, pos_max = 17915, n_swa = 128, size = 14.281 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 18199, pos_max = 18819, n_swa = 128, size = 14.562 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 18464, pos_max = 19090, n_swa = 128, size = 14.703 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 19419, pos_max = 20029, n_swa = 128, size = 14.328 MiB)
slot update_slots: id  0 | task 23737 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.124848
slot update_slots: id  0 | task 23737 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.249695
slot update_slots: id  0 | task 23737 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.374543
slot update_slots: id  0 | task 23737 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.499390
slot update_slots: id  0 | task 23737 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.624238
slot update_slots: id  0 | task 23737 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.749086
slot update_slots: id  0 | task 23737 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.873933
slot update_slots: id  0 | task 23737 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 16340, batch.n_tokens = 2004, progress = 0.996099
slot update_slots: id  0 | task 23737 | n_tokens = 16340, memory_seq_rm [16340, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 16404, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 23737 | prompt done, n_tokens = 16404, batch.n_tokens = 64
slot init_sampler: id  0 | task 23737 | init sampler, took 4.28 ms, tokens: text = 16404, total = 16404
slot update_slots: id  0 | task 23737 | created context checkpoint 1 of 8 (pos_min = 15697, pos_max = 16339, size = 15.078 MiB)
slot print_timing: id  0 | task 23737 | 
prompt eval time =   26032.02 ms / 16404 tokens (    1.59 ms per token,   630.15 tokens per second)
       eval time =   60755.76 ms /  1980 tokens (   30.68 ms per token,    32.59 tokens per second)
      total time =   86787.79 ms / 18384 tokens
slot      release: id  0 | task 23737 | stop processing: n_tokens = 18383, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.892
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 25726 | processing task, is_child = 0
slot update_slots: id  0 | task 25726 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17262
slot update_slots: id  0 | task 25726 | n_past = 16405, slot.prompt.tokens.size() = 18383, seq_id = 0, pos_min = 17740, n_swa = 128
slot update_slots: id  0 | task 25726 | restored context checkpoint (pos_min = 15697, pos_max = 16339, size = 15.078 MiB)
slot update_slots: id  0 | task 25726 | n_tokens = 16339, memory_seq_rm [16339, end)
slot update_slots: id  0 | task 25726 | prompt processing progress, n_tokens = 17198, batch.n_tokens = 859, progress = 0.996292
slot update_slots: id  0 | task 25726 | n_tokens = 17198, memory_seq_rm [17198, end)
slot update_slots: id  0 | task 25726 | prompt processing progress, n_tokens = 17262, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 25726 | prompt done, n_tokens = 17262, batch.n_tokens = 64
slot init_sampler: id  0 | task 25726 | init sampler, took 2.44 ms, tokens: text = 17262, total = 17262
slot update_slots: id  0 | task 25726 | created context checkpoint 2 of 8 (pos_min = 16555, pos_max = 17197, size = 15.078 MiB)
slot print_timing: id  0 | task 25726 | 
prompt eval time =    1940.36 ms /   923 tokens (    2.10 ms per token,   475.69 tokens per second)
       eval time =  126339.09 ms /  4096 tokens (   30.84 ms per token,    32.42 tokens per second)
      total time =  128279.45 ms /  5019 tokens
slot      release: id  0 | task 25726 | stop processing: n_tokens = 21357, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 25726
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.032
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 21357, total state size = 515.877 MiB
srv          load:  - looking for better prompt, base f_keep = 0.032, sim = 0.990
srv        update:  - cache state: 6 prompts, 2579.303 MiB (limits: 8192.000 MiB, 56064 tokens, 269751 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv  get_availabl: prompt cache update took 397.56 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29825 | processing task, is_child = 0
slot update_slots: id  0 | task 29825 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 684
slot update_slots: id  0 | task 29825 | n_past = 677, slot.prompt.tokens.size() = 21357, seq_id = 0, pos_min = 20714, n_swa = 128
slot update_slots: id  0 | task 29825 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 29825 | erased invalidated context checkpoint (pos_min = 15697, pos_max = 16339, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 29825 | erased invalidated context checkpoint (pos_min = 16555, pos_max = 17197, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 29825 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 29825 | prompt processing progress, n_tokens = 620, batch.n_tokens = 620, progress = 0.906433
slot update_slots: id  0 | task 29825 | n_tokens = 620, memory_seq_rm [620, end)
slot update_slots: id  0 | task 29825 | prompt processing progress, n_tokens = 684, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29825 | prompt done, n_tokens = 684, batch.n_tokens = 64
slot init_sampler: id  0 | task 29825 | init sampler, took 0.11 ms, tokens: text = 684, total = 684
slot update_slots: id  0 | task 29825 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 619, size = 14.539 MiB)
slot print_timing: id  0 | task 29825 | 
prompt eval time =    1310.14 ms /   684 tokens (    1.92 ms per token,   522.08 tokens per second)
       eval time =    5188.76 ms /   182 tokens (   28.51 ms per token,    35.08 tokens per second)
      total time =    6498.90 ms /   866 tokens
slot      release: id  0 | task 29825 | stop processing: n_tokens = 865, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 30009 | processing task, is_child = 0
slot update_slots: id  0 | task 30009 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 910
slot update_slots: id  0 | task 30009 | n_tokens = 848, memory_seq_rm [848, end)
slot update_slots: id  0 | task 30009 | prompt processing progress, n_tokens = 910, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  0 | task 30009 | prompt done, n_tokens = 910, batch.n_tokens = 62
slot init_sampler: id  0 | task 30009 | init sampler, took 0.14 ms, tokens: text = 910, total = 910
slot update_slots: id  0 | task 30009 | created context checkpoint 2 of 8 (pos_min = 222, pos_max = 847, size = 14.679 MiB)
slot print_timing: id  0 | task 30009 | 
prompt eval time =     223.84 ms /    62 tokens (    3.61 ms per token,   276.99 tokens per second)
       eval time =     524.52 ms /    19 tokens (   27.61 ms per token,    36.22 tokens per second)
      total time =     748.36 ms /    81 tokens
slot      release: id  0 | task 30009 | stop processing: n_tokens = 928, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.081
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8461, total state size = 201.380 MiB
srv          load:  - looking for better prompt, base f_keep = 0.081, sim = 0.976
srv        update:  - cache state: 7 prompts, 2831.380 MiB (limits: 8192.000 MiB, 56064 tokens, 270215 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv  get_availabl: prompt cache update took 181.57 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30029 | processing task, is_child = 0
slot update_slots: id  1 | task 30029 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 704
slot update_slots: id  1 | task 30029 | n_past = 687, slot.prompt.tokens.size() = 8461, seq_id = 1, pos_min = 8334, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  1 | task 30029 | failed to restore context checkpoint (pos_min = 0, pos_max = 749, size = 17.587 MiB)
slot update_slots: id  1 | task 30029 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 30029 | erased invalidated context checkpoint (pos_min = 5823, pos_max = 6592, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 30029 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 30029 | prompt processing progress, n_tokens = 640, batch.n_tokens = 640, progress = 0.909091
slot update_slots: id  1 | task 30029 | n_tokens = 640, memory_seq_rm [640, end)
slot update_slots: id  1 | task 30029 | prompt processing progress, n_tokens = 704, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30029 | prompt done, n_tokens = 704, batch.n_tokens = 64
slot init_sampler: id  1 | task 30029 | init sampler, took 0.12 ms, tokens: text = 704, total = 704
slot print_timing: id  1 | task 30029 | 
prompt eval time =    1409.63 ms /   704 tokens (    2.00 ms per token,   499.42 tokens per second)
       eval time =    2676.08 ms /    95 tokens (   28.17 ms per token,    35.50 tokens per second)
      total time =    4085.71 ms /   799 tokens
slot      release: id  1 | task 30029 | stop processing: n_tokens = 798, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.307 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30126 | processing task, is_child = 0
slot update_slots: id  1 | task 30126 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2500
slot update_slots: id  1 | task 30126 | n_tokens = 768, memory_seq_rm [768, end)
slot update_slots: id  1 | task 30126 | prompt processing progress, n_tokens = 2436, batch.n_tokens = 1668, progress = 0.974400
slot update_slots: id  1 | task 30126 | n_tokens = 2436, memory_seq_rm [2436, end)
slot update_slots: id  1 | task 30126 | prompt processing progress, n_tokens = 2500, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30126 | prompt done, n_tokens = 2500, batch.n_tokens = 64
slot init_sampler: id  1 | task 30126 | init sampler, took 0.56 ms, tokens: text = 2500, total = 2500
slot update_slots: id  1 | task 30126 | created context checkpoint 3 of 8 (pos_min = 1793, pos_max = 2435, size = 15.078 MiB)
slot print_timing: id  1 | task 30126 | 
prompt eval time =    2835.30 ms /  1732 tokens (    1.64 ms per token,   610.87 tokens per second)
       eval time =    1178.95 ms /    39 tokens (   30.23 ms per token,    33.08 tokens per second)
      total time =    4014.25 ms /  1771 tokens
slot      release: id  1 | task 30126 | stop processing: n_tokens = 2538, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.500 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30167 | processing task, is_child = 0
slot update_slots: id  1 | task 30167 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5020
slot update_slots: id  1 | task 30167 | n_tokens = 2508, memory_seq_rm [2508, end)
slot update_slots: id  1 | task 30167 | prompt processing progress, n_tokens = 4556, batch.n_tokens = 2048, progress = 0.907570
slot update_slots: id  1 | task 30167 | n_tokens = 4556, memory_seq_rm [4556, end)
slot update_slots: id  1 | task 30167 | prompt processing progress, n_tokens = 4956, batch.n_tokens = 400, progress = 0.987251
slot update_slots: id  1 | task 30167 | n_tokens = 4956, memory_seq_rm [4956, end)
slot update_slots: id  1 | task 30167 | prompt processing progress, n_tokens = 5020, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30167 | prompt done, n_tokens = 5020, batch.n_tokens = 64
slot init_sampler: id  1 | task 30167 | init sampler, took 0.77 ms, tokens: text = 5020, total = 5020
slot update_slots: id  1 | task 30167 | created context checkpoint 4 of 8 (pos_min = 4313, pos_max = 4955, size = 15.078 MiB)
slot print_timing: id  1 | task 30167 | 
prompt eval time =    3991.56 ms /  2512 tokens (    1.59 ms per token,   629.33 tokens per second)
       eval time =    1171.18 ms /    39 tokens (   30.03 ms per token,    33.30 tokens per second)
      total time =    5162.74 ms /  2551 tokens
slot      release: id  1 | task 30167 | stop processing: n_tokens = 5058, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.746 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30209 | processing task, is_child = 0
slot update_slots: id  1 | task 30209 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6739
slot update_slots: id  1 | task 30209 | n_tokens = 5028, memory_seq_rm [5028, end)
slot update_slots: id  1 | task 30209 | prompt processing progress, n_tokens = 6675, batch.n_tokens = 1647, progress = 0.990503
slot update_slots: id  1 | task 30209 | n_tokens = 6675, memory_seq_rm [6675, end)
slot update_slots: id  1 | task 30209 | prompt processing progress, n_tokens = 6739, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30209 | prompt done, n_tokens = 6739, batch.n_tokens = 64
slot init_sampler: id  1 | task 30209 | init sampler, took 1.15 ms, tokens: text = 6739, total = 6739
slot update_slots: id  1 | task 30209 | created context checkpoint 5 of 8 (pos_min = 6032, pos_max = 6674, size = 15.078 MiB)
slot print_timing: id  1 | task 30209 | 
prompt eval time =    2960.03 ms /  1711 tokens (    1.73 ms per token,   578.04 tokens per second)
       eval time =    2996.60 ms /   100 tokens (   29.97 ms per token,    33.37 tokens per second)
      total time =    5956.62 ms /  1811 tokens
slot      release: id  1 | task 30209 | stop processing: n_tokens = 6838, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.105 (> 0.100 thold), f_keep = 0.103
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6838, total state size = 175.422 MiB
srv          load:  - looking for better prompt, base f_keep = 0.103, sim = 0.105
srv        update:  - cache state: 8 prompts, 3084.678 MiB (limits: 8192.000 MiB, 56064 tokens, 266186 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv  get_availabl: prompt cache update took 187.39 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30311 | processing task, is_child = 0
slot update_slots: id  1 | task 30311 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6732
slot update_slots: id  1 | task 30311 | n_past = 704, slot.prompt.tokens.size() = 6838, seq_id = 1, pos_min = 6195, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  1 | task 30311 | failed to restore context checkpoint (pos_min = 0, pos_max = 749, size = 17.587 MiB)
slot update_slots: id  1 | task 30311 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 30311 | erased invalidated context checkpoint (pos_min = 1793, pos_max = 2435, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 30311 | erased invalidated context checkpoint (pos_min = 4313, pos_max = 4955, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 30311 | erased invalidated context checkpoint (pos_min = 6032, pos_max = 6674, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 30311 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 30311 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.304219
slot update_slots: id  1 | task 30311 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 30311 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.608437
slot update_slots: id  1 | task 30311 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 30311 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.912656
slot update_slots: id  1 | task 30311 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 30311 | prompt processing progress, n_tokens = 6668, batch.n_tokens = 524, progress = 0.990493
slot update_slots: id  1 | task 30311 | n_tokens = 6668, memory_seq_rm [6668, end)
slot update_slots: id  1 | task 30311 | prompt processing progress, n_tokens = 6732, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30311 | prompt done, n_tokens = 6732, batch.n_tokens = 64
slot init_sampler: id  1 | task 30311 | init sampler, took 0.98 ms, tokens: text = 6732, total = 6732
slot update_slots: id  1 | task 30311 | created context checkpoint 3 of 8 (pos_min = 6025, pos_max = 6667, size = 15.078 MiB)
slot print_timing: id  1 | task 30311 | 
prompt eval time =    9519.14 ms /  6732 tokens (    1.41 ms per token,   707.21 tokens per second)
       eval time =    1095.66 ms /    40 tokens (   27.39 ms per token,    36.51 tokens per second)
      total time =   10614.80 ms /  6772 tokens
slot      release: id  1 | task 30311 | stop processing: n_tokens = 6771, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30356 | processing task, is_child = 0
slot update_slots: id  1 | task 30356 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6867
slot update_slots: id  1 | task 30356 | n_tokens = 6750, memory_seq_rm [6750, end)
slot update_slots: id  1 | task 30356 | prompt processing progress, n_tokens = 6803, batch.n_tokens = 53, progress = 0.990680
slot update_slots: id  1 | task 30356 | n_tokens = 6803, memory_seq_rm [6803, end)
slot update_slots: id  1 | task 30356 | prompt processing progress, n_tokens = 6867, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30356 | prompt done, n_tokens = 6867, batch.n_tokens = 64
slot init_sampler: id  1 | task 30356 | init sampler, took 0.98 ms, tokens: text = 6867, total = 6867
slot update_slots: id  1 | task 30356 | created context checkpoint 4 of 8 (pos_min = 6160, pos_max = 6802, size = 15.078 MiB)
slot print_timing: id  1 | task 30356 | 
prompt eval time =     387.69 ms /   117 tokens (    3.31 ms per token,   301.79 tokens per second)
       eval time =    1182.81 ms /    42 tokens (   28.16 ms per token,    35.51 tokens per second)
      total time =    1570.50 ms /   159 tokens
slot      release: id  1 | task 30356 | stop processing: n_tokens = 6908, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30400 | processing task, is_child = 0
slot update_slots: id  1 | task 30400 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6950
slot update_slots: id  1 | task 30400 | n_tokens = 6877, memory_seq_rm [6877, end)
slot update_slots: id  1 | task 30400 | prompt processing progress, n_tokens = 6886, batch.n_tokens = 9, progress = 0.990791
slot update_slots: id  1 | task 30400 | n_tokens = 6886, memory_seq_rm [6886, end)
slot update_slots: id  1 | task 30400 | prompt processing progress, n_tokens = 6950, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30400 | prompt done, n_tokens = 6950, batch.n_tokens = 64
slot init_sampler: id  1 | task 30400 | init sampler, took 1.01 ms, tokens: text = 6950, total = 6950
slot update_slots: id  1 | task 30400 | created context checkpoint 5 of 8 (pos_min = 6265, pos_max = 6885, size = 14.562 MiB)
slot print_timing: id  1 | task 30400 | 
prompt eval time =     312.56 ms /    73 tokens (    4.28 ms per token,   233.55 tokens per second)
       eval time =    1156.85 ms /    41 tokens (   28.22 ms per token,    35.44 tokens per second)
      total time =    1469.41 ms /   114 tokens
slot      release: id  1 | task 30400 | stop processing: n_tokens = 6990, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.938 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30443 | processing task, is_child = 0
slot update_slots: id  1 | task 30443 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7418
slot update_slots: id  1 | task 30443 | n_tokens = 6960, memory_seq_rm [6960, end)
slot update_slots: id  1 | task 30443 | prompt processing progress, n_tokens = 7354, batch.n_tokens = 394, progress = 0.991372
slot update_slots: id  1 | task 30443 | n_tokens = 7354, memory_seq_rm [7354, end)
slot update_slots: id  1 | task 30443 | prompt processing progress, n_tokens = 7418, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30443 | prompt done, n_tokens = 7418, batch.n_tokens = 64
slot init_sampler: id  1 | task 30443 | init sampler, took 1.40 ms, tokens: text = 7418, total = 7418
slot update_slots: id  1 | task 30443 | created context checkpoint 6 of 8 (pos_min = 6711, pos_max = 7353, size = 15.078 MiB)
slot print_timing: id  1 | task 30443 | 
prompt eval time =     852.28 ms /   458 tokens (    1.86 ms per token,   537.38 tokens per second)
       eval time =    1171.21 ms /    40 tokens (   29.28 ms per token,    34.15 tokens per second)
      total time =    2023.50 ms /   498 tokens
slot      release: id  1 | task 30443 | stop processing: n_tokens = 7457, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30485 | processing task, is_child = 0
slot update_slots: id  1 | task 30485 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7485
slot update_slots: id  1 | task 30485 | n_tokens = 7427, memory_seq_rm [7427, end)
slot update_slots: id  1 | task 30485 | prompt processing progress, n_tokens = 7485, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  1 | task 30485 | prompt done, n_tokens = 7485, batch.n_tokens = 58
slot init_sampler: id  1 | task 30485 | init sampler, took 1.76 ms, tokens: text = 7485, total = 7485
slot update_slots: id  1 | task 30485 | created context checkpoint 7 of 8 (pos_min = 6814, pos_max = 7426, size = 14.374 MiB)
slot print_timing: id  1 | task 30485 | 
prompt eval time =     210.64 ms /    58 tokens (    3.63 ms per token,   275.36 tokens per second)
       eval time =    1631.91 ms /    57 tokens (   28.63 ms per token,    34.93 tokens per second)
      total time =    1842.55 ms /   115 tokens
slot      release: id  1 | task 30485 | stop processing: n_tokens = 7541, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.943 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30543 | processing task, is_child = 0
slot update_slots: id  1 | task 30543 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7973
slot update_slots: id  1 | task 30543 | n_tokens = 7515, memory_seq_rm [7515, end)
slot update_slots: id  1 | task 30543 | prompt processing progress, n_tokens = 7909, batch.n_tokens = 394, progress = 0.991973
slot update_slots: id  1 | task 30543 | n_tokens = 7909, memory_seq_rm [7909, end)
slot update_slots: id  1 | task 30543 | prompt processing progress, n_tokens = 7973, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30543 | prompt done, n_tokens = 7973, batch.n_tokens = 64
slot init_sampler: id  1 | task 30543 | init sampler, took 1.21 ms, tokens: text = 7973, total = 7973
slot update_slots: id  1 | task 30543 | created context checkpoint 8 of 8 (pos_min = 7266, pos_max = 7908, size = 15.078 MiB)
slot print_timing: id  1 | task 30543 | 
prompt eval time =     843.07 ms /   458 tokens (    1.84 ms per token,   543.25 tokens per second)
       eval time =    1306.42 ms /    45 tokens (   29.03 ms per token,    34.45 tokens per second)
      total time =    2149.49 ms /   503 tokens
slot      release: id  1 | task 30543 | stop processing: n_tokens = 8017, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30590 | processing task, is_child = 0
slot update_slots: id  1 | task 30590 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8057
slot update_slots: id  1 | task 30590 | n_tokens = 7997, memory_seq_rm [7997, end)
slot update_slots: id  1 | task 30590 | prompt processing progress, n_tokens = 8057, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  1 | task 30590 | prompt done, n_tokens = 8057, batch.n_tokens = 60
slot init_sampler: id  1 | task 30590 | init sampler, took 1.14 ms, tokens: text = 8057, total = 8057
slot update_slots: id  1 | task 30590 | erasing old context checkpoint (pos_min = 0, pos_max = 641, size = 15.055 MiB)
slot update_slots: id  1 | task 30590 | created context checkpoint 8 of 8 (pos_min = 7374, pos_max = 7996, size = 14.609 MiB)
slot print_timing: id  1 | task 30590 | 
prompt eval time =     211.39 ms /    60 tokens (    3.52 ms per token,   283.83 tokens per second)
       eval time =    1838.03 ms /    63 tokens (   29.18 ms per token,    34.28 tokens per second)
      total time =    2049.42 ms /   123 tokens
slot      release: id  1 | task 30590 | stop processing: n_tokens = 8119, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30654 | processing task, is_child = 0
slot update_slots: id  1 | task 30654 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8263
slot update_slots: id  1 | task 30654 | n_tokens = 8093, memory_seq_rm [8093, end)
slot update_slots: id  1 | task 30654 | prompt processing progress, n_tokens = 8199, batch.n_tokens = 106, progress = 0.992255
slot update_slots: id  1 | task 30654 | n_tokens = 8199, memory_seq_rm [8199, end)
slot update_slots: id  1 | task 30654 | prompt processing progress, n_tokens = 8263, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30654 | prompt done, n_tokens = 8263, batch.n_tokens = 64
slot init_sampler: id  1 | task 30654 | init sampler, took 1.22 ms, tokens: text = 8263, total = 8263
slot update_slots: id  1 | task 30654 | erasing old context checkpoint (pos_min = 0, pos_max = 749, size = 17.587 MiB)
slot update_slots: id  1 | task 30654 | created context checkpoint 8 of 8 (pos_min = 7556, pos_max = 8198, size = 15.078 MiB)
slot print_timing: id  1 | task 30654 | 
prompt eval time =     591.43 ms /   170 tokens (    3.48 ms per token,   287.44 tokens per second)
       eval time =    1062.95 ms /    36 tokens (   29.53 ms per token,    33.87 tokens per second)
      total time =    1654.38 ms /   206 tokens
slot      release: id  1 | task 30654 | stop processing: n_tokens = 8298, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30692 | processing task, is_child = 0
slot update_slots: id  1 | task 30692 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8406
slot update_slots: id  1 | task 30692 | n_tokens = 8272, memory_seq_rm [8272, end)
slot update_slots: id  1 | task 30692 | prompt processing progress, n_tokens = 8342, batch.n_tokens = 70, progress = 0.992386
slot update_slots: id  1 | task 30692 | n_tokens = 8342, memory_seq_rm [8342, end)
slot update_slots: id  1 | task 30692 | prompt processing progress, n_tokens = 8406, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30692 | prompt done, n_tokens = 8406, batch.n_tokens = 64
slot init_sampler: id  1 | task 30692 | init sampler, took 1.20 ms, tokens: text = 8406, total = 8406
slot update_slots: id  1 | task 30692 | erasing old context checkpoint (pos_min = 6025, pos_max = 6667, size = 15.078 MiB)
slot update_slots: id  1 | task 30692 | created context checkpoint 8 of 8 (pos_min = 7699, pos_max = 8341, size = 15.078 MiB)
slot print_timing: id  1 | task 30692 | 
prompt eval time =     543.89 ms /   134 tokens (    4.06 ms per token,   246.37 tokens per second)
       eval time =    1117.76 ms /    38 tokens (   29.41 ms per token,    34.00 tokens per second)
      total time =    1661.65 ms /   172 tokens
slot      release: id  1 | task 30692 | stop processing: n_tokens = 8443, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30732 | processing task, is_child = 0
slot update_slots: id  1 | task 30732 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8737
slot update_slots: id  1 | task 30732 | n_tokens = 8416, memory_seq_rm [8416, end)
slot update_slots: id  1 | task 30732 | prompt processing progress, n_tokens = 8673, batch.n_tokens = 257, progress = 0.992675
slot update_slots: id  1 | task 30732 | n_tokens = 8673, memory_seq_rm [8673, end)
slot update_slots: id  1 | task 30732 | prompt processing progress, n_tokens = 8737, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30732 | prompt done, n_tokens = 8737, batch.n_tokens = 64
slot init_sampler: id  1 | task 30732 | init sampler, took 1.69 ms, tokens: text = 8737, total = 8737
slot update_slots: id  1 | task 30732 | erasing old context checkpoint (pos_min = 6160, pos_max = 6802, size = 15.078 MiB)
slot update_slots: id  1 | task 30732 | created context checkpoint 8 of 8 (pos_min = 8030, pos_max = 8672, size = 15.078 MiB)
slot print_timing: id  1 | task 30732 | 
prompt eval time =     750.82 ms /   321 tokens (    2.34 ms per token,   427.53 tokens per second)
       eval time =    1120.65 ms /    37 tokens (   30.29 ms per token,    33.02 tokens per second)
      total time =    1871.46 ms /   358 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 30732 | stop processing: n_tokens = 8773, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.959 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30771 | processing task, is_child = 0
slot update_slots: id  1 | task 30771 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9125
slot update_slots: id  1 | task 30771 | n_tokens = 8747, memory_seq_rm [8747, end)
slot update_slots: id  1 | task 30771 | prompt processing progress, n_tokens = 9061, batch.n_tokens = 314, progress = 0.992986
slot update_slots: id  1 | task 30771 | n_tokens = 9061, memory_seq_rm [9061, end)
slot update_slots: id  1 | task 30771 | prompt processing progress, n_tokens = 9125, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30771 | prompt done, n_tokens = 9125, batch.n_tokens = 64
slot init_sampler: id  1 | task 30771 | init sampler, took 1.69 ms, tokens: text = 9125, total = 9125
slot update_slots: id  1 | task 30771 | erasing old context checkpoint (pos_min = 6265, pos_max = 6885, size = 14.562 MiB)
slot update_slots: id  1 | task 30771 | created context checkpoint 8 of 8 (pos_min = 8418, pos_max = 9060, size = 15.078 MiB)
slot print_timing: id  1 | task 30771 | 
prompt eval time =     822.13 ms /   378 tokens (    2.17 ms per token,   459.78 tokens per second)
       eval time =    1119.42 ms /    37 tokens (   30.25 ms per token,    33.05 tokens per second)
      total time =    1941.55 ms /   415 tokens
slot      release: id  1 | task 30771 | stop processing: n_tokens = 9161, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30810 | processing task, is_child = 0
slot update_slots: id  1 | task 30810 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9577
slot update_slots: id  1 | task 30810 | n_tokens = 9135, memory_seq_rm [9135, end)
slot update_slots: id  1 | task 30810 | prompt processing progress, n_tokens = 9513, batch.n_tokens = 378, progress = 0.993317
slot update_slots: id  1 | task 30810 | n_tokens = 9513, memory_seq_rm [9513, end)
slot update_slots: id  1 | task 30810 | prompt processing progress, n_tokens = 9577, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30810 | prompt done, n_tokens = 9577, batch.n_tokens = 64
slot init_sampler: id  1 | task 30810 | init sampler, took 1.35 ms, tokens: text = 9577, total = 9577
slot update_slots: id  1 | task 30810 | erasing old context checkpoint (pos_min = 6711, pos_max = 7353, size = 15.078 MiB)
slot update_slots: id  1 | task 30810 | created context checkpoint 8 of 8 (pos_min = 8870, pos_max = 9512, size = 15.078 MiB)
slot print_timing: id  1 | task 30810 | 
prompt eval time =     909.88 ms /   442 tokens (    2.06 ms per token,   485.78 tokens per second)
       eval time =    1108.21 ms /    37 tokens (   29.95 ms per token,    33.39 tokens per second)
      total time =    2018.10 ms /   479 tokens
slot      release: id  1 | task 30810 | stop processing: n_tokens = 9613, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30849 | processing task, is_child = 0
slot update_slots: id  1 | task 30849 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10052
slot update_slots: id  1 | task 30849 | n_tokens = 9587, memory_seq_rm [9587, end)
slot update_slots: id  1 | task 30849 | prompt processing progress, n_tokens = 9988, batch.n_tokens = 401, progress = 0.993633
slot update_slots: id  1 | task 30849 | n_tokens = 9988, memory_seq_rm [9988, end)
slot update_slots: id  1 | task 30849 | prompt processing progress, n_tokens = 10052, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30849 | prompt done, n_tokens = 10052, batch.n_tokens = 64
slot init_sampler: id  1 | task 30849 | init sampler, took 1.99 ms, tokens: text = 10052, total = 10052
slot update_slots: id  1 | task 30849 | erasing old context checkpoint (pos_min = 6814, pos_max = 7426, size = 14.374 MiB)
slot update_slots: id  1 | task 30849 | created context checkpoint 8 of 8 (pos_min = 9345, pos_max = 9987, size = 15.078 MiB)
slot print_timing: id  1 | task 30849 | 
prompt eval time =     906.16 ms /   465 tokens (    1.95 ms per token,   513.15 tokens per second)
       eval time =    1078.67 ms /    36 tokens (   29.96 ms per token,    33.37 tokens per second)
      total time =    1984.84 ms /   501 tokens
slot      release: id  1 | task 30849 | stop processing: n_tokens = 10087, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30887 | processing task, is_child = 0
slot update_slots: id  1 | task 30887 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10257
slot update_slots: id  1 | task 30887 | n_tokens = 10061, memory_seq_rm [10061, end)
slot update_slots: id  1 | task 30887 | prompt processing progress, n_tokens = 10193, batch.n_tokens = 132, progress = 0.993760
slot update_slots: id  1 | task 30887 | n_tokens = 10193, memory_seq_rm [10193, end)
slot update_slots: id  1 | task 30887 | prompt processing progress, n_tokens = 10257, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30887 | prompt done, n_tokens = 10257, batch.n_tokens = 64
slot init_sampler: id  1 | task 30887 | init sampler, took 1.45 ms, tokens: text = 10257, total = 10257
slot update_slots: id  1 | task 30887 | erasing old context checkpoint (pos_min = 7266, pos_max = 7908, size = 15.078 MiB)
slot update_slots: id  1 | task 30887 | created context checkpoint 8 of 8 (pos_min = 9550, pos_max = 10192, size = 15.078 MiB)
slot print_timing: id  1 | task 30887 | 
prompt eval time =     635.01 ms /   196 tokens (    3.24 ms per token,   308.66 tokens per second)
       eval time =    1145.54 ms /    38 tokens (   30.15 ms per token,    33.17 tokens per second)
      total time =    1780.55 ms /   234 tokens
slot      release: id  1 | task 30887 | stop processing: n_tokens = 10294, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30927 | processing task, is_child = 0
slot update_slots: id  1 | task 30927 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10930
slot update_slots: id  1 | task 30927 | n_tokens = 10268, memory_seq_rm [10268, end)
slot update_slots: id  1 | task 30927 | prompt processing progress, n_tokens = 10866, batch.n_tokens = 598, progress = 0.994145
slot update_slots: id  1 | task 30927 | n_tokens = 10866, memory_seq_rm [10866, end)
slot update_slots: id  1 | task 30927 | prompt processing progress, n_tokens = 10930, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30927 | prompt done, n_tokens = 10930, batch.n_tokens = 64
slot init_sampler: id  1 | task 30927 | init sampler, took 1.56 ms, tokens: text = 10930, total = 10930
slot update_slots: id  1 | task 30927 | erasing old context checkpoint (pos_min = 7374, pos_max = 7996, size = 14.609 MiB)
slot update_slots: id  1 | task 30927 | created context checkpoint 8 of 8 (pos_min = 10350, pos_max = 10865, size = 12.100 MiB)
slot print_timing: id  1 | task 30927 | 
prompt eval time =    1433.20 ms /   662 tokens (    2.16 ms per token,   461.90 tokens per second)
       eval time =    1123.81 ms /    37 tokens (   30.37 ms per token,    32.92 tokens per second)
      total time =    2557.02 ms /   699 tokens
slot      release: id  1 | task 30927 | stop processing: n_tokens = 10966, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30966 | processing task, is_child = 0
slot update_slots: id  1 | task 30966 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11207
slot update_slots: id  1 | task 30966 | n_tokens = 10939, memory_seq_rm [10939, end)
slot update_slots: id  1 | task 30966 | prompt processing progress, n_tokens = 11143, batch.n_tokens = 204, progress = 0.994289
slot update_slots: id  1 | task 30966 | n_tokens = 11143, memory_seq_rm [11143, end)
slot update_slots: id  1 | task 30966 | prompt processing progress, n_tokens = 11207, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30966 | prompt done, n_tokens = 11207, batch.n_tokens = 64
slot init_sampler: id  1 | task 30966 | init sampler, took 2.15 ms, tokens: text = 11207, total = 11207
slot update_slots: id  1 | task 30966 | erasing old context checkpoint (pos_min = 7556, pos_max = 8198, size = 15.078 MiB)
slot update_slots: id  1 | task 30966 | created context checkpoint 8 of 8 (pos_min = 10627, pos_max = 11142, size = 12.100 MiB)
slot print_timing: id  1 | task 30966 | 
prompt eval time =     695.13 ms /   268 tokens (    2.59 ms per token,   385.54 tokens per second)
       eval time =    1149.31 ms /    38 tokens (   30.25 ms per token,    33.06 tokens per second)
      total time =    1844.44 ms /   306 tokens
slot      release: id  1 | task 30966 | stop processing: n_tokens = 11244, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31006 | processing task, is_child = 0
slot update_slots: id  1 | task 31006 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11338
slot update_slots: id  1 | task 31006 | n_tokens = 11218, memory_seq_rm [11218, end)
slot update_slots: id  1 | task 31006 | prompt processing progress, n_tokens = 11274, batch.n_tokens = 56, progress = 0.994355
slot update_slots: id  1 | task 31006 | n_tokens = 11274, memory_seq_rm [11274, end)
slot update_slots: id  1 | task 31006 | prompt processing progress, n_tokens = 11338, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31006 | prompt done, n_tokens = 11338, batch.n_tokens = 64
slot init_sampler: id  1 | task 31006 | init sampler, took 1.62 ms, tokens: text = 11338, total = 11338
slot update_slots: id  1 | task 31006 | erasing old context checkpoint (pos_min = 7699, pos_max = 8341, size = 15.078 MiB)
slot update_slots: id  1 | task 31006 | created context checkpoint 8 of 8 (pos_min = 10758, pos_max = 11273, size = 12.100 MiB)
slot print_timing: id  1 | task 31006 | 
prompt eval time =     479.79 ms /   120 tokens (    4.00 ms per token,   250.11 tokens per second)
       eval time =    1064.85 ms /    36 tokens (   29.58 ms per token,    33.81 tokens per second)
      total time =    1544.63 ms /   156 tokens
slot      release: id  1 | task 31006 | stop processing: n_tokens = 11373, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.881 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31044 | processing task, is_child = 0
slot update_slots: id  1 | task 31044 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12881
slot update_slots: id  1 | task 31044 | n_tokens = 11348, memory_seq_rm [11348, end)
slot update_slots: id  1 | task 31044 | prompt processing progress, n_tokens = 12817, batch.n_tokens = 1469, progress = 0.995031
slot update_slots: id  1 | task 31044 | n_tokens = 12817, memory_seq_rm [12817, end)
slot update_slots: id  1 | task 31044 | prompt processing progress, n_tokens = 12881, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31044 | prompt done, n_tokens = 12881, batch.n_tokens = 64
slot init_sampler: id  1 | task 31044 | init sampler, took 1.81 ms, tokens: text = 12881, total = 12881
slot update_slots: id  1 | task 31044 | erasing old context checkpoint (pos_min = 8030, pos_max = 8672, size = 15.078 MiB)
slot update_slots: id  1 | task 31044 | created context checkpoint 8 of 8 (pos_min = 12174, pos_max = 12816, size = 15.078 MiB)
slot print_timing: id  1 | task 31044 | 
prompt eval time =    2559.44 ms /  1533 tokens (    1.67 ms per token,   598.96 tokens per second)
       eval time =    1075.42 ms /    37 tokens (   29.07 ms per token,    34.41 tokens per second)
      total time =    3634.85 ms /  1570 tokens
slot      release: id  1 | task 31044 | stop processing: n_tokens = 12917, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31083 | processing task, is_child = 0
slot update_slots: id  1 | task 31083 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12949
slot update_slots: id  1 | task 31083 | n_tokens = 12892, memory_seq_rm [12892, end)
slot update_slots: id  1 | task 31083 | prompt processing progress, n_tokens = 12949, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  1 | task 31083 | prompt done, n_tokens = 12949, batch.n_tokens = 57
slot init_sampler: id  1 | task 31083 | init sampler, took 1.92 ms, tokens: text = 12949, total = 12949
slot update_slots: id  1 | task 31083 | erasing old context checkpoint (pos_min = 8418, pos_max = 9060, size = 15.078 MiB)
slot update_slots: id  1 | task 31083 | created context checkpoint 8 of 8 (pos_min = 12274, pos_max = 12891, size = 14.492 MiB)
slot print_timing: id  1 | task 31083 | 
prompt eval time =     202.20 ms /    57 tokens (    3.55 ms per token,   281.90 tokens per second)
       eval time =    1013.70 ms /    36 tokens (   28.16 ms per token,    35.51 tokens per second)
      total time =    1215.90 ms /    93 tokens
slot      release: id  1 | task 31083 | stop processing: n_tokens = 12984, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.884 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31120 | processing task, is_child = 0
slot update_slots: id  1 | task 31120 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14656
slot update_slots: id  1 | task 31120 | n_tokens = 12958, memory_seq_rm [12958, end)
slot update_slots: id  1 | task 31120 | prompt processing progress, n_tokens = 14592, batch.n_tokens = 1634, progress = 0.995633
slot update_slots: id  1 | task 31120 | n_tokens = 14592, memory_seq_rm [14592, end)
slot update_slots: id  1 | task 31120 | prompt processing progress, n_tokens = 14656, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31120 | prompt done, n_tokens = 14656, batch.n_tokens = 64
slot init_sampler: id  1 | task 31120 | init sampler, took 2.77 ms, tokens: text = 14656, total = 14656
slot update_slots: id  1 | task 31120 | erasing old context checkpoint (pos_min = 8870, pos_max = 9512, size = 15.078 MiB)
slot update_slots: id  1 | task 31120 | created context checkpoint 8 of 8 (pos_min = 13949, pos_max = 14591, size = 15.078 MiB)
slot print_timing: id  1 | task 31120 | 
prompt eval time =    2988.41 ms /  1698 tokens (    1.76 ms per token,   568.19 tokens per second)
       eval time =    1032.80 ms /    35 tokens (   29.51 ms per token,    33.89 tokens per second)
      total time =    4021.21 ms /  1733 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 31120 | stop processing: n_tokens = 14690, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31157 | processing task, is_child = 0
slot update_slots: id  1 | task 31157 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15238
slot update_slots: id  1 | task 31157 | n_tokens = 14665, memory_seq_rm [14665, end)
slot update_slots: id  1 | task 31157 | prompt processing progress, n_tokens = 15174, batch.n_tokens = 509, progress = 0.995800
slot update_slots: id  1 | task 31157 | n_tokens = 15174, memory_seq_rm [15174, end)
slot update_slots: id  1 | task 31157 | prompt processing progress, n_tokens = 15238, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31157 | prompt done, n_tokens = 15238, batch.n_tokens = 64
slot init_sampler: id  1 | task 31157 | init sampler, took 3.03 ms, tokens: text = 15238, total = 15238
slot update_slots: id  1 | task 31157 | erasing old context checkpoint (pos_min = 9345, pos_max = 9987, size = 15.078 MiB)
slot update_slots: id  1 | task 31157 | created context checkpoint 8 of 8 (pos_min = 14531, pos_max = 15173, size = 15.078 MiB)
slot print_timing: id  1 | task 31157 | 
prompt eval time =    1050.22 ms /   573 tokens (    1.83 ms per token,   545.60 tokens per second)
       eval time =    1091.35 ms /    38 tokens (   28.72 ms per token,    34.82 tokens per second)
      total time =    2141.57 ms /   611 tokens
slot      release: id  1 | task 31157 | stop processing: n_tokens = 15275, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31197 | processing task, is_child = 0
slot update_slots: id  1 | task 31197 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15981
slot update_slots: id  1 | task 31197 | n_tokens = 15248, memory_seq_rm [15248, end)
slot update_slots: id  1 | task 31197 | prompt processing progress, n_tokens = 15917, batch.n_tokens = 669, progress = 0.995995
slot update_slots: id  1 | task 31197 | n_tokens = 15917, memory_seq_rm [15917, end)
slot update_slots: id  1 | task 31197 | prompt processing progress, n_tokens = 15981, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31197 | prompt done, n_tokens = 15981, batch.n_tokens = 64
slot init_sampler: id  1 | task 31197 | init sampler, took 2.24 ms, tokens: text = 15981, total = 15981
slot update_slots: id  1 | task 31197 | erasing old context checkpoint (pos_min = 9550, pos_max = 10192, size = 15.078 MiB)
slot update_slots: id  1 | task 31197 | created context checkpoint 8 of 8 (pos_min = 15274, pos_max = 15916, size = 15.078 MiB)
slot print_timing: id  1 | task 31197 | 
prompt eval time =    1415.22 ms /   733 tokens (    1.93 ms per token,   517.94 tokens per second)
       eval time =     977.45 ms /    34 tokens (   28.75 ms per token,    34.78 tokens per second)
      total time =    2392.67 ms /   767 tokens
slot      release: id  1 | task 31197 | stop processing: n_tokens = 16014, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31233 | processing task, is_child = 0
slot update_slots: id  1 | task 31233 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16471
slot update_slots: id  1 | task 31233 | n_tokens = 15990, memory_seq_rm [15990, end)
slot update_slots: id  1 | task 31233 | prompt processing progress, n_tokens = 16407, batch.n_tokens = 417, progress = 0.996114
slot update_slots: id  1 | task 31233 | n_tokens = 16407, memory_seq_rm [16407, end)
slot update_slots: id  1 | task 31233 | prompt processing progress, n_tokens = 16471, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31233 | prompt done, n_tokens = 16471, batch.n_tokens = 64
slot init_sampler: id  1 | task 31233 | init sampler, took 2.32 ms, tokens: text = 16471, total = 16471
slot update_slots: id  1 | task 31233 | erasing old context checkpoint (pos_min = 10350, pos_max = 10865, size = 12.100 MiB)
slot update_slots: id  1 | task 31233 | created context checkpoint 8 of 8 (pos_min = 15764, pos_max = 16406, size = 15.078 MiB)
slot print_timing: id  1 | task 31233 | 
prompt eval time =     912.25 ms /   481 tokens (    1.90 ms per token,   527.27 tokens per second)
       eval time =     953.40 ms /    34 tokens (   28.04 ms per token,    35.66 tokens per second)
      total time =    1865.65 ms /   515 tokens
slot      release: id  1 | task 31233 | stop processing: n_tokens = 16504, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31269 | processing task, is_child = 0
slot update_slots: id  1 | task 31269 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16551
slot update_slots: id  1 | task 31269 | n_tokens = 16480, memory_seq_rm [16480, end)
slot update_slots: id  1 | task 31269 | prompt processing progress, n_tokens = 16487, batch.n_tokens = 7, progress = 0.996133
slot update_slots: id  1 | task 31269 | n_tokens = 16487, memory_seq_rm [16487, end)
slot update_slots: id  1 | task 31269 | prompt processing progress, n_tokens = 16551, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31269 | prompt done, n_tokens = 16551, batch.n_tokens = 64
slot init_sampler: id  1 | task 31269 | init sampler, took 2.33 ms, tokens: text = 16551, total = 16551
slot update_slots: id  1 | task 31269 | erasing old context checkpoint (pos_min = 10627, pos_max = 11142, size = 12.100 MiB)
slot update_slots: id  1 | task 31269 | created context checkpoint 8 of 8 (pos_min = 15861, pos_max = 16486, size = 14.679 MiB)
slot print_timing: id  1 | task 31269 | 
prompt eval time =     309.90 ms /    71 tokens (    4.36 ms per token,   229.11 tokens per second)
       eval time =    3198.38 ms /   111 tokens (   28.81 ms per token,    34.71 tokens per second)
      total time =    3508.28 ms /   182 tokens
slot      release: id  1 | task 31269 | stop processing: n_tokens = 16661, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.413 (> 0.100 thold), f_keep = 0.404
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 16661, total state size = 405.761 MiB
srv          load:  - looking for better prompt, base f_keep = 0.404, sim = 0.413
srv        update:  - cache state: 9 prompts, 3607.100 MiB (limits: 8192.000 MiB, 56064 tokens, 265472 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv  get_availabl: prompt cache update took 393.49 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31382 | processing task, is_child = 0
slot update_slots: id  1 | task 31382 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16293
slot update_slots: id  1 | task 31382 | n_past = 6732, slot.prompt.tokens.size() = 16661, seq_id = 1, pos_min = 16018, n_swa = 128
slot update_slots: id  1 | task 31382 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 10758, pos_max = 11273, n_swa = 128, size = 12.100 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 12174, pos_max = 12816, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 12274, pos_max = 12891, n_swa = 128, size = 14.492 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 13949, pos_max = 14591, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 14531, pos_max = 15173, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 15274, pos_max = 15916, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 15764, pos_max = 16406, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 15861, pos_max = 16486, n_swa = 128, size = 14.679 MiB)
slot update_slots: id  1 | task 31382 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 31382 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.125698
slot update_slots: id  1 | task 31382 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 31382 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.251396
slot update_slots: id  1 | task 31382 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 31382 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.377094
srv          stop: cancel task, id_task = 31382
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 31382 | stop processing: n_tokens = 6144, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.730
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31387 | processing task, is_child = 0
slot update_slots: id  0 | task 31387 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 725
slot update_slots: id  0 | task 31387 | n_past = 677, slot.prompt.tokens.size() = 928, seq_id = 0, pos_min = 801, n_swa = 128
slot update_slots: id  0 | task 31387 | restored context checkpoint (pos_min = 222, pos_max = 847, size = 14.679 MiB)
slot update_slots: id  0 | task 31387 | n_tokens = 677, memory_seq_rm [677, end)
slot update_slots: id  0 | task 31387 | prompt processing progress, n_tokens = 725, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  0 | task 31387 | prompt done, n_tokens = 725, batch.n_tokens = 48
slot init_sampler: id  0 | task 31387 | init sampler, took 0.16 ms, tokens: text = 725, total = 725
slot print_timing: id  0 | task 31387 | 
prompt eval time =     390.74 ms /    48 tokens (    8.14 ms per token,   122.84 tokens per second)
       eval time =    1121.95 ms /    41 tokens (   27.36 ms per token,    36.54 tokens per second)
      total time =    1512.69 ms /    89 tokens
slot      release: id  0 | task 31387 | stop processing: n_tokens = 765, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.487 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31429 | processing task, is_child = 0
slot update_slots: id  0 | task 31429 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1523
slot update_slots: id  0 | task 31429 | n_tokens = 741, memory_seq_rm [741, end)
slot update_slots: id  0 | task 31429 | prompt processing progress, n_tokens = 1459, batch.n_tokens = 718, progress = 0.957978
slot update_slots: id  0 | task 31429 | n_tokens = 1459, memory_seq_rm [1459, end)
slot update_slots: id  0 | task 31429 | prompt processing progress, n_tokens = 1523, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31429 | prompt done, n_tokens = 1523, batch.n_tokens = 64
slot init_sampler: id  0 | task 31429 | init sampler, took 0.22 ms, tokens: text = 1523, total = 1523
slot update_slots: id  0 | task 31429 | created context checkpoint 3 of 8 (pos_min = 816, pos_max = 1458, size = 15.078 MiB)
slot print_timing: id  0 | task 31429 | 
prompt eval time =    1339.32 ms /   782 tokens (    1.71 ms per token,   583.88 tokens per second)
       eval time =    1560.99 ms /    55 tokens (   28.38 ms per token,    35.23 tokens per second)
      total time =    2900.32 ms /   837 tokens
slot      release: id  0 | task 31429 | stop processing: n_tokens = 1577, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.472 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31486 | processing task, is_child = 0
slot update_slots: id  0 | task 31486 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3283
slot update_slots: id  0 | task 31486 | n_tokens = 1551, memory_seq_rm [1551, end)
slot update_slots: id  0 | task 31486 | prompt processing progress, n_tokens = 3219, batch.n_tokens = 1668, progress = 0.980506
slot update_slots: id  0 | task 31486 | n_tokens = 3219, memory_seq_rm [3219, end)
slot update_slots: id  0 | task 31486 | prompt processing progress, n_tokens = 3283, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31486 | prompt done, n_tokens = 3283, batch.n_tokens = 64
slot init_sampler: id  0 | task 31486 | init sampler, took 0.49 ms, tokens: text = 3283, total = 3283
slot update_slots: id  0 | task 31486 | created context checkpoint 4 of 8 (pos_min = 2576, pos_max = 3218, size = 15.078 MiB)
slot print_timing: id  0 | task 31486 | 
prompt eval time =    2817.74 ms /  1732 tokens (    1.63 ms per token,   614.68 tokens per second)
       eval time =    1133.94 ms /    39 tokens (   29.08 ms per token,    34.39 tokens per second)
      total time =    3951.67 ms /  1771 tokens
slot      release: id  0 | task 31486 | stop processing: n_tokens = 3321, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.567 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31527 | processing task, is_child = 0
slot update_slots: id  0 | task 31527 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5803
slot update_slots: id  0 | task 31527 | n_tokens = 3291, memory_seq_rm [3291, end)
slot update_slots: id  0 | task 31527 | prompt processing progress, n_tokens = 5339, batch.n_tokens = 2048, progress = 0.920041
slot update_slots: id  0 | task 31527 | n_tokens = 5339, memory_seq_rm [5339, end)
slot update_slots: id  0 | task 31527 | prompt processing progress, n_tokens = 5739, batch.n_tokens = 400, progress = 0.988971
slot update_slots: id  0 | task 31527 | n_tokens = 5739, memory_seq_rm [5739, end)
slot update_slots: id  0 | task 31527 | prompt processing progress, n_tokens = 5803, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31527 | prompt done, n_tokens = 5803, batch.n_tokens = 64
slot init_sampler: id  0 | task 31527 | init sampler, took 1.09 ms, tokens: text = 5803, total = 5803
slot update_slots: id  0 | task 31527 | created context checkpoint 5 of 8 (pos_min = 5096, pos_max = 5738, size = 15.078 MiB)
slot print_timing: id  0 | task 31527 | 
prompt eval time =    4023.13 ms /  2512 tokens (    1.60 ms per token,   624.39 tokens per second)
       eval time =    1076.63 ms /    35 tokens (   30.76 ms per token,    32.51 tokens per second)
      total time =    5099.76 ms /  2547 tokens
slot      release: id  0 | task 31527 | stop processing: n_tokens = 5837, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.773 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31565 | processing task, is_child = 0
slot update_slots: id  0 | task 31565 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7522
slot update_slots: id  0 | task 31565 | n_tokens = 5811, memory_seq_rm [5811, end)
slot update_slots: id  0 | task 31565 | prompt processing progress, n_tokens = 7458, batch.n_tokens = 1647, progress = 0.991492
slot update_slots: id  0 | task 31565 | n_tokens = 7458, memory_seq_rm [7458, end)
slot update_slots: id  0 | task 31565 | prompt processing progress, n_tokens = 7522, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31565 | prompt done, n_tokens = 7522, batch.n_tokens = 64
slot init_sampler: id  0 | task 31565 | init sampler, took 1.07 ms, tokens: text = 7522, total = 7522
slot update_slots: id  0 | task 31565 | created context checkpoint 6 of 8 (pos_min = 6815, pos_max = 7457, size = 15.078 MiB)
slot print_timing: id  0 | task 31565 | 
prompt eval time =    3022.88 ms /  1711 tokens (    1.77 ms per token,   566.02 tokens per second)
       eval time =    3374.93 ms /   111 tokens (   30.40 ms per token,    32.89 tokens per second)
      total time =    6397.81 ms /  1822 tokens
slot      release: id  0 | task 31565 | stop processing: n_tokens = 7632, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.943 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31678 | processing task, is_child = 0
slot update_slots: id  0 | task 31678 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8064
slot update_slots: id  0 | task 31678 | n_tokens = 7606, memory_seq_rm [7606, end)
slot update_slots: id  0 | task 31678 | prompt processing progress, n_tokens = 8000, batch.n_tokens = 394, progress = 0.992063
slot update_slots: id  0 | task 31678 | n_tokens = 8000, memory_seq_rm [8000, end)
slot update_slots: id  0 | task 31678 | prompt processing progress, n_tokens = 8064, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31678 | prompt done, n_tokens = 8064, batch.n_tokens = 64
slot init_sampler: id  0 | task 31678 | init sampler, took 1.25 ms, tokens: text = 8064, total = 8064
slot update_slots: id  0 | task 31678 | created context checkpoint 7 of 8 (pos_min = 7357, pos_max = 7999, size = 15.078 MiB)
slot print_timing: id  0 | task 31678 | 
prompt eval time =     943.49 ms /   458 tokens (    2.06 ms per token,   485.43 tokens per second)
       eval time =    1229.29 ms /    40 tokens (   30.73 ms per token,    32.54 tokens per second)
      total time =    2172.79 ms /   498 tokens
slot      release: id  0 | task 31678 | stop processing: n_tokens = 8103, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31720 | processing task, is_child = 0
slot update_slots: id  0 | task 31720 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8131
slot update_slots: id  0 | task 31720 | n_tokens = 8073, memory_seq_rm [8073, end)
slot update_slots: id  0 | task 31720 | prompt processing progress, n_tokens = 8131, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 31720 | prompt done, n_tokens = 8131, batch.n_tokens = 58
slot init_sampler: id  0 | task 31720 | init sampler, took 1.16 ms, tokens: text = 8131, total = 8131
slot update_slots: id  0 | task 31720 | created context checkpoint 8 of 8 (pos_min = 7460, pos_max = 8072, size = 14.374 MiB)
slot print_timing: id  0 | task 31720 | 
prompt eval time =     218.80 ms /    58 tokens (    3.77 ms per token,   265.08 tokens per second)
       eval time =    1598.64 ms /    52 tokens (   30.74 ms per token,    32.53 tokens per second)
      total time =    1817.44 ms /   110 tokens
slot      release: id  0 | task 31720 | stop processing: n_tokens = 8182, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31773 | processing task, is_child = 0
slot update_slots: id  0 | task 31773 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8614
slot update_slots: id  0 | task 31773 | n_tokens = 8156, memory_seq_rm [8156, end)
slot update_slots: id  0 | task 31773 | prompt processing progress, n_tokens = 8550, batch.n_tokens = 394, progress = 0.992570
slot update_slots: id  0 | task 31773 | n_tokens = 8550, memory_seq_rm [8550, end)
slot update_slots: id  0 | task 31773 | prompt processing progress, n_tokens = 8614, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31773 | prompt done, n_tokens = 8614, batch.n_tokens = 64
slot init_sampler: id  0 | task 31773 | init sampler, took 1.63 ms, tokens: text = 8614, total = 8614
slot update_slots: id  0 | task 31773 | erasing old context checkpoint (pos_min = 0, pos_max = 619, size = 14.539 MiB)
slot update_slots: id  0 | task 31773 | created context checkpoint 8 of 8 (pos_min = 7907, pos_max = 8549, size = 15.078 MiB)
slot print_timing: id  0 | task 31773 | 
prompt eval time =     944.28 ms /   458 tokens (    2.06 ms per token,   485.02 tokens per second)
       eval time =    1843.76 ms /    61 tokens (   30.23 ms per token,    33.08 tokens per second)
      total time =    2788.05 ms /   519 tokens
slot      release: id  0 | task 31773 | stop processing: n_tokens = 8674, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31836 | processing task, is_child = 0
slot update_slots: id  0 | task 31836 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8841
slot update_slots: id  0 | task 31836 | n_tokens = 8645, memory_seq_rm [8645, end)
slot update_slots: id  0 | task 31836 | prompt processing progress, n_tokens = 8777, batch.n_tokens = 132, progress = 0.992761
slot update_slots: id  0 | task 31836 | n_tokens = 8777, memory_seq_rm [8777, end)
slot update_slots: id  0 | task 31836 | prompt processing progress, n_tokens = 8841, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31836 | prompt done, n_tokens = 8841, batch.n_tokens = 64
slot init_sampler: id  0 | task 31836 | init sampler, took 1.25 ms, tokens: text = 8841, total = 8841
slot update_slots: id  0 | task 31836 | erasing old context checkpoint (pos_min = 222, pos_max = 847, size = 14.679 MiB)
slot update_slots: id  0 | task 31836 | created context checkpoint 8 of 8 (pos_min = 8134, pos_max = 8776, size = 15.078 MiB)
slot print_timing: id  0 | task 31836 | 
prompt eval time =     650.18 ms /   196 tokens (    3.32 ms per token,   301.45 tokens per second)
       eval time =    2353.68 ms /    78 tokens (   30.18 ms per token,    33.14 tokens per second)
      total time =    3003.86 ms /   274 tokens
slot      release: id  0 | task 31836 | stop processing: n_tokens = 8918, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = 1227256082
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9923, total state size = 235.662 MiB
srv          load:  - looking for better prompt, base f_keep = 0.068, sim = 0.078
srv        update:  - cache state: 10 prompts, 3984.373 MiB (limits: 8192.000 MiB, 56064 tokens, 260737 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv  get_availabl: prompt cache update took 282.19 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31916 | processing task, is_child = 0
slot update_slots: id  3 | task 31916 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8667
slot update_slots: id  3 | task 31916 | n_past = 677, slot.prompt.tokens.size() = 9923, seq_id = 3, pos_min = 9796, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 31916 | failed to restore context checkpoint (pos_min = 0, pos_max = 913, size = 21.433 MiB)
slot update_slots: id  3 | task 31916 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 31916 | erased invalidated context checkpoint (pos_min = 677, pos_max = 1397, n_swa = 128, size = 16.907 MiB)
slot update_slots: id  3 | task 31916 | erased invalidated context checkpoint (pos_min = 2140, pos_max = 3163, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 31916 | erased invalidated context checkpoint (pos_min = 4664, pos_max = 5687, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 31916 | erased invalidated context checkpoint (pos_min = 6378, pos_max = 7401, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 31916 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 31916 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.236299
slot update_slots: id  3 | task 31916 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 31916 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.472597
srv          stop: cancel task, id_task = 31916
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 31916 | stop processing: n_tokens = 4096, truncated = 0
srv  update_slots: all slots are idle
