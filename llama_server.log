ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: using cached file: /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14807 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1763 MiB
llama_params_fit_impl: context size reduced from 131072 to 56064 -> need 1767 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 2.01 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14807 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 56064
llama_context: n_ctx_seq     = 56064
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (56064) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 56064 cells
llama_kv_cache:      CUDA0 KV buffer size =  1314.00 MiB
llama_kv_cache: size = 1314.00 MiB ( 56064 cells,  12 layers,  4/1 seqs), K (f16):  657.00 MiB, V (f16):  657.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   117.15 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 75.50 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  1 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  2 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  3 | task -1 | new slot, n_ctx = 56064
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-12

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 964
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 900, batch.n_tokens = 900, progress = 0.933610
slot update_slots: id  3 | task 0 | n_tokens = 900, memory_seq_rm [900, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 964, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 964, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.19 ms, tokens: text = 964, total = 964
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 899, size = 21.104 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =    1367.91 ms /   964 tokens (    1.42 ms per token,   704.73 tokens per second)
       eval time =    1405.93 ms /    53 tokens (   26.53 ms per token,    37.70 tokens per second)
      total time =    2773.83 ms /  1017 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 1016, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.356 (> 0.100 thold), f_keep = 0.949
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 55 | processing task, is_child = 0
slot update_slots: id  3 | task 55 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2709
slot update_slots: id  3 | task 55 | n_tokens = 964, memory_seq_rm [964, end)
slot update_slots: id  3 | task 55 | prompt processing progress, n_tokens = 2645, batch.n_tokens = 1681, progress = 0.976375
slot update_slots: id  3 | task 55 | n_tokens = 2645, memory_seq_rm [2645, end)
slot update_slots: id  3 | task 55 | prompt processing progress, n_tokens = 2709, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 55 | prompt done, n_tokens = 2709, batch.n_tokens = 64
slot init_sampler: id  3 | task 55 | init sampler, took 0.44 ms, tokens: text = 2709, total = 2709
slot update_slots: id  3 | task 55 | created context checkpoint 2 of 8 (pos_min = 1621, pos_max = 2644, size = 24.012 MiB)
slot print_timing: id  3 | task 55 | 
prompt eval time =    2093.90 ms /  1745 tokens (    1.20 ms per token,   833.37 tokens per second)
       eval time =    1142.85 ms /    41 tokens (   27.87 ms per token,    35.88 tokens per second)
      total time =    3236.75 ms /  1786 tokens
slot      release: id  3 | task 55 | stop processing: n_tokens = 2749, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.736 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 98 | processing task, is_child = 0
slot update_slots: id  3 | task 98 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3683
slot update_slots: id  3 | task 98 | n_tokens = 2709, memory_seq_rm [2709, end)
slot update_slots: id  3 | task 98 | prompt processing progress, n_tokens = 3619, batch.n_tokens = 910, progress = 0.982623
slot update_slots: id  3 | task 98 | n_tokens = 3619, memory_seq_rm [3619, end)
slot update_slots: id  3 | task 98 | prompt processing progress, n_tokens = 3683, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 98 | prompt done, n_tokens = 3683, batch.n_tokens = 64
slot init_sampler: id  3 | task 98 | init sampler, took 0.59 ms, tokens: text = 3683, total = 3683
slot update_slots: id  3 | task 98 | created context checkpoint 3 of 8 (pos_min = 2595, pos_max = 3618, size = 24.012 MiB)
slot print_timing: id  3 | task 98 | 
prompt eval time =    1258.48 ms /   974 tokens (    1.29 ms per token,   773.95 tokens per second)
       eval time =    2232.69 ms /    74 tokens (   30.17 ms per token,    33.14 tokens per second)
      total time =    3491.17 ms /  1048 tokens
slot      release: id  3 | task 98 | stop processing: n_tokens = 3756, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.787 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 174 | processing task, is_child = 0
slot update_slots: id  3 | task 174 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4681
slot update_slots: id  3 | task 174 | n_tokens = 3683, memory_seq_rm [3683, end)
slot update_slots: id  3 | task 174 | prompt processing progress, n_tokens = 4617, batch.n_tokens = 934, progress = 0.986328
slot update_slots: id  3 | task 174 | n_tokens = 4617, memory_seq_rm [4617, end)
slot update_slots: id  3 | task 174 | prompt processing progress, n_tokens = 4681, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 174 | prompt done, n_tokens = 4681, batch.n_tokens = 64
slot init_sampler: id  3 | task 174 | init sampler, took 1.26 ms, tokens: text = 4681, total = 4681
slot update_slots: id  3 | task 174 | created context checkpoint 4 of 8 (pos_min = 3593, pos_max = 4616, size = 24.012 MiB)
slot print_timing: id  3 | task 174 | 
prompt eval time =    1282.69 ms /   998 tokens (    1.29 ms per token,   778.05 tokens per second)
       eval time =    4610.28 ms /   152 tokens (   30.33 ms per token,    32.97 tokens per second)
      total time =    5892.97 ms /  1150 tokens
slot      release: id  3 | task 174 | stop processing: n_tokens = 4832, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 328 | processing task, is_child = 0
slot update_slots: id  3 | task 328 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4756
slot update_slots: id  3 | task 328 | n_tokens = 4681, memory_seq_rm [4681, end)
slot update_slots: id  3 | task 328 | prompt processing progress, n_tokens = 4692, batch.n_tokens = 11, progress = 0.986543
slot update_slots: id  3 | task 328 | n_tokens = 4692, memory_seq_rm [4692, end)
slot update_slots: id  3 | task 328 | prompt processing progress, n_tokens = 4756, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 328 | prompt done, n_tokens = 4756, batch.n_tokens = 64
slot init_sampler: id  3 | task 328 | init sampler, took 0.71 ms, tokens: text = 4756, total = 4756
slot update_slots: id  3 | task 328 | created context checkpoint 5 of 8 (pos_min = 3808, pos_max = 4691, size = 20.729 MiB)
slot print_timing: id  3 | task 328 | 
prompt eval time =     287.97 ms /    75 tokens (    3.84 ms per token,   260.45 tokens per second)
       eval time =    2651.61 ms /    88 tokens (   30.13 ms per token,    33.19 tokens per second)
      total time =    2939.58 ms /   163 tokens
slot      release: id  3 | task 328 | stop processing: n_tokens = 4843, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 418 | processing task, is_child = 0
slot update_slots: id  3 | task 418 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4831
slot update_slots: id  3 | task 418 | n_tokens = 4756, memory_seq_rm [4756, end)
slot update_slots: id  3 | task 418 | prompt processing progress, n_tokens = 4767, batch.n_tokens = 11, progress = 0.986752
slot update_slots: id  3 | task 418 | n_tokens = 4767, memory_seq_rm [4767, end)
slot update_slots: id  3 | task 418 | prompt processing progress, n_tokens = 4831, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 418 | prompt done, n_tokens = 4831, batch.n_tokens = 64
slot init_sampler: id  3 | task 418 | init sampler, took 0.78 ms, tokens: text = 4831, total = 4831
slot update_slots: id  3 | task 418 | created context checkpoint 6 of 8 (pos_min = 3819, pos_max = 4766, size = 22.230 MiB)
slot print_timing: id  3 | task 418 | 
prompt eval time =     288.63 ms /    75 tokens (    3.85 ms per token,   259.85 tokens per second)
       eval time =    3466.14 ms /   115 tokens (   30.14 ms per token,    33.18 tokens per second)
      total time =    3754.76 ms /   190 tokens
slot      release: id  3 | task 418 | stop processing: n_tokens = 4945, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 535 | processing task, is_child = 0
slot update_slots: id  3 | task 535 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4900
slot update_slots: id  3 | task 535 | n_tokens = 4831, memory_seq_rm [4831, end)
slot update_slots: id  3 | task 535 | prompt processing progress, n_tokens = 4836, batch.n_tokens = 5, progress = 0.986939
slot update_slots: id  3 | task 535 | n_tokens = 4836, memory_seq_rm [4836, end)
slot update_slots: id  3 | task 535 | prompt processing progress, n_tokens = 4900, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 535 | prompt done, n_tokens = 4900, batch.n_tokens = 64
slot init_sampler: id  3 | task 535 | init sampler, took 1.05 ms, tokens: text = 4900, total = 4900
slot update_slots: id  3 | task 535 | created context checkpoint 7 of 8 (pos_min = 3921, pos_max = 4835, size = 21.456 MiB)
slot print_timing: id  3 | task 535 | 
prompt eval time =     300.87 ms /    69 tokens (    4.36 ms per token,   229.34 tokens per second)
       eval time =    3127.18 ms /   116 tokens (   26.96 ms per token,    37.09 tokens per second)
      total time =    3428.05 ms /   185 tokens
slot      release: id  3 | task 535 | stop processing: n_tokens = 5015, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.834 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 653 | processing task, is_child = 0
slot update_slots: id  3 | task 653 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5874
slot update_slots: id  3 | task 653 | n_tokens = 4900, memory_seq_rm [4900, end)
slot update_slots: id  3 | task 653 | prompt processing progress, n_tokens = 5810, batch.n_tokens = 910, progress = 0.989105
slot update_slots: id  3 | task 653 | n_tokens = 5810, memory_seq_rm [5810, end)
slot update_slots: id  3 | task 653 | prompt processing progress, n_tokens = 5874, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 653 | prompt done, n_tokens = 5874, batch.n_tokens = 64
slot init_sampler: id  3 | task 653 | init sampler, took 0.90 ms, tokens: text = 5874, total = 5874
slot update_slots: id  3 | task 653 | created context checkpoint 8 of 8 (pos_min = 4786, pos_max = 5809, size = 24.012 MiB)
slot print_timing: id  3 | task 653 | 
prompt eval time =    1198.34 ms /   974 tokens (    1.23 ms per token,   812.79 tokens per second)
       eval time =    7369.62 ms /   274 tokens (   26.90 ms per token,    37.18 tokens per second)
      total time =    8567.96 ms /  1248 tokens
slot      release: id  3 | task 653 | stop processing: n_tokens = 6147, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.956
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 929 | processing task, is_child = 0
slot update_slots: id  3 | task 929 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6001
slot update_slots: id  3 | task 929 | n_tokens = 5874, memory_seq_rm [5874, end)
slot update_slots: id  3 | task 929 | prompt processing progress, n_tokens = 5937, batch.n_tokens = 63, progress = 0.989335
slot update_slots: id  3 | task 929 | n_tokens = 5937, memory_seq_rm [5937, end)
slot update_slots: id  3 | task 929 | prompt processing progress, n_tokens = 6001, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 929 | prompt done, n_tokens = 6001, batch.n_tokens = 64
slot init_sampler: id  3 | task 929 | init sampler, took 1.12 ms, tokens: text = 6001, total = 6001
slot update_slots: id  3 | task 929 | erasing old context checkpoint (pos_min = 0, pos_max = 899, size = 21.104 MiB)
slot update_slots: id  3 | task 929 | created context checkpoint 8 of 8 (pos_min = 5183, pos_max = 5936, size = 17.681 MiB)
slot print_timing: id  3 | task 929 | 
prompt eval time =     355.99 ms /   127 tokens (    2.80 ms per token,   356.75 tokens per second)
       eval time =    4840.25 ms /   182 tokens (   26.59 ms per token,    37.60 tokens per second)
      total time =    5196.24 ms /   309 tokens
slot      release: id  3 | task 929 | stop processing: n_tokens = 6182, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.904 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1113 | processing task, is_child = 0
slot update_slots: id  3 | task 1113 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6635
slot update_slots: id  3 | task 1113 | n_tokens = 6001, memory_seq_rm [6001, end)
slot update_slots: id  3 | task 1113 | prompt processing progress, n_tokens = 6571, batch.n_tokens = 570, progress = 0.990354
slot update_slots: id  3 | task 1113 | n_tokens = 6571, memory_seq_rm [6571, end)
slot update_slots: id  3 | task 1113 | prompt processing progress, n_tokens = 6635, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1113 | prompt done, n_tokens = 6635, batch.n_tokens = 64
slot init_sampler: id  3 | task 1113 | init sampler, took 1.28 ms, tokens: text = 6635, total = 6635
slot update_slots: id  3 | task 1113 | erasing old context checkpoint (pos_min = 1621, pos_max = 2644, size = 24.012 MiB)
slot update_slots: id  3 | task 1113 | created context checkpoint 8 of 8 (pos_min = 5817, pos_max = 6570, size = 17.681 MiB)
slot print_timing: id  3 | task 1113 | 
prompt eval time =     940.96 ms /   634 tokens (    1.48 ms per token,   673.78 tokens per second)
       eval time =    3678.85 ms /   143 tokens (   25.73 ms per token,    38.87 tokens per second)
      total time =    4619.81 ms /   777 tokens
slot      release: id  3 | task 1113 | stop processing: n_tokens = 6777, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.910 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1258 | processing task, is_child = 0
slot update_slots: id  3 | task 1258 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7289
slot update_slots: id  3 | task 1258 | n_tokens = 6635, memory_seq_rm [6635, end)
slot update_slots: id  3 | task 1258 | prompt processing progress, n_tokens = 7225, batch.n_tokens = 590, progress = 0.991220
slot update_slots: id  3 | task 1258 | n_tokens = 7225, memory_seq_rm [7225, end)
slot update_slots: id  3 | task 1258 | prompt processing progress, n_tokens = 7289, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1258 | prompt done, n_tokens = 7289, batch.n_tokens = 64
slot init_sampler: id  3 | task 1258 | init sampler, took 1.21 ms, tokens: text = 7289, total = 7289
slot update_slots: id  3 | task 1258 | erasing old context checkpoint (pos_min = 2595, pos_max = 3618, size = 24.012 MiB)
slot update_slots: id  3 | task 1258 | created context checkpoint 8 of 8 (pos_min = 6201, pos_max = 7224, size = 24.012 MiB)
slot print_timing: id  3 | task 1258 | 
prompt eval time =     962.19 ms /   654 tokens (    1.47 ms per token,   679.70 tokens per second)
       eval time =    4740.03 ms /   182 tokens (   26.04 ms per token,    38.40 tokens per second)
      total time =    5702.22 ms /   836 tokens
slot      release: id  3 | task 1258 | stop processing: n_tokens = 7470, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.854 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1442 | processing task, is_child = 0
slot update_slots: id  3 | task 1442 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8539
slot update_slots: id  3 | task 1442 | n_tokens = 7289, memory_seq_rm [7289, end)
slot update_slots: id  3 | task 1442 | prompt processing progress, n_tokens = 8475, batch.n_tokens = 1186, progress = 0.992505
slot update_slots: id  3 | task 1442 | n_tokens = 8475, memory_seq_rm [8475, end)
slot update_slots: id  3 | task 1442 | prompt processing progress, n_tokens = 8539, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1442 | prompt done, n_tokens = 8539, batch.n_tokens = 64
slot init_sampler: id  3 | task 1442 | init sampler, took 1.30 ms, tokens: text = 8539, total = 8539
slot update_slots: id  3 | task 1442 | erasing old context checkpoint (pos_min = 3593, pos_max = 4616, size = 24.012 MiB)
slot update_slots: id  3 | task 1442 | created context checkpoint 8 of 8 (pos_min = 7451, pos_max = 8474, size = 24.012 MiB)
slot print_timing: id  3 | task 1442 | 
prompt eval time =    1651.56 ms /  1250 tokens (    1.32 ms per token,   756.86 tokens per second)
       eval time =   10356.27 ms /   379 tokens (   27.33 ms per token,    36.60 tokens per second)
      total time =   12007.83 ms /  1629 tokens
slot      release: id  3 | task 1442 | stop processing: n_tokens = 8917, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.955 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1823 | processing task, is_child = 0
slot update_slots: id  3 | task 1823 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8943
slot update_slots: id  3 | task 1823 | n_tokens = 8539, memory_seq_rm [8539, end)
slot update_slots: id  3 | task 1823 | prompt processing progress, n_tokens = 8879, batch.n_tokens = 340, progress = 0.992844
slot update_slots: id  3 | task 1823 | n_tokens = 8879, memory_seq_rm [8879, end)
slot update_slots: id  3 | task 1823 | prompt processing progress, n_tokens = 8943, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1823 | prompt done, n_tokens = 8943, batch.n_tokens = 64
slot init_sampler: id  3 | task 1823 | init sampler, took 1.40 ms, tokens: text = 8943, total = 8943
slot update_slots: id  3 | task 1823 | erasing old context checkpoint (pos_min = 3808, pos_max = 4691, size = 20.729 MiB)
slot update_slots: id  3 | task 1823 | created context checkpoint 8 of 8 (pos_min = 7893, pos_max = 8878, size = 23.121 MiB)
slot print_timing: id  3 | task 1823 | 
prompt eval time =     686.56 ms /   404 tokens (    1.70 ms per token,   588.44 tokens per second)
       eval time =    1600.84 ms /    56 tokens (   28.59 ms per token,    34.98 tokens per second)
      total time =    2287.39 ms /   460 tokens
slot      release: id  3 | task 1823 | stop processing: n_tokens = 8998, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.902 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1881 | processing task, is_child = 0
slot update_slots: id  3 | task 1881 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9917
slot update_slots: id  3 | task 1881 | n_tokens = 8943, memory_seq_rm [8943, end)
slot update_slots: id  3 | task 1881 | prompt processing progress, n_tokens = 9853, batch.n_tokens = 910, progress = 0.993546
slot update_slots: id  3 | task 1881 | n_tokens = 9853, memory_seq_rm [9853, end)
slot update_slots: id  3 | task 1881 | prompt processing progress, n_tokens = 9917, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1881 | prompt done, n_tokens = 9917, batch.n_tokens = 64
slot init_sampler: id  3 | task 1881 | init sampler, took 1.79 ms, tokens: text = 9917, total = 9917
slot update_slots: id  3 | task 1881 | erasing old context checkpoint (pos_min = 3819, pos_max = 4766, size = 22.230 MiB)
slot update_slots: id  3 | task 1881 | created context checkpoint 8 of 8 (pos_min = 8832, pos_max = 9852, size = 23.942 MiB)
slot print_timing: id  3 | task 1881 | 
prompt eval time =    1338.93 ms /   974 tokens (    1.37 ms per token,   727.45 tokens per second)
       eval time =    1994.51 ms /    66 tokens (   30.22 ms per token,    33.09 tokens per second)
      total time =    3333.44 ms /  1040 tokens
slot      release: id  3 | task 1881 | stop processing: n_tokens = 9982, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1949 | processing task, is_child = 0
slot update_slots: id  3 | task 1949 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9973
slot update_slots: id  3 | task 1949 | n_tokens = 9917, memory_seq_rm [9917, end)
slot update_slots: id  3 | task 1949 | prompt processing progress, n_tokens = 9973, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  3 | task 1949 | prompt done, n_tokens = 9973, batch.n_tokens = 56
slot init_sampler: id  3 | task 1949 | init sampler, took 2.03 ms, tokens: text = 9973, total = 9973
slot print_timing: id  3 | task 1949 | 
prompt eval time =     192.22 ms /    56 tokens (    3.43 ms per token,   291.33 tokens per second)
       eval time =    1607.53 ms /    54 tokens (   29.77 ms per token,    33.59 tokens per second)
      total time =    1799.75 ms /   110 tokens
slot      release: id  3 | task 1949 | stop processing: n_tokens = 10026, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2004 | processing task, is_child = 0
slot update_slots: id  3 | task 2004 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10028
slot update_slots: id  3 | task 2004 | n_tokens = 9973, memory_seq_rm [9973, end)
slot update_slots: id  3 | task 2004 | prompt processing progress, n_tokens = 10028, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 2004 | prompt done, n_tokens = 10028, batch.n_tokens = 55
slot init_sampler: id  3 | task 2004 | init sampler, took 1.57 ms, tokens: text = 10028, total = 10028
slot update_slots: id  3 | task 2004 | erasing old context checkpoint (pos_min = 3921, pos_max = 4835, size = 21.456 MiB)
slot update_slots: id  3 | task 2004 | created context checkpoint 8 of 8 (pos_min = 9052, pos_max = 9972, size = 21.597 MiB)
slot print_timing: id  3 | task 2004 | 
prompt eval time =     204.37 ms /    55 tokens (    3.72 ms per token,   269.12 tokens per second)
       eval time =    3969.29 ms /   131 tokens (   30.30 ms per token,    33.00 tokens per second)
      total time =    4173.67 ms /   186 tokens
slot      release: id  3 | task 2004 | stop processing: n_tokens = 10158, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2136 | processing task, is_child = 0
slot update_slots: id  3 | task 2136 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10093
slot update_slots: id  3 | task 2136 | n_tokens = 10028, memory_seq_rm [10028, end)
slot update_slots: id  3 | task 2136 | prompt processing progress, n_tokens = 10029, batch.n_tokens = 1, progress = 0.993659
slot update_slots: id  3 | task 2136 | n_tokens = 10029, memory_seq_rm [10029, end)
slot update_slots: id  3 | task 2136 | prompt processing progress, n_tokens = 10093, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2136 | prompt done, n_tokens = 10093, batch.n_tokens = 64
slot init_sampler: id  3 | task 2136 | init sampler, took 2.12 ms, tokens: text = 10093, total = 10093
slot print_timing: id  3 | task 2136 | 
prompt eval time =     290.73 ms /    65 tokens (    4.47 ms per token,   223.57 tokens per second)
       eval time =    2127.20 ms /    76 tokens (   27.99 ms per token,    35.73 tokens per second)
      total time =    2417.93 ms /   141 tokens
slot      release: id  3 | task 2136 | stop processing: n_tokens = 10168, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2214 | processing task, is_child = 0
slot update_slots: id  3 | task 2214 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10153
slot update_slots: id  3 | task 2214 | n_tokens = 10093, memory_seq_rm [10093, end)
slot update_slots: id  3 | task 2214 | prompt processing progress, n_tokens = 10153, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  3 | task 2214 | prompt done, n_tokens = 10153, batch.n_tokens = 60
slot init_sampler: id  3 | task 2214 | init sampler, took 3.92 ms, tokens: text = 10153, total = 10153
slot update_slots: id  3 | task 2214 | erasing old context checkpoint (pos_min = 4786, pos_max = 5809, size = 24.012 MiB)
slot update_slots: id  3 | task 2214 | created context checkpoint 8 of 8 (pos_min = 9194, pos_max = 10092, size = 21.081 MiB)
slot print_timing: id  3 | task 2214 | 
prompt eval time =     222.47 ms /    60 tokens (    3.71 ms per token,   269.70 tokens per second)
       eval time =    3024.88 ms /   108 tokens (   28.01 ms per token,    35.70 tokens per second)
      total time =    3247.35 ms /   168 tokens
slot      release: id  3 | task 2214 | stop processing: n_tokens = 10260, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.869 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2323 | processing task, is_child = 0
slot update_slots: id  3 | task 2323 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11686
slot update_slots: id  3 | task 2323 | n_tokens = 10153, memory_seq_rm [10153, end)
slot update_slots: id  3 | task 2323 | prompt processing progress, n_tokens = 11622, batch.n_tokens = 1469, progress = 0.994523
slot update_slots: id  3 | task 2323 | n_tokens = 11622, memory_seq_rm [11622, end)
slot update_slots: id  3 | task 2323 | prompt processing progress, n_tokens = 11686, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2323 | prompt done, n_tokens = 11686, batch.n_tokens = 64
slot init_sampler: id  3 | task 2323 | init sampler, took 1.75 ms, tokens: text = 11686, total = 11686
slot update_slots: id  3 | task 2323 | erasing old context checkpoint (pos_min = 5183, pos_max = 5936, size = 17.681 MiB)
slot update_slots: id  3 | task 2323 | created context checkpoint 8 of 8 (pos_min = 10598, pos_max = 11621, size = 24.012 MiB)
slot print_timing: id  3 | task 2323 | 
prompt eval time =    2027.66 ms /  1533 tokens (    1.32 ms per token,   756.04 tokens per second)
       eval time =    3189.64 ms /   115 tokens (   27.74 ms per token,    36.05 tokens per second)
      total time =    5217.30 ms /  1648 tokens
slot      release: id  3 | task 2323 | stop processing: n_tokens = 11800, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2440 | processing task, is_child = 0
slot update_slots: id  3 | task 2440 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11761
slot update_slots: id  3 | task 2440 | n_tokens = 11686, memory_seq_rm [11686, end)
slot update_slots: id  3 | task 2440 | prompt processing progress, n_tokens = 11697, batch.n_tokens = 11, progress = 0.994558
slot update_slots: id  3 | task 2440 | n_tokens = 11697, memory_seq_rm [11697, end)
slot update_slots: id  3 | task 2440 | prompt processing progress, n_tokens = 11761, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2440 | prompt done, n_tokens = 11761, batch.n_tokens = 64
slot init_sampler: id  3 | task 2440 | init sampler, took 1.71 ms, tokens: text = 11761, total = 11761
slot update_slots: id  3 | task 2440 | erasing old context checkpoint (pos_min = 5817, pos_max = 6570, size = 17.681 MiB)
slot update_slots: id  3 | task 2440 | created context checkpoint 8 of 8 (pos_min = 10776, pos_max = 11696, size = 21.597 MiB)
slot print_timing: id  3 | task 2440 | 
prompt eval time =     279.28 ms /    75 tokens (    3.72 ms per token,   268.55 tokens per second)
       eval time =    3995.75 ms /   139 tokens (   28.75 ms per token,    34.79 tokens per second)
      total time =    4275.03 ms /   214 tokens
slot      release: id  3 | task 2440 | stop processing: n_tokens = 11899, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2581 | processing task, is_child = 0
slot update_slots: id  3 | task 2581 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11847
slot update_slots: id  3 | task 2581 | n_tokens = 11761, memory_seq_rm [11761, end)
slot update_slots: id  3 | task 2581 | prompt processing progress, n_tokens = 11783, batch.n_tokens = 22, progress = 0.994598
slot update_slots: id  3 | task 2581 | n_tokens = 11783, memory_seq_rm [11783, end)
slot update_slots: id  3 | task 2581 | prompt processing progress, n_tokens = 11847, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2581 | prompt done, n_tokens = 11847, batch.n_tokens = 64
slot init_sampler: id  3 | task 2581 | init sampler, took 2.28 ms, tokens: text = 11847, total = 11847
slot update_slots: id  3 | task 2581 | erasing old context checkpoint (pos_min = 6201, pos_max = 7224, size = 24.012 MiB)
slot update_slots: id  3 | task 2581 | created context checkpoint 8 of 8 (pos_min = 10875, pos_max = 11782, size = 21.292 MiB)
slot print_timing: id  3 | task 2581 | 
prompt eval time =     336.11 ms /    86 tokens (    3.91 ms per token,   255.87 tokens per second)
       eval time =    7001.27 ms /   232 tokens (   30.18 ms per token,    33.14 tokens per second)
      total time =    7337.38 ms /   318 tokens
slot      release: id  3 | task 2581 | stop processing: n_tokens = 12078, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2815 | processing task, is_child = 0
slot update_slots: id  3 | task 2815 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11914
slot update_slots: id  3 | task 2815 | n_tokens = 11847, memory_seq_rm [11847, end)
slot update_slots: id  3 | task 2815 | prompt processing progress, n_tokens = 11850, batch.n_tokens = 3, progress = 0.994628
slot update_slots: id  3 | task 2815 | n_tokens = 11850, memory_seq_rm [11850, end)
slot update_slots: id  3 | task 2815 | prompt processing progress, n_tokens = 11914, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2815 | prompt done, n_tokens = 11914, batch.n_tokens = 64
slot init_sampler: id  3 | task 2815 | init sampler, took 2.40 ms, tokens: text = 11914, total = 11914
slot update_slots: id  3 | task 2815 | erasing old context checkpoint (pos_min = 7451, pos_max = 8474, size = 24.012 MiB)
slot update_slots: id  3 | task 2815 | created context checkpoint 8 of 8 (pos_min = 11054, pos_max = 11849, size = 18.666 MiB)
slot print_timing: id  3 | task 2815 | 
prompt eval time =     348.39 ms /    67 tokens (    5.20 ms per token,   192.31 tokens per second)
       eval time =    2951.32 ms /   110 tokens (   26.83 ms per token,    37.27 tokens per second)
      total time =    3299.71 ms /   177 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 2815 | stop processing: n_tokens = 12023, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.909 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2927 | processing task, is_child = 0
slot update_slots: id  3 | task 2927 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13100
slot update_slots: id  3 | task 2927 | n_tokens = 11914, memory_seq_rm [11914, end)
slot update_slots: id  3 | task 2927 | prompt processing progress, n_tokens = 13036, batch.n_tokens = 1122, progress = 0.995115
slot update_slots: id  3 | task 2927 | n_tokens = 13036, memory_seq_rm [13036, end)
slot update_slots: id  3 | task 2927 | prompt processing progress, n_tokens = 13100, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2927 | prompt done, n_tokens = 13100, batch.n_tokens = 64
slot init_sampler: id  3 | task 2927 | init sampler, took 2.81 ms, tokens: text = 13100, total = 13100
slot update_slots: id  3 | task 2927 | erasing old context checkpoint (pos_min = 7893, pos_max = 8878, size = 23.121 MiB)
slot update_slots: id  3 | task 2927 | created context checkpoint 8 of 8 (pos_min = 12012, pos_max = 13035, size = 24.012 MiB)
slot print_timing: id  3 | task 2927 | 
prompt eval time =    1726.64 ms /  1186 tokens (    1.46 ms per token,   686.88 tokens per second)
       eval time =   32880.81 ms /  1104 tokens (   29.78 ms per token,    33.58 tokens per second)
      total time =   34607.44 ms /  2290 tokens
slot      release: id  3 | task 2927 | stop processing: n_tokens = 14203, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.922
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4033 | processing task, is_child = 0
slot update_slots: id  3 | task 4033 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13504
slot update_slots: id  3 | task 4033 | n_past = 13100, slot.prompt.tokens.size() = 14203, seq_id = 3, pos_min = 13179, n_swa = 128
slot update_slots: id  3 | task 4033 | restored context checkpoint (pos_min = 12012, pos_max = 13035, size = 24.012 MiB)
slot update_slots: id  3 | task 4033 | n_tokens = 13035, memory_seq_rm [13035, end)
slot update_slots: id  3 | task 4033 | prompt processing progress, n_tokens = 13440, batch.n_tokens = 405, progress = 0.995261
slot update_slots: id  3 | task 4033 | n_tokens = 13440, memory_seq_rm [13440, end)
slot update_slots: id  3 | task 4033 | prompt processing progress, n_tokens = 13504, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4033 | prompt done, n_tokens = 13504, batch.n_tokens = 64
slot init_sampler: id  3 | task 4033 | init sampler, took 1.89 ms, tokens: text = 13504, total = 13504
slot update_slots: id  3 | task 4033 | erasing old context checkpoint (pos_min = 8832, pos_max = 9852, size = 23.942 MiB)
slot update_slots: id  3 | task 4033 | created context checkpoint 8 of 8 (pos_min = 12416, pos_max = 13439, size = 24.012 MiB)
slot print_timing: id  3 | task 4033 | 
prompt eval time =     780.60 ms /   469 tokens (    1.66 ms per token,   600.82 tokens per second)
       eval time =    1294.77 ms /    45 tokens (   28.77 ms per token,    34.76 tokens per second)
      total time =    2075.37 ms /   514 tokens
slot      release: id  3 | task 4033 | stop processing: n_tokens = 13548, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4080 | processing task, is_child = 0
slot update_slots: id  3 | task 4080 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13559
slot update_slots: id  3 | task 4080 | n_tokens = 13504, memory_seq_rm [13504, end)
slot update_slots: id  3 | task 4080 | prompt processing progress, n_tokens = 13559, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 4080 | prompt done, n_tokens = 13559, batch.n_tokens = 55
slot init_sampler: id  3 | task 4080 | init sampler, took 1.89 ms, tokens: text = 13559, total = 13559
slot print_timing: id  3 | task 4080 | 
prompt eval time =     188.87 ms /    55 tokens (    3.43 ms per token,   291.20 tokens per second)
       eval time =    2452.22 ms /    87 tokens (   28.19 ms per token,    35.48 tokens per second)
      total time =    2641.10 ms /   142 tokens
slot      release: id  3 | task 4080 | stop processing: n_tokens = 13645, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4168 | processing task, is_child = 0
slot update_slots: id  3 | task 4168 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13614
slot update_slots: id  3 | task 4168 | n_tokens = 13559, memory_seq_rm [13559, end)
slot update_slots: id  3 | task 4168 | prompt processing progress, n_tokens = 13614, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 4168 | prompt done, n_tokens = 13614, batch.n_tokens = 55
slot init_sampler: id  3 | task 4168 | init sampler, took 2.08 ms, tokens: text = 13614, total = 13614
slot update_slots: id  3 | task 4168 | erasing old context checkpoint (pos_min = 9052, pos_max = 9972, size = 21.597 MiB)
slot update_slots: id  3 | task 4168 | created context checkpoint 8 of 8 (pos_min = 12621, pos_max = 13558, size = 21.995 MiB)
slot print_timing: id  3 | task 4168 | 
prompt eval time =     201.41 ms /    55 tokens (    3.66 ms per token,   273.08 tokens per second)
       eval time =    1781.28 ms /    63 tokens (   28.27 ms per token,    35.37 tokens per second)
      total time =    1982.69 ms /   118 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 4168 | stop processing: n_tokens = 13676, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4232 | processing task, is_child = 0
slot update_slots: id  3 | task 4232 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13668
slot update_slots: id  3 | task 4232 | n_tokens = 13614, memory_seq_rm [13614, end)
slot update_slots: id  3 | task 4232 | prompt processing progress, n_tokens = 13668, batch.n_tokens = 54, progress = 1.000000
slot update_slots: id  3 | task 4232 | prompt done, n_tokens = 13668, batch.n_tokens = 54
slot init_sampler: id  3 | task 4232 | init sampler, took 2.83 ms, tokens: text = 13668, total = 13668
slot print_timing: id  3 | task 4232 | 
prompt eval time =     188.40 ms /    54 tokens (    3.49 ms per token,   286.63 tokens per second)
       eval time =    1698.13 ms /    61 tokens (   27.84 ms per token,    35.92 tokens per second)
      total time =    1886.53 ms /   115 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 4232 | stop processing: n_tokens = 13728, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4294 | processing task, is_child = 0
slot update_slots: id  3 | task 4294 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13923
slot update_slots: id  3 | task 4294 | n_tokens = 13668, memory_seq_rm [13668, end)
slot update_slots: id  3 | task 4294 | prompt processing progress, n_tokens = 13859, batch.n_tokens = 191, progress = 0.995403
slot update_slots: id  3 | task 4294 | n_tokens = 13859, memory_seq_rm [13859, end)
slot update_slots: id  3 | task 4294 | prompt processing progress, n_tokens = 13923, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4294 | prompt done, n_tokens = 13923, batch.n_tokens = 64
slot init_sampler: id  3 | task 4294 | init sampler, took 1.99 ms, tokens: text = 13923, total = 13923
slot update_slots: id  3 | task 4294 | erasing old context checkpoint (pos_min = 9194, pos_max = 10092, size = 21.081 MiB)
slot update_slots: id  3 | task 4294 | created context checkpoint 8 of 8 (pos_min = 12835, pos_max = 13858, size = 24.012 MiB)
slot print_timing: id  3 | task 4294 | 
prompt eval time =     541.11 ms /   255 tokens (    2.12 ms per token,   471.25 tokens per second)
       eval time =    1255.49 ms /    46 tokens (   27.29 ms per token,    36.64 tokens per second)
      total time =    1796.60 ms /   301 tokens
slot      release: id  3 | task 4294 | stop processing: n_tokens = 13968, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4342 | processing task, is_child = 0
slot update_slots: id  3 | task 4342 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14421
slot update_slots: id  3 | task 4342 | n_tokens = 13923, memory_seq_rm [13923, end)
slot update_slots: id  3 | task 4342 | prompt processing progress, n_tokens = 14357, batch.n_tokens = 434, progress = 0.995562
slot update_slots: id  3 | task 4342 | n_tokens = 14357, memory_seq_rm [14357, end)
slot update_slots: id  3 | task 4342 | prompt processing progress, n_tokens = 14421, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4342 | prompt done, n_tokens = 14421, batch.n_tokens = 64
slot init_sampler: id  3 | task 4342 | init sampler, took 2.03 ms, tokens: text = 14421, total = 14421
slot update_slots: id  3 | task 4342 | erasing old context checkpoint (pos_min = 10598, pos_max = 11621, size = 24.012 MiB)
slot update_slots: id  3 | task 4342 | created context checkpoint 8 of 8 (pos_min = 13333, pos_max = 14356, size = 24.012 MiB)
slot print_timing: id  3 | task 4342 | 
prompt eval time =     755.35 ms /   498 tokens (    1.52 ms per token,   659.30 tokens per second)
       eval time =    8931.36 ms /   330 tokens (   27.06 ms per token,    36.95 tokens per second)
      total time =    9686.71 ms /   828 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 4342 | stop processing: n_tokens = 14750, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.952 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4674 | processing task, is_child = 0
slot update_slots: id  3 | task 4674 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15154
slot update_slots: id  3 | task 4674 | n_tokens = 14421, memory_seq_rm [14421, end)
slot update_slots: id  3 | task 4674 | prompt processing progress, n_tokens = 15090, batch.n_tokens = 669, progress = 0.995777
slot update_slots: id  3 | task 4674 | n_tokens = 15090, memory_seq_rm [15090, end)
slot update_slots: id  3 | task 4674 | prompt processing progress, n_tokens = 15154, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4674 | prompt done, n_tokens = 15154, batch.n_tokens = 64
slot init_sampler: id  3 | task 4674 | init sampler, took 2.84 ms, tokens: text = 15154, total = 15154
slot update_slots: id  3 | task 4674 | erasing old context checkpoint (pos_min = 10776, pos_max = 11696, size = 21.597 MiB)
slot update_slots: id  3 | task 4674 | created context checkpoint 8 of 8 (pos_min = 14066, pos_max = 15089, size = 24.012 MiB)
slot print_timing: id  3 | task 4674 | 
prompt eval time =    1183.04 ms /   733 tokens (    1.61 ms per token,   619.59 tokens per second)
       eval time =    7674.61 ms /   285 tokens (   26.93 ms per token,    37.14 tokens per second)
      total time =    8857.65 ms /  1018 tokens
slot      release: id  3 | task 4674 | stop processing: n_tokens = 15438, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4961 | processing task, is_child = 0
slot update_slots: id  3 | task 4961 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16404
slot update_slots: id  3 | task 4961 | n_tokens = 15154, memory_seq_rm [15154, end)
slot update_slots: id  3 | task 4961 | prompt processing progress, n_tokens = 16340, batch.n_tokens = 1186, progress = 0.996099
slot update_slots: id  3 | task 4961 | n_tokens = 16340, memory_seq_rm [16340, end)
slot update_slots: id  3 | task 4961 | prompt processing progress, n_tokens = 16404, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4961 | prompt done, n_tokens = 16404, batch.n_tokens = 64
slot init_sampler: id  3 | task 4961 | init sampler, took 2.33 ms, tokens: text = 16404, total = 16404
slot update_slots: id  3 | task 4961 | erasing old context checkpoint (pos_min = 10875, pos_max = 11782, size = 21.292 MiB)
slot update_slots: id  3 | task 4961 | created context checkpoint 8 of 8 (pos_min = 15316, pos_max = 16339, size = 24.012 MiB)
slot print_timing: id  3 | task 4961 | 
prompt eval time =    1807.93 ms /  1250 tokens (    1.45 ms per token,   691.40 tokens per second)
       eval time =    3515.17 ms /   128 tokens (   27.46 ms per token,    36.41 tokens per second)
      total time =    5323.10 ms /  1378 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 4961 | stop processing: n_tokens = 16531, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5091 | processing task, is_child = 0
slot update_slots: id  3 | task 5091 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17058
slot update_slots: id  3 | task 5091 | n_tokens = 16404, memory_seq_rm [16404, end)
slot update_slots: id  3 | task 5091 | prompt processing progress, n_tokens = 16994, batch.n_tokens = 590, progress = 0.996248
slot update_slots: id  3 | task 5091 | n_tokens = 16994, memory_seq_rm [16994, end)
slot update_slots: id  3 | task 5091 | prompt processing progress, n_tokens = 17058, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5091 | prompt done, n_tokens = 17058, batch.n_tokens = 64
slot init_sampler: id  3 | task 5091 | init sampler, took 3.60 ms, tokens: text = 17058, total = 17058
slot update_slots: id  3 | task 5091 | erasing old context checkpoint (pos_min = 11054, pos_max = 11849, size = 18.666 MiB)
slot update_slots: id  3 | task 5091 | created context checkpoint 8 of 8 (pos_min = 15970, pos_max = 16993, size = 24.012 MiB)
slot print_timing: id  3 | task 5091 | 
prompt eval time =    1100.24 ms /   654 tokens (    1.68 ms per token,   594.42 tokens per second)
       eval time =   16785.47 ms /   598 tokens (   28.07 ms per token,    35.63 tokens per second)
      total time =   17885.70 ms /  1252 tokens
slot      release: id  3 | task 5091 | stop processing: n_tokens = 17655, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5691 | processing task, is_child = 0
slot update_slots: id  3 | task 5691 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17462
slot update_slots: id  3 | task 5691 | n_tokens = 17058, memory_seq_rm [17058, end)
slot update_slots: id  3 | task 5691 | prompt processing progress, n_tokens = 17398, batch.n_tokens = 340, progress = 0.996335
slot update_slots: id  3 | task 5691 | n_tokens = 17398, memory_seq_rm [17398, end)
slot update_slots: id  3 | task 5691 | prompt processing progress, n_tokens = 17462, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5691 | prompt done, n_tokens = 17462, batch.n_tokens = 64
slot init_sampler: id  3 | task 5691 | init sampler, took 2.51 ms, tokens: text = 17462, total = 17462
slot update_slots: id  3 | task 5691 | erasing old context checkpoint (pos_min = 12012, pos_max = 13035, size = 24.012 MiB)
slot update_slots: id  3 | task 5691 | created context checkpoint 8 of 8 (pos_min = 16631, pos_max = 17397, size = 17.986 MiB)
slot print_timing: id  3 | task 5691 | 
prompt eval time =     744.00 ms /   404 tokens (    1.84 ms per token,   543.01 tokens per second)
       eval time =    1945.18 ms /    68 tokens (   28.61 ms per token,    34.96 tokens per second)
      total time =    2689.18 ms /   472 tokens
slot      release: id  3 | task 5691 | stop processing: n_tokens = 17529, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5761 | processing task, is_child = 0
slot update_slots: id  3 | task 5761 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17517
slot update_slots: id  3 | task 5761 | n_tokens = 17462, memory_seq_rm [17462, end)
slot update_slots: id  3 | task 5761 | prompt processing progress, n_tokens = 17517, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 5761 | prompt done, n_tokens = 17517, batch.n_tokens = 55
slot init_sampler: id  3 | task 5761 | init sampler, took 2.44 ms, tokens: text = 17517, total = 17517
slot print_timing: id  3 | task 5761 | 
prompt eval time =     190.78 ms /    55 tokens (    3.47 ms per token,   288.29 tokens per second)
       eval time =    1726.59 ms /    60 tokens (   28.78 ms per token,    34.75 tokens per second)
      total time =    1917.37 ms /   115 tokens
slot      release: id  3 | task 5761 | stop processing: n_tokens = 17576, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5822 | processing task, is_child = 0
slot update_slots: id  3 | task 5822 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17572
slot update_slots: id  3 | task 5822 | n_tokens = 17517, memory_seq_rm [17517, end)
slot update_slots: id  3 | task 5822 | prompt processing progress, n_tokens = 17572, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 5822 | prompt done, n_tokens = 17572, batch.n_tokens = 55
slot init_sampler: id  3 | task 5822 | init sampler, took 2.53 ms, tokens: text = 17572, total = 17572
slot update_slots: id  3 | task 5822 | erasing old context checkpoint (pos_min = 12416, pos_max = 13439, size = 24.012 MiB)
slot update_slots: id  3 | task 5822 | created context checkpoint 8 of 8 (pos_min = 16631, pos_max = 17516, size = 20.776 MiB)
slot print_timing: id  3 | task 5822 | 
prompt eval time =     203.35 ms /    55 tokens (    3.70 ms per token,   270.47 tokens per second)
       eval time =    1718.60 ms /    59 tokens (   29.13 ms per token,    34.33 tokens per second)
      total time =    1921.95 ms /   114 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5822 | stop processing: n_tokens = 17630, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5882 | processing task, is_child = 0
slot update_slots: id  3 | task 5882 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17732
slot update_slots: id  3 | task 5882 | n_tokens = 17572, memory_seq_rm [17572, end)
slot update_slots: id  3 | task 5882 | prompt processing progress, n_tokens = 17668, batch.n_tokens = 96, progress = 0.996391
slot update_slots: id  3 | task 5882 | n_tokens = 17668, memory_seq_rm [17668, end)
slot update_slots: id  3 | task 5882 | prompt processing progress, n_tokens = 17732, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5882 | prompt done, n_tokens = 17732, batch.n_tokens = 64
slot init_sampler: id  3 | task 5882 | init sampler, took 3.43 ms, tokens: text = 17732, total = 17732
slot update_slots: id  3 | task 5882 | erasing old context checkpoint (pos_min = 12621, pos_max = 13558, size = 21.995 MiB)
slot update_slots: id  3 | task 5882 | created context checkpoint 8 of 8 (pos_min = 16644, pos_max = 17667, size = 24.012 MiB)
slot print_timing: id  3 | task 5882 | 
prompt eval time =     489.25 ms /   160 tokens (    3.06 ms per token,   327.03 tokens per second)
       eval time =    1399.92 ms /    49 tokens (   28.57 ms per token,    35.00 tokens per second)
      total time =    1889.16 ms /   209 tokens
slot      release: id  3 | task 5882 | stop processing: n_tokens = 17780, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5933 | processing task, is_child = 0
slot update_slots: id  3 | task 5933 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17788
slot update_slots: id  3 | task 5933 | n_tokens = 17732, memory_seq_rm [17732, end)
slot update_slots: id  3 | task 5933 | prompt processing progress, n_tokens = 17788, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  3 | task 5933 | prompt done, n_tokens = 17788, batch.n_tokens = 56
slot init_sampler: id  3 | task 5933 | init sampler, took 2.53 ms, tokens: text = 17788, total = 17788
slot print_timing: id  3 | task 5933 | 
prompt eval time =     194.62 ms /    56 tokens (    3.48 ms per token,   287.74 tokens per second)
       eval time =    1582.92 ms /    55 tokens (   28.78 ms per token,    34.75 tokens per second)
      total time =    1777.54 ms /   111 tokens
slot      release: id  3 | task 5933 | stop processing: n_tokens = 17842, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5989 | processing task, is_child = 0
slot update_slots: id  3 | task 5989 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17847
slot update_slots: id  3 | task 5989 | n_tokens = 17788, memory_seq_rm [17788, end)
slot update_slots: id  3 | task 5989 | prompt processing progress, n_tokens = 17847, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 5989 | prompt done, n_tokens = 17847, batch.n_tokens = 59
slot init_sampler: id  3 | task 5989 | init sampler, took 2.52 ms, tokens: text = 17847, total = 17847
slot update_slots: id  3 | task 5989 | erasing old context checkpoint (pos_min = 12835, pos_max = 13858, size = 24.012 MiB)
slot update_slots: id  3 | task 5989 | created context checkpoint 8 of 8 (pos_min = 16818, pos_max = 17787, size = 22.746 MiB)
slot print_timing: id  3 | task 5989 | 
prompt eval time =     210.38 ms /    59 tokens (    3.57 ms per token,   280.44 tokens per second)
       eval time =    3381.57 ms /   116 tokens (   29.15 ms per token,    34.30 tokens per second)
      total time =    3591.95 ms /   175 tokens
slot      release: id  3 | task 5989 | stop processing: n_tokens = 17962, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6106 | processing task, is_child = 0
slot update_slots: id  3 | task 6106 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17974
slot update_slots: id  3 | task 6106 | n_tokens = 17847, memory_seq_rm [17847, end)
slot update_slots: id  3 | task 6106 | prompt processing progress, n_tokens = 17910, batch.n_tokens = 63, progress = 0.996439
slot update_slots: id  3 | task 6106 | n_tokens = 17910, memory_seq_rm [17910, end)
slot update_slots: id  3 | task 6106 | prompt processing progress, n_tokens = 17974, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6106 | prompt done, n_tokens = 17974, batch.n_tokens = 64
slot init_sampler: id  3 | task 6106 | init sampler, took 2.59 ms, tokens: text = 17974, total = 17974
slot update_slots: id  3 | task 6106 | erasing old context checkpoint (pos_min = 13333, pos_max = 14356, size = 24.012 MiB)
slot update_slots: id  3 | task 6106 | created context checkpoint 8 of 8 (pos_min = 16938, pos_max = 17909, size = 22.793 MiB)
slot print_timing: id  3 | task 6106 | 
prompt eval time =     424.44 ms /   127 tokens (    3.34 ms per token,   299.22 tokens per second)
       eval time =    3423.76 ms /   118 tokens (   29.01 ms per token,    34.47 tokens per second)
      total time =    3848.19 ms /   245 tokens
slot      release: id  3 | task 6106 | stop processing: n_tokens = 18091, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6226 | processing task, is_child = 0
slot update_slots: id  3 | task 6226 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18948
slot update_slots: id  3 | task 6226 | n_tokens = 17974, memory_seq_rm [17974, end)
slot update_slots: id  3 | task 6226 | prompt processing progress, n_tokens = 18884, batch.n_tokens = 910, progress = 0.996622
slot update_slots: id  3 | task 6226 | n_tokens = 18884, memory_seq_rm [18884, end)
slot update_slots: id  3 | task 6226 | prompt processing progress, n_tokens = 18948, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6226 | prompt done, n_tokens = 18948, batch.n_tokens = 64
slot init_sampler: id  3 | task 6226 | init sampler, took 3.82 ms, tokens: text = 18948, total = 18948
slot update_slots: id  3 | task 6226 | erasing old context checkpoint (pos_min = 14066, pos_max = 15089, size = 24.012 MiB)
slot update_slots: id  3 | task 6226 | created context checkpoint 8 of 8 (pos_min = 17877, pos_max = 18883, size = 23.613 MiB)
slot print_timing: id  3 | task 6226 | 
prompt eval time =    1490.14 ms /   974 tokens (    1.53 ms per token,   653.63 tokens per second)
       eval time =   10778.90 ms /   373 tokens (   28.90 ms per token,    34.60 tokens per second)
      total time =   12269.04 ms /  1347 tokens
slot      release: id  3 | task 6226 | stop processing: n_tokens = 19320, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6601 | processing task, is_child = 0
slot update_slots: id  3 | task 6601 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19521
slot update_slots: id  3 | task 6601 | n_tokens = 18948, memory_seq_rm [18948, end)
slot update_slots: id  3 | task 6601 | prompt processing progress, n_tokens = 19457, batch.n_tokens = 509, progress = 0.996722
slot update_slots: id  3 | task 6601 | n_tokens = 19457, memory_seq_rm [19457, end)
slot update_slots: id  3 | task 6601 | prompt processing progress, n_tokens = 19521, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6601 | prompt done, n_tokens = 19521, batch.n_tokens = 64
slot init_sampler: id  3 | task 6601 | init sampler, took 3.64 ms, tokens: text = 19521, total = 19521
slot update_slots: id  3 | task 6601 | erasing old context checkpoint (pos_min = 15316, pos_max = 16339, size = 24.012 MiB)
slot update_slots: id  3 | task 6601 | created context checkpoint 8 of 8 (pos_min = 18483, pos_max = 19456, size = 22.840 MiB)
slot print_timing: id  3 | task 6601 | 
prompt eval time =     941.82 ms /   573 tokens (    1.64 ms per token,   608.40 tokens per second)
       eval time =    2222.97 ms /    78 tokens (   28.50 ms per token,    35.09 tokens per second)
      total time =    3164.78 ms /   651 tokens
slot      release: id  3 | task 6601 | stop processing: n_tokens = 19598, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6681 | processing task, is_child = 0
slot update_slots: id  3 | task 6681 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19584
slot update_slots: id  3 | task 6681 | n_tokens = 19521, memory_seq_rm [19521, end)
slot update_slots: id  3 | task 6681 | prompt processing progress, n_tokens = 19584, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 6681 | prompt done, n_tokens = 19584, batch.n_tokens = 63
slot init_sampler: id  3 | task 6681 | init sampler, took 4.00 ms, tokens: text = 19584, total = 19584
slot print_timing: id  3 | task 6681 | 
prompt eval time =     289.81 ms /    63 tokens (    4.60 ms per token,   217.38 tokens per second)
       eval time =   23944.26 ms /   828 tokens (   28.92 ms per token,    34.58 tokens per second)
      total time =   24234.07 ms /   891 tokens
slot      release: id  3 | task 6681 | stop processing: n_tokens = 20411, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7510 | processing task, is_child = 0
slot update_slots: id  3 | task 7510 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 20735
slot update_slots: id  3 | task 7510 | n_tokens = 19584, memory_seq_rm [19584, end)
slot update_slots: id  3 | task 7510 | prompt processing progress, n_tokens = 20671, batch.n_tokens = 1087, progress = 0.996913
slot update_slots: id  3 | task 7510 | n_tokens = 20671, memory_seq_rm [20671, end)
slot update_slots: id  3 | task 7510 | prompt processing progress, n_tokens = 20735, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7510 | prompt done, n_tokens = 20735, batch.n_tokens = 64
slot init_sampler: id  3 | task 7510 | init sampler, took 3.18 ms, tokens: text = 20735, total = 20735
slot update_slots: id  3 | task 7510 | erasing old context checkpoint (pos_min = 15970, pos_max = 16993, size = 24.012 MiB)
slot update_slots: id  3 | task 7510 | created context checkpoint 8 of 8 (pos_min = 19774, pos_max = 20670, size = 21.034 MiB)
slot print_timing: id  3 | task 7510 | 
prompt eval time =    2024.48 ms /  1151 tokens (    1.76 ms per token,   568.54 tokens per second)
       eval time =    3461.71 ms /   110 tokens (   31.47 ms per token,    31.78 tokens per second)
      total time =    5486.19 ms /  1261 tokens
slot      release: id  3 | task 7510 | stop processing: n_tokens = 20844, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7622 | processing task, is_child = 0
slot update_slots: id  3 | task 7622 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21239
slot update_slots: id  3 | task 7622 | n_tokens = 20735, memory_seq_rm [20735, end)
slot update_slots: id  3 | task 7622 | prompt processing progress, n_tokens = 21175, batch.n_tokens = 440, progress = 0.996987
slot update_slots: id  3 | task 7622 | n_tokens = 21175, memory_seq_rm [21175, end)
slot update_slots: id  3 | task 7622 | prompt processing progress, n_tokens = 21239, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7622 | prompt done, n_tokens = 21239, batch.n_tokens = 64
slot init_sampler: id  3 | task 7622 | init sampler, took 2.94 ms, tokens: text = 21239, total = 21239
slot update_slots: id  3 | task 7622 | erasing old context checkpoint (pos_min = 16631, pos_max = 17397, size = 17.986 MiB)
slot update_slots: id  3 | task 7622 | created context checkpoint 8 of 8 (pos_min = 20151, pos_max = 21174, size = 24.012 MiB)
slot print_timing: id  3 | task 7622 | 
prompt eval time =     947.18 ms /   504 tokens (    1.88 ms per token,   532.11 tokens per second)
       eval time =  181672.62 ms /  6096 tokens (   29.80 ms per token,    33.55 tokens per second)
      total time =  182619.80 ms /  6600 tokens
slot      release: id  3 | task 7622 | stop processing: n_tokens = 27334, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.777
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13720 | processing task, is_child = 0
slot update_slots: id  3 | task 13720 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21402
slot update_slots: id  3 | task 13720 | n_past = 21239, slot.prompt.tokens.size() = 27334, seq_id = 3, pos_min = 26310, n_swa = 128
slot update_slots: id  3 | task 13720 | restored context checkpoint (pos_min = 20151, pos_max = 21174, size = 24.012 MiB)
slot update_slots: id  3 | task 13720 | n_tokens = 21174, memory_seq_rm [21174, end)
slot update_slots: id  3 | task 13720 | prompt processing progress, n_tokens = 21338, batch.n_tokens = 164, progress = 0.997010
slot update_slots: id  3 | task 13720 | n_tokens = 21338, memory_seq_rm [21338, end)
slot update_slots: id  3 | task 13720 | prompt processing progress, n_tokens = 21402, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13720 | prompt done, n_tokens = 21402, batch.n_tokens = 64
slot init_sampler: id  3 | task 13720 | init sampler, took 4.34 ms, tokens: text = 21402, total = 21402
slot update_slots: id  3 | task 13720 | erasing old context checkpoint (pos_min = 16631, pos_max = 17516, size = 20.776 MiB)
slot update_slots: id  3 | task 13720 | created context checkpoint 8 of 8 (pos_min = 20314, pos_max = 21337, size = 24.012 MiB)
slot print_timing: id  3 | task 13720 | 
prompt eval time =     613.06 ms /   228 tokens (    2.69 ms per token,   371.90 tokens per second)
       eval time =   11175.79 ms /   379 tokens (   29.49 ms per token,    33.91 tokens per second)
      total time =   11788.85 ms /   607 tokens
slot      release: id  3 | task 13720 | stop processing: n_tokens = 21780, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14101 | processing task, is_child = 0
slot update_slots: id  3 | task 14101 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21489
slot update_slots: id  3 | task 14101 | n_tokens = 21402, memory_seq_rm [21402, end)
slot update_slots: id  3 | task 14101 | prompt processing progress, n_tokens = 21425, batch.n_tokens = 23, progress = 0.997022
slot update_slots: id  3 | task 14101 | n_tokens = 21425, memory_seq_rm [21425, end)
slot update_slots: id  3 | task 14101 | prompt processing progress, n_tokens = 21489, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 14101 | prompt done, n_tokens = 21489, batch.n_tokens = 64
slot init_sampler: id  3 | task 14101 | init sampler, took 4.02 ms, tokens: text = 21489, total = 21489
slot update_slots: id  3 | task 14101 | erasing old context checkpoint (pos_min = 16644, pos_max = 17667, size = 24.012 MiB)
slot update_slots: id  3 | task 14101 | created context checkpoint 8 of 8 (pos_min = 20756, pos_max = 21424, size = 15.688 MiB)
slot print_timing: id  3 | task 14101 | 
prompt eval time =     349.03 ms /    87 tokens (    4.01 ms per token,   249.26 tokens per second)
       eval time = 1077728.25 ms / 34575 tokens (   31.17 ms per token,    32.08 tokens per second)
      total time = 1078077.28 ms / 34662 tokens
slot      release: id  3 | task 14101 | stop processing: n_tokens = 56063, truncated = 1
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 14101
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.012
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 56063, total state size = 1338.630 MiB
srv          load:  - looking for better prompt, base f_keep = 0.012, sim = 0.962
srv        update:  - cache state: 1 prompts, 1515.367 MiB (limits: 8192.000 MiB, 56064 tokens, 303073 est)
srv        update:    - prompt 0x59f268cdc130:   56063 tokens, checkpoints:  8,  1515.367 MiB
srv  get_availabl: prompt cache update took 1274.76 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 48679 | processing task, is_child = 0
slot update_slots: id  3 | task 48679 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 704
slot update_slots: id  3 | task 48679 | n_past = 677, slot.prompt.tokens.size() = 56063, seq_id = 3, pos_min = 55039, n_swa = 128
slot update_slots: id  3 | task 48679 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 48679 | erased invalidated context checkpoint (pos_min = 16818, pos_max = 17787, n_swa = 128, size = 22.746 MiB)
slot update_slots: id  3 | task 48679 | erased invalidated context checkpoint (pos_min = 16938, pos_max = 17909, n_swa = 128, size = 22.793 MiB)
slot update_slots: id  3 | task 48679 | erased invalidated context checkpoint (pos_min = 17877, pos_max = 18883, n_swa = 128, size = 23.613 MiB)
slot update_slots: id  3 | task 48679 | erased invalidated context checkpoint (pos_min = 18483, pos_max = 19456, n_swa = 128, size = 22.840 MiB)
slot update_slots: id  3 | task 48679 | erased invalidated context checkpoint (pos_min = 19774, pos_max = 20670, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 48679 | erased invalidated context checkpoint (pos_min = 20151, pos_max = 21174, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 48679 | erased invalidated context checkpoint (pos_min = 20314, pos_max = 21337, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 48679 | erased invalidated context checkpoint (pos_min = 20756, pos_max = 21424, n_swa = 128, size = 15.688 MiB)
slot update_slots: id  3 | task 48679 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 48679 | prompt processing progress, n_tokens = 640, batch.n_tokens = 640, progress = 0.909091
slot update_slots: id  3 | task 48679 | n_tokens = 640, memory_seq_rm [640, end)
slot update_slots: id  3 | task 48679 | prompt processing progress, n_tokens = 704, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 48679 | prompt done, n_tokens = 704, batch.n_tokens = 64
slot init_sampler: id  3 | task 48679 | init sampler, took 0.12 ms, tokens: text = 704, total = 704
slot update_slots: id  3 | task 48679 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot print_timing: id  3 | task 48679 | 
prompt eval time =    1043.06 ms /   704 tokens (    1.48 ms per token,   674.94 tokens per second)
       eval time =    1833.99 ms /    70 tokens (   26.20 ms per token,    38.17 tokens per second)
      total time =    2877.05 ms /   774 tokens
slot      release: id  3 | task 48679 | stop processing: n_tokens = 773, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.287 (> 0.100 thold), f_keep = 0.911
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 48751 | processing task, is_child = 0
slot update_slots: id  3 | task 48751 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2453
slot update_slots: id  3 | task 48751 | n_tokens = 704, memory_seq_rm [704, end)
slot update_slots: id  3 | task 48751 | prompt processing progress, n_tokens = 2389, batch.n_tokens = 1685, progress = 0.973909
slot update_slots: id  3 | task 48751 | n_tokens = 2389, memory_seq_rm [2389, end)
slot update_slots: id  3 | task 48751 | prompt processing progress, n_tokens = 2453, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 48751 | prompt done, n_tokens = 2453, batch.n_tokens = 64
slot init_sampler: id  3 | task 48751 | init sampler, took 0.43 ms, tokens: text = 2453, total = 2453
slot update_slots: id  3 | task 48751 | created context checkpoint 2 of 8 (pos_min = 1365, pos_max = 2388, size = 24.012 MiB)
slot print_timing: id  3 | task 48751 | 
prompt eval time =    2012.67 ms /  1749 tokens (    1.15 ms per token,   868.99 tokens per second)
       eval time =   17029.79 ms /   585 tokens (   29.11 ms per token,    34.35 tokens per second)
      total time =   19042.46 ms /  2334 tokens
slot      release: id  3 | task 48751 | stop processing: n_tokens = 3037, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.350 (> 0.100 thold), f_keep = 0.222
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3037, total state size = 95.227 MiB
srv          load:  - looking for better prompt, base f_keep = 0.222, sim = 0.350
srv        update:  - cache state: 2 prompts, 1649.614 MiB (limits: 8192.000 MiB, 56064 tokens, 293491 est)
srv        update:    - prompt 0x59f268cdc130:   56063 tokens, checkpoints:  8,  1515.367 MiB
srv        update:    - prompt 0x59f268cdb230:    3037 tokens, checkpoints:  2,   134.246 MiB
srv  get_availabl: prompt cache update took 53.95 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 49338 | processing task, is_child = 0
slot update_slots: id  3 | task 49338 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1930
slot update_slots: id  3 | task 49338 | n_past = 675, slot.prompt.tokens.size() = 3037, seq_id = 3, pos_min = 2013, n_swa = 128
slot update_slots: id  3 | task 49338 | restored context checkpoint (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot update_slots: id  3 | task 49338 | erased invalidated context checkpoint (pos_min = 1365, pos_max = 2388, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 49338 | n_tokens = 639, memory_seq_rm [639, end)
slot update_slots: id  3 | task 49338 | prompt processing progress, n_tokens = 1866, batch.n_tokens = 1227, progress = 0.966839
slot update_slots: id  3 | task 49338 | n_tokens = 1866, memory_seq_rm [1866, end)
slot update_slots: id  3 | task 49338 | prompt processing progress, n_tokens = 1930, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 49338 | prompt done, n_tokens = 1930, batch.n_tokens = 64
slot init_sampler: id  3 | task 49338 | init sampler, took 0.39 ms, tokens: text = 1930, total = 1930
slot update_slots: id  3 | task 49338 | created context checkpoint 2 of 8 (pos_min = 842, pos_max = 1865, size = 24.012 MiB)
slot print_timing: id  3 | task 49338 | 
prompt eval time =    1566.17 ms /  1291 tokens (    1.21 ms per token,   824.30 tokens per second)
       eval time =   19536.40 ms /   728 tokens (   26.84 ms per token,    37.26 tokens per second)
      total time =   21102.57 ms /  2019 tokens
slot      release: id  3 | task 49338 | stop processing: n_tokens = 2657, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.394 (> 0.100 thold), f_keep = 0.484
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2657, total state size = 86.316 MiB
srv          load:  - looking for better prompt, base f_keep = 0.484, sim = 0.394
srv        update:  - cache state: 3 prompts, 1774.950 MiB (limits: 8192.000 MiB, 56064 tokens, 285029 est)
srv        update:    - prompt 0x59f268cdc130:   56063 tokens, checkpoints:  8,  1515.367 MiB
srv        update:    - prompt 0x59f268cdb230:    3037 tokens, checkpoints:  2,   134.246 MiB
srv        update:    - prompt 0x59f267be5e10:    2657 tokens, checkpoints:  2,   125.336 MiB
srv  get_availabl: prompt cache update took 94.91 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 50068 | processing task, is_child = 0
slot update_slots: id  3 | task 50068 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3268
slot update_slots: id  3 | task 50068 | n_past = 1287, slot.prompt.tokens.size() = 2657, seq_id = 3, pos_min = 1633, n_swa = 128
slot update_slots: id  3 | task 50068 | restored context checkpoint (pos_min = 842, pos_max = 1865, size = 24.012 MiB)
slot update_slots: id  3 | task 50068 | n_tokens = 1287, memory_seq_rm [1287, end)
slot update_slots: id  3 | task 50068 | prompt processing progress, n_tokens = 3204, batch.n_tokens = 1917, progress = 0.980416
slot update_slots: id  3 | task 50068 | n_tokens = 3204, memory_seq_rm [3204, end)
slot update_slots: id  3 | task 50068 | prompt processing progress, n_tokens = 3268, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 50068 | prompt done, n_tokens = 3268, batch.n_tokens = 64
slot init_sampler: id  3 | task 50068 | init sampler, took 0.58 ms, tokens: text = 3268, total = 3268
slot update_slots: id  3 | task 50068 | created context checkpoint 3 of 8 (pos_min = 2180, pos_max = 3203, size = 24.012 MiB)
slot print_timing: id  3 | task 50068 | 
prompt eval time =    2276.45 ms /  1981 tokens (    1.15 ms per token,   870.22 tokens per second)
       eval time =    1103.10 ms /    42 tokens (   26.26 ms per token,    38.07 tokens per second)
      total time =    3379.54 ms /  2023 tokens
slot      release: id  3 | task 50068 | stop processing: n_tokens = 3309, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.766 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 50112 | processing task, is_child = 0
slot update_slots: id  3 | task 50112 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4269
slot update_slots: id  3 | task 50112 | n_tokens = 3268, memory_seq_rm [3268, end)
slot update_slots: id  3 | task 50112 | prompt processing progress, n_tokens = 4205, batch.n_tokens = 937, progress = 0.985008
slot update_slots: id  3 | task 50112 | n_tokens = 4205, memory_seq_rm [4205, end)
slot update_slots: id  3 | task 50112 | prompt processing progress, n_tokens = 4269, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 50112 | prompt done, n_tokens = 4269, batch.n_tokens = 64
slot init_sampler: id  3 | task 50112 | init sampler, took 0.65 ms, tokens: text = 4269, total = 4269
slot update_slots: id  3 | task 50112 | created context checkpoint 4 of 8 (pos_min = 3181, pos_max = 4204, size = 24.012 MiB)
slot print_timing: id  3 | task 50112 | 
prompt eval time =    1174.36 ms /  1001 tokens (    1.17 ms per token,   852.38 tokens per second)
       eval time =    2861.19 ms /   105 tokens (   27.25 ms per token,    36.70 tokens per second)
      total time =    4035.56 ms /  1106 tokens
slot      release: id  3 | task 50112 | stop processing: n_tokens = 4373, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 50219 | processing task, is_child = 0
slot update_slots: id  3 | task 50219 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4338
slot update_slots: id  3 | task 50219 | n_tokens = 4269, memory_seq_rm [4269, end)
slot update_slots: id  3 | task 50219 | prompt processing progress, n_tokens = 4274, batch.n_tokens = 5, progress = 0.985247
slot update_slots: id  3 | task 50219 | n_tokens = 4274, memory_seq_rm [4274, end)
slot update_slots: id  3 | task 50219 | prompt processing progress, n_tokens = 4338, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 50219 | prompt done, n_tokens = 4338, batch.n_tokens = 64
slot init_sampler: id  3 | task 50219 | init sampler, took 0.65 ms, tokens: text = 4338, total = 4338
slot update_slots: id  3 | task 50219 | created context checkpoint 5 of 8 (pos_min = 3349, pos_max = 4273, size = 21.691 MiB)
slot print_timing: id  3 | task 50219 | 
prompt eval time =     333.35 ms /    69 tokens (    4.83 ms per token,   206.99 tokens per second)
       eval time =    5034.54 ms /   178 tokens (   28.28 ms per token,    35.36 tokens per second)
      total time =    5367.89 ms /   247 tokens
slot      release: id  3 | task 50219 | stop processing: n_tokens = 4515, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.868 (> 0.100 thold), f_keep = 0.961
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 50399 | processing task, is_child = 0
slot update_slots: id  3 | task 50399 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4996
slot update_slots: id  3 | task 50399 | n_tokens = 4338, memory_seq_rm [4338, end)
slot update_slots: id  3 | task 50399 | prompt processing progress, n_tokens = 4932, batch.n_tokens = 594, progress = 0.987190
slot update_slots: id  3 | task 50399 | n_tokens = 4932, memory_seq_rm [4932, end)
slot update_slots: id  3 | task 50399 | prompt processing progress, n_tokens = 4996, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 50399 | prompt done, n_tokens = 4996, batch.n_tokens = 64
slot init_sampler: id  3 | task 50399 | init sampler, took 0.76 ms, tokens: text = 4996, total = 4996
slot update_slots: id  3 | task 50399 | created context checkpoint 6 of 8 (pos_min = 3908, pos_max = 4931, size = 24.012 MiB)
slot print_timing: id  3 | task 50399 | 
prompt eval time =    1063.21 ms /   658 tokens (    1.62 ms per token,   618.88 tokens per second)
       eval time =   37299.68 ms /  1321 tokens (   28.24 ms per token,    35.42 tokens per second)
      total time =   38362.89 ms /  1979 tokens
slot      release: id  3 | task 50399 | stop processing: n_tokens = 6316, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.927 (> 0.100 thold), f_keep = 0.791
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 51722 | processing task, is_child = 0
slot update_slots: id  3 | task 51722 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5392
slot update_slots: id  3 | task 51722 | n_past = 4996, slot.prompt.tokens.size() = 6316, seq_id = 3, pos_min = 5292, n_swa = 128
slot update_slots: id  3 | task 51722 | restored context checkpoint (pos_min = 3908, pos_max = 4931, size = 24.012 MiB)
slot update_slots: id  3 | task 51722 | n_tokens = 4931, memory_seq_rm [4931, end)
slot update_slots: id  3 | task 51722 | prompt processing progress, n_tokens = 5328, batch.n_tokens = 397, progress = 0.988131
slot update_slots: id  3 | task 51722 | n_tokens = 5328, memory_seq_rm [5328, end)
slot update_slots: id  3 | task 51722 | prompt processing progress, n_tokens = 5392, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 51722 | prompt done, n_tokens = 5392, batch.n_tokens = 64
slot init_sampler: id  3 | task 51722 | init sampler, took 0.81 ms, tokens: text = 5392, total = 5392
slot update_slots: id  3 | task 51722 | created context checkpoint 7 of 8 (pos_min = 4304, pos_max = 5327, size = 24.012 MiB)
slot print_timing: id  3 | task 51722 | 
prompt eval time =     683.28 ms /   461 tokens (    1.48 ms per token,   674.68 tokens per second)
       eval time =    5587.77 ms /   213 tokens (   26.23 ms per token,    38.12 tokens per second)
      total time =    6271.06 ms /   674 tokens
slot      release: id  3 | task 51722 | stop processing: n_tokens = 5604, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 51937 | processing task, is_child = 0
slot update_slots: id  3 | task 51937 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5473
slot update_slots: id  3 | task 51937 | n_tokens = 5392, memory_seq_rm [5392, end)
slot update_slots: id  3 | task 51937 | prompt processing progress, n_tokens = 5409, batch.n_tokens = 17, progress = 0.988306
slot update_slots: id  3 | task 51937 | n_tokens = 5409, memory_seq_rm [5409, end)
slot update_slots: id  3 | task 51937 | prompt processing progress, n_tokens = 5473, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 51937 | prompt done, n_tokens = 5473, batch.n_tokens = 64
slot init_sampler: id  3 | task 51937 | init sampler, took 0.95 ms, tokens: text = 5473, total = 5473
slot update_slots: id  3 | task 51937 | created context checkpoint 8 of 8 (pos_min = 4580, pos_max = 5408, size = 19.439 MiB)
slot print_timing: id  3 | task 51937 | 
prompt eval time =     267.03 ms /    81 tokens (    3.30 ms per token,   303.33 tokens per second)
       eval time =    1727.10 ms /    65 tokens (   26.57 ms per token,    37.64 tokens per second)
      total time =    1994.14 ms /   146 tokens
slot      release: id  3 | task 51937 | stop processing: n_tokens = 5537, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 52004 | processing task, is_child = 0
slot update_slots: id  3 | task 52004 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5552
slot update_slots: id  3 | task 52004 | n_tokens = 5473, memory_seq_rm [5473, end)
slot update_slots: id  3 | task 52004 | prompt processing progress, n_tokens = 5488, batch.n_tokens = 15, progress = 0.988473
slot update_slots: id  3 | task 52004 | n_tokens = 5488, memory_seq_rm [5488, end)
slot update_slots: id  3 | task 52004 | prompt processing progress, n_tokens = 5552, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 52004 | prompt done, n_tokens = 5552, batch.n_tokens = 64
slot init_sampler: id  3 | task 52004 | init sampler, took 0.81 ms, tokens: text = 5552, total = 5552
slot update_slots: id  3 | task 52004 | erasing old context checkpoint (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot update_slots: id  3 | task 52004 | created context checkpoint 8 of 8 (pos_min = 4580, pos_max = 5487, size = 21.292 MiB)
slot print_timing: id  3 | task 52004 | 
prompt eval time =     258.83 ms /    79 tokens (    3.28 ms per token,   305.22 tokens per second)
       eval time =    1556.42 ms /    60 tokens (   25.94 ms per token,    38.55 tokens per second)
      total time =    1815.25 ms /   139 tokens
slot      release: id  3 | task 52004 | stop processing: n_tokens = 5611, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 52066 | processing task, is_child = 0
slot update_slots: id  3 | task 52066 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5631
slot update_slots: id  3 | task 52066 | n_tokens = 5552, memory_seq_rm [5552, end)
slot update_slots: id  3 | task 52066 | prompt processing progress, n_tokens = 5567, batch.n_tokens = 15, progress = 0.988634
slot update_slots: id  3 | task 52066 | n_tokens = 5567, memory_seq_rm [5567, end)
slot update_slots: id  3 | task 52066 | prompt processing progress, n_tokens = 5631, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 52066 | prompt done, n_tokens = 5631, batch.n_tokens = 64
slot init_sampler: id  3 | task 52066 | init sampler, took 0.84 ms, tokens: text = 5631, total = 5631
slot update_slots: id  3 | task 52066 | erasing old context checkpoint (pos_min = 842, pos_max = 1865, size = 24.012 MiB)
slot update_slots: id  3 | task 52066 | created context checkpoint 8 of 8 (pos_min = 4587, pos_max = 5566, size = 22.980 MiB)
slot print_timing: id  3 | task 52066 | 
prompt eval time =     268.41 ms /    79 tokens (    3.40 ms per token,   294.33 tokens per second)
       eval time =    6991.13 ms /   269 tokens (   25.99 ms per token,    38.48 tokens per second)
      total time =    7259.53 ms /   348 tokens
slot      release: id  3 | task 52066 | stop processing: n_tokens = 5899, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.893 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 52337 | processing task, is_child = 0
slot update_slots: id  3 | task 52337 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6304
slot update_slots: id  3 | task 52337 | n_tokens = 5631, memory_seq_rm [5631, end)
slot update_slots: id  3 | task 52337 | prompt processing progress, n_tokens = 6240, batch.n_tokens = 609, progress = 0.989848
slot update_slots: id  3 | task 52337 | n_tokens = 6240, memory_seq_rm [6240, end)
slot update_slots: id  3 | task 52337 | prompt processing progress, n_tokens = 6304, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 52337 | prompt done, n_tokens = 6304, batch.n_tokens = 64
slot init_sampler: id  3 | task 52337 | init sampler, took 1.19 ms, tokens: text = 6304, total = 6304
slot update_slots: id  3 | task 52337 | erasing old context checkpoint (pos_min = 2180, pos_max = 3203, size = 24.012 MiB)
slot update_slots: id  3 | task 52337 | created context checkpoint 8 of 8 (pos_min = 5216, pos_max = 6239, size = 24.012 MiB)
slot print_timing: id  3 | task 52337 | 
prompt eval time =    1019.79 ms /   673 tokens (    1.52 ms per token,   659.94 tokens per second)
       eval time =    5711.10 ms /   216 tokens (   26.44 ms per token,    37.82 tokens per second)
      total time =    6730.89 ms /   889 tokens
slot      release: id  3 | task 52337 | stop processing: n_tokens = 6519, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 52555 | processing task, is_child = 0
slot update_slots: id  3 | task 52555 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6385
slot update_slots: id  3 | task 52555 | n_tokens = 6304, memory_seq_rm [6304, end)
slot update_slots: id  3 | task 52555 | prompt processing progress, n_tokens = 6321, batch.n_tokens = 17, progress = 0.989977
slot update_slots: id  3 | task 52555 | n_tokens = 6321, memory_seq_rm [6321, end)
slot update_slots: id  3 | task 52555 | prompt processing progress, n_tokens = 6385, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 52555 | prompt done, n_tokens = 6385, batch.n_tokens = 64
slot init_sampler: id  3 | task 52555 | init sampler, took 0.99 ms, tokens: text = 6385, total = 6385
slot update_slots: id  3 | task 52555 | erasing old context checkpoint (pos_min = 3181, pos_max = 4204, size = 24.012 MiB)
slot update_slots: id  3 | task 52555 | created context checkpoint 8 of 8 (pos_min = 5495, pos_max = 6320, size = 19.369 MiB)
slot print_timing: id  3 | task 52555 | 
prompt eval time =     274.68 ms /    81 tokens (    3.39 ms per token,   294.89 tokens per second)
       eval time =    7187.28 ms /   269 tokens (   26.72 ms per token,    37.43 tokens per second)
      total time =    7461.95 ms /   350 tokens
slot      release: id  3 | task 52555 | stop processing: n_tokens = 6653, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.960
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 52826 | processing task, is_child = 0
slot update_slots: id  3 | task 52826 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6466
slot update_slots: id  3 | task 52826 | n_tokens = 6385, memory_seq_rm [6385, end)
slot update_slots: id  3 | task 52826 | prompt processing progress, n_tokens = 6402, batch.n_tokens = 17, progress = 0.990102
slot update_slots: id  3 | task 52826 | n_tokens = 6402, memory_seq_rm [6402, end)
slot update_slots: id  3 | task 52826 | prompt processing progress, n_tokens = 6466, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 52826 | prompt done, n_tokens = 6466, batch.n_tokens = 64
slot init_sampler: id  3 | task 52826 | init sampler, took 1.31 ms, tokens: text = 6466, total = 6466
slot update_slots: id  3 | task 52826 | erasing old context checkpoint (pos_min = 3349, pos_max = 4273, size = 21.691 MiB)
slot update_slots: id  3 | task 52826 | created context checkpoint 8 of 8 (pos_min = 5629, pos_max = 6401, size = 18.126 MiB)
slot print_timing: id  3 | task 52826 | 
prompt eval time =     284.58 ms /    81 tokens (    3.51 ms per token,   284.63 tokens per second)
       eval time =   11048.01 ms /   410 tokens (   26.95 ms per token,    37.11 tokens per second)
      total time =   11332.59 ms /   491 tokens
slot      release: id  3 | task 52826 | stop processing: n_tokens = 6875, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 53238 | processing task, is_child = 0
slot update_slots: id  3 | task 53238 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6657
slot update_slots: id  3 | task 53238 | n_tokens = 6466, memory_seq_rm [6466, end)
slot update_slots: id  3 | task 53238 | prompt processing progress, n_tokens = 6593, batch.n_tokens = 127, progress = 0.990386
slot update_slots: id  3 | task 53238 | n_tokens = 6593, memory_seq_rm [6593, end)
slot update_slots: id  3 | task 53238 | prompt processing progress, n_tokens = 6657, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 53238 | prompt done, n_tokens = 6657, batch.n_tokens = 64
slot init_sampler: id  3 | task 53238 | init sampler, took 0.98 ms, tokens: text = 6657, total = 6657
slot update_slots: id  3 | task 53238 | erasing old context checkpoint (pos_min = 3908, pos_max = 4931, size = 24.012 MiB)
slot update_slots: id  3 | task 53238 | created context checkpoint 8 of 8 (pos_min = 5851, pos_max = 6592, size = 17.399 MiB)
slot print_timing: id  3 | task 53238 | 
prompt eval time =     515.85 ms /   191 tokens (    2.70 ms per token,   370.26 tokens per second)
       eval time =    3904.02 ms /   142 tokens (   27.49 ms per token,    36.37 tokens per second)
      total time =    4419.86 ms /   333 tokens
slot      release: id  3 | task 53238 | stop processing: n_tokens = 6798, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 53382 | processing task, is_child = 0
slot update_slots: id  3 | task 53382 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6793
slot update_slots: id  3 | task 53382 | n_tokens = 6657, memory_seq_rm [6657, end)
slot update_slots: id  3 | task 53382 | prompt processing progress, n_tokens = 6729, batch.n_tokens = 72, progress = 0.990579
slot update_slots: id  3 | task 53382 | n_tokens = 6729, memory_seq_rm [6729, end)
slot update_slots: id  3 | task 53382 | prompt processing progress, n_tokens = 6793, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 53382 | prompt done, n_tokens = 6793, batch.n_tokens = 64
slot init_sampler: id  3 | task 53382 | init sampler, took 1.30 ms, tokens: text = 6793, total = 6793
slot update_slots: id  3 | task 53382 | erasing old context checkpoint (pos_min = 4304, pos_max = 5327, size = 24.012 MiB)
slot update_slots: id  3 | task 53382 | created context checkpoint 8 of 8 (pos_min = 5851, pos_max = 6728, size = 20.588 MiB)
slot print_timing: id  3 | task 53382 | 
prompt eval time =     396.79 ms /   136 tokens (    2.92 ms per token,   342.75 tokens per second)
       eval time =    4172.50 ms /   152 tokens (   27.45 ms per token,    36.43 tokens per second)
      total time =    4569.30 ms /   288 tokens
slot      release: id  3 | task 53382 | stop processing: n_tokens = 6944, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.844 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 53536 | processing task, is_child = 0
slot update_slots: id  3 | task 53536 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8047
slot update_slots: id  3 | task 53536 | n_tokens = 6793, memory_seq_rm [6793, end)
slot update_slots: id  3 | task 53536 | prompt processing progress, n_tokens = 7983, batch.n_tokens = 1190, progress = 0.992047
slot update_slots: id  3 | task 53536 | n_tokens = 7983, memory_seq_rm [7983, end)
slot update_slots: id  3 | task 53536 | prompt processing progress, n_tokens = 8047, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 53536 | prompt done, n_tokens = 8047, batch.n_tokens = 64
slot init_sampler: id  3 | task 53536 | init sampler, took 1.26 ms, tokens: text = 8047, total = 8047
slot update_slots: id  3 | task 53536 | erasing old context checkpoint (pos_min = 4580, pos_max = 5408, size = 19.439 MiB)
slot update_slots: id  3 | task 53536 | created context checkpoint 8 of 8 (pos_min = 6959, pos_max = 7982, size = 24.012 MiB)
slot print_timing: id  3 | task 53536 | 
prompt eval time =    1733.86 ms /  1254 tokens (    1.38 ms per token,   723.24 tokens per second)
       eval time =    7510.16 ms /   268 tokens (   28.02 ms per token,    35.68 tokens per second)
      total time =    9244.03 ms /  1522 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 53536 | stop processing: n_tokens = 8314, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.968
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 53806 | processing task, is_child = 0
slot update_slots: id  3 | task 53806 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8646
slot update_slots: id  3 | task 53806 | n_tokens = 8047, memory_seq_rm [8047, end)
slot update_slots: id  3 | task 53806 | prompt processing progress, n_tokens = 8582, batch.n_tokens = 535, progress = 0.992598
slot update_slots: id  3 | task 53806 | n_tokens = 8582, memory_seq_rm [8582, end)
slot update_slots: id  3 | task 53806 | prompt processing progress, n_tokens = 8646, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 53806 | prompt done, n_tokens = 8646, batch.n_tokens = 64
slot init_sampler: id  3 | task 53806 | init sampler, took 1.30 ms, tokens: text = 8646, total = 8646
slot update_slots: id  3 | task 53806 | erasing old context checkpoint (pos_min = 4580, pos_max = 5487, size = 21.292 MiB)
slot update_slots: id  3 | task 53806 | created context checkpoint 8 of 8 (pos_min = 7703, pos_max = 8581, size = 20.612 MiB)
slot print_timing: id  3 | task 53806 | 
prompt eval time =     895.55 ms /   599 tokens (    1.50 ms per token,   668.86 tokens per second)
       eval time =   18272.56 ms /   667 tokens (   27.40 ms per token,    36.50 tokens per second)
      total time =   19168.11 ms /  1266 tokens
slot      release: id  3 | task 53806 | stop processing: n_tokens = 9312, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.879 (> 0.100 thold), f_keep = 0.928
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 54475 | processing task, is_child = 0
slot update_slots: id  3 | task 54475 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9836
slot update_slots: id  3 | task 54475 | n_tokens = 8646, memory_seq_rm [8646, end)
slot update_slots: id  3 | task 54475 | prompt processing progress, n_tokens = 9772, batch.n_tokens = 1126, progress = 0.993493
slot update_slots: id  3 | task 54475 | n_tokens = 9772, memory_seq_rm [9772, end)
slot update_slots: id  3 | task 54475 | prompt processing progress, n_tokens = 9836, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 54475 | prompt done, n_tokens = 9836, batch.n_tokens = 64
slot init_sampler: id  3 | task 54475 | init sampler, took 1.45 ms, tokens: text = 9836, total = 9836
slot update_slots: id  3 | task 54475 | erasing old context checkpoint (pos_min = 4587, pos_max = 5566, size = 22.980 MiB)
slot update_slots: id  3 | task 54475 | created context checkpoint 8 of 8 (pos_min = 8875, pos_max = 9771, size = 21.034 MiB)
slot print_timing: id  3 | task 54475 | 
prompt eval time =    1703.59 ms /  1190 tokens (    1.43 ms per token,   698.52 tokens per second)
       eval time =    9161.83 ms /   339 tokens (   27.03 ms per token,    37.00 tokens per second)
      total time =   10865.42 ms /  1529 tokens
slot      release: id  3 | task 54475 | stop processing: n_tokens = 10174, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.930 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 54816 | processing task, is_child = 0
slot update_slots: id  3 | task 54816 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10573
slot update_slots: id  3 | task 54816 | n_tokens = 9836, memory_seq_rm [9836, end)
slot update_slots: id  3 | task 54816 | prompt processing progress, n_tokens = 10509, batch.n_tokens = 673, progress = 0.993947
slot update_slots: id  3 | task 54816 | n_tokens = 10509, memory_seq_rm [10509, end)
slot update_slots: id  3 | task 54816 | prompt processing progress, n_tokens = 10573, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 54816 | prompt done, n_tokens = 10573, batch.n_tokens = 64
slot init_sampler: id  3 | task 54816 | init sampler, took 1.66 ms, tokens: text = 10573, total = 10573
slot update_slots: id  3 | task 54816 | erasing old context checkpoint (pos_min = 5216, pos_max = 6239, size = 24.012 MiB)
slot update_slots: id  3 | task 54816 | created context checkpoint 8 of 8 (pos_min = 9485, pos_max = 10508, size = 24.012 MiB)
slot print_timing: id  3 | task 54816 | 
prompt eval time =    1145.50 ms /   737 tokens (    1.55 ms per token,   643.39 tokens per second)
       eval time =    6867.60 ms /   254 tokens (   27.04 ms per token,    36.99 tokens per second)
      total time =    8013.10 ms /   991 tokens
slot      release: id  3 | task 54816 | stop processing: n_tokens = 10826, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.890 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 55072 | processing task, is_child = 0
slot update_slots: id  3 | task 55072 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11876
slot update_slots: id  3 | task 55072 | n_tokens = 10573, memory_seq_rm [10573, end)
slot update_slots: id  3 | task 55072 | prompt processing progress, n_tokens = 11812, batch.n_tokens = 1239, progress = 0.994611
slot update_slots: id  3 | task 55072 | n_tokens = 11812, memory_seq_rm [11812, end)
slot update_slots: id  3 | task 55072 | prompt processing progress, n_tokens = 11876, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 55072 | prompt done, n_tokens = 11876, batch.n_tokens = 64
slot init_sampler: id  3 | task 55072 | init sampler, took 2.64 ms, tokens: text = 11876, total = 11876
slot update_slots: id  3 | task 55072 | erasing old context checkpoint (pos_min = 5495, pos_max = 6320, size = 19.369 MiB)
slot update_slots: id  3 | task 55072 | created context checkpoint 8 of 8 (pos_min = 10788, pos_max = 11811, size = 24.012 MiB)
slot print_timing: id  3 | task 55072 | 
prompt eval time =    1837.16 ms /  1303 tokens (    1.41 ms per token,   709.25 tokens per second)
       eval time =    8727.67 ms /   323 tokens (   27.02 ms per token,    37.01 tokens per second)
      total time =   10564.83 ms /  1626 tokens
slot      release: id  3 | task 55072 | stop processing: n_tokens = 12198, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 55397 | processing task, is_child = 0
slot update_slots: id  3 | task 55397 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12453
slot update_slots: id  3 | task 55397 | n_tokens = 11876, memory_seq_rm [11876, end)
slot update_slots: id  3 | task 55397 | prompt processing progress, n_tokens = 12389, batch.n_tokens = 513, progress = 0.994861
slot update_slots: id  3 | task 55397 | n_tokens = 12389, memory_seq_rm [12389, end)
slot update_slots: id  3 | task 55397 | prompt processing progress, n_tokens = 12453, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 55397 | prompt done, n_tokens = 12453, batch.n_tokens = 64
slot init_sampler: id  3 | task 55397 | init sampler, took 1.80 ms, tokens: text = 12453, total = 12453
slot update_slots: id  3 | task 55397 | erasing old context checkpoint (pos_min = 5629, pos_max = 6401, size = 18.126 MiB)
slot update_slots: id  3 | task 55397 | created context checkpoint 8 of 8 (pos_min = 11365, pos_max = 12388, size = 24.012 MiB)
slot print_timing: id  3 | task 55397 | 
prompt eval time =     864.86 ms /   577 tokens (    1.50 ms per token,   667.16 tokens per second)
       eval time =    6273.55 ms /   229 tokens (   27.40 ms per token,    36.50 tokens per second)
      total time =    7138.41 ms /   806 tokens
slot      release: id  3 | task 55397 | stop processing: n_tokens = 12681, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.883 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 55628 | processing task, is_child = 0
slot update_slots: id  3 | task 55628 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14109
slot update_slots: id  3 | task 55628 | n_tokens = 12453, memory_seq_rm [12453, end)
slot update_slots: id  3 | task 55628 | prompt processing progress, n_tokens = 14045, batch.n_tokens = 1592, progress = 0.995464
slot update_slots: id  3 | task 55628 | n_tokens = 14045, memory_seq_rm [14045, end)
slot update_slots: id  3 | task 55628 | prompt processing progress, n_tokens = 14109, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 55628 | prompt done, n_tokens = 14109, batch.n_tokens = 64
slot init_sampler: id  3 | task 55628 | init sampler, took 2.04 ms, tokens: text = 14109, total = 14109
slot update_slots: id  3 | task 55628 | erasing old context checkpoint (pos_min = 5851, pos_max = 6592, size = 17.399 MiB)
slot update_slots: id  3 | task 55628 | created context checkpoint 8 of 8 (pos_min = 13021, pos_max = 14044, size = 24.012 MiB)
slot print_timing: id  3 | task 55628 | 
prompt eval time =    2356.82 ms /  1656 tokens (    1.42 ms per token,   702.64 tokens per second)
       eval time =    4320.09 ms /   156 tokens (   27.69 ms per token,    36.11 tokens per second)
      total time =    6676.91 ms /  1812 tokens
slot      release: id  3 | task 55628 | stop processing: n_tokens = 14264, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 55786 | processing task, is_child = 0
slot update_slots: id  3 | task 55786 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14245
slot update_slots: id  3 | task 55786 | n_tokens = 14109, memory_seq_rm [14109, end)
slot update_slots: id  3 | task 55786 | prompt processing progress, n_tokens = 14181, batch.n_tokens = 72, progress = 0.995507
slot update_slots: id  3 | task 55786 | n_tokens = 14181, memory_seq_rm [14181, end)
slot update_slots: id  3 | task 55786 | prompt processing progress, n_tokens = 14245, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 55786 | prompt done, n_tokens = 14245, batch.n_tokens = 64
slot init_sampler: id  3 | task 55786 | init sampler, took 2.15 ms, tokens: text = 14245, total = 14245
slot update_slots: id  3 | task 55786 | erasing old context checkpoint (pos_min = 5851, pos_max = 6728, size = 20.588 MiB)
slot update_slots: id  3 | task 55786 | created context checkpoint 8 of 8 (pos_min = 13240, pos_max = 14180, size = 22.066 MiB)
slot print_timing: id  3 | task 55786 | 
prompt eval time =     402.43 ms /   136 tokens (    2.96 ms per token,   337.95 tokens per second)
       eval time =    9252.43 ms /   330 tokens (   28.04 ms per token,    35.67 tokens per second)
      total time =    9654.86 ms /   466 tokens
slot      release: id  3 | task 55786 | stop processing: n_tokens = 14574, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56118 | processing task, is_child = 0
slot update_slots: id  3 | task 56118 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14378
slot update_slots: id  3 | task 56118 | n_tokens = 14245, memory_seq_rm [14245, end)
slot update_slots: id  3 | task 56118 | prompt processing progress, n_tokens = 14314, batch.n_tokens = 69, progress = 0.995549
slot update_slots: id  3 | task 56118 | n_tokens = 14314, memory_seq_rm [14314, end)
slot update_slots: id  3 | task 56118 | prompt processing progress, n_tokens = 14378, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 56118 | prompt done, n_tokens = 14378, batch.n_tokens = 64
slot init_sampler: id  3 | task 56118 | init sampler, took 2.07 ms, tokens: text = 14378, total = 14378
slot update_slots: id  3 | task 56118 | erasing old context checkpoint (pos_min = 6959, pos_max = 7982, size = 24.012 MiB)
slot update_slots: id  3 | task 56118 | created context checkpoint 8 of 8 (pos_min = 13550, pos_max = 14313, size = 17.915 MiB)
slot print_timing: id  3 | task 56118 | 
prompt eval time =     428.57 ms /   133 tokens (    3.22 ms per token,   310.33 tokens per second)
       eval time =    6897.90 ms /   244 tokens (   28.27 ms per token,    35.37 tokens per second)
      total time =    7326.48 ms /   377 tokens
slot      release: id  3 | task 56118 | stop processing: n_tokens = 14621, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.948 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56364 | processing task, is_child = 0
slot update_slots: id  3 | task 56364 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15166
slot update_slots: id  3 | task 56364 | n_tokens = 14378, memory_seq_rm [14378, end)
slot update_slots: id  3 | task 56364 | prompt processing progress, n_tokens = 15102, batch.n_tokens = 724, progress = 0.995780
slot update_slots: id  3 | task 56364 | n_tokens = 15102, memory_seq_rm [15102, end)
slot update_slots: id  3 | task 56364 | prompt processing progress, n_tokens = 15166, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 56364 | prompt done, n_tokens = 15166, batch.n_tokens = 64
slot init_sampler: id  3 | task 56364 | init sampler, took 2.21 ms, tokens: text = 15166, total = 15166
slot update_slots: id  3 | task 56364 | erasing old context checkpoint (pos_min = 7703, pos_max = 8581, size = 20.612 MiB)
slot update_slots: id  3 | task 56364 | created context checkpoint 8 of 8 (pos_min = 14245, pos_max = 15101, size = 20.096 MiB)
slot print_timing: id  3 | task 56364 | 
prompt eval time =    1273.99 ms /   788 tokens (    1.62 ms per token,   618.53 tokens per second)
       eval time =    1579.06 ms /    57 tokens (   27.70 ms per token,    36.10 tokens per second)
      total time =    2853.05 ms /   845 tokens
slot      release: id  3 | task 56364 | stop processing: n_tokens = 15222, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56423 | processing task, is_child = 0
slot update_slots: id  3 | task 56423 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15405
slot update_slots: id  3 | task 56423 | n_tokens = 15166, memory_seq_rm [15166, end)
slot update_slots: id  3 | task 56423 | prompt processing progress, n_tokens = 15341, batch.n_tokens = 175, progress = 0.995845
slot update_slots: id  3 | task 56423 | n_tokens = 15341, memory_seq_rm [15341, end)
slot update_slots: id  3 | task 56423 | prompt processing progress, n_tokens = 15405, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 56423 | prompt done, n_tokens = 15405, batch.n_tokens = 64
slot init_sampler: id  3 | task 56423 | init sampler, took 2.22 ms, tokens: text = 15405, total = 15405
slot update_slots: id  3 | task 56423 | erasing old context checkpoint (pos_min = 8875, pos_max = 9771, size = 21.034 MiB)
slot update_slots: id  3 | task 56423 | created context checkpoint 8 of 8 (pos_min = 14317, pos_max = 15340, size = 24.012 MiB)
slot print_timing: id  3 | task 56423 | 
prompt eval time =     525.92 ms /   239 tokens (    2.20 ms per token,   454.45 tokens per second)
       eval time =    7014.82 ms /   250 tokens (   28.06 ms per token,    35.64 tokens per second)
      total time =    7540.74 ms /   489 tokens
slot      release: id  3 | task 56423 | stop processing: n_tokens = 15654, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56675 | processing task, is_child = 0
slot update_slots: id  3 | task 56675 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15681
slot update_slots: id  3 | task 56675 | n_tokens = 15405, memory_seq_rm [15405, end)
slot update_slots: id  3 | task 56675 | prompt processing progress, n_tokens = 15617, batch.n_tokens = 212, progress = 0.995919
slot update_slots: id  3 | task 56675 | n_tokens = 15617, memory_seq_rm [15617, end)
slot update_slots: id  3 | task 56675 | prompt processing progress, n_tokens = 15681, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 56675 | prompt done, n_tokens = 15681, batch.n_tokens = 64
slot init_sampler: id  3 | task 56675 | init sampler, took 2.20 ms, tokens: text = 15681, total = 15681
slot update_slots: id  3 | task 56675 | erasing old context checkpoint (pos_min = 9485, pos_max = 10508, size = 24.012 MiB)
slot update_slots: id  3 | task 56675 | created context checkpoint 8 of 8 (pos_min = 14630, pos_max = 15616, size = 23.144 MiB)
slot print_timing: id  3 | task 56675 | 
prompt eval time =     608.23 ms /   276 tokens (    2.20 ms per token,   453.77 tokens per second)
       eval time =    1006.82 ms /    36 tokens (   27.97 ms per token,    35.76 tokens per second)
      total time =    1615.05 ms /   312 tokens
slot      release: id  3 | task 56675 | stop processing: n_tokens = 15716, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56713 | processing task, is_child = 0
slot update_slots: id  3 | task 56713 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15752
slot update_slots: id  3 | task 56713 | n_tokens = 15681, memory_seq_rm [15681, end)
slot update_slots: id  3 | task 56713 | prompt processing progress, n_tokens = 15688, batch.n_tokens = 7, progress = 0.995937
slot update_slots: id  3 | task 56713 | n_tokens = 15688, memory_seq_rm [15688, end)
slot update_slots: id  3 | task 56713 | prompt processing progress, n_tokens = 15752, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 56713 | prompt done, n_tokens = 15752, batch.n_tokens = 64
slot init_sampler: id  3 | task 56713 | init sampler, took 2.42 ms, tokens: text = 15752, total = 15752
slot update_slots: id  3 | task 56713 | erasing old context checkpoint (pos_min = 10788, pos_max = 11811, size = 24.012 MiB)
slot update_slots: id  3 | task 56713 | created context checkpoint 8 of 8 (pos_min = 14692, pos_max = 15687, size = 23.355 MiB)
slot print_timing: id  3 | task 56713 | 
prompt eval time =     324.01 ms /    71 tokens (    4.56 ms per token,   219.13 tokens per second)
       eval time =    1019.42 ms /    37 tokens (   27.55 ms per token,    36.30 tokens per second)
      total time =    1343.43 ms /   108 tokens
slot      release: id  3 | task 56713 | stop processing: n_tokens = 15788, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56752 | processing task, is_child = 0
slot update_slots: id  3 | task 56752 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15821
slot update_slots: id  3 | task 56752 | n_tokens = 15752, memory_seq_rm [15752, end)
slot update_slots: id  3 | task 56752 | prompt processing progress, n_tokens = 15757, batch.n_tokens = 5, progress = 0.995955
slot update_slots: id  3 | task 56752 | n_tokens = 15757, memory_seq_rm [15757, end)
slot update_slots: id  3 | task 56752 | prompt processing progress, n_tokens = 15821, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 56752 | prompt done, n_tokens = 15821, batch.n_tokens = 64
slot init_sampler: id  3 | task 56752 | init sampler, took 2.96 ms, tokens: text = 15821, total = 15821
slot update_slots: id  3 | task 56752 | erasing old context checkpoint (pos_min = 11365, pos_max = 12388, size = 24.012 MiB)
slot update_slots: id  3 | task 56752 | created context checkpoint 8 of 8 (pos_min = 14764, pos_max = 15756, size = 23.285 MiB)
slot print_timing: id  3 | task 56752 | 
prompt eval time =     318.87 ms /    69 tokens (    4.62 ms per token,   216.39 tokens per second)
       eval time =    1274.65 ms /    48 tokens (   26.56 ms per token,    37.66 tokens per second)
      total time =    1593.52 ms /   117 tokens
slot      release: id  3 | task 56752 | stop processing: n_tokens = 15868, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.043
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 15868, total state size = 396.100 MiB
srv          load:  - looking for better prompt, base f_keep = 0.043, sim = 0.953
srv        update:  - cache state: 4 prompts, 2348.936 MiB (limits: 8192.000 MiB, 56064 tokens, 270720 est)
srv        update:    - prompt 0x59f268cdc130:   56063 tokens, checkpoints:  8,  1515.367 MiB
srv        update:    - prompt 0x59f268cdb230:    3037 tokens, checkpoints:  2,   134.246 MiB
srv        update:    - prompt 0x59f267be5e10:    2657 tokens, checkpoints:  2,   125.336 MiB
srv        update:    - prompt 0x59f260c53670:   15868 tokens, checkpoints:  8,   573.986 MiB
srv  get_availabl: prompt cache update took 603.79 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56802 | processing task, is_child = 0
slot update_slots: id  3 | task 56802 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 708
slot update_slots: id  3 | task 56802 | n_past = 675, slot.prompt.tokens.size() = 15868, seq_id = 3, pos_min = 14844, n_swa = 128
slot update_slots: id  3 | task 56802 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 56802 | erased invalidated context checkpoint (pos_min = 13021, pos_max = 14044, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 56802 | erased invalidated context checkpoint (pos_min = 13240, pos_max = 14180, n_swa = 128, size = 22.066 MiB)
slot update_slots: id  3 | task 56802 | erased invalidated context checkpoint (pos_min = 13550, pos_max = 14313, n_swa = 128, size = 17.915 MiB)
slot update_slots: id  3 | task 56802 | erased invalidated context checkpoint (pos_min = 14245, pos_max = 15101, n_swa = 128, size = 20.096 MiB)
slot update_slots: id  3 | task 56802 | erased invalidated context checkpoint (pos_min = 14317, pos_max = 15340, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 56802 | erased invalidated context checkpoint (pos_min = 14630, pos_max = 15616, n_swa = 128, size = 23.144 MiB)
slot update_slots: id  3 | task 56802 | erased invalidated context checkpoint (pos_min = 14692, pos_max = 15687, n_swa = 128, size = 23.355 MiB)
slot update_slots: id  3 | task 56802 | erased invalidated context checkpoint (pos_min = 14764, pos_max = 15756, n_swa = 128, size = 23.285 MiB)
slot update_slots: id  3 | task 56802 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 56802 | prompt processing progress, n_tokens = 644, batch.n_tokens = 644, progress = 0.909605
slot update_slots: id  3 | task 56802 | n_tokens = 644, memory_seq_rm [644, end)
slot update_slots: id  3 | task 56802 | prompt processing progress, n_tokens = 708, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 56802 | prompt done, n_tokens = 708, batch.n_tokens = 64
slot init_sampler: id  3 | task 56802 | init sampler, took 0.24 ms, tokens: text = 708, total = 708
slot update_slots: id  3 | task 56802 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 643, size = 15.101 MiB)
slot print_timing: id  3 | task 56802 | 
prompt eval time =     988.51 ms /   708 tokens (    1.40 ms per token,   716.23 tokens per second)
       eval time =    2482.89 ms /    91 tokens (   27.28 ms per token,    36.65 tokens per second)
      total time =    3471.39 ms /   799 tokens
slot      release: id  3 | task 56802 | stop processing: n_tokens = 798, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.289 (> 0.100 thold), f_keep = 0.887
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56895 | processing task, is_child = 0
slot update_slots: id  3 | task 56895 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2453
slot update_slots: id  3 | task 56895 | n_tokens = 708, memory_seq_rm [708, end)
slot update_slots: id  3 | task 56895 | prompt processing progress, n_tokens = 2389, batch.n_tokens = 1681, progress = 0.973909
slot update_slots: id  3 | task 56895 | n_tokens = 2389, memory_seq_rm [2389, end)
slot update_slots: id  3 | task 56895 | prompt processing progress, n_tokens = 2453, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 56895 | prompt done, n_tokens = 2453, batch.n_tokens = 64
slot init_sampler: id  3 | task 56895 | init sampler, took 0.40 ms, tokens: text = 2453, total = 2453
slot update_slots: id  3 | task 56895 | created context checkpoint 2 of 8 (pos_min = 1365, pos_max = 2388, size = 24.012 MiB)
slot print_timing: id  3 | task 56895 | 
prompt eval time =    2130.33 ms /  1745 tokens (    1.22 ms per token,   819.12 tokens per second)
       eval time =   18109.69 ms /   600 tokens (   30.18 ms per token,    33.13 tokens per second)
      total time =   20240.02 ms /  2345 tokens
slot      release: id  3 | task 56895 | stop processing: n_tokens = 3052, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.403 (> 0.100 thold), f_keep = 0.221
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3052, total state size = 95.578 MiB
srv          load:  - looking for better prompt, base f_keep = 0.221, sim = 0.403
srv          load:  - found better prompt with f_keep = 0.264, sim = 0.419
srv        update:  - cache state: 4 prompts, 2358.292 MiB (limits: 8192.000 MiB, 56064 tokens, 271018 est)
srv        update:    - prompt 0x59f268cdc130:   56063 tokens, checkpoints:  8,  1515.367 MiB
srv        update:    - prompt 0x59f268cdb230:    3037 tokens, checkpoints:  2,   134.246 MiB
srv        update:    - prompt 0x59f260c53670:   15868 tokens, checkpoints:  8,   573.986 MiB
srv        update:    - prompt 0x59f25768a8b0:    3052 tokens, checkpoints:  2,   134.692 MiB
srv  get_availabl: prompt cache update took 152.06 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 57497 | processing task, is_child = 0
slot update_slots: id  3 | task 57497 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1674
slot update_slots: id  3 | task 57497 | n_past = 702, slot.prompt.tokens.size() = 2657, seq_id = 3, pos_min = 1633, n_swa = 128
slot update_slots: id  3 | task 57497 | restored context checkpoint (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot update_slots: id  3 | task 57497 | erased invalidated context checkpoint (pos_min = 842, pos_max = 1865, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 57497 | n_tokens = 639, memory_seq_rm [639, end)
slot update_slots: id  3 | task 57497 | prompt processing progress, n_tokens = 1610, batch.n_tokens = 971, progress = 0.961768
slot update_slots: id  3 | task 57497 | n_tokens = 1610, memory_seq_rm [1610, end)
slot update_slots: id  3 | task 57497 | prompt processing progress, n_tokens = 1674, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 57497 | prompt done, n_tokens = 1674, batch.n_tokens = 64
slot init_sampler: id  3 | task 57497 | init sampler, took 0.30 ms, tokens: text = 1674, total = 1674
slot update_slots: id  3 | task 57497 | created context checkpoint 2 of 8 (pos_min = 586, pos_max = 1609, size = 24.012 MiB)
slot print_timing: id  3 | task 57497 | 
prompt eval time =    1209.50 ms /  1035 tokens (    1.17 ms per token,   855.72 tokens per second)
       eval time =    1785.62 ms /    73 tokens (   24.46 ms per token,    40.88 tokens per second)
      total time =    2995.12 ms /  1108 tokens
slot      release: id  3 | task 57497 | stop processing: n_tokens = 1746, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.621 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 57572 | processing task, is_child = 0
slot update_slots: id  3 | task 57572 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2695
slot update_slots: id  3 | task 57572 | n_tokens = 1674, memory_seq_rm [1674, end)
slot update_slots: id  3 | task 57572 | prompt processing progress, n_tokens = 2631, batch.n_tokens = 957, progress = 0.976252
slot update_slots: id  3 | task 57572 | n_tokens = 2631, memory_seq_rm [2631, end)
slot update_slots: id  3 | task 57572 | prompt processing progress, n_tokens = 2695, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 57572 | prompt done, n_tokens = 2695, batch.n_tokens = 64
slot init_sampler: id  3 | task 57572 | init sampler, took 0.46 ms, tokens: text = 2695, total = 2695
slot update_slots: id  3 | task 57572 | created context checkpoint 3 of 8 (pos_min = 1607, pos_max = 2630, size = 24.012 MiB)
slot print_timing: id  3 | task 57572 | 
prompt eval time =    1089.53 ms /  1021 tokens (    1.07 ms per token,   937.10 tokens per second)
       eval time =    1380.66 ms /    55 tokens (   25.10 ms per token,    39.84 tokens per second)
      total time =    2470.19 ms /  1076 tokens
slot      release: id  3 | task 57572 | stop processing: n_tokens = 2749, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.685 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 57629 | processing task, is_child = 0
slot update_slots: id  3 | task 57629 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3935
slot update_slots: id  3 | task 57629 | n_tokens = 2695, memory_seq_rm [2695, end)
slot update_slots: id  3 | task 57629 | prompt processing progress, n_tokens = 3871, batch.n_tokens = 1176, progress = 0.983736
slot update_slots: id  3 | task 57629 | n_tokens = 3871, memory_seq_rm [3871, end)
slot update_slots: id  3 | task 57629 | prompt processing progress, n_tokens = 3935, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 57629 | prompt done, n_tokens = 3935, batch.n_tokens = 64
slot init_sampler: id  3 | task 57629 | init sampler, took 0.61 ms, tokens: text = 3935, total = 3935
slot update_slots: id  3 | task 57629 | created context checkpoint 4 of 8 (pos_min = 2847, pos_max = 3870, size = 24.012 MiB)
slot print_timing: id  3 | task 57629 | 
prompt eval time =    1501.73 ms /  1240 tokens (    1.21 ms per token,   825.71 tokens per second)
       eval time =    1001.51 ms /    40 tokens (   25.04 ms per token,    39.94 tokens per second)
      total time =    2503.24 ms /  1280 tokens
slot      release: id  3 | task 57629 | stop processing: n_tokens = 3974, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.720 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 57671 | processing task, is_child = 0
slot update_slots: id  3 | task 57671 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5468
slot update_slots: id  3 | task 57671 | n_tokens = 3935, memory_seq_rm [3935, end)
slot update_slots: id  3 | task 57671 | prompt processing progress, n_tokens = 5404, batch.n_tokens = 1469, progress = 0.988296
slot update_slots: id  3 | task 57671 | n_tokens = 5404, memory_seq_rm [5404, end)
slot update_slots: id  3 | task 57671 | prompt processing progress, n_tokens = 5468, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 57671 | prompt done, n_tokens = 5468, batch.n_tokens = 64
slot init_sampler: id  3 | task 57671 | init sampler, took 0.84 ms, tokens: text = 5468, total = 5468
slot update_slots: id  3 | task 57671 | created context checkpoint 5 of 8 (pos_min = 4380, pos_max = 5403, size = 24.012 MiB)
slot print_timing: id  3 | task 57671 | 
prompt eval time =    1797.00 ms /  1533 tokens (    1.17 ms per token,   853.09 tokens per second)
       eval time =    2239.95 ms /    84 tokens (   26.67 ms per token,    37.50 tokens per second)
      total time =    4036.95 ms /  1617 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 57671 | stop processing: n_tokens = 5551, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.905 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 57757 | processing task, is_child = 0
slot update_slots: id  3 | task 57757 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6041
slot update_slots: id  3 | task 57757 | n_tokens = 5468, memory_seq_rm [5468, end)
slot update_slots: id  3 | task 57757 | prompt processing progress, n_tokens = 5977, batch.n_tokens = 509, progress = 0.989406
slot update_slots: id  3 | task 57757 | n_tokens = 5977, memory_seq_rm [5977, end)
slot update_slots: id  3 | task 57757 | prompt processing progress, n_tokens = 6041, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 57757 | prompt done, n_tokens = 6041, batch.n_tokens = 64
slot init_sampler: id  3 | task 57757 | init sampler, took 1.15 ms, tokens: text = 6041, total = 6041
slot update_slots: id  3 | task 57757 | created context checkpoint 6 of 8 (pos_min = 4953, pos_max = 5976, size = 24.012 MiB)
slot print_timing: id  3 | task 57757 | 
prompt eval time =     761.48 ms /   573 tokens (    1.33 ms per token,   752.48 tokens per second)
       eval time =    2277.25 ms /    85 tokens (   26.79 ms per token,    37.33 tokens per second)
      total time =    3038.73 ms /   658 tokens
slot      release: id  3 | task 57757 | stop processing: n_tokens = 6125, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.892 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 57844 | processing task, is_child = 0
slot update_slots: id  3 | task 57844 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6774
slot update_slots: id  3 | task 57844 | n_tokens = 6041, memory_seq_rm [6041, end)
slot update_slots: id  3 | task 57844 | prompt processing progress, n_tokens = 6710, batch.n_tokens = 669, progress = 0.990552
slot update_slots: id  3 | task 57844 | n_tokens = 6710, memory_seq_rm [6710, end)
slot update_slots: id  3 | task 57844 | prompt processing progress, n_tokens = 6774, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 57844 | prompt done, n_tokens = 6774, batch.n_tokens = 64
slot init_sampler: id  3 | task 57844 | init sampler, took 1.00 ms, tokens: text = 6774, total = 6774
slot update_slots: id  3 | task 57844 | created context checkpoint 7 of 8 (pos_min = 5686, pos_max = 6709, size = 24.012 MiB)
slot print_timing: id  3 | task 57844 | 
prompt eval time =    1087.42 ms /   733 tokens (    1.48 ms per token,   674.07 tokens per second)
       eval time =   41648.04 ms /  1451 tokens (   28.70 ms per token,    34.84 tokens per second)
      total time =   42735.46 ms /  2184 tokens
slot      release: id  3 | task 57844 | stop processing: n_tokens = 8224, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.225 (> 0.100 thold), f_keep = 0.139
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8224, total state size = 216.856 MiB
srv          load:  - looking for better prompt, base f_keep = 0.139, sim = 0.225
srv        update:  - cache state: 5 prompts, 2734.228 MiB (limits: 8192.000 MiB, 56064 tokens, 258394 est)
srv        update:    - prompt 0x59f268cdc130:   56063 tokens, checkpoints:  8,  1515.367 MiB
srv        update:    - prompt 0x59f268cdb230:    3037 tokens, checkpoints:  2,   134.246 MiB
srv        update:    - prompt 0x59f260c53670:   15868 tokens, checkpoints:  8,   573.986 MiB
srv        update:    - prompt 0x59f25768a8b0:    3052 tokens, checkpoints:  2,   134.692 MiB
srv        update:    - prompt 0x59f267bde450:    8224 tokens, checkpoints:  7,   375.936 MiB
srv  get_availabl: prompt cache update took 307.82 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 59297 | processing task, is_child = 0
slot update_slots: id  3 | task 59297 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5082
slot update_slots: id  3 | task 59297 | n_past = 1142, slot.prompt.tokens.size() = 8224, seq_id = 3, pos_min = 7200, n_swa = 128
slot update_slots: id  3 | task 59297 | restored context checkpoint (pos_min = 586, pos_max = 1609, size = 24.012 MiB)
slot update_slots: id  3 | task 59297 | erased invalidated context checkpoint (pos_min = 1607, pos_max = 2630, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 59297 | erased invalidated context checkpoint (pos_min = 2847, pos_max = 3870, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 59297 | erased invalidated context checkpoint (pos_min = 4380, pos_max = 5403, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 59297 | erased invalidated context checkpoint (pos_min = 4953, pos_max = 5976, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 59297 | erased invalidated context checkpoint (pos_min = 5686, pos_max = 6709, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 59297 | n_tokens = 1142, memory_seq_rm [1142, end)
slot update_slots: id  3 | task 59297 | prompt processing progress, n_tokens = 3190, batch.n_tokens = 2048, progress = 0.627706
slot update_slots: id  3 | task 59297 | n_tokens = 3190, memory_seq_rm [3190, end)
slot update_slots: id  3 | task 59297 | prompt processing progress, n_tokens = 5018, batch.n_tokens = 1828, progress = 0.987407
slot update_slots: id  3 | task 59297 | n_tokens = 5018, memory_seq_rm [5018, end)
slot update_slots: id  3 | task 59297 | prompt processing progress, n_tokens = 5082, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 59297 | prompt done, n_tokens = 5082, batch.n_tokens = 64
slot init_sampler: id  3 | task 59297 | init sampler, took 0.76 ms, tokens: text = 5082, total = 5082
slot update_slots: id  3 | task 59297 | created context checkpoint 3 of 8 (pos_min = 3994, pos_max = 5017, size = 24.012 MiB)
slot print_timing: id  3 | task 59297 | 
prompt eval time =    4154.74 ms /  3940 tokens (    1.05 ms per token,   948.31 tokens per second)
       eval time =    1391.42 ms /    56 tokens (   24.85 ms per token,    40.25 tokens per second)
      total time =    5546.17 ms /  3996 tokens
slot      release: id  3 | task 59297 | stop processing: n_tokens = 5137, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.804 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 59356 | processing task, is_child = 0
slot update_slots: id  3 | task 59356 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6322
slot update_slots: id  3 | task 59356 | n_tokens = 5082, memory_seq_rm [5082, end)
slot update_slots: id  3 | task 59356 | prompt processing progress, n_tokens = 6258, batch.n_tokens = 1176, progress = 0.989877
slot update_slots: id  3 | task 59356 | n_tokens = 6258, memory_seq_rm [6258, end)
slot update_slots: id  3 | task 59356 | prompt processing progress, n_tokens = 6322, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 59356 | prompt done, n_tokens = 6322, batch.n_tokens = 64
slot init_sampler: id  3 | task 59356 | init sampler, took 0.94 ms, tokens: text = 6322, total = 6322
slot update_slots: id  3 | task 59356 | created context checkpoint 4 of 8 (pos_min = 5234, pos_max = 6257, size = 24.012 MiB)
slot print_timing: id  3 | task 59356 | 
prompt eval time =    1535.35 ms /  1240 tokens (    1.24 ms per token,   807.63 tokens per second)
       eval time =   67680.34 ms /  2462 tokens (   27.49 ms per token,    36.38 tokens per second)
      total time =   69215.69 ms /  3702 tokens
slot      release: id  3 | task 59356 | stop processing: n_tokens = 8783, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.877 (> 0.100 thold), f_keep = 0.720
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 61820 | processing task, is_child = 0
slot update_slots: id  3 | task 61820 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7207
slot update_slots: id  3 | task 61820 | n_past = 6322, slot.prompt.tokens.size() = 8783, seq_id = 3, pos_min = 7759, n_swa = 128
slot update_slots: id  3 | task 61820 | restored context checkpoint (pos_min = 5234, pos_max = 6257, size = 24.012 MiB)
slot update_slots: id  3 | task 61820 | n_tokens = 6257, memory_seq_rm [6257, end)
slot update_slots: id  3 | task 61820 | prompt processing progress, n_tokens = 7143, batch.n_tokens = 886, progress = 0.991120
slot update_slots: id  3 | task 61820 | n_tokens = 7143, memory_seq_rm [7143, end)
slot update_slots: id  3 | task 61820 | prompt processing progress, n_tokens = 7207, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 61820 | prompt done, n_tokens = 7207, batch.n_tokens = 64
slot init_sampler: id  3 | task 61820 | init sampler, took 1.12 ms, tokens: text = 7207, total = 7207
slot update_slots: id  3 | task 61820 | created context checkpoint 5 of 8 (pos_min = 6119, pos_max = 7142, size = 24.012 MiB)
slot print_timing: id  3 | task 61820 | 
prompt eval time =    1257.95 ms /   950 tokens (    1.32 ms per token,   755.20 tokens per second)
       eval time =    1892.31 ms /    71 tokens (   26.65 ms per token,    37.52 tokens per second)
      total time =    3150.26 ms /  1021 tokens
slot      release: id  3 | task 61820 | stop processing: n_tokens = 7277, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.936 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 61893 | processing task, is_child = 0
slot update_slots: id  3 | task 61893 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7702
slot update_slots: id  3 | task 61893 | n_tokens = 7207, memory_seq_rm [7207, end)
slot update_slots: id  3 | task 61893 | prompt processing progress, n_tokens = 7638, batch.n_tokens = 431, progress = 0.991690
slot update_slots: id  3 | task 61893 | n_tokens = 7638, memory_seq_rm [7638, end)
slot update_slots: id  3 | task 61893 | prompt processing progress, n_tokens = 7702, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 61893 | prompt done, n_tokens = 7702, batch.n_tokens = 64
slot init_sampler: id  3 | task 61893 | init sampler, took 1.19 ms, tokens: text = 7702, total = 7702
slot update_slots: id  3 | task 61893 | created context checkpoint 6 of 8 (pos_min = 6614, pos_max = 7637, size = 24.012 MiB)
slot print_timing: id  3 | task 61893 | 
prompt eval time =     690.78 ms /   495 tokens (    1.40 ms per token,   716.58 tokens per second)
       eval time =    5349.16 ms /   200 tokens (   26.75 ms per token,    37.39 tokens per second)
      total time =    6039.94 ms /   695 tokens
slot      release: id  3 | task 61893 | stop processing: n_tokens = 7901, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.834 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 62095 | processing task, is_child = 0
slot update_slots: id  3 | task 62095 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9235
slot update_slots: id  3 | task 62095 | n_tokens = 7702, memory_seq_rm [7702, end)
slot update_slots: id  3 | task 62095 | prompt processing progress, n_tokens = 9171, batch.n_tokens = 1469, progress = 0.993070
slot update_slots: id  3 | task 62095 | n_tokens = 9171, memory_seq_rm [9171, end)
slot update_slots: id  3 | task 62095 | prompt processing progress, n_tokens = 9235, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 62095 | prompt done, n_tokens = 9235, batch.n_tokens = 64
slot init_sampler: id  3 | task 62095 | init sampler, took 1.99 ms, tokens: text = 9235, total = 9235
slot update_slots: id  3 | task 62095 | created context checkpoint 7 of 8 (pos_min = 8147, pos_max = 9170, size = 24.012 MiB)
slot print_timing: id  3 | task 62095 | 
prompt eval time =    1934.03 ms /  1533 tokens (    1.26 ms per token,   792.65 tokens per second)
       eval time =   33126.14 ms /  1222 tokens (   27.11 ms per token,    36.89 tokens per second)
      total time =   35060.17 ms /  2755 tokens
slot      release: id  3 | task 62095 | stop processing: n_tokens = 10456, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.883
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 63319 | processing task, is_child = 0
slot update_slots: id  3 | task 63319 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9516
slot update_slots: id  3 | task 63319 | n_past = 9235, slot.prompt.tokens.size() = 10456, seq_id = 3, pos_min = 9432, n_swa = 128
slot update_slots: id  3 | task 63319 | restored context checkpoint (pos_min = 8147, pos_max = 9170, size = 24.012 MiB)
slot update_slots: id  3 | task 63319 | n_tokens = 9170, memory_seq_rm [9170, end)
slot update_slots: id  3 | task 63319 | prompt processing progress, n_tokens = 9452, batch.n_tokens = 282, progress = 0.993275
slot update_slots: id  3 | task 63319 | n_tokens = 9452, memory_seq_rm [9452, end)
slot update_slots: id  3 | task 63319 | prompt processing progress, n_tokens = 9516, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 63319 | prompt done, n_tokens = 9516, batch.n_tokens = 64
slot init_sampler: id  3 | task 63319 | init sampler, took 1.41 ms, tokens: text = 9516, total = 9516
slot update_slots: id  3 | task 63319 | created context checkpoint 8 of 8 (pos_min = 8428, pos_max = 9451, size = 24.012 MiB)
slot print_timing: id  3 | task 63319 | 
prompt eval time =     616.33 ms /   346 tokens (    1.78 ms per token,   561.39 tokens per second)
       eval time =   57213.89 ms /  2100 tokens (   27.24 ms per token,    36.70 tokens per second)
      total time =   57830.22 ms /  2446 tokens
slot      release: id  3 | task 63319 | stop processing: n_tokens = 11615, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.873 (> 0.100 thold), f_keep = 0.819
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 65421 | processing task, is_child = 0
slot update_slots: id  3 | task 65421 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10899
slot update_slots: id  3 | task 65421 | n_past = 9516, slot.prompt.tokens.size() = 11615, seq_id = 3, pos_min = 10591, n_swa = 128
slot update_slots: id  3 | task 65421 | restored context checkpoint (pos_min = 8428, pos_max = 9451, size = 24.012 MiB)
slot update_slots: id  3 | task 65421 | n_tokens = 9451, memory_seq_rm [9451, end)
slot update_slots: id  3 | task 65421 | prompt processing progress, n_tokens = 10835, batch.n_tokens = 1384, progress = 0.994128
slot update_slots: id  3 | task 65421 | n_tokens = 10835, memory_seq_rm [10835, end)
slot update_slots: id  3 | task 65421 | prompt processing progress, n_tokens = 10899, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 65421 | prompt done, n_tokens = 10899, batch.n_tokens = 64
slot init_sampler: id  3 | task 65421 | init sampler, took 1.56 ms, tokens: text = 10899, total = 10899
slot update_slots: id  3 | task 65421 | erasing old context checkpoint (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot update_slots: id  3 | task 65421 | created context checkpoint 8 of 8 (pos_min = 9811, pos_max = 10834, size = 24.012 MiB)
slot print_timing: id  3 | task 65421 | 
prompt eval time =    1964.12 ms /  1448 tokens (    1.36 ms per token,   737.23 tokens per second)
       eval time =    2820.46 ms /   105 tokens (   26.86 ms per token,    37.23 tokens per second)
      total time =    4784.58 ms /  1553 tokens
slot      release: id  3 | task 65421 | stop processing: n_tokens = 11003, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 65528 | processing task, is_child = 0
slot update_slots: id  3 | task 65528 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11003
slot update_slots: id  3 | task 65528 | n_tokens = 10899, memory_seq_rm [10899, end)
slot update_slots: id  3 | task 65528 | prompt processing progress, n_tokens = 10939, batch.n_tokens = 40, progress = 0.994183
slot update_slots: id  3 | task 65528 | n_tokens = 10939, memory_seq_rm [10939, end)
slot update_slots: id  3 | task 65528 | prompt processing progress, n_tokens = 11003, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 65528 | prompt done, n_tokens = 11003, batch.n_tokens = 64
slot init_sampler: id  3 | task 65528 | init sampler, took 1.61 ms, tokens: text = 11003, total = 11003
slot update_slots: id  3 | task 65528 | erasing old context checkpoint (pos_min = 586, pos_max = 1609, size = 24.012 MiB)
slot update_slots: id  3 | task 65528 | created context checkpoint 8 of 8 (pos_min = 9979, pos_max = 10938, size = 22.511 MiB)
slot print_timing: id  3 | task 65528 | 
prompt eval time =     340.06 ms /   104 tokens (    3.27 ms per token,   305.83 tokens per second)
       eval time =    3079.35 ms /   114 tokens (   27.01 ms per token,    37.02 tokens per second)
      total time =    3419.41 ms /   218 tokens
slot      release: id  3 | task 65528 | stop processing: n_tokens = 11116, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 65644 | processing task, is_child = 0
slot update_slots: id  3 | task 65644 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11576
slot update_slots: id  3 | task 65644 | n_tokens = 11003, memory_seq_rm [11003, end)
slot update_slots: id  3 | task 65644 | prompt processing progress, n_tokens = 11512, batch.n_tokens = 509, progress = 0.994471
slot update_slots: id  3 | task 65644 | n_tokens = 11512, memory_seq_rm [11512, end)
slot update_slots: id  3 | task 65644 | prompt processing progress, n_tokens = 11576, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 65644 | prompt done, n_tokens = 11576, batch.n_tokens = 64
slot init_sampler: id  3 | task 65644 | init sampler, took 1.81 ms, tokens: text = 11576, total = 11576
slot update_slots: id  3 | task 65644 | erasing old context checkpoint (pos_min = 3994, pos_max = 5017, size = 24.012 MiB)
slot update_slots: id  3 | task 65644 | created context checkpoint 8 of 8 (pos_min = 10488, pos_max = 11511, size = 24.012 MiB)
slot print_timing: id  3 | task 65644 | 
prompt eval time =     823.43 ms /   573 tokens (    1.44 ms per token,   695.87 tokens per second)
       eval time =   19721.34 ms /   720 tokens (   27.39 ms per token,    36.51 tokens per second)
      total time =   20544.77 ms /  1293 tokens
slot      release: id  3 | task 65644 | stop processing: n_tokens = 12295, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.942
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 66366 | processing task, is_child = 0
slot update_slots: id  3 | task 66366 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12056
slot update_slots: id  3 | task 66366 | n_tokens = 11576, memory_seq_rm [11576, end)
slot update_slots: id  3 | task 66366 | prompt processing progress, n_tokens = 11992, batch.n_tokens = 416, progress = 0.994691
slot update_slots: id  3 | task 66366 | n_tokens = 11992, memory_seq_rm [11992, end)
slot update_slots: id  3 | task 66366 | prompt processing progress, n_tokens = 12056, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 66366 | prompt done, n_tokens = 12056, batch.n_tokens = 64
slot init_sampler: id  3 | task 66366 | init sampler, took 1.69 ms, tokens: text = 12056, total = 12056
slot update_slots: id  3 | task 66366 | erasing old context checkpoint (pos_min = 5234, pos_max = 6257, size = 24.012 MiB)
slot update_slots: id  3 | task 66366 | created context checkpoint 8 of 8 (pos_min = 11449, pos_max = 11991, size = 12.733 MiB)
slot print_timing: id  3 | task 66366 | 
prompt eval time =     746.85 ms /   480 tokens (    1.56 ms per token,   642.70 tokens per second)
       eval time =   28071.53 ms /  1007 tokens (   27.88 ms per token,    35.87 tokens per second)
      total time =   28818.38 ms /  1487 tokens
slot      release: id  3 | task 66366 | stop processing: n_tokens = 13062, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.923
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 67375 | processing task, is_child = 0
slot update_slots: id  3 | task 67375 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12411
slot update_slots: id  3 | task 67375 | n_past = 12056, slot.prompt.tokens.size() = 13062, seq_id = 3, pos_min = 12038, n_swa = 128
slot update_slots: id  3 | task 67375 | restored context checkpoint (pos_min = 11449, pos_max = 11991, size = 12.733 MiB)
slot update_slots: id  3 | task 67375 | n_tokens = 11991, memory_seq_rm [11991, end)
slot update_slots: id  3 | task 67375 | prompt processing progress, n_tokens = 12347, batch.n_tokens = 356, progress = 0.994843
slot update_slots: id  3 | task 67375 | n_tokens = 12347, memory_seq_rm [12347, end)
slot update_slots: id  3 | task 67375 | prompt processing progress, n_tokens = 12411, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 67375 | prompt done, n_tokens = 12411, batch.n_tokens = 64
slot init_sampler: id  3 | task 67375 | init sampler, took 1.81 ms, tokens: text = 12411, total = 12411
slot update_slots: id  3 | task 67375 | erasing old context checkpoint (pos_min = 6119, pos_max = 7142, size = 24.012 MiB)
slot update_slots: id  3 | task 67375 | created context checkpoint 8 of 8 (pos_min = 11449, pos_max = 12346, size = 21.057 MiB)
slot print_timing: id  3 | task 67375 | 
prompt eval time =     731.92 ms /   420 tokens (    1.74 ms per token,   573.83 tokens per second)
       eval time =   22979.85 ms /   831 tokens (   27.65 ms per token,    36.16 tokens per second)
      total time =   23711.76 ms /  1251 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 67375 | stop processing: n_tokens = 13241, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.937
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 68208 | processing task, is_child = 0
slot update_slots: id  3 | task 68208 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12902
slot update_slots: id  3 | task 68208 | n_tokens = 12411, memory_seq_rm [12411, end)
slot update_slots: id  3 | task 68208 | prompt processing progress, n_tokens = 12838, batch.n_tokens = 427, progress = 0.995040
slot update_slots: id  3 | task 68208 | n_tokens = 12838, memory_seq_rm [12838, end)
slot update_slots: id  3 | task 68208 | prompt processing progress, n_tokens = 12902, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 68208 | prompt done, n_tokens = 12902, batch.n_tokens = 64
slot init_sampler: id  3 | task 68208 | init sampler, took 2.44 ms, tokens: text = 12902, total = 12902
slot update_slots: id  3 | task 68208 | erasing old context checkpoint (pos_min = 6614, pos_max = 7637, size = 24.012 MiB)
slot update_slots: id  3 | task 68208 | created context checkpoint 8 of 8 (pos_min = 12217, pos_max = 12837, size = 14.562 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 68208
slot      release: id  3 | task 68208 | stop processing: n_tokens = 21867, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.031
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 21867, total state size = 536.770 MiB
srv          load:  - looking for better prompt, base f_keep = 0.031, sim = 0.951
srv        update:  - cache state: 6 prompts, 3437.910 MiB (limits: 8192.000 MiB, 56064 tokens, 257611 est)
srv        update:    - prompt 0x59f268cdc130:   56063 tokens, checkpoints:  8,  1515.367 MiB
srv        update:    - prompt 0x59f268cdb230:    3037 tokens, checkpoints:  2,   134.246 MiB
srv        update:    - prompt 0x59f260c53670:   15868 tokens, checkpoints:  8,   573.986 MiB
srv        update:    - prompt 0x59f25768a8b0:    3052 tokens, checkpoints:  2,   134.692 MiB
srv        update:    - prompt 0x59f267bde450:    8224 tokens, checkpoints:  7,   375.936 MiB
srv        update:    - prompt 0x59f2695613f0:   21867 tokens, checkpoints:  8,   703.682 MiB
srv  get_availabl: prompt cache update took 704.41 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 77177 | processing task, is_child = 0
slot update_slots: id  3 | task 77177 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 710
slot update_slots: id  3 | task 77177 | n_past = 675, slot.prompt.tokens.size() = 21867, seq_id = 3, pos_min = 20843, n_swa = 128
slot update_slots: id  3 | task 77177 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 77177 | erased invalidated context checkpoint (pos_min = 8147, pos_max = 9170, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 77177 | erased invalidated context checkpoint (pos_min = 8428, pos_max = 9451, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 77177 | erased invalidated context checkpoint (pos_min = 9811, pos_max = 10834, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 77177 | erased invalidated context checkpoint (pos_min = 9979, pos_max = 10938, n_swa = 128, size = 22.511 MiB)
slot update_slots: id  3 | task 77177 | erased invalidated context checkpoint (pos_min = 10488, pos_max = 11511, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 77177 | erased invalidated context checkpoint (pos_min = 11449, pos_max = 11991, n_swa = 128, size = 12.733 MiB)
slot update_slots: id  3 | task 77177 | erased invalidated context checkpoint (pos_min = 11449, pos_max = 12346, n_swa = 128, size = 21.057 MiB)
slot update_slots: id  3 | task 77177 | erased invalidated context checkpoint (pos_min = 12217, pos_max = 12837, n_swa = 128, size = 14.562 MiB)
slot update_slots: id  3 | task 77177 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 77177 | prompt processing progress, n_tokens = 646, batch.n_tokens = 646, progress = 0.909859
slot update_slots: id  3 | task 77177 | n_tokens = 646, memory_seq_rm [646, end)
slot update_slots: id  3 | task 77177 | prompt processing progress, n_tokens = 710, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 77177 | prompt done, n_tokens = 710, batch.n_tokens = 64
slot init_sampler: id  3 | task 77177 | init sampler, took 0.13 ms, tokens: text = 710, total = 710
slot update_slots: id  3 | task 77177 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 645, size = 15.148 MiB)
slot print_timing: id  3 | task 77177 | 
prompt eval time =     969.26 ms /   710 tokens (    1.37 ms per token,   732.52 tokens per second)
       eval time =    1740.16 ms /    63 tokens (   27.62 ms per token,    36.20 tokens per second)
      total time =    2709.42 ms /   773 tokens
slot      release: id  3 | task 77177 | stop processing: n_tokens = 772, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.935 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 77242 | processing task, is_child = 0
slot update_slots: id  3 | task 77242 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 759
slot update_slots: id  3 | task 77242 | n_tokens = 710, memory_seq_rm [710, end)
slot update_slots: id  3 | task 77242 | prompt processing progress, n_tokens = 759, batch.n_tokens = 49, progress = 1.000000
slot update_slots: id  3 | task 77242 | prompt done, n_tokens = 759, batch.n_tokens = 49
slot init_sampler: id  3 | task 77242 | init sampler, took 0.15 ms, tokens: text = 759, total = 759
slot print_timing: id  3 | task 77242 | 
prompt eval time =     255.55 ms /    49 tokens (    5.22 ms per token,   191.74 tokens per second)
       eval time =    2820.85 ms /   106 tokens (   26.61 ms per token,    37.58 tokens per second)
      total time =    3076.40 ms /   155 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 77242 | stop processing: n_tokens = 864, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.900 (> 0.100 thold), f_keep = 0.878
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 77349 | processing task, is_child = 0
slot update_slots: id  3 | task 77349 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 843
slot update_slots: id  3 | task 77349 | n_tokens = 759, memory_seq_rm [759, end)
slot update_slots: id  3 | task 77349 | prompt processing progress, n_tokens = 779, batch.n_tokens = 20, progress = 0.924081
slot update_slots: id  3 | task 77349 | n_tokens = 779, memory_seq_rm [779, end)
slot update_slots: id  3 | task 77349 | prompt processing progress, n_tokens = 843, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 77349 | prompt done, n_tokens = 843, batch.n_tokens = 64
slot init_sampler: id  3 | task 77349 | init sampler, took 0.18 ms, tokens: text = 843, total = 843
slot update_slots: id  3 | task 77349 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 778, size = 18.267 MiB)
slot print_timing: id  3 | task 77349 | 
prompt eval time =     361.05 ms /    84 tokens (    4.30 ms per token,   232.65 tokens per second)
       eval time =    1361.13 ms /    51 tokens (   26.69 ms per token,    37.47 tokens per second)
      total time =    1722.19 ms /   135 tokens
slot      release: id  3 | task 77349 | stop processing: n_tokens = 893, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.445 (> 0.100 thold), f_keep = 0.944
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 77402 | processing task, is_child = 0
slot update_slots: id  3 | task 77402 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1896
slot update_slots: id  3 | task 77402 | n_tokens = 843, memory_seq_rm [843, end)
slot update_slots: id  3 | task 77402 | prompt processing progress, n_tokens = 1832, batch.n_tokens = 989, progress = 0.966245
slot update_slots: id  3 | task 77402 | n_tokens = 1832, memory_seq_rm [1832, end)
slot update_slots: id  3 | task 77402 | prompt processing progress, n_tokens = 1896, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 77402 | prompt done, n_tokens = 1896, batch.n_tokens = 64
slot init_sampler: id  3 | task 77402 | init sampler, took 0.35 ms, tokens: text = 1896, total = 1896
slot update_slots: id  3 | task 77402 | created context checkpoint 3 of 8 (pos_min = 808, pos_max = 1831, size = 24.012 MiB)
slot print_timing: id  3 | task 77402 | 
prompt eval time =    1190.11 ms /  1053 tokens (    1.13 ms per token,   884.79 tokens per second)
       eval time =    1404.04 ms /    51 tokens (   27.53 ms per token,    36.32 tokens per second)
      total time =    2594.15 ms /  1104 tokens
slot      release: id  3 | task 77402 | stop processing: n_tokens = 1946, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 77455 | processing task, is_child = 0
slot update_slots: id  3 | task 77455 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1969
slot update_slots: id  3 | task 77455 | n_tokens = 1896, memory_seq_rm [1896, end)
slot update_slots: id  3 | task 77455 | prompt processing progress, n_tokens = 1905, batch.n_tokens = 9, progress = 0.967496
slot update_slots: id  3 | task 77455 | n_tokens = 1905, memory_seq_rm [1905, end)
slot update_slots: id  3 | task 77455 | prompt processing progress, n_tokens = 1969, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 77455 | prompt done, n_tokens = 1969, batch.n_tokens = 64
slot init_sampler: id  3 | task 77455 | init sampler, took 0.39 ms, tokens: text = 1969, total = 1969
slot update_slots: id  3 | task 77455 | created context checkpoint 4 of 8 (pos_min = 922, pos_max = 1904, size = 23.051 MiB)
slot print_timing: id  3 | task 77455 | 
prompt eval time =     305.24 ms /    73 tokens (    4.18 ms per token,   239.16 tokens per second)
       eval time =    4676.90 ms /   164 tokens (   28.52 ms per token,    35.07 tokens per second)
      total time =    4982.14 ms /   237 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 77455 | stop processing: n_tokens = 2132, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.924
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 77621 | processing task, is_child = 0
slot update_slots: id  3 | task 77621 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2042
slot update_slots: id  3 | task 77621 | n_tokens = 1969, memory_seq_rm [1969, end)
slot update_slots: id  3 | task 77621 | prompt processing progress, n_tokens = 1978, batch.n_tokens = 9, progress = 0.968658
slot update_slots: id  3 | task 77621 | n_tokens = 1978, memory_seq_rm [1978, end)
slot update_slots: id  3 | task 77621 | prompt processing progress, n_tokens = 2042, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 77621 | prompt done, n_tokens = 2042, batch.n_tokens = 64
slot init_sampler: id  3 | task 77621 | init sampler, took 0.34 ms, tokens: text = 2042, total = 2042
slot update_slots: id  3 | task 77621 | created context checkpoint 5 of 8 (pos_min = 1108, pos_max = 1977, size = 20.401 MiB)
slot print_timing: id  3 | task 77621 | 
prompt eval time =     336.61 ms /    73 tokens (    4.61 ms per token,   216.87 tokens per second)
       eval time =    2745.84 ms /    94 tokens (   29.21 ms per token,    34.23 tokens per second)
      total time =    3082.45 ms /   167 tokens
slot      release: id  3 | task 77621 | stop processing: n_tokens = 2135, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
