ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14992 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1578 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 64000 -> need 1580 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.67 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 64000
llama_context: n_ctx_seq     = 64000
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (64000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 64000 cells
llama_kv_cache:      CUDA0 KV buffer size =  1500.00 MiB
llama_kv_cache: size = 1500.00 MiB ( 64000 cells,  12 layers,  4/1 seqs), K (f16):  750.00 MiB, V (f16):  750.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   132.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 61.66 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv  log_server_r: request: GET /health 127.0.0.1 503
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  1 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  2 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  3 | task -1 | new slot, n_ctx = 64000
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-10

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 575
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 511, batch.n_tokens = 511, progress = 0.888696
slot update_slots: id  3 | task 0 | n_tokens = 511, memory_seq_rm [511, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 575, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 575, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.15 ms, tokens: text = 575, total = 575
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 510, size = 11.983 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =     871.95 ms /   575 tokens (    1.52 ms per token,   659.44 tokens per second)
       eval time =     667.97 ms /    30 tokens (   22.27 ms per token,    44.91 tokens per second)
      total time =    1539.92 ms /   605 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 604, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.890 (> 0.100 thold), f_keep = 0.952
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32 | processing task, is_child = 0
slot update_slots: id  3 | task 32 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 646
slot update_slots: id  3 | task 32 | n_tokens = 575, memory_seq_rm [575, end)
slot update_slots: id  3 | task 32 | prompt processing progress, n_tokens = 582, batch.n_tokens = 7, progress = 0.900929
slot update_slots: id  3 | task 32 | n_tokens = 582, memory_seq_rm [582, end)
slot update_slots: id  3 | task 32 | prompt processing progress, n_tokens = 646, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 32 | prompt done, n_tokens = 646, batch.n_tokens = 64
slot init_sampler: id  3 | task 32 | init sampler, took 0.12 ms, tokens: text = 646, total = 646
slot update_slots: id  3 | task 32 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 581, size = 13.648 MiB)
slot print_timing: id  3 | task 32 | 
prompt eval time =     287.56 ms /    71 tokens (    4.05 ms per token,   246.90 tokens per second)
       eval time =    1613.65 ms /    74 tokens (   21.81 ms per token,    45.86 tokens per second)
      total time =    1901.21 ms /   145 tokens
slot      release: id  3 | task 32 | stop processing: n_tokens = 719, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.891 (> 0.100 thold), f_keep = 0.787
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 108 | processing task, is_child = 0
slot update_slots: id  3 | task 108 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 635
slot update_slots: id  3 | task 108 | n_tokens = 566, memory_seq_rm [566, end)
slot update_slots: id  3 | task 108 | prompt processing progress, n_tokens = 571, batch.n_tokens = 5, progress = 0.899213
slot update_slots: id  3 | task 108 | n_tokens = 571, memory_seq_rm [571, end)
slot update_slots: id  3 | task 108 | prompt processing progress, n_tokens = 635, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 108 | prompt done, n_tokens = 635, batch.n_tokens = 64
slot init_sampler: id  3 | task 108 | init sampler, took 0.11 ms, tokens: text = 635, total = 635
slot print_timing: id  3 | task 108 | 
prompt eval time =     267.58 ms /    69 tokens (    3.88 ms per token,   257.86 tokens per second)
       eval time =     733.22 ms /    35 tokens (   20.95 ms per token,    47.73 tokens per second)
      total time =    1000.80 ms /   104 tokens
slot      release: id  3 | task 108 | stop processing: n_tokens = 669, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.906 (> 0.100 thold), f_keep = 0.949
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 145 | processing task, is_child = 0
slot update_slots: id  3 | task 145 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 701
slot update_slots: id  3 | task 145 | n_tokens = 635, memory_seq_rm [635, end)
slot update_slots: id  3 | task 145 | prompt processing progress, n_tokens = 637, batch.n_tokens = 2, progress = 0.908702
slot update_slots: id  3 | task 145 | n_tokens = 637, memory_seq_rm [637, end)
slot update_slots: id  3 | task 145 | prompt processing progress, n_tokens = 701, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 145 | prompt done, n_tokens = 701, batch.n_tokens = 64
slot init_sampler: id  3 | task 145 | init sampler, took 0.13 ms, tokens: text = 701, total = 701
slot print_timing: id  3 | task 145 | 
prompt eval time =     250.96 ms /    66 tokens (    3.80 ms per token,   262.99 tokens per second)
       eval time =     266.90 ms /    13 tokens (   20.53 ms per token,    48.71 tokens per second)
      total time =     517.86 ms /    79 tokens
slot      release: id  3 | task 145 | stop processing: n_tokens = 713, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.887 (> 0.100 thold), f_keep = 0.879
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 160 | processing task, is_child = 0
slot update_slots: id  3 | task 160 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 707
slot update_slots: id  3 | task 160 | n_tokens = 627, memory_seq_rm [627, end)
slot update_slots: id  3 | task 160 | prompt processing progress, n_tokens = 643, batch.n_tokens = 16, progress = 0.909477
slot update_slots: id  3 | task 160 | n_tokens = 643, memory_seq_rm [643, end)
slot update_slots: id  3 | task 160 | prompt processing progress, n_tokens = 707, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 160 | prompt done, n_tokens = 707, batch.n_tokens = 64
slot init_sampler: id  3 | task 160 | init sampler, took 0.13 ms, tokens: text = 707, total = 707
slot print_timing: id  3 | task 160 | 
prompt eval time =     320.34 ms /    80 tokens (    4.00 ms per token,   249.73 tokens per second)
       eval time =     672.25 ms /    32 tokens (   21.01 ms per token,    47.60 tokens per second)
      total time =     992.59 ms /   112 tokens
slot      release: id  3 | task 160 | stop processing: n_tokens = 738, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.530 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 194 | processing task, is_child = 0
slot update_slots: id  3 | task 194 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1333
slot update_slots: id  3 | task 194 | n_tokens = 707, memory_seq_rm [707, end)
slot update_slots: id  3 | task 194 | prompt processing progress, n_tokens = 1269, batch.n_tokens = 562, progress = 0.951988
slot update_slots: id  3 | task 194 | n_tokens = 1269, memory_seq_rm [1269, end)
slot update_slots: id  3 | task 194 | prompt processing progress, n_tokens = 1333, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 194 | prompt done, n_tokens = 1333, batch.n_tokens = 64
slot init_sampler: id  3 | task 194 | init sampler, took 0.25 ms, tokens: text = 1333, total = 1333
slot update_slots: id  3 | task 194 | created context checkpoint 3 of 8 (pos_min = 245, pos_max = 1268, size = 24.012 MiB)
slot print_timing: id  3 | task 194 | 
prompt eval time =     711.03 ms /   626 tokens (    1.14 ms per token,   880.42 tokens per second)
       eval time =     943.75 ms /    42 tokens (   22.47 ms per token,    44.50 tokens per second)
      total time =    1654.78 ms /   668 tokens
slot      release: id  3 | task 194 | stop processing: n_tokens = 1374, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.410 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 238 | processing task, is_child = 0
slot update_slots: id  3 | task 238 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3255
slot update_slots: id  3 | task 238 | n_tokens = 1333, memory_seq_rm [1333, end)
slot update_slots: id  3 | task 238 | prompt processing progress, n_tokens = 3191, batch.n_tokens = 1858, progress = 0.980338
slot update_slots: id  3 | task 238 | n_tokens = 3191, memory_seq_rm [3191, end)
slot update_slots: id  3 | task 238 | prompt processing progress, n_tokens = 3255, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 238 | prompt done, n_tokens = 3255, batch.n_tokens = 64
slot init_sampler: id  3 | task 238 | init sampler, took 0.64 ms, tokens: text = 3255, total = 3255
slot update_slots: id  3 | task 238 | created context checkpoint 4 of 8 (pos_min = 2167, pos_max = 3190, size = 24.012 MiB)
slot print_timing: id  3 | task 238 | 
prompt eval time =    1860.62 ms /  1922 tokens (    0.97 ms per token,  1032.99 tokens per second)
       eval time =     705.56 ms /    30 tokens (   23.52 ms per token,    42.52 tokens per second)
      total time =    2566.18 ms /  1952 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 238 | stop processing: n_tokens = 3284, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.747 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 270 | processing task, is_child = 0
slot update_slots: id  3 | task 270 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4360
slot update_slots: id  3 | task 270 | n_tokens = 3255, memory_seq_rm [3255, end)
slot update_slots: id  3 | task 270 | prompt processing progress, n_tokens = 4296, batch.n_tokens = 1041, progress = 0.985321
slot update_slots: id  3 | task 270 | n_tokens = 4296, memory_seq_rm [4296, end)
slot update_slots: id  3 | task 270 | prompt processing progress, n_tokens = 4360, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 270 | prompt done, n_tokens = 4360, batch.n_tokens = 64
slot init_sampler: id  3 | task 270 | init sampler, took 0.69 ms, tokens: text = 4360, total = 4360
slot update_slots: id  3 | task 270 | created context checkpoint 5 of 8 (pos_min = 3272, pos_max = 4295, size = 24.012 MiB)
slot print_timing: id  3 | task 270 | 
prompt eval time =    1171.36 ms /  1105 tokens (    1.06 ms per token,   943.35 tokens per second)
       eval time =   11721.16 ms /   514 tokens (   22.80 ms per token,    43.85 tokens per second)
      total time =   12892.52 ms /  1619 tokens
slot      release: id  3 | task 270 | stop processing: n_tokens = 4873, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.602 (> 0.100 thold), f_keep = 0.137
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4873, total state size = 138.279 MiB
srv          load:  - looking for better prompt, base f_keep = 0.137, sim = 0.602
srv        update:  - cache state: 1 prompts, 235.945 MiB (limits: 8192.000 MiB, 64000 tokens, 169190 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv  get_availabl: prompt cache update took 182.77 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 786 | processing task, is_child = 0
slot update_slots: id  3 | task 786 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1109
slot update_slots: id  3 | task 786 | n_past = 668, slot.prompt.tokens.size() = 4873, seq_id = 3, pos_min = 3849, n_swa = 128
slot update_slots: id  3 | task 786 | restored context checkpoint (pos_min = 245, pos_max = 1268, size = 24.012 MiB)
slot update_slots: id  3 | task 786 | erased invalidated context checkpoint (pos_min = 2167, pos_max = 3190, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 786 | erased invalidated context checkpoint (pos_min = 3272, pos_max = 4295, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 786 | n_tokens = 668, memory_seq_rm [668, end)
slot update_slots: id  3 | task 786 | prompt processing progress, n_tokens = 1045, batch.n_tokens = 377, progress = 0.942290
slot update_slots: id  3 | task 786 | n_tokens = 1045, memory_seq_rm [1045, end)
slot update_slots: id  3 | task 786 | prompt processing progress, n_tokens = 1109, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 786 | prompt done, n_tokens = 1109, batch.n_tokens = 64
slot init_sampler: id  3 | task 786 | init sampler, took 0.24 ms, tokens: text = 1109, total = 1109
slot print_timing: id  3 | task 786 | 
prompt eval time =     638.54 ms /   441 tokens (    1.45 ms per token,   690.64 tokens per second)
       eval time =    1345.47 ms /    60 tokens (   22.42 ms per token,    44.59 tokens per second)
      total time =    1984.01 ms /   501 tokens
slot      release: id  3 | task 786 | stop processing: n_tokens = 1168, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.774 (> 0.100 thold), f_keep = 0.949
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 848 | processing task, is_child = 0
slot update_slots: id  3 | task 848 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1433
slot update_slots: id  3 | task 848 | n_tokens = 1109, memory_seq_rm [1109, end)
slot update_slots: id  3 | task 848 | prompt processing progress, n_tokens = 1369, batch.n_tokens = 260, progress = 0.955338
slot update_slots: id  3 | task 848 | n_tokens = 1369, memory_seq_rm [1369, end)
slot update_slots: id  3 | task 848 | prompt processing progress, n_tokens = 1433, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 848 | prompt done, n_tokens = 1433, batch.n_tokens = 64
slot init_sampler: id  3 | task 848 | init sampler, took 0.28 ms, tokens: text = 1433, total = 1433
slot update_slots: id  3 | task 848 | created context checkpoint 4 of 8 (pos_min = 668, pos_max = 1368, size = 16.438 MiB)
slot print_timing: id  3 | task 848 | 
prompt eval time =     430.48 ms /   324 tokens (    1.33 ms per token,   752.64 tokens per second)
       eval time =    1530.81 ms /    69 tokens (   22.19 ms per token,    45.07 tokens per second)
      total time =    1961.29 ms /   393 tokens
slot      release: id  3 | task 848 | stop processing: n_tokens = 1501, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.427 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 919 | processing task, is_child = 0
slot update_slots: id  3 | task 919 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3355
slot update_slots: id  3 | task 919 | n_tokens = 1433, memory_seq_rm [1433, end)
slot update_slots: id  3 | task 919 | prompt processing progress, n_tokens = 3291, batch.n_tokens = 1858, progress = 0.980924
slot update_slots: id  3 | task 919 | n_tokens = 3291, memory_seq_rm [3291, end)
slot update_slots: id  3 | task 919 | prompt processing progress, n_tokens = 3355, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 919 | prompt done, n_tokens = 3355, batch.n_tokens = 64
slot init_sampler: id  3 | task 919 | init sampler, took 0.65 ms, tokens: text = 3355, total = 3355
slot update_slots: id  3 | task 919 | created context checkpoint 5 of 8 (pos_min = 2267, pos_max = 3290, size = 24.012 MiB)
slot print_timing: id  3 | task 919 | 
prompt eval time =    1896.07 ms /  1922 tokens (    0.99 ms per token,  1013.67 tokens per second)
       eval time =    1192.71 ms /    52 tokens (   22.94 ms per token,    43.60 tokens per second)
      total time =    3088.79 ms /  1974 tokens
slot      release: id  3 | task 919 | stop processing: n_tokens = 3406, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.752 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 973 | processing task, is_child = 0
slot update_slots: id  3 | task 973 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4460
slot update_slots: id  3 | task 973 | n_tokens = 3355, memory_seq_rm [3355, end)
slot update_slots: id  3 | task 973 | prompt processing progress, n_tokens = 4396, batch.n_tokens = 1041, progress = 0.985650
slot update_slots: id  3 | task 973 | n_tokens = 4396, memory_seq_rm [4396, end)
slot update_slots: id  3 | task 973 | prompt processing progress, n_tokens = 4460, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 973 | prompt done, n_tokens = 4460, batch.n_tokens = 64
slot init_sampler: id  3 | task 973 | init sampler, took 0.69 ms, tokens: text = 4460, total = 4460
slot update_slots: id  3 | task 973 | created context checkpoint 6 of 8 (pos_min = 3372, pos_max = 4395, size = 24.012 MiB)
slot print_timing: id  3 | task 973 | 
prompt eval time =    1188.24 ms /  1105 tokens (    1.08 ms per token,   929.95 tokens per second)
       eval time =   12717.61 ms /   545 tokens (   23.34 ms per token,    42.85 tokens per second)
      total time =   13905.85 ms /  1650 tokens
slot      release: id  3 | task 973 | stop processing: n_tokens = 5004, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.891
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1520 | processing task, is_child = 0
slot update_slots: id  3 | task 1520 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4680
slot update_slots: id  3 | task 1520 | n_tokens = 4460, memory_seq_rm [4460, end)
slot update_slots: id  3 | task 1520 | prompt processing progress, n_tokens = 4616, batch.n_tokens = 156, progress = 0.986325
slot update_slots: id  3 | task 1520 | n_tokens = 4616, memory_seq_rm [4616, end)
slot update_slots: id  3 | task 1520 | prompt processing progress, n_tokens = 4680, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1520 | prompt done, n_tokens = 4680, batch.n_tokens = 64
slot init_sampler: id  3 | task 1520 | init sampler, took 0.89 ms, tokens: text = 4680, total = 4680
slot update_slots: id  3 | task 1520 | created context checkpoint 7 of 8 (pos_min = 3980, pos_max = 4615, size = 14.914 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 1520
slot      release: id  3 | task 1520 | stop processing: n_tokens = 4947, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.748 (> 0.100 thold), f_keep = 0.212
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4947, total state size = 132.886 MiB
srv          load:  - looking for better prompt, base f_keep = 0.212, sim = 0.748
srv        update:  - cache state: 2 prompts, 497.849 MiB (limits: 8192.000 MiB, 64000 tokens, 161585 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv  get_availabl: prompt cache update took 178.21 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1791 | processing task, is_child = 0
slot update_slots: id  3 | task 1791 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1404
slot update_slots: id  3 | task 1791 | n_past = 1050, slot.prompt.tokens.size() = 4947, seq_id = 3, pos_min = 4227, n_swa = 128
slot update_slots: id  3 | task 1791 | restored context checkpoint (pos_min = 668, pos_max = 1368, size = 16.438 MiB)
slot update_slots: id  3 | task 1791 | erased invalidated context checkpoint (pos_min = 2267, pos_max = 3290, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1791 | erased invalidated context checkpoint (pos_min = 3372, pos_max = 4395, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1791 | erased invalidated context checkpoint (pos_min = 3980, pos_max = 4615, n_swa = 128, size = 14.914 MiB)
slot update_slots: id  3 | task 1791 | n_tokens = 1050, memory_seq_rm [1050, end)
slot update_slots: id  3 | task 1791 | prompt processing progress, n_tokens = 1340, batch.n_tokens = 290, progress = 0.954416
slot update_slots: id  3 | task 1791 | n_tokens = 1340, memory_seq_rm [1340, end)
slot update_slots: id  3 | task 1791 | prompt processing progress, n_tokens = 1404, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1791 | prompt done, n_tokens = 1404, batch.n_tokens = 64
slot init_sampler: id  3 | task 1791 | init sampler, took 0.24 ms, tokens: text = 1404, total = 1404
slot print_timing: id  3 | task 1791 | 
prompt eval time =     587.43 ms /   354 tokens (    1.66 ms per token,   602.62 tokens per second)
       eval time =    1067.12 ms /    46 tokens (   23.20 ms per token,    43.11 tokens per second)
      total time =    1654.55 ms /   400 tokens
slot      release: id  3 | task 1791 | stop processing: n_tokens = 1449, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.692 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1839 | processing task, is_child = 0
slot update_slots: id  3 | task 1839 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2030
slot update_slots: id  3 | task 1839 | n_tokens = 1404, memory_seq_rm [1404, end)
slot update_slots: id  3 | task 1839 | prompt processing progress, n_tokens = 1966, batch.n_tokens = 562, progress = 0.968473
slot update_slots: id  3 | task 1839 | n_tokens = 1966, memory_seq_rm [1966, end)
slot update_slots: id  3 | task 1839 | prompt processing progress, n_tokens = 2030, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1839 | prompt done, n_tokens = 2030, batch.n_tokens = 64
slot init_sampler: id  3 | task 1839 | init sampler, took 0.34 ms, tokens: text = 2030, total = 2030
slot update_slots: id  3 | task 1839 | created context checkpoint 5 of 8 (pos_min = 942, pos_max = 1965, size = 24.012 MiB)
slot print_timing: id  3 | task 1839 | 
prompt eval time =     754.08 ms /   626 tokens (    1.20 ms per token,   830.15 tokens per second)
       eval time =     777.46 ms /    33 tokens (   23.56 ms per token,    42.45 tokens per second)
      total time =    1531.53 ms /   659 tokens
slot      release: id  3 | task 1839 | stop processing: n_tokens = 2062, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1874 | processing task, is_child = 0
slot update_slots: id  3 | task 1874 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2096
slot update_slots: id  3 | task 1874 | n_tokens = 2030, memory_seq_rm [2030, end)
slot update_slots: id  3 | task 1874 | prompt processing progress, n_tokens = 2032, batch.n_tokens = 2, progress = 0.969466
slot update_slots: id  3 | task 1874 | n_tokens = 2032, memory_seq_rm [2032, end)
slot update_slots: id  3 | task 1874 | prompt processing progress, n_tokens = 2096, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1874 | prompt done, n_tokens = 2096, batch.n_tokens = 64
slot init_sampler: id  3 | task 1874 | init sampler, took 0.33 ms, tokens: text = 2096, total = 2096
slot update_slots: id  3 | task 1874 | created context checkpoint 6 of 8 (pos_min = 1038, pos_max = 2031, size = 23.309 MiB)
slot print_timing: id  3 | task 1874 | 
prompt eval time =     277.91 ms /    66 tokens (    4.21 ms per token,   237.49 tokens per second)
       eval time =    2642.16 ms /   113 tokens (   23.38 ms per token,    42.77 tokens per second)
      total time =    2920.07 ms /   179 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 1874 | stop processing: n_tokens = 2208, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.866 (> 0.100 thold), f_keep = 0.949
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1989 | processing task, is_child = 0
slot update_slots: id  3 | task 1989 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2420
slot update_slots: id  3 | task 1989 | n_tokens = 2096, memory_seq_rm [2096, end)
slot update_slots: id  3 | task 1989 | prompt processing progress, n_tokens = 2356, batch.n_tokens = 260, progress = 0.973554
slot update_slots: id  3 | task 1989 | n_tokens = 2356, memory_seq_rm [2356, end)
slot update_slots: id  3 | task 1989 | prompt processing progress, n_tokens = 2420, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1989 | prompt done, n_tokens = 2420, batch.n_tokens = 64
slot init_sampler: id  3 | task 1989 | init sampler, took 0.38 ms, tokens: text = 2420, total = 2420
slot update_slots: id  3 | task 1989 | created context checkpoint 7 of 8 (pos_min = 1332, pos_max = 2355, size = 24.012 MiB)
slot print_timing: id  3 | task 1989 | 
prompt eval time =     473.69 ms /   324 tokens (    1.46 ms per token,   683.99 tokens per second)
       eval time =    3014.94 ms /   129 tokens (   23.37 ms per token,    42.79 tokens per second)
      total time =    3488.63 ms /   453 tokens
slot      release: id  3 | task 1989 | stop processing: n_tokens = 2548, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.870 (> 0.100 thold), f_keep = 0.547
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2120 | processing task, is_child = 0
slot update_slots: id  3 | task 2120 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1602
slot update_slots: id  3 | task 2120 | n_past = 1394, slot.prompt.tokens.size() = 2548, seq_id = 3, pos_min = 1524, n_swa = 128
slot update_slots: id  3 | task 2120 | restored context checkpoint (pos_min = 1038, pos_max = 2031, size = 23.309 MiB)
slot update_slots: id  3 | task 2120 | erased invalidated context checkpoint (pos_min = 1332, pos_max = 2355, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 2120 | n_tokens = 1394, memory_seq_rm [1394, end)
slot update_slots: id  3 | task 2120 | prompt processing progress, n_tokens = 1538, batch.n_tokens = 144, progress = 0.960050
slot update_slots: id  3 | task 2120 | n_tokens = 1538, memory_seq_rm [1538, end)
slot update_slots: id  3 | task 2120 | prompt processing progress, n_tokens = 1602, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2120 | prompt done, n_tokens = 1602, batch.n_tokens = 64
slot init_sampler: id  3 | task 2120 | init sampler, took 0.31 ms, tokens: text = 1602, total = 1602
slot print_timing: id  3 | task 2120 | 
prompt eval time =     510.77 ms /   208 tokens (    2.46 ms per token,   407.23 tokens per second)
       eval time =    7147.67 ms /   306 tokens (   23.36 ms per token,    42.81 tokens per second)
      total time =    7658.44 ms /   514 tokens
slot      release: id  3 | task 2120 | stop processing: n_tokens = 1907, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.860 (> 0.100 thold), f_keep = 0.840
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2428 | processing task, is_child = 0
slot update_slots: id  3 | task 2428 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1863
slot update_slots: id  3 | task 2428 | n_tokens = 1602, memory_seq_rm [1602, end)
slot update_slots: id  3 | task 2428 | prompt processing progress, n_tokens = 1799, batch.n_tokens = 197, progress = 0.965647
slot update_slots: id  3 | task 2428 | n_tokens = 1799, memory_seq_rm [1799, end)
slot update_slots: id  3 | task 2428 | prompt processing progress, n_tokens = 1863, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2428 | prompt done, n_tokens = 1863, batch.n_tokens = 64
slot init_sampler: id  3 | task 2428 | init sampler, took 0.34 ms, tokens: text = 1863, total = 1863
slot print_timing: id  3 | task 2428 | 
prompt eval time =     510.06 ms /   261 tokens (    1.95 ms per token,   511.71 tokens per second)
       eval time =    5038.74 ms /   217 tokens (   23.22 ms per token,    43.07 tokens per second)
      total time =    5548.80 ms /   478 tokens
slot      release: id  3 | task 2428 | stop processing: n_tokens = 2079, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.826 (> 0.100 thold), f_keep = 0.764
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2647 | processing task, is_child = 0
slot update_slots: id  3 | task 2647 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1922
slot update_slots: id  3 | task 2647 | n_tokens = 1588, memory_seq_rm [1588, end)
slot update_slots: id  3 | task 2647 | prompt processing progress, n_tokens = 1858, batch.n_tokens = 270, progress = 0.966701
slot update_slots: id  3 | task 2647 | n_tokens = 1858, memory_seq_rm [1858, end)
slot update_slots: id  3 | task 2647 | prompt processing progress, n_tokens = 1922, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2647 | prompt done, n_tokens = 1922, batch.n_tokens = 64
slot init_sampler: id  3 | task 2647 | init sampler, took 0.33 ms, tokens: text = 1922, total = 1922
slot print_timing: id  3 | task 2647 | 
prompt eval time =     580.40 ms /   334 tokens (    1.74 ms per token,   575.47 tokens per second)
       eval time =    6722.91 ms /   288 tokens (   23.34 ms per token,    42.84 tokens per second)
      total time =    7303.31 ms /   622 tokens
slot      release: id  3 | task 2647 | stop processing: n_tokens = 2209, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.587 (> 0.100 thold), f_keep = 0.870
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2937 | processing task, is_child = 0
slot update_slots: id  3 | task 2937 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3275
slot update_slots: id  3 | task 2937 | n_tokens = 1922, memory_seq_rm [1922, end)
slot update_slots: id  3 | task 2937 | prompt processing progress, n_tokens = 3211, batch.n_tokens = 1289, progress = 0.980458
slot update_slots: id  3 | task 2937 | n_tokens = 3211, memory_seq_rm [3211, end)
slot update_slots: id  3 | task 2937 | prompt processing progress, n_tokens = 3275, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2937 | prompt done, n_tokens = 3275, batch.n_tokens = 64
slot init_sampler: id  3 | task 2937 | init sampler, took 0.51 ms, tokens: text = 3275, total = 3275
slot update_slots: id  3 | task 2937 | created context checkpoint 7 of 8 (pos_min = 2187, pos_max = 3210, size = 24.012 MiB)
slot print_timing: id  3 | task 2937 | 
prompt eval time =    1439.71 ms /  1353 tokens (    1.06 ms per token,   939.77 tokens per second)
       eval time =    8152.65 ms /   344 tokens (   23.70 ms per token,    42.19 tokens per second)
      total time =    9592.37 ms /  1697 tokens
slot      release: id  3 | task 2937 | stop processing: n_tokens = 3618, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.795 (> 0.100 thold), f_keep = 0.525
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3283 | processing task, is_child = 0
slot update_slots: id  3 | task 3283 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2390
slot update_slots: id  3 | task 3283 | n_past = 1900, slot.prompt.tokens.size() = 3618, seq_id = 3, pos_min = 2594, n_swa = 128
slot update_slots: id  3 | task 3283 | restored context checkpoint (pos_min = 1038, pos_max = 2031, size = 23.309 MiB)
slot update_slots: id  3 | task 3283 | erased invalidated context checkpoint (pos_min = 2187, pos_max = 3210, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 3283 | n_tokens = 1900, memory_seq_rm [1900, end)
slot update_slots: id  3 | task 3283 | prompt processing progress, n_tokens = 2326, batch.n_tokens = 426, progress = 0.973222
slot update_slots: id  3 | task 3283 | n_tokens = 2326, memory_seq_rm [2326, end)
slot update_slots: id  3 | task 3283 | prompt processing progress, n_tokens = 2390, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3283 | prompt done, n_tokens = 2390, batch.n_tokens = 64
slot init_sampler: id  3 | task 3283 | init sampler, took 0.47 ms, tokens: text = 2390, total = 2390
slot update_slots: id  3 | task 3283 | created context checkpoint 7 of 8 (pos_min = 1332, pos_max = 2325, size = 23.309 MiB)
slot print_timing: id  3 | task 3283 | 
prompt eval time =     681.33 ms /   490 tokens (    1.39 ms per token,   719.18 tokens per second)
       eval time =   18279.78 ms /   762 tokens (   23.99 ms per token,    41.69 tokens per second)
      total time =   18961.11 ms /  1252 tokens
slot      release: id  3 | task 3283 | stop processing: n_tokens = 3151, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.888 (> 0.100 thold), f_keep = 0.758
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4047 | processing task, is_child = 0
slot update_slots: id  3 | task 4047 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2692
slot update_slots: id  3 | task 4047 | n_tokens = 2390, memory_seq_rm [2390, end)
slot update_slots: id  3 | task 4047 | prompt processing progress, n_tokens = 2628, batch.n_tokens = 238, progress = 0.976226
slot update_slots: id  3 | task 4047 | n_tokens = 2628, memory_seq_rm [2628, end)
slot update_slots: id  3 | task 4047 | prompt processing progress, n_tokens = 2692, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4047 | prompt done, n_tokens = 2692, batch.n_tokens = 64
slot init_sampler: id  3 | task 4047 | init sampler, took 0.41 ms, tokens: text = 2692, total = 2692
slot update_slots: id  3 | task 4047 | created context checkpoint 8 of 8 (pos_min = 2127, pos_max = 2627, size = 11.748 MiB)
slot print_timing: id  3 | task 4047 | 
prompt eval time =     526.25 ms /   302 tokens (    1.74 ms per token,   573.87 tokens per second)
       eval time =   10438.84 ms /   429 tokens (   24.33 ms per token,    41.10 tokens per second)
      total time =   10965.10 ms /   731 tokens
slot      release: id  3 | task 4047 | stop processing: n_tokens = 3120, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.881 (> 0.100 thold), f_keep = 0.863
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4478 | processing task, is_child = 0
slot update_slots: id  3 | task 4478 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3054
slot update_slots: id  3 | task 4478 | n_tokens = 2692, memory_seq_rm [2692, end)
slot update_slots: id  3 | task 4478 | prompt processing progress, n_tokens = 2990, batch.n_tokens = 298, progress = 0.979044
slot update_slots: id  3 | task 4478 | n_tokens = 2990, memory_seq_rm [2990, end)
slot update_slots: id  3 | task 4478 | prompt processing progress, n_tokens = 3054, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4478 | prompt done, n_tokens = 3054, batch.n_tokens = 64
slot init_sampler: id  3 | task 4478 | init sampler, took 0.48 ms, tokens: text = 3054, total = 3054
slot update_slots: id  3 | task 4478 | erasing old context checkpoint (pos_min = 0, pos_max = 510, size = 11.983 MiB)
slot update_slots: id  3 | task 4478 | created context checkpoint 8 of 8 (pos_min = 2390, pos_max = 2989, size = 14.070 MiB)
slot print_timing: id  3 | task 4478 | 
prompt eval time =     521.11 ms /   362 tokens (    1.44 ms per token,   694.67 tokens per second)
       eval time =    1403.49 ms /    57 tokens (   24.62 ms per token,    40.61 tokens per second)
      total time =    1924.60 ms /   419 tokens
slot      release: id  3 | task 4478 | stop processing: n_tokens = 3110, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4537 | processing task, is_child = 0
slot update_slots: id  3 | task 4537 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3120
slot update_slots: id  3 | task 4537 | n_tokens = 3054, memory_seq_rm [3054, end)
slot update_slots: id  3 | task 4537 | prompt processing progress, n_tokens = 3056, batch.n_tokens = 2, progress = 0.979487
slot update_slots: id  3 | task 4537 | n_tokens = 3056, memory_seq_rm [3056, end)
slot update_slots: id  3 | task 4537 | prompt processing progress, n_tokens = 3120, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4537 | prompt done, n_tokens = 3120, batch.n_tokens = 64
slot init_sampler: id  3 | task 4537 | init sampler, took 0.50 ms, tokens: text = 3120, total = 3120
slot update_slots: id  3 | task 4537 | erasing old context checkpoint (pos_min = 0, pos_max = 581, size = 13.648 MiB)
slot update_slots: id  3 | task 4537 | created context checkpoint 8 of 8 (pos_min = 2390, pos_max = 3055, size = 15.617 MiB)
slot print_timing: id  3 | task 4537 | 
prompt eval time =     260.30 ms /    66 tokens (    3.94 ms per token,   253.56 tokens per second)
       eval time =    3497.56 ms /   148 tokens (   23.63 ms per token,    42.32 tokens per second)
      total time =    3757.86 ms /   214 tokens
slot      release: id  3 | task 4537 | stop processing: n_tokens = 3267, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.741 (> 0.100 thold), f_keep = 0.721
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4687 | processing task, is_child = 0
slot update_slots: id  3 | task 4687 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3175
slot update_slots: id  3 | task 4687 | n_past = 2354, slot.prompt.tokens.size() = 3267, seq_id = 3, pos_min = 2390, n_swa = 128
slot update_slots: id  3 | task 4687 | restored context checkpoint (pos_min = 2127, pos_max = 2627, size = 11.748 MiB)
slot update_slots: id  3 | task 4687 | erased invalidated context checkpoint (pos_min = 2390, pos_max = 2989, n_swa = 128, size = 14.070 MiB)
slot update_slots: id  3 | task 4687 | erased invalidated context checkpoint (pos_min = 2390, pos_max = 3055, n_swa = 128, size = 15.617 MiB)
slot update_slots: id  3 | task 4687 | n_tokens = 2354, memory_seq_rm [2354, end)
slot update_slots: id  3 | task 4687 | prompt processing progress, n_tokens = 3111, batch.n_tokens = 757, progress = 0.979843
slot update_slots: id  3 | task 4687 | n_tokens = 3111, memory_seq_rm [3111, end)
slot update_slots: id  3 | task 4687 | prompt processing progress, n_tokens = 3175, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4687 | prompt done, n_tokens = 3175, batch.n_tokens = 64
slot init_sampler: id  3 | task 4687 | init sampler, took 0.49 ms, tokens: text = 3175, total = 3175
slot update_slots: id  3 | task 4687 | created context checkpoint 7 of 8 (pos_min = 2227, pos_max = 3110, size = 20.729 MiB)
slot print_timing: id  3 | task 4687 | 
prompt eval time =    1076.77 ms /   821 tokens (    1.31 ms per token,   762.47 tokens per second)
       eval time =    2032.92 ms /    86 tokens (   23.64 ms per token,    42.30 tokens per second)
      total time =    3109.69 ms /   907 tokens
slot      release: id  3 | task 4687 | stop processing: n_tokens = 3260, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.940 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4775 | processing task, is_child = 0
slot update_slots: id  3 | task 4775 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3378
slot update_slots: id  3 | task 4775 | n_tokens = 3175, memory_seq_rm [3175, end)
slot update_slots: id  3 | task 4775 | prompt processing progress, n_tokens = 3314, batch.n_tokens = 139, progress = 0.981054
slot update_slots: id  3 | task 4775 | n_tokens = 3314, memory_seq_rm [3314, end)
slot update_slots: id  3 | task 4775 | prompt processing progress, n_tokens = 3378, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4775 | prompt done, n_tokens = 3378, batch.n_tokens = 64
slot init_sampler: id  3 | task 4775 | init sampler, took 0.51 ms, tokens: text = 3378, total = 3378
slot update_slots: id  3 | task 4775 | created context checkpoint 8 of 8 (pos_min = 2493, pos_max = 3313, size = 19.252 MiB)
slot print_timing: id  3 | task 4775 | 
prompt eval time =     447.53 ms /   203 tokens (    2.20 ms per token,   453.60 tokens per second)
       eval time =    1374.88 ms /    59 tokens (   23.30 ms per token,    42.91 tokens per second)
      total time =    1822.40 ms /   262 tokens
slot      release: id  3 | task 4775 | stop processing: n_tokens = 3436, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.637 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4836 | processing task, is_child = 0
slot update_slots: id  3 | task 4836 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5300
slot update_slots: id  3 | task 4836 | n_tokens = 3378, memory_seq_rm [3378, end)
slot update_slots: id  3 | task 4836 | prompt processing progress, n_tokens = 5236, batch.n_tokens = 1858, progress = 0.987925
slot update_slots: id  3 | task 4836 | n_tokens = 5236, memory_seq_rm [5236, end)
slot update_slots: id  3 | task 4836 | prompt processing progress, n_tokens = 5300, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4836 | prompt done, n_tokens = 5300, batch.n_tokens = 64
slot init_sampler: id  3 | task 4836 | init sampler, took 0.80 ms, tokens: text = 5300, total = 5300
slot update_slots: id  3 | task 4836 | erasing old context checkpoint (pos_min = 245, pos_max = 1268, size = 24.012 MiB)
slot update_slots: id  3 | task 4836 | created context checkpoint 8 of 8 (pos_min = 4212, pos_max = 5235, size = 24.012 MiB)
slot print_timing: id  3 | task 4836 | 
prompt eval time =    2049.80 ms /  1922 tokens (    1.07 ms per token,   937.65 tokens per second)
       eval time =    1165.34 ms /    49 tokens (   23.78 ms per token,    42.05 tokens per second)
      total time =    3215.15 ms /  1971 tokens
slot      release: id  3 | task 4836 | stop processing: n_tokens = 5348, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.881 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4887 | processing task, is_child = 0
slot update_slots: id  3 | task 4887 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6019
slot update_slots: id  3 | task 4887 | n_tokens = 5300, memory_seq_rm [5300, end)
slot update_slots: id  3 | task 4887 | prompt processing progress, n_tokens = 5955, batch.n_tokens = 655, progress = 0.989367
slot update_slots: id  3 | task 4887 | n_tokens = 5955, memory_seq_rm [5955, end)
slot update_slots: id  3 | task 4887 | prompt processing progress, n_tokens = 6019, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4887 | prompt done, n_tokens = 6019, batch.n_tokens = 64
slot init_sampler: id  3 | task 4887 | init sampler, took 0.94 ms, tokens: text = 6019, total = 6019
slot update_slots: id  3 | task 4887 | erasing old context checkpoint (pos_min = 668, pos_max = 1368, size = 16.438 MiB)
slot update_slots: id  3 | task 4887 | created context checkpoint 8 of 8 (pos_min = 4931, pos_max = 5954, size = 24.012 MiB)
slot print_timing: id  3 | task 4887 | 
prompt eval time =     930.12 ms /   719 tokens (    1.29 ms per token,   773.02 tokens per second)
       eval time =    1287.90 ms /    54 tokens (   23.85 ms per token,    41.93 tokens per second)
      total time =    2218.02 ms /   773 tokens
slot      release: id  3 | task 4887 | stop processing: n_tokens = 6072, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4943 | processing task, is_child = 0
slot update_slots: id  3 | task 4943 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6076
slot update_slots: id  3 | task 4943 | n_tokens = 6019, memory_seq_rm [6019, end)
slot update_slots: id  3 | task 4943 | prompt processing progress, n_tokens = 6076, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 4943 | prompt done, n_tokens = 6076, batch.n_tokens = 57
slot init_sampler: id  3 | task 4943 | init sampler, took 0.96 ms, tokens: text = 6076, total = 6076
slot print_timing: id  3 | task 4943 | 
prompt eval time =     153.20 ms /    57 tokens (    2.69 ms per token,   372.07 tokens per second)
       eval time =    7066.09 ms /   291 tokens (   24.28 ms per token,    41.18 tokens per second)
      total time =    7219.29 ms /   348 tokens
slot      release: id  3 | task 4943 | stop processing: n_tokens = 6366, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.952 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5235 | processing task, is_child = 0
slot update_slots: id  3 | task 5235 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6380
slot update_slots: id  3 | task 5235 | n_tokens = 6076, memory_seq_rm [6076, end)
slot update_slots: id  3 | task 5235 | prompt processing progress, n_tokens = 6316, batch.n_tokens = 240, progress = 0.989969
slot update_slots: id  3 | task 5235 | n_tokens = 6316, memory_seq_rm [6316, end)
slot update_slots: id  3 | task 5235 | prompt processing progress, n_tokens = 6380, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5235 | prompt done, n_tokens = 6380, batch.n_tokens = 64
slot init_sampler: id  3 | task 5235 | init sampler, took 0.92 ms, tokens: text = 6380, total = 6380
slot update_slots: id  3 | task 5235 | erasing old context checkpoint (pos_min = 942, pos_max = 1965, size = 24.012 MiB)
slot update_slots: id  3 | task 5235 | created context checkpoint 8 of 8 (pos_min = 5463, pos_max = 6315, size = 20.002 MiB)
slot print_timing: id  3 | task 5235 | 
prompt eval time =     548.43 ms /   304 tokens (    1.80 ms per token,   554.31 tokens per second)
       eval time =    1004.88 ms /    41 tokens (   24.51 ms per token,    40.80 tokens per second)
      total time =    1553.31 ms /   345 tokens
slot      release: id  3 | task 5235 | stop processing: n_tokens = 6420, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5278 | processing task, is_child = 0
slot update_slots: id  3 | task 5278 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6579
slot update_slots: id  3 | task 5278 | n_tokens = 6380, memory_seq_rm [6380, end)
slot update_slots: id  3 | task 5278 | prompt processing progress, n_tokens = 6515, batch.n_tokens = 135, progress = 0.990272
slot update_slots: id  3 | task 5278 | n_tokens = 6515, memory_seq_rm [6515, end)
slot update_slots: id  3 | task 5278 | prompt processing progress, n_tokens = 6579, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5278 | prompt done, n_tokens = 6579, batch.n_tokens = 64
slot init_sampler: id  3 | task 5278 | init sampler, took 1.26 ms, tokens: text = 6579, total = 6579
slot update_slots: id  3 | task 5278 | erasing old context checkpoint (pos_min = 1038, pos_max = 2031, size = 23.309 MiB)
slot update_slots: id  3 | task 5278 | created context checkpoint 8 of 8 (pos_min = 5662, pos_max = 6514, size = 20.002 MiB)
slot print_timing: id  3 | task 5278 | 
prompt eval time =     461.24 ms /   199 tokens (    2.32 ms per token,   431.44 tokens per second)
       eval time =    1822.76 ms /    74 tokens (   24.63 ms per token,    40.60 tokens per second)
      total time =    2284.00 ms /   273 tokens
slot      release: id  3 | task 5278 | stop processing: n_tokens = 6652, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.907 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5354 | processing task, is_child = 0
slot update_slots: id  3 | task 5354 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7255
slot update_slots: id  3 | task 5354 | n_tokens = 6579, memory_seq_rm [6579, end)
slot update_slots: id  3 | task 5354 | prompt processing progress, n_tokens = 7191, batch.n_tokens = 612, progress = 0.991179
slot update_slots: id  3 | task 5354 | n_tokens = 7191, memory_seq_rm [7191, end)
slot update_slots: id  3 | task 5354 | prompt processing progress, n_tokens = 7255, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5354 | prompt done, n_tokens = 7255, batch.n_tokens = 64
slot init_sampler: id  3 | task 5354 | init sampler, took 1.08 ms, tokens: text = 7255, total = 7255
slot update_slots: id  3 | task 5354 | erasing old context checkpoint (pos_min = 1332, pos_max = 2325, size = 23.309 MiB)
slot update_slots: id  3 | task 5354 | created context checkpoint 8 of 8 (pos_min = 6167, pos_max = 7190, size = 24.012 MiB)
slot print_timing: id  3 | task 5354 | 
prompt eval time =     951.75 ms /   676 tokens (    1.41 ms per token,   710.27 tokens per second)
       eval time =    3041.47 ms /   124 tokens (   24.53 ms per token,    40.77 tokens per second)
      total time =    3993.22 ms /   800 tokens
slot      release: id  3 | task 5354 | stop processing: n_tokens = 7378, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5480 | processing task, is_child = 0
slot update_slots: id  3 | task 5480 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7313
slot update_slots: id  3 | task 5480 | n_tokens = 7255, memory_seq_rm [7255, end)
slot update_slots: id  3 | task 5480 | prompt processing progress, n_tokens = 7313, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 5480 | prompt done, n_tokens = 7313, batch.n_tokens = 58
slot init_sampler: id  3 | task 5480 | init sampler, took 1.12 ms, tokens: text = 7313, total = 7313
slot print_timing: id  3 | task 5480 | 
prompt eval time =     161.87 ms /    58 tokens (    2.79 ms per token,   358.31 tokens per second)
       eval time =    2171.00 ms /    88 tokens (   24.67 ms per token,    40.53 tokens per second)
      total time =    2332.87 ms /   146 tokens
slot      release: id  3 | task 5480 | stop processing: n_tokens = 7400, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5569 | processing task, is_child = 0
slot update_slots: id  3 | task 5569 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7370
slot update_slots: id  3 | task 5569 | n_tokens = 7313, memory_seq_rm [7313, end)
slot update_slots: id  3 | task 5569 | prompt processing progress, n_tokens = 7370, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 5569 | prompt done, n_tokens = 7370, batch.n_tokens = 57
slot init_sampler: id  3 | task 5569 | init sampler, took 1.14 ms, tokens: text = 7370, total = 7370
slot update_slots: id  3 | task 5569 | erasing old context checkpoint (pos_min = 2127, pos_max = 2627, size = 11.748 MiB)
slot update_slots: id  3 | task 5569 | created context checkpoint 8 of 8 (pos_min = 6376, pos_max = 7312, size = 21.972 MiB)
slot print_timing: id  3 | task 5569 | 
prompt eval time =     172.27 ms /    57 tokens (    3.02 ms per token,   330.88 tokens per second)
       eval time =    1898.65 ms /    77 tokens (   24.66 ms per token,    40.56 tokens per second)
      total time =    2070.91 ms /   134 tokens
slot      release: id  3 | task 5569 | stop processing: n_tokens = 7446, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.943 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5647 | processing task, is_child = 0
slot update_slots: id  3 | task 5647 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7813
slot update_slots: id  3 | task 5647 | n_tokens = 7370, memory_seq_rm [7370, end)
slot update_slots: id  3 | task 5647 | prompt processing progress, n_tokens = 7749, batch.n_tokens = 379, progress = 0.991809
slot update_slots: id  3 | task 5647 | n_tokens = 7749, memory_seq_rm [7749, end)
slot update_slots: id  3 | task 5647 | prompt processing progress, n_tokens = 7813, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5647 | prompt done, n_tokens = 7813, batch.n_tokens = 64
slot init_sampler: id  3 | task 5647 | init sampler, took 1.55 ms, tokens: text = 7813, total = 7813
slot update_slots: id  3 | task 5647 | erasing old context checkpoint (pos_min = 2227, pos_max = 3110, size = 20.729 MiB)
slot update_slots: id  3 | task 5647 | created context checkpoint 8 of 8 (pos_min = 6725, pos_max = 7748, size = 24.012 MiB)
slot print_timing: id  3 | task 5647 | 
prompt eval time =     620.22 ms /   443 tokens (    1.40 ms per token,   714.26 tokens per second)
       eval time =    2598.44 ms /   101 tokens (   25.73 ms per token,    38.87 tokens per second)
      total time =    3218.66 ms /   544 tokens
slot      release: id  3 | task 5647 | stop processing: n_tokens = 7913, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5750 | processing task, is_child = 0
slot update_slots: id  3 | task 5750 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7871
slot update_slots: id  3 | task 5750 | n_tokens = 7813, memory_seq_rm [7813, end)
slot update_slots: id  3 | task 5750 | prompt processing progress, n_tokens = 7871, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 5750 | prompt done, n_tokens = 7871, batch.n_tokens = 58
slot init_sampler: id  3 | task 5750 | init sampler, took 1.15 ms, tokens: text = 7871, total = 7871
slot print_timing: id  3 | task 5750 | 
prompt eval time =     162.36 ms /    58 tokens (    2.80 ms per token,   357.24 tokens per second)
       eval time =    1575.98 ms /    64 tokens (   24.62 ms per token,    40.61 tokens per second)
      total time =    1738.33 ms /   122 tokens
slot      release: id  3 | task 5750 | stop processing: n_tokens = 7934, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.804 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5815 | processing task, is_child = 0
slot update_slots: id  3 | task 5815 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9793
slot update_slots: id  3 | task 5815 | n_tokens = 7871, memory_seq_rm [7871, end)
slot update_slots: id  3 | task 5815 | prompt processing progress, n_tokens = 9729, batch.n_tokens = 1858, progress = 0.993465
slot update_slots: id  3 | task 5815 | n_tokens = 9729, memory_seq_rm [9729, end)
slot update_slots: id  3 | task 5815 | prompt processing progress, n_tokens = 9793, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5815 | prompt done, n_tokens = 9793, batch.n_tokens = 64
slot init_sampler: id  3 | task 5815 | init sampler, took 1.95 ms, tokens: text = 9793, total = 9793
slot update_slots: id  3 | task 5815 | erasing old context checkpoint (pos_min = 2493, pos_max = 3313, size = 19.252 MiB)
slot update_slots: id  3 | task 5815 | created context checkpoint 8 of 8 (pos_min = 8705, pos_max = 9728, size = 24.012 MiB)
slot print_timing: id  3 | task 5815 | 
prompt eval time =    2193.19 ms /  1922 tokens (    1.14 ms per token,   876.35 tokens per second)
       eval time =    2982.79 ms /   120 tokens (   24.86 ms per token,    40.23 tokens per second)
      total time =    5175.98 ms /  2042 tokens
slot      release: id  3 | task 5815 | stop processing: n_tokens = 9912, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5937 | processing task, is_child = 0
slot update_slots: id  3 | task 5937 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9851
slot update_slots: id  3 | task 5937 | n_tokens = 9793, memory_seq_rm [9793, end)
slot update_slots: id  3 | task 5937 | prompt processing progress, n_tokens = 9851, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 5937 | prompt done, n_tokens = 9851, batch.n_tokens = 58
slot init_sampler: id  3 | task 5937 | init sampler, took 1.64 ms, tokens: text = 9851, total = 9851
slot print_timing: id  3 | task 5937 | 
prompt eval time =     165.32 ms /    58 tokens (    2.85 ms per token,   350.84 tokens per second)
       eval time =    7112.85 ms /   285 tokens (   24.96 ms per token,    40.07 tokens per second)
      total time =    7278.17 ms /   343 tokens
slot      release: id  3 | task 5937 | stop processing: n_tokens = 10135, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6223 | processing task, is_child = 0
slot update_slots: id  3 | task 6223 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10155
slot update_slots: id  3 | task 6223 | n_tokens = 9851, memory_seq_rm [9851, end)
slot update_slots: id  3 | task 6223 | prompt processing progress, n_tokens = 10091, batch.n_tokens = 240, progress = 0.993698
slot update_slots: id  3 | task 6223 | n_tokens = 10091, memory_seq_rm [10091, end)
slot update_slots: id  3 | task 6223 | prompt processing progress, n_tokens = 10155, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6223 | prompt done, n_tokens = 10155, batch.n_tokens = 64
slot init_sampler: id  3 | task 6223 | init sampler, took 1.48 ms, tokens: text = 10155, total = 10155
slot update_slots: id  3 | task 6223 | erasing old context checkpoint (pos_min = 4212, pos_max = 5235, size = 24.012 MiB)
slot update_slots: id  3 | task 6223 | created context checkpoint 8 of 8 (pos_min = 9111, pos_max = 10090, size = 22.980 MiB)
slot print_timing: id  3 | task 6223 | 
prompt eval time =     566.94 ms /   304 tokens (    1.86 ms per token,   536.21 tokens per second)
       eval time =    3240.65 ms /   131 tokens (   24.74 ms per token,    40.42 tokens per second)
      total time =    3807.59 ms /   435 tokens
slot      release: id  3 | task 6223 | stop processing: n_tokens = 10285, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6356 | processing task, is_child = 0
slot update_slots: id  3 | task 6356 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10221
slot update_slots: id  3 | task 6356 | n_tokens = 10155, memory_seq_rm [10155, end)
slot update_slots: id  3 | task 6356 | prompt processing progress, n_tokens = 10157, batch.n_tokens = 2, progress = 0.993738
slot update_slots: id  3 | task 6356 | n_tokens = 10157, memory_seq_rm [10157, end)
slot update_slots: id  3 | task 6356 | prompt processing progress, n_tokens = 10221, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6356 | prompt done, n_tokens = 10221, batch.n_tokens = 64
slot init_sampler: id  3 | task 6356 | init sampler, took 1.44 ms, tokens: text = 10221, total = 10221
slot update_slots: id  3 | task 6356 | erasing old context checkpoint (pos_min = 4931, pos_max = 5954, size = 24.012 MiB)
slot update_slots: id  3 | task 6356 | created context checkpoint 8 of 8 (pos_min = 9261, pos_max = 10156, size = 21.011 MiB)
slot print_timing: id  3 | task 6356 | 
prompt eval time =     211.57 ms /    66 tokens (    3.21 ms per token,   311.96 tokens per second)
       eval time =    1553.97 ms /    62 tokens (   25.06 ms per token,    39.90 tokens per second)
      total time =    1765.54 ms /   128 tokens
slot      release: id  3 | task 6356 | stop processing: n_tokens = 10282, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6420 | processing task, is_child = 0
slot update_slots: id  3 | task 6420 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10607
slot update_slots: id  3 | task 6420 | n_tokens = 10221, memory_seq_rm [10221, end)
slot update_slots: id  3 | task 6420 | prompt processing progress, n_tokens = 10543, batch.n_tokens = 322, progress = 0.993966
slot update_slots: id  3 | task 6420 | n_tokens = 10543, memory_seq_rm [10543, end)
slot update_slots: id  3 | task 6420 | prompt processing progress, n_tokens = 10607, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6420 | prompt done, n_tokens = 10607, batch.n_tokens = 64
slot init_sampler: id  3 | task 6420 | init sampler, took 1.52 ms, tokens: text = 10607, total = 10607
slot update_slots: id  3 | task 6420 | erasing old context checkpoint (pos_min = 5463, pos_max = 6315, size = 20.002 MiB)
slot update_slots: id  3 | task 6420 | created context checkpoint 8 of 8 (pos_min = 9536, pos_max = 10542, size = 23.613 MiB)
slot print_timing: id  3 | task 6420 | 
prompt eval time =     563.41 ms /   386 tokens (    1.46 ms per token,   685.12 tokens per second)
       eval time =    2683.01 ms /   108 tokens (   24.84 ms per token,    40.25 tokens per second)
      total time =    3246.42 ms /   494 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 6420 | stop processing: n_tokens = 10714, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6530 | processing task, is_child = 0
slot update_slots: id  3 | task 6530 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10665
slot update_slots: id  3 | task 6530 | n_tokens = 10607, memory_seq_rm [10607, end)
slot update_slots: id  3 | task 6530 | prompt processing progress, n_tokens = 10665, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 6530 | prompt done, n_tokens = 10665, batch.n_tokens = 58
slot init_sampler: id  3 | task 6530 | init sampler, took 2.18 ms, tokens: text = 10665, total = 10665
slot print_timing: id  3 | task 6530 | 
prompt eval time =     165.52 ms /    58 tokens (    2.85 ms per token,   350.40 tokens per second)
       eval time =    1627.41 ms /    65 tokens (   25.04 ms per token,    39.94 tokens per second)
      total time =    1792.94 ms /   123 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 6530 | stop processing: n_tokens = 10729, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6596 | processing task, is_child = 0
slot update_slots: id  3 | task 6596 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11233
slot update_slots: id  3 | task 6596 | n_tokens = 10665, memory_seq_rm [10665, end)
slot update_slots: id  3 | task 6596 | prompt processing progress, n_tokens = 11169, batch.n_tokens = 504, progress = 0.994303
slot update_slots: id  3 | task 6596 | n_tokens = 11169, memory_seq_rm [11169, end)
slot update_slots: id  3 | task 6596 | prompt processing progress, n_tokens = 11233, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6596 | prompt done, n_tokens = 11233, batch.n_tokens = 64
slot init_sampler: id  3 | task 6596 | init sampler, took 1.60 ms, tokens: text = 11233, total = 11233
slot update_slots: id  3 | task 6596 | erasing old context checkpoint (pos_min = 5662, pos_max = 6514, size = 20.002 MiB)
slot update_slots: id  3 | task 6596 | created context checkpoint 8 of 8 (pos_min = 10155, pos_max = 11168, size = 23.778 MiB)
slot print_timing: id  3 | task 6596 | 
prompt eval time =     714.45 ms /   568 tokens (    1.26 ms per token,   795.01 tokens per second)
       eval time =    5764.23 ms /   235 tokens (   24.53 ms per token,    40.77 tokens per second)
      total time =    6478.69 ms /   803 tokens
slot      release: id  3 | task 6596 | stop processing: n_tokens = 11467, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6833 | processing task, is_child = 0
slot update_slots: id  3 | task 6833 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11315
slot update_slots: id  3 | task 6833 | n_tokens = 11233, memory_seq_rm [11233, end)
slot update_slots: id  3 | task 6833 | prompt processing progress, n_tokens = 11251, batch.n_tokens = 18, progress = 0.994344
slot update_slots: id  3 | task 6833 | n_tokens = 11251, memory_seq_rm [11251, end)
slot update_slots: id  3 | task 6833 | prompt processing progress, n_tokens = 11315, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6833 | prompt done, n_tokens = 11315, batch.n_tokens = 64
slot init_sampler: id  3 | task 6833 | init sampler, took 1.79 ms, tokens: text = 11315, total = 11315
slot update_slots: id  3 | task 6833 | erasing old context checkpoint (pos_min = 6167, pos_max = 7190, size = 24.012 MiB)
slot update_slots: id  3 | task 6833 | created context checkpoint 8 of 8 (pos_min = 10443, pos_max = 11250, size = 18.947 MiB)
slot print_timing: id  3 | task 6833 | 
prompt eval time =     269.28 ms /    82 tokens (    3.28 ms per token,   304.52 tokens per second)
       eval time =    8862.89 ms /   360 tokens (   24.62 ms per token,    40.62 tokens per second)
      total time =    9132.17 ms /   442 tokens
slot      release: id  3 | task 6833 | stop processing: n_tokens = 11674, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7195 | processing task, is_child = 0
slot update_slots: id  3 | task 7195 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11620
slot update_slots: id  3 | task 7195 | n_tokens = 11315, memory_seq_rm [11315, end)
slot update_slots: id  3 | task 7195 | prompt processing progress, n_tokens = 11556, batch.n_tokens = 241, progress = 0.994492
slot update_slots: id  3 | task 7195 | n_tokens = 11556, memory_seq_rm [11556, end)
slot update_slots: id  3 | task 7195 | prompt processing progress, n_tokens = 11620, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7195 | prompt done, n_tokens = 11620, batch.n_tokens = 64
slot init_sampler: id  3 | task 7195 | init sampler, took 1.71 ms, tokens: text = 11620, total = 11620
slot update_slots: id  3 | task 7195 | erasing old context checkpoint (pos_min = 6376, pos_max = 7312, size = 21.972 MiB)
slot update_slots: id  3 | task 7195 | created context checkpoint 8 of 8 (pos_min = 10650, pos_max = 11555, size = 21.245 MiB)
slot print_timing: id  3 | task 7195 | 
prompt eval time =     565.79 ms /   305 tokens (    1.86 ms per token,   539.07 tokens per second)
       eval time =    1872.24 ms /    76 tokens (   24.63 ms per token,    40.59 tokens per second)
      total time =    2438.03 ms /   381 tokens
slot      release: id  3 | task 7195 | stop processing: n_tokens = 11695, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7273 | processing task, is_child = 0
slot update_slots: id  3 | task 7273 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11678
slot update_slots: id  3 | task 7273 | n_tokens = 11620, memory_seq_rm [11620, end)
slot update_slots: id  3 | task 7273 | prompt processing progress, n_tokens = 11678, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 7273 | prompt done, n_tokens = 11678, batch.n_tokens = 58
slot init_sampler: id  3 | task 7273 | init sampler, took 1.64 ms, tokens: text = 11678, total = 11678
slot print_timing: id  3 | task 7273 | 
prompt eval time =     161.71 ms /    58 tokens (    2.79 ms per token,   358.66 tokens per second)
       eval time =    1279.29 ms /    52 tokens (   24.60 ms per token,    40.65 tokens per second)
      total time =    1441.00 ms /   110 tokens
slot      release: id  3 | task 7273 | stop processing: n_tokens = 11729, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7326 | processing task, is_child = 0
slot update_slots: id  3 | task 7326 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12026
slot update_slots: id  3 | task 7326 | n_tokens = 11678, memory_seq_rm [11678, end)
slot update_slots: id  3 | task 7326 | prompt processing progress, n_tokens = 11962, batch.n_tokens = 284, progress = 0.994678
slot update_slots: id  3 | task 7326 | n_tokens = 11962, memory_seq_rm [11962, end)
slot update_slots: id  3 | task 7326 | prompt processing progress, n_tokens = 12026, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7326 | prompt done, n_tokens = 12026, batch.n_tokens = 64
slot init_sampler: id  3 | task 7326 | init sampler, took 1.76 ms, tokens: text = 12026, total = 12026
slot update_slots: id  3 | task 7326 | erasing old context checkpoint (pos_min = 6725, pos_max = 7748, size = 24.012 MiB)
slot update_slots: id  3 | task 7326 | created context checkpoint 8 of 8 (pos_min = 10938, pos_max = 11961, size = 24.012 MiB)
slot print_timing: id  3 | task 7326 | 
prompt eval time =     531.32 ms /   348 tokens (    1.53 ms per token,   654.98 tokens per second)
       eval time =    1580.24 ms /    64 tokens (   24.69 ms per token,    40.50 tokens per second)
      total time =    2111.56 ms /   412 tokens
slot      release: id  3 | task 7326 | stop processing: n_tokens = 12089, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7392 | processing task, is_child = 0
slot update_slots: id  3 | task 7392 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12499
slot update_slots: id  3 | task 7392 | n_tokens = 12026, memory_seq_rm [12026, end)
slot update_slots: id  3 | task 7392 | prompt processing progress, n_tokens = 12435, batch.n_tokens = 409, progress = 0.994880
slot update_slots: id  3 | task 7392 | n_tokens = 12435, memory_seq_rm [12435, end)
slot update_slots: id  3 | task 7392 | prompt processing progress, n_tokens = 12499, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7392 | prompt done, n_tokens = 12499, batch.n_tokens = 64
slot init_sampler: id  3 | task 7392 | init sampler, took 2.40 ms, tokens: text = 12499, total = 12499
slot update_slots: id  3 | task 7392 | erasing old context checkpoint (pos_min = 8705, pos_max = 9728, size = 24.012 MiB)
slot update_slots: id  3 | task 7392 | created context checkpoint 8 of 8 (pos_min = 11411, pos_max = 12434, size = 24.012 MiB)
slot print_timing: id  3 | task 7392 | 
prompt eval time =     636.62 ms /   473 tokens (    1.35 ms per token,   742.98 tokens per second)
       eval time =    9677.65 ms /   389 tokens (   24.88 ms per token,    40.20 tokens per second)
      total time =   10314.27 ms /   862 tokens
slot      release: id  3 | task 7392 | stop processing: n_tokens = 12887, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7783 | processing task, is_child = 0
slot update_slots: id  3 | task 7783 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12974
slot update_slots: id  3 | task 7783 | n_tokens = 12499, memory_seq_rm [12499, end)
slot update_slots: id  3 | task 7783 | prompt processing progress, n_tokens = 12910, batch.n_tokens = 411, progress = 0.995067
slot update_slots: id  3 | task 7783 | n_tokens = 12910, memory_seq_rm [12910, end)
slot update_slots: id  3 | task 7783 | prompt processing progress, n_tokens = 12974, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7783 | prompt done, n_tokens = 12974, batch.n_tokens = 64
slot init_sampler: id  3 | task 7783 | init sampler, took 1.87 ms, tokens: text = 12974, total = 12974
slot update_slots: id  3 | task 7783 | erasing old context checkpoint (pos_min = 9111, pos_max = 10090, size = 22.980 MiB)
slot update_slots: id  3 | task 7783 | created context checkpoint 8 of 8 (pos_min = 11886, pos_max = 12909, size = 24.012 MiB)
slot print_timing: id  3 | task 7783 | 
prompt eval time =     649.59 ms /   475 tokens (    1.37 ms per token,   731.23 tokens per second)
       eval time =    5853.29 ms /   234 tokens (   25.01 ms per token,    39.98 tokens per second)
      total time =    6502.88 ms /   709 tokens
slot      release: id  3 | task 7783 | stop processing: n_tokens = 13207, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.967 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8019 | processing task, is_child = 0
slot update_slots: id  3 | task 8019 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13423
slot update_slots: id  3 | task 8019 | n_tokens = 12974, memory_seq_rm [12974, end)
slot update_slots: id  3 | task 8019 | prompt processing progress, n_tokens = 13359, batch.n_tokens = 385, progress = 0.995232
slot update_slots: id  3 | task 8019 | n_tokens = 13359, memory_seq_rm [13359, end)
slot update_slots: id  3 | task 8019 | prompt processing progress, n_tokens = 13423, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8019 | prompt done, n_tokens = 13423, batch.n_tokens = 64
slot init_sampler: id  3 | task 8019 | init sampler, took 1.97 ms, tokens: text = 13423, total = 13423
slot update_slots: id  3 | task 8019 | erasing old context checkpoint (pos_min = 9261, pos_max = 10156, size = 21.011 MiB)
slot update_slots: id  3 | task 8019 | created context checkpoint 8 of 8 (pos_min = 12335, pos_max = 13358, size = 24.012 MiB)
slot print_timing: id  3 | task 8019 | 
prompt eval time =     630.00 ms /   449 tokens (    1.40 ms per token,   712.70 tokens per second)
       eval time =    2696.09 ms /   107 tokens (   25.20 ms per token,    39.69 tokens per second)
      total time =    3326.09 ms /   556 tokens
slot      release: id  3 | task 8019 | stop processing: n_tokens = 13529, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8128 | processing task, is_child = 0
slot update_slots: id  3 | task 8128 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13480
slot update_slots: id  3 | task 8128 | n_tokens = 13423, memory_seq_rm [13423, end)
slot update_slots: id  3 | task 8128 | prompt processing progress, n_tokens = 13480, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 8128 | prompt done, n_tokens = 13480, batch.n_tokens = 57
slot init_sampler: id  3 | task 8128 | init sampler, took 2.00 ms, tokens: text = 13480, total = 13480
slot print_timing: id  3 | task 8128 | 
prompt eval time =     163.23 ms /    57 tokens (    2.86 ms per token,   349.20 tokens per second)
       eval time =    1643.84 ms /    65 tokens (   25.29 ms per token,    39.54 tokens per second)
      total time =    1807.07 ms /   122 tokens
slot      release: id  3 | task 8128 | stop processing: n_tokens = 13544, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8194 | processing task, is_child = 0
slot update_slots: id  3 | task 8194 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14042
slot update_slots: id  3 | task 8194 | n_tokens = 13480, memory_seq_rm [13480, end)
slot update_slots: id  3 | task 8194 | prompt processing progress, n_tokens = 13978, batch.n_tokens = 498, progress = 0.995442
slot update_slots: id  3 | task 8194 | n_tokens = 13978, memory_seq_rm [13978, end)
slot update_slots: id  3 | task 8194 | prompt processing progress, n_tokens = 14042, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8194 | prompt done, n_tokens = 14042, batch.n_tokens = 64
slot init_sampler: id  3 | task 8194 | init sampler, took 2.09 ms, tokens: text = 14042, total = 14042
slot update_slots: id  3 | task 8194 | erasing old context checkpoint (pos_min = 9536, pos_max = 10542, size = 23.613 MiB)
slot update_slots: id  3 | task 8194 | created context checkpoint 8 of 8 (pos_min = 12954, pos_max = 13977, size = 24.012 MiB)
slot print_timing: id  3 | task 8194 | 
prompt eval time =     748.74 ms /   562 tokens (    1.33 ms per token,   750.60 tokens per second)
       eval time =    3716.07 ms /   145 tokens (   25.63 ms per token,    39.02 tokens per second)
      total time =    4464.81 ms /   707 tokens
slot      release: id  3 | task 8194 | stop processing: n_tokens = 14186, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.880 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8341 | processing task, is_child = 0
slot update_slots: id  3 | task 8341 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15964
slot update_slots: id  3 | task 8341 | n_tokens = 14042, memory_seq_rm [14042, end)
slot update_slots: id  3 | task 8341 | prompt processing progress, n_tokens = 15900, batch.n_tokens = 1858, progress = 0.995991
slot update_slots: id  3 | task 8341 | n_tokens = 15900, memory_seq_rm [15900, end)
slot update_slots: id  3 | task 8341 | prompt processing progress, n_tokens = 15964, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8341 | prompt done, n_tokens = 15964, batch.n_tokens = 64
slot init_sampler: id  3 | task 8341 | init sampler, took 2.69 ms, tokens: text = 15964, total = 15964
slot update_slots: id  3 | task 8341 | erasing old context checkpoint (pos_min = 10155, pos_max = 11168, size = 23.778 MiB)
slot update_slots: id  3 | task 8341 | created context checkpoint 8 of 8 (pos_min = 14876, pos_max = 15899, size = 24.012 MiB)
slot print_timing: id  3 | task 8341 | 
prompt eval time =    2349.41 ms /  1922 tokens (    1.22 ms per token,   818.08 tokens per second)
       eval time =   10359.81 ms /   410 tokens (   25.27 ms per token,    39.58 tokens per second)
      total time =   12709.22 ms /  2332 tokens
slot      release: id  3 | task 8341 | stop processing: n_tokens = 16373, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8753 | processing task, is_child = 0
slot update_slots: id  3 | task 8753 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16269
slot update_slots: id  3 | task 8753 | n_tokens = 15964, memory_seq_rm [15964, end)
slot update_slots: id  3 | task 8753 | prompt processing progress, n_tokens = 16205, batch.n_tokens = 241, progress = 0.996066
slot update_slots: id  3 | task 8753 | n_tokens = 16205, memory_seq_rm [16205, end)
slot update_slots: id  3 | task 8753 | prompt processing progress, n_tokens = 16269, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8753 | prompt done, n_tokens = 16269, batch.n_tokens = 64
slot init_sampler: id  3 | task 8753 | init sampler, took 3.40 ms, tokens: text = 16269, total = 16269
slot update_slots: id  3 | task 8753 | erasing old context checkpoint (pos_min = 10443, pos_max = 11250, size = 18.947 MiB)
slot update_slots: id  3 | task 8753 | created context checkpoint 8 of 8 (pos_min = 15516, pos_max = 16204, size = 16.157 MiB)
slot print_timing: id  3 | task 8753 | 
prompt eval time =     600.30 ms /   305 tokens (    1.97 ms per token,   508.08 tokens per second)
       eval time =    1831.29 ms /    73 tokens (   25.09 ms per token,    39.86 tokens per second)
      total time =    2431.59 ms /   378 tokens
slot      release: id  3 | task 8753 | stop processing: n_tokens = 16341, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8828 | processing task, is_child = 0
slot update_slots: id  3 | task 8828 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16327
slot update_slots: id  3 | task 8828 | n_tokens = 16269, memory_seq_rm [16269, end)
slot update_slots: id  3 | task 8828 | prompt processing progress, n_tokens = 16327, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 8828 | prompt done, n_tokens = 16327, batch.n_tokens = 58
slot init_sampler: id  3 | task 8828 | init sampler, took 2.58 ms, tokens: text = 16327, total = 16327
slot print_timing: id  3 | task 8828 | 
prompt eval time =     167.51 ms /    58 tokens (    2.89 ms per token,   346.25 tokens per second)
       eval time =    2520.53 ms /   100 tokens (   25.21 ms per token,    39.67 tokens per second)
      total time =    2688.04 ms /   158 tokens
slot      release: id  3 | task 8828 | stop processing: n_tokens = 16426, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8929 | processing task, is_child = 0
slot update_slots: id  3 | task 8929 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16384
slot update_slots: id  3 | task 8929 | n_tokens = 16327, memory_seq_rm [16327, end)
slot update_slots: id  3 | task 8929 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 8929 | prompt done, n_tokens = 16384, batch.n_tokens = 57
slot init_sampler: id  3 | task 8929 | init sampler, took 2.29 ms, tokens: text = 16384, total = 16384
slot update_slots: id  3 | task 8929 | erasing old context checkpoint (pos_min = 10650, pos_max = 11555, size = 21.245 MiB)
slot update_slots: id  3 | task 8929 | created context checkpoint 8 of 8 (pos_min = 15737, pos_max = 16326, size = 13.835 MiB)
slot print_timing: id  3 | task 8929 | 
prompt eval time =     175.05 ms /    57 tokens (    3.07 ms per token,   325.62 tokens per second)
       eval time =    3652.08 ms /   145 tokens (   25.19 ms per token,    39.70 tokens per second)
      total time =    3827.14 ms /   202 tokens
slot      release: id  3 | task 8929 | stop processing: n_tokens = 16528, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.967 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9075 | processing task, is_child = 0
slot update_slots: id  3 | task 9075 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16946
slot update_slots: id  3 | task 9075 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  3 | task 9075 | prompt processing progress, n_tokens = 16882, batch.n_tokens = 498, progress = 0.996223
slot update_slots: id  3 | task 9075 | n_tokens = 16882, memory_seq_rm [16882, end)
slot update_slots: id  3 | task 9075 | prompt processing progress, n_tokens = 16946, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9075 | prompt done, n_tokens = 16946, batch.n_tokens = 64
slot init_sampler: id  3 | task 9075 | init sampler, took 2.42 ms, tokens: text = 16946, total = 16946
slot update_slots: id  3 | task 9075 | erasing old context checkpoint (pos_min = 10938, pos_max = 11961, size = 24.012 MiB)
slot update_slots: id  3 | task 9075 | created context checkpoint 8 of 8 (pos_min = 15964, pos_max = 16881, size = 21.526 MiB)
slot print_timing: id  3 | task 9075 | 
prompt eval time =     774.77 ms /   562 tokens (    1.38 ms per token,   725.38 tokens per second)
       eval time =    2130.06 ms /    82 tokens (   25.98 ms per token,    38.50 tokens per second)
      total time =    2904.83 ms /   644 tokens
slot      release: id  3 | task 9075 | stop processing: n_tokens = 17027, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9159 | processing task, is_child = 0
slot update_slots: id  3 | task 9159 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17495
slot update_slots: id  3 | task 9159 | n_tokens = 16946, memory_seq_rm [16946, end)
slot update_slots: id  3 | task 9159 | prompt processing progress, n_tokens = 17431, batch.n_tokens = 485, progress = 0.996342
slot update_slots: id  3 | task 9159 | n_tokens = 17431, memory_seq_rm [17431, end)
slot update_slots: id  3 | task 9159 | prompt processing progress, n_tokens = 17495, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9159 | prompt done, n_tokens = 17495, batch.n_tokens = 64
slot init_sampler: id  3 | task 9159 | init sampler, took 2.45 ms, tokens: text = 17495, total = 17495
slot update_slots: id  3 | task 9159 | erasing old context checkpoint (pos_min = 11411, pos_max = 12434, size = 24.012 MiB)
slot update_slots: id  3 | task 9159 | created context checkpoint 8 of 8 (pos_min = 16449, pos_max = 17430, size = 23.027 MiB)
slot print_timing: id  3 | task 9159 | 
prompt eval time =     766.19 ms /   549 tokens (    1.40 ms per token,   716.53 tokens per second)
       eval time =    3746.30 ms /   148 tokens (   25.31 ms per token,    39.51 tokens per second)
      total time =    4512.49 ms /   697 tokens
slot      release: id  3 | task 9159 | stop processing: n_tokens = 17642, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.901 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9309 | processing task, is_child = 0
slot update_slots: id  3 | task 9309 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19417
slot update_slots: id  3 | task 9309 | n_tokens = 17495, memory_seq_rm [17495, end)
slot update_slots: id  3 | task 9309 | prompt processing progress, n_tokens = 19353, batch.n_tokens = 1858, progress = 0.996704
slot update_slots: id  3 | task 9309 | n_tokens = 19353, memory_seq_rm [19353, end)
slot update_slots: id  3 | task 9309 | prompt processing progress, n_tokens = 19417, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9309 | prompt done, n_tokens = 19417, batch.n_tokens = 64
slot init_sampler: id  3 | task 9309 | init sampler, took 2.88 ms, tokens: text = 19417, total = 19417
slot update_slots: id  3 | task 9309 | erasing old context checkpoint (pos_min = 11886, pos_max = 12909, size = 24.012 MiB)
slot update_slots: id  3 | task 9309 | created context checkpoint 8 of 8 (pos_min = 18329, pos_max = 19352, size = 24.012 MiB)
slot print_timing: id  3 | task 9309 | 
prompt eval time =    2440.70 ms /  1922 tokens (    1.27 ms per token,   787.48 tokens per second)
       eval time =    3495.88 ms /   136 tokens (   25.71 ms per token,    38.90 tokens per second)
      total time =    5936.58 ms /  2058 tokens
slot      release: id  3 | task 9309 | stop processing: n_tokens = 19552, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9447 | processing task, is_child = 0
slot update_slots: id  3 | task 9447 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19803
slot update_slots: id  3 | task 9447 | n_tokens = 19417, memory_seq_rm [19417, end)
slot update_slots: id  3 | task 9447 | prompt processing progress, n_tokens = 19739, batch.n_tokens = 322, progress = 0.996768
slot update_slots: id  3 | task 9447 | n_tokens = 19739, memory_seq_rm [19739, end)
slot update_slots: id  3 | task 9447 | prompt processing progress, n_tokens = 19803, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9447 | prompt done, n_tokens = 19803, batch.n_tokens = 64
slot init_sampler: id  3 | task 9447 | init sampler, took 4.47 ms, tokens: text = 19803, total = 19803
slot update_slots: id  3 | task 9447 | erasing old context checkpoint (pos_min = 12335, pos_max = 13358, size = 24.012 MiB)
slot update_slots: id  3 | task 9447 | created context checkpoint 8 of 8 (pos_min = 18715, pos_max = 19738, size = 24.012 MiB)
slot print_timing: id  3 | task 9447 | 
prompt eval time =     628.33 ms /   386 tokens (    1.63 ms per token,   614.33 tokens per second)
       eval time =    2031.78 ms /    79 tokens (   25.72 ms per token,    38.88 tokens per second)
      total time =    2660.11 ms /   465 tokens
slot      release: id  3 | task 9447 | stop processing: n_tokens = 19881, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9528 | processing task, is_child = 0
slot update_slots: id  3 | task 9528 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20365
slot update_slots: id  3 | task 9528 | n_tokens = 19803, memory_seq_rm [19803, end)
slot update_slots: id  3 | task 9528 | prompt processing progress, n_tokens = 20301, batch.n_tokens = 498, progress = 0.996857
slot update_slots: id  3 | task 9528 | n_tokens = 20301, memory_seq_rm [20301, end)
slot update_slots: id  3 | task 9528 | prompt processing progress, n_tokens = 20365, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9528 | prompt done, n_tokens = 20365, batch.n_tokens = 64
slot init_sampler: id  3 | task 9528 | init sampler, took 2.87 ms, tokens: text = 20365, total = 20365
slot update_slots: id  3 | task 9528 | erasing old context checkpoint (pos_min = 12954, pos_max = 13977, size = 24.012 MiB)
slot update_slots: id  3 | task 9528 | created context checkpoint 8 of 8 (pos_min = 19277, pos_max = 20300, size = 24.012 MiB)
slot print_timing: id  3 | task 9528 | 
prompt eval time =     808.77 ms /   562 tokens (    1.44 ms per token,   694.88 tokens per second)
       eval time =    9201.82 ms /   359 tokens (   25.63 ms per token,    39.01 tokens per second)
      total time =   10010.60 ms /   921 tokens
slot      release: id  3 | task 9528 | stop processing: n_tokens = 20723, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9889 | processing task, is_child = 0
slot update_slots: id  3 | task 9889 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20670
slot update_slots: id  3 | task 9889 | n_tokens = 20365, memory_seq_rm [20365, end)
slot update_slots: id  3 | task 9889 | prompt processing progress, n_tokens = 20606, batch.n_tokens = 241, progress = 0.996904
slot update_slots: id  3 | task 9889 | n_tokens = 20606, memory_seq_rm [20606, end)
slot update_slots: id  3 | task 9889 | prompt processing progress, n_tokens = 20670, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9889 | prompt done, n_tokens = 20670, batch.n_tokens = 64
slot init_sampler: id  3 | task 9889 | init sampler, took 4.00 ms, tokens: text = 20670, total = 20670
slot update_slots: id  3 | task 9889 | erasing old context checkpoint (pos_min = 14876, pos_max = 15899, size = 24.012 MiB)
slot update_slots: id  3 | task 9889 | created context checkpoint 8 of 8 (pos_min = 19699, pos_max = 20605, size = 21.268 MiB)
slot print_timing: id  3 | task 9889 | 
prompt eval time =     633.50 ms /   305 tokens (    2.08 ms per token,   481.45 tokens per second)
       eval time =    1230.90 ms /    48 tokens (   25.64 ms per token,    39.00 tokens per second)
      total time =    1864.40 ms /   353 tokens
slot      release: id  3 | task 9889 | stop processing: n_tokens = 20717, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9939 | processing task, is_child = 0
slot update_slots: id  3 | task 9939 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20727
slot update_slots: id  3 | task 9939 | n_tokens = 20670, memory_seq_rm [20670, end)
slot update_slots: id  3 | task 9939 | prompt processing progress, n_tokens = 20727, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 9939 | prompt done, n_tokens = 20727, batch.n_tokens = 57
slot init_sampler: id  3 | task 9939 | init sampler, took 3.22 ms, tokens: text = 20727, total = 20727
slot print_timing: id  3 | task 9939 | 
prompt eval time =     173.97 ms /    57 tokens (    3.05 ms per token,   327.63 tokens per second)
       eval time =    1814.34 ms /    71 tokens (   25.55 ms per token,    39.13 tokens per second)
      total time =    1988.31 ms /   128 tokens
slot      release: id  3 | task 9939 | stop processing: n_tokens = 20797, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10011 | processing task, is_child = 0
slot update_slots: id  3 | task 10011 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20784
slot update_slots: id  3 | task 10011 | n_tokens = 20727, memory_seq_rm [20727, end)
slot update_slots: id  3 | task 10011 | prompt processing progress, n_tokens = 20784, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 10011 | prompt done, n_tokens = 20784, batch.n_tokens = 57
slot init_sampler: id  3 | task 10011 | init sampler, took 2.97 ms, tokens: text = 20784, total = 20784
slot update_slots: id  3 | task 10011 | erasing old context checkpoint (pos_min = 15516, pos_max = 16204, size = 16.157 MiB)
slot update_slots: id  3 | task 10011 | created context checkpoint 8 of 8 (pos_min = 19773, pos_max = 20726, size = 22.371 MiB)
slot print_timing: id  3 | task 10011 | 
prompt eval time =     191.29 ms /    57 tokens (    3.36 ms per token,   297.97 tokens per second)
       eval time =    2043.24 ms /    80 tokens (   25.54 ms per token,    39.15 tokens per second)
      total time =    2234.54 ms /   137 tokens
slot      release: id  3 | task 10011 | stop processing: n_tokens = 20863, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10092 | processing task, is_child = 0
slot update_slots: id  3 | task 10092 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21352
slot update_slots: id  3 | task 10092 | n_tokens = 20784, memory_seq_rm [20784, end)
slot update_slots: id  3 | task 10092 | prompt processing progress, n_tokens = 21288, batch.n_tokens = 504, progress = 0.997003
slot update_slots: id  3 | task 10092 | n_tokens = 21288, memory_seq_rm [21288, end)
slot update_slots: id  3 | task 10092 | prompt processing progress, n_tokens = 21352, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10092 | prompt done, n_tokens = 21352, batch.n_tokens = 64
slot init_sampler: id  3 | task 10092 | init sampler, took 3.53 ms, tokens: text = 21352, total = 21352
slot update_slots: id  3 | task 10092 | erasing old context checkpoint (pos_min = 15737, pos_max = 16326, size = 13.835 MiB)
slot update_slots: id  3 | task 10092 | created context checkpoint 8 of 8 (pos_min = 20264, pos_max = 21287, size = 24.012 MiB)
slot print_timing: id  3 | task 10092 | 
prompt eval time =     829.81 ms /   568 tokens (    1.46 ms per token,   684.50 tokens per second)
       eval time =    2194.86 ms /    85 tokens (   25.82 ms per token,    38.73 tokens per second)
      total time =    3024.67 ms /   653 tokens
slot      release: id  3 | task 10092 | stop processing: n_tokens = 21436, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10179 | processing task, is_child = 0
slot update_slots: id  3 | task 10179 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21901
slot update_slots: id  3 | task 10179 | n_tokens = 21352, memory_seq_rm [21352, end)
slot update_slots: id  3 | task 10179 | prompt processing progress, n_tokens = 21837, batch.n_tokens = 485, progress = 0.997078
slot update_slots: id  3 | task 10179 | n_tokens = 21837, memory_seq_rm [21837, end)
slot update_slots: id  3 | task 10179 | prompt processing progress, n_tokens = 21901, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10179 | prompt done, n_tokens = 21901, batch.n_tokens = 64
slot init_sampler: id  3 | task 10179 | init sampler, took 4.33 ms, tokens: text = 21901, total = 21901
slot update_slots: id  3 | task 10179 | erasing old context checkpoint (pos_min = 15964, pos_max = 16881, size = 21.526 MiB)
slot update_slots: id  3 | task 10179 | created context checkpoint 8 of 8 (pos_min = 20813, pos_max = 21836, size = 24.012 MiB)
slot print_timing: id  3 | task 10179 | 
prompt eval time =     809.81 ms /   549 tokens (    1.48 ms per token,   677.94 tokens per second)
       eval time =    2106.73 ms /    81 tokens (   26.01 ms per token,    38.45 tokens per second)
      total time =    2916.55 ms /   630 tokens
slot      release: id  3 | task 10179 | stop processing: n_tokens = 21981, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10262 | processing task, is_child = 0
slot update_slots: id  3 | task 10262 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 21958
slot update_slots: id  3 | task 10262 | n_tokens = 21901, memory_seq_rm [21901, end)
slot update_slots: id  3 | task 10262 | prompt processing progress, n_tokens = 21958, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 10262 | prompt done, n_tokens = 21958, batch.n_tokens = 57
slot init_sampler: id  3 | task 10262 | init sampler, took 3.25 ms, tokens: text = 21958, total = 21958
slot print_timing: id  3 | task 10262 | 
prompt eval time =     174.58 ms /    57 tokens (    3.06 ms per token,   326.50 tokens per second)
       eval time =    1753.53 ms /    68 tokens (   25.79 ms per token,    38.78 tokens per second)
      total time =    1928.11 ms /   125 tokens
slot      release: id  3 | task 10262 | stop processing: n_tokens = 22025, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.920 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10331 | processing task, is_child = 0
slot update_slots: id  3 | task 10331 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 23880
slot update_slots: id  3 | task 10331 | n_tokens = 21958, memory_seq_rm [21958, end)
slot update_slots: id  3 | task 10331 | prompt processing progress, n_tokens = 23816, batch.n_tokens = 1858, progress = 0.997320
slot update_slots: id  3 | task 10331 | n_tokens = 23816, memory_seq_rm [23816, end)
slot update_slots: id  3 | task 10331 | prompt processing progress, n_tokens = 23880, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10331 | prompt done, n_tokens = 23880, batch.n_tokens = 64
slot init_sampler: id  3 | task 10331 | init sampler, took 3.43 ms, tokens: text = 23880, total = 23880
slot update_slots: id  3 | task 10331 | erasing old context checkpoint (pos_min = 16449, pos_max = 17430, size = 23.027 MiB)
slot update_slots: id  3 | task 10331 | created context checkpoint 8 of 8 (pos_min = 22792, pos_max = 23815, size = 24.012 MiB)
slot print_timing: id  3 | task 10331 | 
prompt eval time =    2577.40 ms /  1922 tokens (    1.34 ms per token,   745.71 tokens per second)
       eval time =    1559.48 ms /    60 tokens (   25.99 ms per token,    38.47 tokens per second)
      total time =    4136.88 ms /  1982 tokens
slot      release: id  3 | task 10331 | stop processing: n_tokens = 23939, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10393 | processing task, is_child = 0
slot update_slots: id  3 | task 10393 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 23937
slot update_slots: id  3 | task 10393 | n_tokens = 23880, memory_seq_rm [23880, end)
slot update_slots: id  3 | task 10393 | prompt processing progress, n_tokens = 23937, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 10393 | prompt done, n_tokens = 23937, batch.n_tokens = 57
slot init_sampler: id  3 | task 10393 | init sampler, took 3.34 ms, tokens: text = 23937, total = 23937
slot print_timing: id  3 | task 10393 | 
prompt eval time =     174.07 ms /    57 tokens (    3.05 ms per token,   327.46 tokens per second)
       eval time =   11114.09 ms /   426 tokens (   26.09 ms per token,    38.33 tokens per second)
      total time =   11288.16 ms /   483 tokens
slot      release: id  3 | task 10393 | stop processing: n_tokens = 24362, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10820 | processing task, is_child = 0
slot update_slots: id  3 | task 10820 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24242
slot update_slots: id  3 | task 10820 | n_tokens = 23937, memory_seq_rm [23937, end)
slot update_slots: id  3 | task 10820 | prompt processing progress, n_tokens = 24178, batch.n_tokens = 241, progress = 0.997360
slot update_slots: id  3 | task 10820 | n_tokens = 24178, memory_seq_rm [24178, end)
slot update_slots: id  3 | task 10820 | prompt processing progress, n_tokens = 24242, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10820 | prompt done, n_tokens = 24242, batch.n_tokens = 64
slot init_sampler: id  3 | task 10820 | init sampler, took 3.44 ms, tokens: text = 24242, total = 24242
slot update_slots: id  3 | task 10820 | erasing old context checkpoint (pos_min = 18329, pos_max = 19352, size = 24.012 MiB)
slot update_slots: id  3 | task 10820 | created context checkpoint 8 of 8 (pos_min = 23338, pos_max = 24177, size = 19.697 MiB)
slot print_timing: id  3 | task 10820 | 
prompt eval time =     646.99 ms /   305 tokens (    2.12 ms per token,   471.42 tokens per second)
       eval time =    3298.76 ms /   125 tokens (   26.39 ms per token,    37.89 tokens per second)
      total time =    3945.75 ms /   430 tokens
slot      release: id  3 | task 10820 | stop processing: n_tokens = 24366, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10947 | processing task, is_child = 0
slot update_slots: id  3 | task 10947 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24867
slot update_slots: id  3 | task 10947 | n_tokens = 24242, memory_seq_rm [24242, end)
slot update_slots: id  3 | task 10947 | prompt processing progress, n_tokens = 24803, batch.n_tokens = 561, progress = 0.997426
slot update_slots: id  3 | task 10947 | n_tokens = 24803, memory_seq_rm [24803, end)
slot update_slots: id  3 | task 10947 | prompt processing progress, n_tokens = 24867, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10947 | prompt done, n_tokens = 24867, batch.n_tokens = 64
slot init_sampler: id  3 | task 10947 | init sampler, took 4.91 ms, tokens: text = 24867, total = 24867
slot update_slots: id  3 | task 10947 | erasing old context checkpoint (pos_min = 18715, pos_max = 19738, size = 24.012 MiB)
slot update_slots: id  3 | task 10947 | created context checkpoint 8 of 8 (pos_min = 23937, pos_max = 24802, size = 20.307 MiB)
slot print_timing: id  3 | task 10947 | 
prompt eval time =    1052.88 ms /   625 tokens (    1.68 ms per token,   593.61 tokens per second)
       eval time =    3272.82 ms /   125 tokens (   26.18 ms per token,    38.19 tokens per second)
      total time =    4325.70 ms /   750 tokens
slot      release: id  3 | task 10947 | stop processing: n_tokens = 24991, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11074 | processing task, is_child = 0
slot update_slots: id  3 | task 11074 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24925
slot update_slots: id  3 | task 11074 | n_tokens = 24867, memory_seq_rm [24867, end)
slot update_slots: id  3 | task 11074 | prompt processing progress, n_tokens = 24925, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 11074 | prompt done, n_tokens = 24925, batch.n_tokens = 58
slot init_sampler: id  3 | task 11074 | init sampler, took 3.54 ms, tokens: text = 24925, total = 24925
slot print_timing: id  3 | task 11074 | 
prompt eval time =     178.32 ms /    58 tokens (    3.07 ms per token,   325.26 tokens per second)
       eval time =    9964.63 ms /   377 tokens (   26.43 ms per token,    37.83 tokens per second)
      total time =   10142.95 ms /   435 tokens
slot      release: id  3 | task 11074 | stop processing: n_tokens = 25301, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11452 | processing task, is_child = 0
slot update_slots: id  3 | task 11452 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 25230
slot update_slots: id  3 | task 11452 | n_tokens = 24925, memory_seq_rm [24925, end)
slot update_slots: id  3 | task 11452 | prompt processing progress, n_tokens = 25166, batch.n_tokens = 241, progress = 0.997463
slot update_slots: id  3 | task 11452 | n_tokens = 25166, memory_seq_rm [25166, end)
slot update_slots: id  3 | task 11452 | prompt processing progress, n_tokens = 25230, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11452 | prompt done, n_tokens = 25230, batch.n_tokens = 64
slot init_sampler: id  3 | task 11452 | init sampler, took 3.52 ms, tokens: text = 25230, total = 25230
slot update_slots: id  3 | task 11452 | erasing old context checkpoint (pos_min = 19277, pos_max = 20300, size = 24.012 MiB)
slot update_slots: id  3 | task 11452 | created context checkpoint 8 of 8 (pos_min = 24371, pos_max = 25165, size = 18.642 MiB)
slot print_timing: id  3 | task 11452 | 
prompt eval time =     651.77 ms /   305 tokens (    2.14 ms per token,   467.96 tokens per second)
       eval time =    2151.46 ms /    82 tokens (   26.24 ms per token,    38.11 tokens per second)
      total time =    2803.23 ms /   387 tokens
slot      release: id  3 | task 11452 | stop processing: n_tokens = 25311, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11536 | processing task, is_child = 0
slot update_slots: id  3 | task 11536 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 25287
slot update_slots: id  3 | task 11536 | n_tokens = 25230, memory_seq_rm [25230, end)
slot update_slots: id  3 | task 11536 | prompt processing progress, n_tokens = 25287, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 11536 | prompt done, n_tokens = 25287, batch.n_tokens = 57
slot init_sampler: id  3 | task 11536 | init sampler, took 3.52 ms, tokens: text = 25287, total = 25287
slot print_timing: id  3 | task 11536 | 
prompt eval time =     177.43 ms /    57 tokens (    3.11 ms per token,   321.26 tokens per second)
       eval time =    1264.48 ms /    48 tokens (   26.34 ms per token,    37.96 tokens per second)
      total time =    1441.91 ms /   105 tokens
slot      release: id  3 | task 11536 | stop processing: n_tokens = 25334, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11585 | processing task, is_child = 0
slot update_slots: id  3 | task 11585 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 25836
slot update_slots: id  3 | task 11585 | n_tokens = 25287, memory_seq_rm [25287, end)
slot update_slots: id  3 | task 11585 | prompt processing progress, n_tokens = 25772, batch.n_tokens = 485, progress = 0.997523
slot update_slots: id  3 | task 11585 | n_tokens = 25772, memory_seq_rm [25772, end)
slot update_slots: id  3 | task 11585 | prompt processing progress, n_tokens = 25836, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11585 | prompt done, n_tokens = 25836, batch.n_tokens = 64
slot init_sampler: id  3 | task 11585 | init sampler, took 3.68 ms, tokens: text = 25836, total = 25836
slot update_slots: id  3 | task 11585 | erasing old context checkpoint (pos_min = 19699, pos_max = 20605, size = 21.268 MiB)
slot update_slots: id  3 | task 11585 | created context checkpoint 8 of 8 (pos_min = 24842, pos_max = 25771, size = 21.808 MiB)
slot print_timing: id  3 | task 11585 | 
prompt eval time =     859.17 ms /   549 tokens (    1.56 ms per token,   638.99 tokens per second)
       eval time =    2258.16 ms /    86 tokens (   26.26 ms per token,    38.08 tokens per second)
      total time =    3117.33 ms /   635 tokens
slot      release: id  3 | task 11585 | stop processing: n_tokens = 25921, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11673 | processing task, is_child = 0
slot update_slots: id  3 | task 11673 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 26512
slot update_slots: id  3 | task 11673 | n_tokens = 25836, memory_seq_rm [25836, end)
slot update_slots: id  3 | task 11673 | prompt processing progress, n_tokens = 26448, batch.n_tokens = 612, progress = 0.997586
slot update_slots: id  3 | task 11673 | n_tokens = 26448, memory_seq_rm [26448, end)
slot update_slots: id  3 | task 11673 | prompt processing progress, n_tokens = 26512, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11673 | prompt done, n_tokens = 26512, batch.n_tokens = 64
slot init_sampler: id  3 | task 11673 | init sampler, took 4.21 ms, tokens: text = 26512, total = 26512
slot update_slots: id  3 | task 11673 | erasing old context checkpoint (pos_min = 19773, pos_max = 20726, size = 22.371 MiB)
slot update_slots: id  3 | task 11673 | created context checkpoint 8 of 8 (pos_min = 25479, pos_max = 26447, size = 22.722 MiB)
slot print_timing: id  3 | task 11673 | 
prompt eval time =    1122.23 ms /   676 tokens (    1.66 ms per token,   602.37 tokens per second)
       eval time =    2946.43 ms /   111 tokens (   26.54 ms per token,    37.67 tokens per second)
      total time =    4068.66 ms /   787 tokens
slot      release: id  3 | task 11673 | stop processing: n_tokens = 26622, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11786 | processing task, is_child = 0
slot update_slots: id  3 | task 11786 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 26985
slot update_slots: id  3 | task 11786 | n_tokens = 26512, memory_seq_rm [26512, end)
slot update_slots: id  3 | task 11786 | prompt processing progress, n_tokens = 26921, batch.n_tokens = 409, progress = 0.997628
slot update_slots: id  3 | task 11786 | n_tokens = 26921, memory_seq_rm [26921, end)
slot update_slots: id  3 | task 11786 | prompt processing progress, n_tokens = 26985, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11786 | prompt done, n_tokens = 26985, batch.n_tokens = 64
slot init_sampler: id  3 | task 11786 | init sampler, took 3.75 ms, tokens: text = 26985, total = 26985
slot update_slots: id  3 | task 11786 | erasing old context checkpoint (pos_min = 20264, pos_max = 21287, size = 24.012 MiB)
slot update_slots: id  3 | task 11786 | created context checkpoint 8 of 8 (pos_min = 25897, pos_max = 26920, size = 24.012 MiB)
slot print_timing: id  3 | task 11786 | 
prompt eval time =     758.94 ms /   473 tokens (    1.60 ms per token,   623.24 tokens per second)
       eval time =    3320.05 ms /   126 tokens (   26.35 ms per token,    37.95 tokens per second)
      total time =    4078.99 ms /   599 tokens
slot      release: id  3 | task 11786 | stop processing: n_tokens = 27110, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11914 | processing task, is_child = 0
slot update_slots: id  3 | task 11914 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 27042
slot update_slots: id  3 | task 11914 | n_tokens = 26985, memory_seq_rm [26985, end)
slot update_slots: id  3 | task 11914 | prompt processing progress, n_tokens = 27042, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 11914 | prompt done, n_tokens = 27042, batch.n_tokens = 57
slot init_sampler: id  3 | task 11914 | init sampler, took 3.81 ms, tokens: text = 27042, total = 27042
slot print_timing: id  3 | task 11914 | 
prompt eval time =     180.62 ms /    57 tokens (    3.17 ms per token,   315.57 tokens per second)
       eval time =    3324.66 ms /   126 tokens (   26.39 ms per token,    37.90 tokens per second)
      total time =    3505.28 ms /   183 tokens
slot      release: id  3 | task 11914 | stop processing: n_tokens = 27167, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12041 | processing task, is_child = 0
slot update_slots: id  3 | task 12041 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 27497
slot update_slots: id  3 | task 12041 | n_tokens = 27042, memory_seq_rm [27042, end)
slot update_slots: id  3 | task 12041 | prompt processing progress, n_tokens = 27433, batch.n_tokens = 391, progress = 0.997672
slot update_slots: id  3 | task 12041 | n_tokens = 27433, memory_seq_rm [27433, end)
slot update_slots: id  3 | task 12041 | prompt processing progress, n_tokens = 27497, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12041 | prompt done, n_tokens = 27497, batch.n_tokens = 64
slot init_sampler: id  3 | task 12041 | init sampler, took 5.56 ms, tokens: text = 27497, total = 27497
slot update_slots: id  3 | task 12041 | erasing old context checkpoint (pos_min = 20813, pos_max = 21836, size = 24.012 MiB)
slot update_slots: id  3 | task 12041 | created context checkpoint 8 of 8 (pos_min = 26409, pos_max = 27432, size = 24.012 MiB)
slot print_timing: id  3 | task 12041 | 
prompt eval time =     748.94 ms /   455 tokens (    1.65 ms per token,   607.53 tokens per second)
       eval time =   10128.38 ms /   383 tokens (   26.44 ms per token,    37.81 tokens per second)
      total time =   10877.32 ms /   838 tokens
slot      release: id  3 | task 12041 | stop processing: n_tokens = 27879, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12426 | processing task, is_child = 0
slot update_slots: id  3 | task 12426 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 27817
slot update_slots: id  3 | task 12426 | n_tokens = 27497, memory_seq_rm [27497, end)
slot update_slots: id  3 | task 12426 | prompt processing progress, n_tokens = 27753, batch.n_tokens = 256, progress = 0.997699
slot update_slots: id  3 | task 12426 | n_tokens = 27753, memory_seq_rm [27753, end)
slot update_slots: id  3 | task 12426 | prompt processing progress, n_tokens = 27817, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12426 | prompt done, n_tokens = 27817, batch.n_tokens = 64
slot init_sampler: id  3 | task 12426 | init sampler, took 4.00 ms, tokens: text = 27817, total = 27817
slot update_slots: id  3 | task 12426 | erasing old context checkpoint (pos_min = 22792, pos_max = 23815, size = 24.012 MiB)
slot update_slots: id  3 | task 12426 | created context checkpoint 8 of 8 (pos_min = 26855, pos_max = 27752, size = 21.057 MiB)
slot print_timing: id  3 | task 12426 | 
prompt eval time =     677.41 ms /   320 tokens (    2.12 ms per token,   472.38 tokens per second)
       eval time =    2968.94 ms /   110 tokens (   26.99 ms per token,    37.05 tokens per second)
      total time =    3646.36 ms /   430 tokens
slot      release: id  3 | task 12426 | stop processing: n_tokens = 27926, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12538 | processing task, is_child = 0
slot update_slots: id  3 | task 12538 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 27875
slot update_slots: id  3 | task 12538 | n_tokens = 27817, memory_seq_rm [27817, end)
slot update_slots: id  3 | task 12538 | prompt processing progress, n_tokens = 27875, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 12538 | prompt done, n_tokens = 27875, batch.n_tokens = 58
slot init_sampler: id  3 | task 12538 | init sampler, took 3.88 ms, tokens: text = 27875, total = 27875
slot print_timing: id  3 | task 12538 | 
prompt eval time =     182.84 ms /    58 tokens (    3.15 ms per token,   317.22 tokens per second)
       eval time =    1517.00 ms /    56 tokens (   27.09 ms per token,    36.91 tokens per second)
      total time =    1699.84 ms /   114 tokens
slot      release: id  3 | task 12538 | stop processing: n_tokens = 27930, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.864 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12595 | processing task, is_child = 0
slot update_slots: id  3 | task 12595 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 32281
slot update_slots: id  3 | task 12595 | n_tokens = 27875, memory_seq_rm [27875, end)
slot update_slots: id  3 | task 12595 | prompt processing progress, n_tokens = 29923, batch.n_tokens = 2048, progress = 0.926954
slot update_slots: id  3 | task 12595 | n_tokens = 29923, memory_seq_rm [29923, end)
slot update_slots: id  3 | task 12595 | prompt processing progress, n_tokens = 31971, batch.n_tokens = 2048, progress = 0.990397
slot update_slots: id  3 | task 12595 | n_tokens = 31971, memory_seq_rm [31971, end)
slot update_slots: id  3 | task 12595 | prompt processing progress, n_tokens = 32217, batch.n_tokens = 246, progress = 0.998017
slot update_slots: id  3 | task 12595 | n_tokens = 32217, memory_seq_rm [32217, end)
slot update_slots: id  3 | task 12595 | prompt processing progress, n_tokens = 32281, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12595 | prompt done, n_tokens = 32281, batch.n_tokens = 64
slot init_sampler: id  3 | task 12595 | init sampler, took 5.29 ms, tokens: text = 32281, total = 32281
slot update_slots: id  3 | task 12595 | erasing old context checkpoint (pos_min = 23338, pos_max = 24177, size = 19.697 MiB)
slot update_slots: id  3 | task 12595 | created context checkpoint 8 of 8 (pos_min = 31193, pos_max = 32216, size = 24.012 MiB)
slot print_timing: id  3 | task 12595 | 
prompt eval time =    6249.41 ms /  4406 tokens (    1.42 ms per token,   705.03 tokens per second)
       eval time =    1598.03 ms /    58 tokens (   27.55 ms per token,    36.29 tokens per second)
      total time =    7847.44 ms /  4464 tokens
slot      release: id  3 | task 12595 | stop processing: n_tokens = 32338, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12657 | processing task, is_child = 0
slot update_slots: id  3 | task 12657 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 33173
slot update_slots: id  3 | task 12657 | n_tokens = 32281, memory_seq_rm [32281, end)
slot update_slots: id  3 | task 12657 | prompt processing progress, n_tokens = 33109, batch.n_tokens = 828, progress = 0.998071
slot update_slots: id  3 | task 12657 | n_tokens = 33109, memory_seq_rm [33109, end)
slot update_slots: id  3 | task 12657 | prompt processing progress, n_tokens = 33173, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12657 | prompt done, n_tokens = 33173, batch.n_tokens = 64
slot init_sampler: id  3 | task 12657 | init sampler, took 4.97 ms, tokens: text = 33173, total = 33173
slot update_slots: id  3 | task 12657 | erasing old context checkpoint (pos_min = 23937, pos_max = 24802, size = 20.307 MiB)
slot update_slots: id  3 | task 12657 | created context checkpoint 8 of 8 (pos_min = 32085, pos_max = 33108, size = 24.012 MiB)
slot print_timing: id  3 | task 12657 | 
prompt eval time =    1459.86 ms /   892 tokens (    1.64 ms per token,   611.02 tokens per second)
       eval time =   13701.40 ms /   506 tokens (   27.08 ms per token,    36.93 tokens per second)
      total time =   15161.25 ms /  1398 tokens
slot      release: id  3 | task 12657 | stop processing: n_tokens = 33678, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13165 | processing task, is_child = 0
slot update_slots: id  3 | task 13165 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 33520
slot update_slots: id  3 | task 13165 | n_tokens = 33173, memory_seq_rm [33173, end)
slot update_slots: id  3 | task 13165 | prompt processing progress, n_tokens = 33456, batch.n_tokens = 283, progress = 0.998091
slot update_slots: id  3 | task 13165 | n_tokens = 33456, memory_seq_rm [33456, end)
slot update_slots: id  3 | task 13165 | prompt processing progress, n_tokens = 33520, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13165 | prompt done, n_tokens = 33520, batch.n_tokens = 64
slot init_sampler: id  3 | task 13165 | init sampler, took 4.68 ms, tokens: text = 33520, total = 33520
slot update_slots: id  3 | task 13165 | erasing old context checkpoint (pos_min = 24371, pos_max = 25165, size = 18.642 MiB)
slot update_slots: id  3 | task 13165 | created context checkpoint 8 of 8 (pos_min = 32654, pos_max = 33455, size = 18.806 MiB)
slot print_timing: id  3 | task 13165 | 
prompt eval time =     695.94 ms /   347 tokens (    2.01 ms per token,   498.61 tokens per second)
       eval time =    1501.25 ms /    55 tokens (   27.30 ms per token,    36.64 tokens per second)
      total time =    2197.19 ms /   402 tokens
slot      release: id  3 | task 13165 | stop processing: n_tokens = 33574, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13222 | processing task, is_child = 0
slot update_slots: id  3 | task 13222 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 33578
slot update_slots: id  3 | task 13222 | n_tokens = 33520, memory_seq_rm [33520, end)
slot update_slots: id  3 | task 13222 | prompt processing progress, n_tokens = 33578, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 13222 | prompt done, n_tokens = 33578, batch.n_tokens = 58
slot init_sampler: id  3 | task 13222 | init sampler, took 4.60 ms, tokens: text = 33578, total = 33578
slot print_timing: id  3 | task 13222 | 
prompt eval time =     186.02 ms /    58 tokens (    3.21 ms per token,   311.80 tokens per second)
       eval time =    1792.15 ms /    66 tokens (   27.15 ms per token,    36.83 tokens per second)
      total time =    1978.17 ms /   124 tokens
slot      release: id  3 | task 13222 | stop processing: n_tokens = 33643, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13289 | processing task, is_child = 0
slot update_slots: id  3 | task 13289 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 34244
slot update_slots: id  3 | task 13289 | n_tokens = 33578, memory_seq_rm [33578, end)
slot update_slots: id  3 | task 13289 | prompt processing progress, n_tokens = 34180, batch.n_tokens = 602, progress = 0.998131
slot update_slots: id  3 | task 13289 | n_tokens = 34180, memory_seq_rm [34180, end)
slot update_slots: id  3 | task 13289 | prompt processing progress, n_tokens = 34244, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13289 | prompt done, n_tokens = 34244, batch.n_tokens = 64
slot init_sampler: id  3 | task 13289 | init sampler, took 4.85 ms, tokens: text = 34244, total = 34244
slot update_slots: id  3 | task 13289 | erasing old context checkpoint (pos_min = 24842, pos_max = 25771, size = 21.808 MiB)
slot update_slots: id  3 | task 13289 | created context checkpoint 8 of 8 (pos_min = 33156, pos_max = 34179, size = 24.012 MiB)
slot print_timing: id  3 | task 13289 | 
prompt eval time =    1218.57 ms /   666 tokens (    1.83 ms per token,   546.54 tokens per second)
       eval time =    1547.14 ms /    56 tokens (   27.63 ms per token,    36.20 tokens per second)
      total time =    2765.71 ms /   722 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 13289 | stop processing: n_tokens = 34299, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13347 | processing task, is_child = 0
slot update_slots: id  3 | task 13347 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 34299
slot update_slots: id  3 | task 13347 | n_tokens = 34244, memory_seq_rm [34244, end)
slot update_slots: id  3 | task 13347 | prompt processing progress, n_tokens = 34299, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 13347 | prompt done, n_tokens = 34299, batch.n_tokens = 55
slot init_sampler: id  3 | task 13347 | init sampler, took 7.37 ms, tokens: text = 34299, total = 34299
slot print_timing: id  3 | task 13347 | 
prompt eval time =     189.05 ms /    55 tokens (    3.44 ms per token,   290.93 tokens per second)
       eval time =    4168.10 ms /   153 tokens (   27.24 ms per token,    36.71 tokens per second)
      total time =    4357.14 ms /   208 tokens
slot      release: id  3 | task 13347 | stop processing: n_tokens = 34451, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13501 | processing task, is_child = 0
slot update_slots: id  3 | task 13501 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 35293
slot update_slots: id  3 | task 13501 | n_tokens = 34299, memory_seq_rm [34299, end)
slot update_slots: id  3 | task 13501 | prompt processing progress, n_tokens = 35229, batch.n_tokens = 930, progress = 0.998187
slot update_slots: id  3 | task 13501 | n_tokens = 35229, memory_seq_rm [35229, end)
slot update_slots: id  3 | task 13501 | prompt processing progress, n_tokens = 35293, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13501 | prompt done, n_tokens = 35293, batch.n_tokens = 64
slot init_sampler: id  3 | task 13501 | init sampler, took 5.17 ms, tokens: text = 35293, total = 35293
slot update_slots: id  3 | task 13501 | erasing old context checkpoint (pos_min = 25479, pos_max = 26447, size = 22.722 MiB)
slot update_slots: id  3 | task 13501 | created context checkpoint 8 of 8 (pos_min = 34205, pos_max = 35228, size = 24.012 MiB)
slot print_timing: id  3 | task 13501 | 
prompt eval time =    1587.30 ms /   994 tokens (    1.60 ms per token,   626.22 tokens per second)
       eval time =    3902.26 ms /   143 tokens (   27.29 ms per token,    36.65 tokens per second)
      total time =    5489.55 ms /  1137 tokens
slot      release: id  3 | task 13501 | stop processing: n_tokens = 35435, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13646 | processing task, is_child = 0
slot update_slots: id  3 | task 13646 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 35375
slot update_slots: id  3 | task 13646 | n_tokens = 35293, memory_seq_rm [35293, end)
slot update_slots: id  3 | task 13646 | prompt processing progress, n_tokens = 35311, batch.n_tokens = 18, progress = 0.998191
slot update_slots: id  3 | task 13646 | n_tokens = 35311, memory_seq_rm [35311, end)
slot update_slots: id  3 | task 13646 | prompt processing progress, n_tokens = 35375, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13646 | prompt done, n_tokens = 35375, batch.n_tokens = 64
slot init_sampler: id  3 | task 13646 | init sampler, took 6.95 ms, tokens: text = 35375, total = 35375
slot update_slots: id  3 | task 13646 | erasing old context checkpoint (pos_min = 25897, pos_max = 26920, size = 24.012 MiB)
slot update_slots: id  3 | task 13646 | created context checkpoint 8 of 8 (pos_min = 34411, pos_max = 35310, size = 21.104 MiB)
slot print_timing: id  3 | task 13646 | 
prompt eval time =     325.43 ms /    82 tokens (    3.97 ms per token,   251.98 tokens per second)
       eval time =   79543.41 ms /  2875 tokens (   27.67 ms per token,    36.14 tokens per second)
      total time =   79868.84 ms /  2957 tokens
slot      release: id  3 | task 13646 | stop processing: n_tokens = 38249, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.928 (> 0.100 thold), f_keep = 0.925
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 16523 | processing task, is_child = 0
slot update_slots: id  3 | task 16523 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 38135
slot update_slots: id  3 | task 16523 | n_past = 35375, slot.prompt.tokens.size() = 38249, seq_id = 3, pos_min = 37225, n_swa = 128
slot update_slots: id  3 | task 16523 | restored context checkpoint (pos_min = 34411, pos_max = 35310, size = 21.104 MiB)
slot update_slots: id  3 | task 16523 | n_tokens = 35310, memory_seq_rm [35310, end)
slot update_slots: id  3 | task 16523 | prompt processing progress, n_tokens = 37358, batch.n_tokens = 2048, progress = 0.979625
slot update_slots: id  3 | task 16523 | n_tokens = 37358, memory_seq_rm [37358, end)
slot update_slots: id  3 | task 16523 | prompt processing progress, n_tokens = 38071, batch.n_tokens = 713, progress = 0.998322
slot update_slots: id  3 | task 16523 | n_tokens = 38071, memory_seq_rm [38071, end)
slot update_slots: id  3 | task 16523 | prompt processing progress, n_tokens = 38135, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 16523 | prompt done, n_tokens = 38135, batch.n_tokens = 64
slot init_sampler: id  3 | task 16523 | init sampler, took 5.70 ms, tokens: text = 38135, total = 38135
slot update_slots: id  3 | task 16523 | erasing old context checkpoint (pos_min = 26409, pos_max = 27432, size = 24.012 MiB)
slot update_slots: id  3 | task 16523 | created context checkpoint 8 of 8 (pos_min = 37047, pos_max = 38070, size = 24.012 MiB)
slot print_timing: id  3 | task 16523 | 
prompt eval time =    4420.67 ms /  2825 tokens (    1.56 ms per token,   639.04 tokens per second)
       eval time =     937.16 ms /    35 tokens (   26.78 ms per token,    37.35 tokens per second)
      total time =    5357.84 ms /  2860 tokens
slot      release: id  3 | task 16523 | stop processing: n_tokens = 38169, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.362 (> 0.100 thold), f_keep = 0.007
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 38169, total state size = 919.035 MiB
srv          load:  - looking for better prompt, base f_keep = 0.007, sim = 0.362
srv        update:  - cache state: 3 prompts, 1597.912 MiB (limits: 8192.000 MiB, 64000 tokens, 246024 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv  get_availabl: prompt cache update took 883.42 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 16561 | processing task, is_child = 0
slot update_slots: id  3 | task 16561 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 740
slot update_slots: id  3 | task 16561 | n_past = 268, slot.prompt.tokens.size() = 38169, seq_id = 3, pos_min = 37145, n_swa = 128
slot update_slots: id  3 | task 16561 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 16561 | erased invalidated context checkpoint (pos_min = 26855, pos_max = 27752, n_swa = 128, size = 21.057 MiB)
slot update_slots: id  3 | task 16561 | erased invalidated context checkpoint (pos_min = 31193, pos_max = 32216, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 16561 | erased invalidated context checkpoint (pos_min = 32085, pos_max = 33108, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 16561 | erased invalidated context checkpoint (pos_min = 32654, pos_max = 33455, n_swa = 128, size = 18.806 MiB)
slot update_slots: id  3 | task 16561 | erased invalidated context checkpoint (pos_min = 33156, pos_max = 34179, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 16561 | erased invalidated context checkpoint (pos_min = 34205, pos_max = 35228, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 16561 | erased invalidated context checkpoint (pos_min = 34411, pos_max = 35310, n_swa = 128, size = 21.104 MiB)
slot update_slots: id  3 | task 16561 | erased invalidated context checkpoint (pos_min = 37047, pos_max = 38070, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 16561 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 16561 | prompt processing progress, n_tokens = 676, batch.n_tokens = 676, progress = 0.913514
slot update_slots: id  3 | task 16561 | n_tokens = 676, memory_seq_rm [676, end)
slot update_slots: id  3 | task 16561 | prompt processing progress, n_tokens = 740, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 16561 | prompt done, n_tokens = 740, batch.n_tokens = 64
slot init_sampler: id  3 | task 16561 | init sampler, took 0.14 ms, tokens: text = 740, total = 740
slot update_slots: id  3 | task 16561 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 675, size = 15.852 MiB)
slot print_timing: id  3 | task 16561 | 
prompt eval time =     910.24 ms /   740 tokens (    1.23 ms per token,   812.98 tokens per second)
       eval time =   33081.88 ms /  1384 tokens (   23.90 ms per token,    41.84 tokens per second)
      total time =   33992.12 ms /  2124 tokens
slot      release: id  3 | task 16561 | stop processing: n_tokens = 2123, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17947 | processing task, is_child = 0
slot update_slots: id  2 | task 17947 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11179
slot update_slots: id  2 | task 17947 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 17947 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.183201
slot update_slots: id  2 | task 17947 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 17947 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.366401
slot update_slots: id  2 | task 17947 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 17947 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.549602
slot update_slots: id  2 | task 17947 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 17947 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.732803
slot update_slots: id  2 | task 17947 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 17947 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.916003
slot update_slots: id  2 | task 17947 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 17947 | prompt processing progress, n_tokens = 11115, batch.n_tokens = 875, progress = 0.994275
slot update_slots: id  2 | task 17947 | n_tokens = 11115, memory_seq_rm [11115, end)
slot update_slots: id  2 | task 17947 | prompt processing progress, n_tokens = 11179, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17947 | prompt done, n_tokens = 11179, batch.n_tokens = 64
slot init_sampler: id  2 | task 17947 | init sampler, took 1.95 ms, tokens: text = 11179, total = 11179
slot update_slots: id  2 | task 17947 | created context checkpoint 1 of 8 (pos_min = 10218, pos_max = 11114, size = 21.034 MiB)
slot print_timing: id  2 | task 17947 | 
prompt eval time =   11690.70 ms / 11179 tokens (    1.05 ms per token,   956.23 tokens per second)
       eval time =    1155.08 ms /    47 tokens (   24.58 ms per token,    40.69 tokens per second)
      total time =   12845.78 ms / 11226 tokens
slot      release: id  2 | task 17947 | stop processing: n_tokens = 11225, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18001 | processing task, is_child = 0
slot update_slots: id  2 | task 18001 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11353
slot update_slots: id  2 | task 18001 | n_tokens = 11179, memory_seq_rm [11179, end)
slot update_slots: id  2 | task 18001 | prompt processing progress, n_tokens = 11289, batch.n_tokens = 110, progress = 0.994363
slot update_slots: id  2 | task 18001 | n_tokens = 11289, memory_seq_rm [11289, end)
slot update_slots: id  2 | task 18001 | prompt processing progress, n_tokens = 11353, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 18001 | prompt done, n_tokens = 11353, batch.n_tokens = 64
slot init_sampler: id  2 | task 18001 | init sampler, took 2.19 ms, tokens: text = 11353, total = 11353
slot update_slots: id  2 | task 18001 | created context checkpoint 2 of 8 (pos_min = 10392, pos_max = 11288, size = 21.034 MiB)
slot print_timing: id  2 | task 18001 | 
prompt eval time =     514.57 ms /   174 tokens (    2.96 ms per token,   338.15 tokens per second)
       eval time =    1724.65 ms /    69 tokens (   24.99 ms per token,    40.01 tokens per second)
      total time =    2239.22 ms /   243 tokens
slot      release: id  2 | task 18001 | stop processing: n_tokens = 11421, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18072 | processing task, is_child = 0
slot update_slots: id  2 | task 18072 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11661
slot update_slots: id  2 | task 18072 | n_tokens = 11353, memory_seq_rm [11353, end)
slot update_slots: id  2 | task 18072 | prompt processing progress, n_tokens = 11597, batch.n_tokens = 244, progress = 0.994512
slot update_slots: id  2 | task 18072 | n_tokens = 11597, memory_seq_rm [11597, end)
slot update_slots: id  2 | task 18072 | prompt processing progress, n_tokens = 11661, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 18072 | prompt done, n_tokens = 11661, batch.n_tokens = 64
slot init_sampler: id  2 | task 18072 | init sampler, took 1.76 ms, tokens: text = 11661, total = 11661
slot update_slots: id  2 | task 18072 | created context checkpoint 3 of 8 (pos_min = 10700, pos_max = 11596, size = 21.034 MiB)
slot print_timing: id  2 | task 18072 | 
prompt eval time =     554.04 ms /   308 tokens (    1.80 ms per token,   555.91 tokens per second)
       eval time =    1833.84 ms /    72 tokens (   25.47 ms per token,    39.26 tokens per second)
      total time =    2387.88 ms /   380 tokens
slot      release: id  2 | task 18072 | stop processing: n_tokens = 11732, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18146 | processing task, is_child = 0
slot update_slots: id  2 | task 18146 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12076
slot update_slots: id  2 | task 18146 | n_tokens = 11661, memory_seq_rm [11661, end)
slot update_slots: id  2 | task 18146 | prompt processing progress, n_tokens = 12012, batch.n_tokens = 351, progress = 0.994700
slot update_slots: id  2 | task 18146 | n_tokens = 12012, memory_seq_rm [12012, end)
slot update_slots: id  2 | task 18146 | prompt processing progress, n_tokens = 12076, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 18146 | prompt done, n_tokens = 12076, batch.n_tokens = 64
slot init_sampler: id  2 | task 18146 | init sampler, took 1.72 ms, tokens: text = 12076, total = 12076
slot update_slots: id  2 | task 18146 | created context checkpoint 4 of 8 (pos_min = 11115, pos_max = 12011, size = 21.034 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.540 (> 0.100 thold), f_keep = 0.216
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2123, total state size = 52.761 MiB
srv          load:  - looking for better prompt, base f_keep = 0.216, sim = 0.540
srv        update:  - cache state: 4 prompts, 1666.525 MiB (limits: 8192.000 MiB, 64000 tokens, 246331 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv        update:    - prompt 0x5a234dc80090:    2123 tokens, checkpoints:  1,    68.612 MiB
srv  get_availabl: prompt cache update took 30.77 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18246 | processing task, is_child = 0
slot update_slots: id  3 | task 18246 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 850
slot update_slots: id  3 | task 18246 | n_past = 459, slot.prompt.tokens.size() = 2123, seq_id = 3, pos_min = 1996, n_swa = 128
slot update_slots: id  3 | task 18246 | restored context checkpoint (pos_min = 0, pos_max = 675, size = 15.852 MiB)
slot update_slots: id  3 | task 18246 | n_tokens = 459, memory_seq_rm [459, end)
slot update_slots: id  3 | task 18246 | prompt processing progress, n_tokens = 786, batch.n_tokens = 328, progress = 0.924706
slot update_slots: id  3 | task 18246 | n_tokens = 786, memory_seq_rm [786, end)
slot update_slots: id  3 | task 18246 | prompt processing progress, n_tokens = 850, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 18246 | prompt done, n_tokens = 850, batch.n_tokens = 65
slot init_sampler: id  3 | task 18246 | init sampler, took 0.14 ms, tokens: text = 850, total = 850
slot update_slots: id  3 | task 18246 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 785, size = 18.431 MiB)
slot print_timing: id  2 | task 18146 | 
prompt eval time =     635.70 ms /   415 tokens (    1.53 ms per token,   652.83 tokens per second)
       eval time =    5456.45 ms /   144 tokens (   37.89 ms per token,    26.39 tokens per second)
      total time =    6092.14 ms /   559 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 18146 | stop processing: n_tokens = 12219, truncated = 0
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18396 | processing task, is_child = 0
slot update_slots: id  2 | task 18396 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12192
slot update_slots: id  2 | task 18396 | n_past = 12076, slot.prompt.tokens.size() = 12219, seq_id = 2, pos_min = 12046, n_swa = 128
slot update_slots: id  2 | task 18396 | restored context checkpoint (pos_min = 11115, pos_max = 12011, size = 21.034 MiB)
slot update_slots: id  2 | task 18396 | n_tokens = 12011, memory_seq_rm [12011, end)
slot update_slots: id  2 | task 18396 | prompt processing progress, n_tokens = 12128, batch.n_tokens = 118, progress = 0.994751
slot update_slots: id  2 | task 18396 | n_tokens = 12128, memory_seq_rm [12128, end)
slot update_slots: id  2 | task 18396 | prompt processing progress, n_tokens = 12192, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  2 | task 18396 | prompt done, n_tokens = 12192, batch.n_tokens = 65
slot init_sampler: id  2 | task 18396 | init sampler, took 2.17 ms, tokens: text = 12192, total = 12192
slot update_slots: id  2 | task 18396 | created context checkpoint 5 of 8 (pos_min = 11232, pos_max = 12127, size = 21.011 MiB)
slot print_timing: id  2 | task 18396 | 
prompt eval time =     674.18 ms /   181 tokens (    3.72 ms per token,   268.48 tokens per second)
       eval time =    2325.52 ms /    51 tokens (   45.60 ms per token,    21.93 tokens per second)
      total time =    2999.70 ms /   232 tokens
slot      release: id  2 | task 18396 | stop processing: n_tokens = 12242, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18527 | processing task, is_child = 0
slot update_slots: id  2 | task 18527 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12724
slot update_slots: id  2 | task 18527 | n_tokens = 12192, memory_seq_rm [12192, end)
slot update_slots: id  2 | task 18527 | prompt processing progress, n_tokens = 12660, batch.n_tokens = 469, progress = 0.994970
slot update_slots: id  2 | task 18527 | n_tokens = 12660, memory_seq_rm [12660, end)
slot update_slots: id  2 | task 18527 | prompt processing progress, n_tokens = 12724, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  2 | task 18527 | prompt done, n_tokens = 12724, batch.n_tokens = 65
slot init_sampler: id  2 | task 18527 | init sampler, took 2.26 ms, tokens: text = 12724, total = 12724
slot update_slots: id  2 | task 18527 | created context checkpoint 6 of 8 (pos_min = 11766, pos_max = 12659, size = 20.964 MiB)
slot print_timing: id  2 | task 18527 | 
prompt eval time =     776.25 ms /   532 tokens (    1.46 ms per token,   685.34 tokens per second)
       eval time =    3014.33 ms /    66 tokens (   45.67 ms per token,    21.90 tokens per second)
      total time =    3790.59 ms /   598 tokens
slot      release: id  2 | task 18527 | stop processing: n_tokens = 12789, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 18246 | 
prompt eval time =     762.25 ms /   391 tokens (    1.95 ms per token,   512.95 tokens per second)
       eval time =   16589.91 ms /   457 tokens (   36.30 ms per token,    27.55 tokens per second)
      total time =   17352.17 ms /   848 tokens
slot      release: id  3 | task 18246 | stop processing: n_tokens = 1306, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18707 | processing task, is_child = 0
slot update_slots: id  2 | task 18707 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13018
slot update_slots: id  2 | task 18707 | n_tokens = 12724, memory_seq_rm [12724, end)
slot update_slots: id  2 | task 18707 | prompt processing progress, n_tokens = 12954, batch.n_tokens = 230, progress = 0.995084
slot update_slots: id  2 | task 18707 | n_tokens = 12954, memory_seq_rm [12954, end)
slot update_slots: id  2 | task 18707 | prompt processing progress, n_tokens = 13018, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 18707 | prompt done, n_tokens = 13018, batch.n_tokens = 64
slot init_sampler: id  2 | task 18707 | init sampler, took 2.57 ms, tokens: text = 13018, total = 13018
slot update_slots: id  2 | task 18707 | created context checkpoint 7 of 8 (pos_min = 12186, pos_max = 12953, size = 18.009 MiB)
slot print_timing: id  2 | task 18707 | 
prompt eval time =     585.18 ms /   294 tokens (    1.99 ms per token,   502.41 tokens per second)
       eval time =    2919.38 ms /   116 tokens (   25.17 ms per token,    39.73 tokens per second)
      total time =    3504.56 ms /   410 tokens
slot      release: id  2 | task 18707 | stop processing: n_tokens = 13133, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18825 | processing task, is_child = 0
slot update_slots: id  2 | task 18825 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13075
slot update_slots: id  2 | task 18825 | n_tokens = 13018, memory_seq_rm [13018, end)
slot update_slots: id  2 | task 18825 | prompt processing progress, n_tokens = 13075, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  2 | task 18825 | prompt done, n_tokens = 13075, batch.n_tokens = 57
slot init_sampler: id  2 | task 18825 | init sampler, took 1.95 ms, tokens: text = 13075, total = 13075
slot print_timing: id  2 | task 18825 | 
prompt eval time =     164.03 ms /    57 tokens (    2.88 ms per token,   347.50 tokens per second)
       eval time =    4103.77 ms /   164 tokens (   25.02 ms per token,    39.96 tokens per second)
      total time =    4267.80 ms /   221 tokens
slot      release: id  2 | task 18825 | stop processing: n_tokens = 13238, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18990 | processing task, is_child = 0
slot update_slots: id  2 | task 18990 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13171
slot update_slots: id  2 | task 18990 | n_tokens = 13075, memory_seq_rm [13075, end)
slot update_slots: id  2 | task 18990 | prompt processing progress, n_tokens = 13107, batch.n_tokens = 32, progress = 0.995141
slot update_slots: id  2 | task 18990 | n_tokens = 13107, memory_seq_rm [13107, end)
slot update_slots: id  2 | task 18990 | prompt processing progress, n_tokens = 13171, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 18990 | prompt done, n_tokens = 13171, batch.n_tokens = 64
slot init_sampler: id  2 | task 18990 | init sampler, took 1.88 ms, tokens: text = 13171, total = 13171
slot update_slots: id  2 | task 18990 | created context checkpoint 8 of 8 (pos_min = 12342, pos_max = 13106, size = 17.939 MiB)
slot print_timing: id  2 | task 18990 | 
prompt eval time =     304.14 ms /    96 tokens (    3.17 ms per token,   315.65 tokens per second)
       eval time =    1700.06 ms /    67 tokens (   25.37 ms per token,    39.41 tokens per second)
      total time =    2004.20 ms /   163 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 18990 | stop processing: n_tokens = 13237, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 19059 | processing task, is_child = 0
slot update_slots: id  2 | task 19059 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13228
slot update_slots: id  2 | task 19059 | n_tokens = 13171, memory_seq_rm [13171, end)
slot update_slots: id  2 | task 19059 | prompt processing progress, n_tokens = 13228, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  2 | task 19059 | prompt done, n_tokens = 13228, batch.n_tokens = 57
slot init_sampler: id  2 | task 19059 | init sampler, took 2.80 ms, tokens: text = 13228, total = 13228
slot print_timing: id  2 | task 19059 | 
prompt eval time =     257.62 ms /    57 tokens (    4.52 ms per token,   221.26 tokens per second)
       eval time =    2370.78 ms /    94 tokens (   25.22 ms per token,    39.65 tokens per second)
      total time =    2628.39 ms /   151 tokens
slot      release: id  2 | task 19059 | stop processing: n_tokens = 13321, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.873 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 19154 | processing task, is_child = 0
slot update_slots: id  2 | task 19154 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15160
slot update_slots: id  2 | task 19154 | n_tokens = 13228, memory_seq_rm [13228, end)
slot update_slots: id  2 | task 19154 | prompt processing progress, n_tokens = 15096, batch.n_tokens = 1868, progress = 0.995778
slot update_slots: id  2 | task 19154 | n_tokens = 15096, memory_seq_rm [15096, end)
slot update_slots: id  2 | task 19154 | prompt processing progress, n_tokens = 15160, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 19154 | prompt done, n_tokens = 15160, batch.n_tokens = 64
slot init_sampler: id  2 | task 19154 | init sampler, took 2.23 ms, tokens: text = 15160, total = 15160
slot update_slots: id  2 | task 19154 | erasing old context checkpoint (pos_min = 10218, pos_max = 11114, size = 21.034 MiB)
slot update_slots: id  2 | task 19154 | created context checkpoint 8 of 8 (pos_min = 14199, pos_max = 15095, size = 21.034 MiB)
slot print_timing: id  2 | task 19154 | 
prompt eval time =    2367.80 ms /  1932 tokens (    1.23 ms per token,   815.95 tokens per second)
       eval time =    1722.61 ms /    69 tokens (   24.97 ms per token,    40.06 tokens per second)
      total time =    4090.41 ms /  2001 tokens
slot      release: id  2 | task 19154 | stop processing: n_tokens = 15228, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.943 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 19225 | processing task, is_child = 0
slot update_slots: id  2 | task 19225 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16083
slot update_slots: id  2 | task 19225 | n_tokens = 15160, memory_seq_rm [15160, end)
slot update_slots: id  2 | task 19225 | prompt processing progress, n_tokens = 16019, batch.n_tokens = 859, progress = 0.996021
slot update_slots: id  2 | task 19225 | n_tokens = 16019, memory_seq_rm [16019, end)
slot update_slots: id  2 | task 19225 | prompt processing progress, n_tokens = 16083, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 19225 | prompt done, n_tokens = 16083, batch.n_tokens = 64
slot init_sampler: id  2 | task 19225 | init sampler, took 2.27 ms, tokens: text = 16083, total = 16083
slot update_slots: id  2 | task 19225 | erasing old context checkpoint (pos_min = 10392, pos_max = 11288, size = 21.034 MiB)
slot update_slots: id  2 | task 19225 | created context checkpoint 8 of 8 (pos_min = 15122, pos_max = 16018, size = 21.034 MiB)
slot print_timing: id  2 | task 19225 | 
prompt eval time =    1231.90 ms /   923 tokens (    1.33 ms per token,   749.25 tokens per second)
       eval time =    3134.11 ms /   123 tokens (   25.48 ms per token,    39.25 tokens per second)
      total time =    4366.01 ms /  1046 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 19225 | stop processing: n_tokens = 16205, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 19350 | processing task, is_child = 0
slot update_slots: id  2 | task 19350 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16140
slot update_slots: id  2 | task 19350 | n_tokens = 16083, memory_seq_rm [16083, end)
slot update_slots: id  2 | task 19350 | prompt processing progress, n_tokens = 16140, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  2 | task 19350 | prompt done, n_tokens = 16140, batch.n_tokens = 57
slot init_sampler: id  2 | task 19350 | init sampler, took 2.36 ms, tokens: text = 16140, total = 16140
slot print_timing: id  2 | task 19350 | 
prompt eval time =     200.17 ms /    57 tokens (    3.51 ms per token,   284.76 tokens per second)
       eval time =    1025.86 ms /    41 tokens (   25.02 ms per token,    39.97 tokens per second)
      total time =    1226.02 ms /    98 tokens
slot      release: id  2 | task 19350 | stop processing: n_tokens = 16180, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 19392 | processing task, is_child = 0
slot update_slots: id  2 | task 19392 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16314
slot update_slots: id  2 | task 19392 | n_tokens = 16140, memory_seq_rm [16140, end)
slot update_slots: id  2 | task 19392 | prompt processing progress, n_tokens = 16250, batch.n_tokens = 110, progress = 0.996077
slot update_slots: id  2 | task 19392 | n_tokens = 16250, memory_seq_rm [16250, end)
slot update_slots: id  2 | task 19392 | prompt processing progress, n_tokens = 16314, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 19392 | prompt done, n_tokens = 16314, batch.n_tokens = 64
slot init_sampler: id  2 | task 19392 | init sampler, took 2.46 ms, tokens: text = 16314, total = 16314
slot update_slots: id  2 | task 19392 | erasing old context checkpoint (pos_min = 10700, pos_max = 11596, size = 21.034 MiB)
slot update_slots: id  2 | task 19392 | created context checkpoint 8 of 8 (pos_min = 15353, pos_max = 16249, size = 21.034 MiB)
slot print_timing: id  2 | task 19392 | 
prompt eval time =     454.98 ms /   174 tokens (    2.61 ms per token,   382.43 tokens per second)
       eval time =    1532.79 ms /    61 tokens (   25.13 ms per token,    39.80 tokens per second)
      total time =    1987.77 ms /   235 tokens
slot      release: id  2 | task 19392 | stop processing: n_tokens = 16374, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 19455 | processing task, is_child = 0
slot update_slots: id  2 | task 19455 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16622
slot update_slots: id  2 | task 19455 | n_tokens = 16314, memory_seq_rm [16314, end)
slot update_slots: id  2 | task 19455 | prompt processing progress, n_tokens = 16558, batch.n_tokens = 244, progress = 0.996150
slot update_slots: id  2 | task 19455 | n_tokens = 16558, memory_seq_rm [16558, end)
slot update_slots: id  2 | task 19455 | prompt processing progress, n_tokens = 16622, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 19455 | prompt done, n_tokens = 16622, batch.n_tokens = 64
slot init_sampler: id  2 | task 19455 | init sampler, took 2.42 ms, tokens: text = 16622, total = 16622
slot update_slots: id  2 | task 19455 | erasing old context checkpoint (pos_min = 11115, pos_max = 12011, size = 21.034 MiB)
slot update_slots: id  2 | task 19455 | created context checkpoint 8 of 8 (pos_min = 15661, pos_max = 16557, size = 21.034 MiB)
slot print_timing: id  2 | task 19455 | 
prompt eval time =     570.58 ms /   308 tokens (    1.85 ms per token,   539.80 tokens per second)
       eval time =    1471.13 ms /    58 tokens (   25.36 ms per token,    39.43 tokens per second)
      total time =    2041.71 ms /   366 tokens
slot      release: id  2 | task 19455 | stop processing: n_tokens = 16679, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 19515 | processing task, is_child = 0
slot update_slots: id  2 | task 19515 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16930
slot update_slots: id  2 | task 19515 | n_tokens = 16622, memory_seq_rm [16622, end)
slot update_slots: id  2 | task 19515 | prompt processing progress, n_tokens = 16866, batch.n_tokens = 244, progress = 0.996220
slot update_slots: id  2 | task 19515 | n_tokens = 16866, memory_seq_rm [16866, end)
slot update_slots: id  2 | task 19515 | prompt processing progress, n_tokens = 16930, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 19515 | prompt done, n_tokens = 16930, batch.n_tokens = 64
slot init_sampler: id  2 | task 19515 | init sampler, took 2.37 ms, tokens: text = 16930, total = 16930
slot update_slots: id  2 | task 19515 | erasing old context checkpoint (pos_min = 11232, pos_max = 12127, size = 21.011 MiB)
slot update_slots: id  2 | task 19515 | created context checkpoint 8 of 8 (pos_min = 15969, pos_max = 16865, size = 21.034 MiB)
slot print_timing: id  2 | task 19515 | 
prompt eval time =     576.21 ms /   308 tokens (    1.87 ms per token,   534.53 tokens per second)
       eval time =    1503.72 ms /    58 tokens (   25.93 ms per token,    38.57 tokens per second)
      total time =    2079.93 ms /   366 tokens
slot      release: id  2 | task 19515 | stop processing: n_tokens = 16987, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 19575 | processing task, is_child = 0
slot update_slots: id  2 | task 19575 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17336
slot update_slots: id  2 | task 19575 | n_tokens = 16930, memory_seq_rm [16930, end)
slot update_slots: id  2 | task 19575 | prompt processing progress, n_tokens = 17272, batch.n_tokens = 342, progress = 0.996308
slot update_slots: id  2 | task 19575 | n_tokens = 17272, memory_seq_rm [17272, end)
slot update_slots: id  2 | task 19575 | prompt processing progress, n_tokens = 17336, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 19575 | prompt done, n_tokens = 17336, batch.n_tokens = 64
slot init_sampler: id  2 | task 19575 | init sampler, took 2.75 ms, tokens: text = 17336, total = 17336
slot update_slots: id  2 | task 19575 | erasing old context checkpoint (pos_min = 11766, pos_max = 12659, size = 20.964 MiB)
slot update_slots: id  2 | task 19575 | created context checkpoint 8 of 8 (pos_min = 16375, pos_max = 17271, size = 21.034 MiB)
slot print_timing: id  2 | task 19575 | 
prompt eval time =     715.11 ms /   406 tokens (    1.76 ms per token,   567.75 tokens per second)
       eval time =    1944.99 ms /    76 tokens (   25.59 ms per token,    39.07 tokens per second)
      total time =    2660.10 ms /   482 tokens
slot      release: id  2 | task 19575 | stop processing: n_tokens = 17411, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 19653 | processing task, is_child = 0
slot update_slots: id  2 | task 19653 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17405
slot update_slots: id  2 | task 19653 | n_tokens = 17336, memory_seq_rm [17336, end)
slot update_slots: id  2 | task 19653 | prompt processing progress, n_tokens = 17341, batch.n_tokens = 5, progress = 0.996323
slot update_slots: id  2 | task 19653 | n_tokens = 17341, memory_seq_rm [17341, end)
slot update_slots: id  2 | task 19653 | prompt processing progress, n_tokens = 17405, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 19653 | prompt done, n_tokens = 17405, batch.n_tokens = 64
slot init_sampler: id  2 | task 19653 | init sampler, took 2.42 ms, tokens: text = 17405, total = 17405
slot update_slots: id  2 | task 19653 | erasing old context checkpoint (pos_min = 12186, pos_max = 12953, size = 18.009 MiB)
slot update_slots: id  2 | task 19653 | created context checkpoint 8 of 8 (pos_min = 16514, pos_max = 17340, size = 19.393 MiB)
slot print_timing: id  2 | task 19653 | 
prompt eval time =     246.77 ms /    69 tokens (    3.58 ms per token,   279.61 tokens per second)
       eval time =    1162.53 ms /    47 tokens (   24.73 ms per token,    40.43 tokens per second)
      total time =    1409.30 ms /   116 tokens
slot      release: id  2 | task 19653 | stop processing: n_tokens = 17451, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.660 (> 0.100 thold), f_keep = 0.639
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19702 | processing task, is_child = 0
slot update_slots: id  3 | task 19702 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1263
slot update_slots: id  3 | task 19702 | n_past = 834, slot.prompt.tokens.size() = 1306, seq_id = 3, pos_min = 1179, n_swa = 128
slot update_slots: id  3 | task 19702 | restored context checkpoint (pos_min = 0, pos_max = 785, size = 18.431 MiB)
slot update_slots: id  3 | task 19702 | n_tokens = 785, memory_seq_rm [785, end)
slot update_slots: id  3 | task 19702 | prompt processing progress, n_tokens = 1199, batch.n_tokens = 414, progress = 0.949327
slot update_slots: id  3 | task 19702 | n_tokens = 1199, memory_seq_rm [1199, end)
slot update_slots: id  3 | task 19702 | prompt processing progress, n_tokens = 1263, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 19702 | prompt done, n_tokens = 1263, batch.n_tokens = 64
slot init_sampler: id  3 | task 19702 | init sampler, took 0.21 ms, tokens: text = 1263, total = 1263
slot update_slots: id  3 | task 19702 | created context checkpoint 3 of 8 (pos_min = 302, pos_max = 1198, size = 21.034 MiB)
slot print_timing: id  3 | task 19702 | 
prompt eval time =     903.10 ms /   478 tokens (    1.89 ms per token,   529.29 tokens per second)
       eval time =   16960.41 ms /   659 tokens (   25.74 ms per token,    38.86 tokens per second)
      total time =   17863.51 ms /  1137 tokens
slot      release: id  3 | task 19702 | stop processing: n_tokens = 1921, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.919 (> 0.100 thold), f_keep = 0.640
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20363 | processing task, is_child = 0
slot update_slots: id  2 | task 20363 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12141
slot update_slots: id  2 | task 20363 | n_past = 11163, slot.prompt.tokens.size() = 17451, seq_id = 2, pos_min = 17324, n_swa = 128
slot update_slots: id  2 | task 20363 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 20363 | erased invalidated context checkpoint (pos_min = 12342, pos_max = 13106, n_swa = 128, size = 17.939 MiB)
slot update_slots: id  2 | task 20363 | erased invalidated context checkpoint (pos_min = 14199, pos_max = 15095, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 20363 | erased invalidated context checkpoint (pos_min = 15122, pos_max = 16018, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 20363 | erased invalidated context checkpoint (pos_min = 15353, pos_max = 16249, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 20363 | erased invalidated context checkpoint (pos_min = 15661, pos_max = 16557, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 20363 | erased invalidated context checkpoint (pos_min = 15969, pos_max = 16865, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 20363 | erased invalidated context checkpoint (pos_min = 16375, pos_max = 17271, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 20363 | erased invalidated context checkpoint (pos_min = 16514, pos_max = 17340, n_swa = 128, size = 19.393 MiB)
slot update_slots: id  2 | task 20363 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 20363 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.168685
slot update_slots: id  2 | task 20363 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 20363 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.337369
slot update_slots: id  2 | task 20363 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 20363 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.506054
slot update_slots: id  2 | task 20363 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 20363 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.674738
slot update_slots: id  2 | task 20363 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 20363 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.843423
slot update_slots: id  2 | task 20363 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 20363 | prompt processing progress, n_tokens = 12077, batch.n_tokens = 1837, progress = 0.994729
slot update_slots: id  2 | task 20363 | n_tokens = 12077, memory_seq_rm [12077, end)
slot update_slots: id  2 | task 20363 | prompt processing progress, n_tokens = 12141, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 20363 | prompt done, n_tokens = 12141, batch.n_tokens = 64
slot init_sampler: id  2 | task 20363 | init sampler, took 1.82 ms, tokens: text = 12141, total = 12141
slot update_slots: id  2 | task 20363 | created context checkpoint 1 of 8 (pos_min = 11180, pos_max = 12076, size = 21.034 MiB)
slot print_timing: id  2 | task 20363 | 
prompt eval time =   14940.70 ms / 12141 tokens (    1.23 ms per token,   812.61 tokens per second)
       eval time =   71021.48 ms /  2767 tokens (   25.67 ms per token,    38.96 tokens per second)
      total time =   85962.18 ms / 14908 tokens
slot      release: id  2 | task 20363 | stop processing: n_tokens = 14907, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.815 (> 0.100 thold), f_keep = 0.814
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 23137 | processing task, is_child = 0
slot update_slots: id  2 | task 23137 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14900
slot update_slots: id  2 | task 23137 | n_past = 12141, slot.prompt.tokens.size() = 14907, seq_id = 2, pos_min = 14010, n_swa = 128
slot update_slots: id  2 | task 23137 | restored context checkpoint (pos_min = 11180, pos_max = 12076, size = 21.034 MiB)
slot update_slots: id  2 | task 23137 | n_tokens = 12076, memory_seq_rm [12076, end)
slot update_slots: id  2 | task 23137 | prompt processing progress, n_tokens = 14124, batch.n_tokens = 2048, progress = 0.947919
slot update_slots: id  2 | task 23137 | n_tokens = 14124, memory_seq_rm [14124, end)
slot update_slots: id  2 | task 23137 | prompt processing progress, n_tokens = 14836, batch.n_tokens = 712, progress = 0.995705
slot update_slots: id  2 | task 23137 | n_tokens = 14836, memory_seq_rm [14836, end)
slot update_slots: id  2 | task 23137 | prompt processing progress, n_tokens = 14900, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 23137 | prompt done, n_tokens = 14900, batch.n_tokens = 64
slot init_sampler: id  2 | task 23137 | init sampler, took 3.81 ms, tokens: text = 14900, total = 14900
slot update_slots: id  2 | task 23137 | created context checkpoint 2 of 8 (pos_min = 13939, pos_max = 14835, size = 21.034 MiB)
slot print_timing: id  2 | task 23137 | 
prompt eval time =    3781.26 ms /  2824 tokens (    1.34 ms per token,   746.84 tokens per second)
       eval time =     779.84 ms /    31 tokens (   25.16 ms per token,    39.75 tokens per second)
      total time =    4561.10 ms /  2855 tokens
slot      release: id  2 | task 23137 | stop processing: n_tokens = 14930, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23171 | processing task, is_child = 0
slot update_slots: id  1 | task 23171 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15029
slot update_slots: id  1 | task 23171 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 23171 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.136270
slot update_slots: id  1 | task 23171 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 23171 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.272540
slot update_slots: id  1 | task 23171 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 23171 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.408810
slot update_slots: id  1 | task 23171 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 23171 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.545080
slot update_slots: id  1 | task 23171 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  1 | task 23171 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.681349
slot update_slots: id  1 | task 23171 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  1 | task 23171 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.817619
slot update_slots: id  1 | task 23171 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  1 | task 23171 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.953889
slot update_slots: id  1 | task 23171 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  1 | task 23171 | prompt processing progress, n_tokens = 14965, batch.n_tokens = 629, progress = 0.995742
slot update_slots: id  1 | task 23171 | n_tokens = 14965, memory_seq_rm [14965, end)
slot update_slots: id  1 | task 23171 | prompt processing progress, n_tokens = 15029, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23171 | prompt done, n_tokens = 15029, batch.n_tokens = 64
slot init_sampler: id  1 | task 23171 | init sampler, took 2.15 ms, tokens: text = 15029, total = 15029
slot update_slots: id  1 | task 23171 | created context checkpoint 1 of 8 (pos_min = 14195, pos_max = 14964, size = 18.056 MiB)
slot print_timing: id  1 | task 23171 | 
prompt eval time =   19671.69 ms / 15029 tokens (    1.31 ms per token,   763.99 tokens per second)
       eval time =     753.97 ms /    26 tokens (   29.00 ms per token,    34.48 tokens per second)
      total time =   20425.66 ms / 15055 tokens
slot      release: id  1 | task 23171 | stop processing: n_tokens = 15054, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23206 | processing task, is_child = 0
slot update_slots: id  1 | task 23206 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15095
slot update_slots: id  1 | task 23206 | n_tokens = 15029, memory_seq_rm [15029, end)
slot update_slots: id  1 | task 23206 | prompt processing progress, n_tokens = 15031, batch.n_tokens = 2, progress = 0.995760
slot update_slots: id  1 | task 23206 | n_tokens = 15031, memory_seq_rm [15031, end)
slot update_slots: id  1 | task 23206 | prompt processing progress, n_tokens = 15095, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23206 | prompt done, n_tokens = 15095, batch.n_tokens = 64
slot init_sampler: id  1 | task 23206 | init sampler, took 2.25 ms, tokens: text = 15095, total = 15095
slot update_slots: id  1 | task 23206 | created context checkpoint 2 of 8 (pos_min = 14284, pos_max = 15030, size = 17.517 MiB)
slot print_timing: id  1 | task 23206 | 
prompt eval time =     340.05 ms /    66 tokens (    5.15 ms per token,   194.09 tokens per second)
       eval time =     718.57 ms /    28 tokens (   25.66 ms per token,    38.97 tokens per second)
      total time =    1058.63 ms /    94 tokens
slot      release: id  1 | task 23206 | stop processing: n_tokens = 15122, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23236 | processing task, is_child = 0
slot update_slots: id  1 | task 23236 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15092
slot update_slots: id  1 | task 23236 | n_tokens = 15020, memory_seq_rm [15020, end)
slot update_slots: id  1 | task 23236 | prompt processing progress, n_tokens = 15028, batch.n_tokens = 8, progress = 0.995759
slot update_slots: id  1 | task 23236 | n_tokens = 15028, memory_seq_rm [15028, end)
slot update_slots: id  1 | task 23236 | prompt processing progress, n_tokens = 15092, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23236 | prompt done, n_tokens = 15092, batch.n_tokens = 64
slot init_sampler: id  1 | task 23236 | init sampler, took 2.60 ms, tokens: text = 15092, total = 15092
slot print_timing: id  1 | task 23236 | 
prompt eval time =     383.12 ms /    72 tokens (    5.32 ms per token,   187.93 tokens per second)
       eval time =    4167.86 ms /   156 tokens (   26.72 ms per token,    37.43 tokens per second)
      total time =    4550.98 ms /   228 tokens
slot      release: id  1 | task 23236 | stop processing: n_tokens = 15247, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23394 | processing task, is_child = 0
slot update_slots: id  1 | task 23394 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15175
slot update_slots: id  1 | task 23394 | n_tokens = 15092, memory_seq_rm [15092, end)
slot update_slots: id  1 | task 23394 | prompt processing progress, n_tokens = 15111, batch.n_tokens = 19, progress = 0.995783
slot update_slots: id  1 | task 23394 | n_tokens = 15111, memory_seq_rm [15111, end)
slot update_slots: id  1 | task 23394 | prompt processing progress, n_tokens = 15175, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23394 | prompt done, n_tokens = 15175, batch.n_tokens = 64
slot init_sampler: id  1 | task 23394 | init sampler, took 3.02 ms, tokens: text = 15175, total = 15175
slot update_slots: id  1 | task 23394 | created context checkpoint 3 of 8 (pos_min = 14477, pos_max = 15110, size = 14.867 MiB)
slot print_timing: id  1 | task 23394 | 
prompt eval time =     390.63 ms /    83 tokens (    4.71 ms per token,   212.48 tokens per second)
       eval time =     847.47 ms /    32 tokens (   26.48 ms per token,    37.76 tokens per second)
      total time =    1238.10 ms /   115 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 23394 | stop processing: n_tokens = 15206, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.305 (> 0.100 thold), f_keep = 0.037
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 15206, total state size = 373.660 MiB
srv          load:  - looking for better prompt, base f_keep = 0.037, sim = 0.305
srv        update:  - cache state: 5 prompts, 2090.624 MiB (limits: 8192.000 MiB, 64000 tokens, 255945 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv        update:    - prompt 0x5a234dc80090:    2123 tokens, checkpoints:  1,    68.612 MiB
srv        update:    - prompt 0x5a234ccbfee0:   15206 tokens, checkpoints:  3,   424.099 MiB
srv  get_availabl: prompt cache update took 303.77 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23428 | processing task, is_child = 0
slot update_slots: id  1 | task 23428 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1861
slot update_slots: id  1 | task 23428 | n_past = 567, slot.prompt.tokens.size() = 15206, seq_id = 1, pos_min = 14477, n_swa = 128
slot update_slots: id  1 | task 23428 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 23428 | erased invalidated context checkpoint (pos_min = 14195, pos_max = 14964, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 23428 | erased invalidated context checkpoint (pos_min = 14284, pos_max = 15030, n_swa = 128, size = 17.517 MiB)
slot update_slots: id  1 | task 23428 | erased invalidated context checkpoint (pos_min = 14477, pos_max = 15110, n_swa = 128, size = 14.867 MiB)
slot update_slots: id  1 | task 23428 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 23428 | prompt processing progress, n_tokens = 1797, batch.n_tokens = 1797, progress = 0.965610
slot update_slots: id  1 | task 23428 | n_tokens = 1797, memory_seq_rm [1797, end)
slot update_slots: id  1 | task 23428 | prompt processing progress, n_tokens = 1861, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23428 | prompt done, n_tokens = 1861, batch.n_tokens = 64
slot init_sampler: id  1 | task 23428 | init sampler, took 0.35 ms, tokens: text = 1861, total = 1861
slot update_slots: id  1 | task 23428 | created context checkpoint 1 of 8 (pos_min = 1027, pos_max = 1796, size = 18.056 MiB)
slot print_timing: id  1 | task 23428 | 
prompt eval time =    2474.79 ms /  1861 tokens (    1.33 ms per token,   751.98 tokens per second)
       eval time =   13632.35 ms /   525 tokens (   25.97 ms per token,    38.51 tokens per second)
      total time =   16107.14 ms /  2386 tokens
slot      release: id  1 | task 23428 | stop processing: n_tokens = 2385, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.790 (> 0.100 thold), f_keep = 0.776
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23955 | processing task, is_child = 0
slot update_slots: id  1 | task 23955 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2343
slot update_slots: id  1 | task 23955 | n_tokens = 1850, memory_seq_rm [1850, end)
slot update_slots: id  1 | task 23955 | prompt processing progress, n_tokens = 2279, batch.n_tokens = 429, progress = 0.972685
slot update_slots: id  1 | task 23955 | n_tokens = 2279, memory_seq_rm [2279, end)
slot update_slots: id  1 | task 23955 | prompt processing progress, n_tokens = 2343, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23955 | prompt done, n_tokens = 2343, batch.n_tokens = 64
slot init_sampler: id  1 | task 23955 | init sampler, took 0.40 ms, tokens: text = 2343, total = 2343
slot update_slots: id  1 | task 23955 | created context checkpoint 2 of 8 (pos_min = 1723, pos_max = 2278, size = 13.038 MiB)
slot print_timing: id  1 | task 23955 | 
prompt eval time =     820.39 ms /   493 tokens (    1.66 ms per token,   600.93 tokens per second)
       eval time =   15150.27 ms /   582 tokens (   26.03 ms per token,    38.42 tokens per second)
      total time =   15970.66 ms /  1075 tokens
slot      release: id  1 | task 23955 | stop processing: n_tokens = 2924, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.194
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2924, total state size = 86.621 MiB
srv          load:  - looking for better prompt, base f_keep = 0.194, sim = 0.984
srv        update:  - cache state: 6 prompts, 2208.339 MiB (limits: 8192.000 MiB, 64000 tokens, 253148 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv        update:    - prompt 0x5a234dc80090:    2123 tokens, checkpoints:  1,    68.612 MiB
srv        update:    - prompt 0x5a234ccbfee0:   15206 tokens, checkpoints:  3,   424.099 MiB
srv        update:    - prompt 0x5a234dbdca00:    2924 tokens, checkpoints:  2,   117.715 MiB
srv  get_availabl: prompt cache update took 107.75 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24539 | processing task, is_child = 0
slot update_slots: id  1 | task 24539 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 575
slot update_slots: id  1 | task 24539 | n_past = 566, slot.prompt.tokens.size() = 2924, seq_id = 1, pos_min = 2154, n_swa = 128
slot update_slots: id  1 | task 24539 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 24539 | erased invalidated context checkpoint (pos_min = 1027, pos_max = 1796, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 24539 | erased invalidated context checkpoint (pos_min = 1723, pos_max = 2278, n_swa = 128, size = 13.038 MiB)
slot update_slots: id  1 | task 24539 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 24539 | prompt processing progress, n_tokens = 511, batch.n_tokens = 511, progress = 0.888696
slot update_slots: id  1 | task 24539 | n_tokens = 511, memory_seq_rm [511, end)
slot update_slots: id  1 | task 24539 | prompt processing progress, n_tokens = 575, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24539 | prompt done, n_tokens = 575, batch.n_tokens = 64
slot init_sampler: id  1 | task 24539 | init sampler, took 0.21 ms, tokens: text = 575, total = 575
slot update_slots: id  1 | task 24539 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 510, size = 11.983 MiB)
slot print_timing: id  1 | task 24539 | 
prompt eval time =     846.79 ms /   575 tokens (    1.47 ms per token,   679.03 tokens per second)
       eval time =    1441.34 ms /    59 tokens (   24.43 ms per token,    40.93 tokens per second)
      total time =    2288.13 ms /   634 tokens
slot      release: id  1 | task 24539 | stop processing: n_tokens = 633, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.897 (> 0.100 thold), f_keep = 0.908
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24600 | processing task, is_child = 0
slot update_slots: id  1 | task 24600 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 641
slot update_slots: id  1 | task 24600 | n_tokens = 575, memory_seq_rm [575, end)
slot update_slots: id  1 | task 24600 | prompt processing progress, n_tokens = 577, batch.n_tokens = 2, progress = 0.900156
slot update_slots: id  1 | task 24600 | n_tokens = 577, memory_seq_rm [577, end)
slot update_slots: id  1 | task 24600 | prompt processing progress, n_tokens = 641, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24600 | prompt done, n_tokens = 641, batch.n_tokens = 64
slot init_sampler: id  1 | task 24600 | init sampler, took 0.13 ms, tokens: text = 641, total = 641
slot update_slots: id  1 | task 24600 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 576, size = 13.530 MiB)
slot print_timing: id  1 | task 24600 | 
prompt eval time =     303.51 ms /    66 tokens (    4.60 ms per token,   217.46 tokens per second)
       eval time =    1447.97 ms /    62 tokens (   23.35 ms per token,    42.82 tokens per second)
      total time =    1751.48 ms /   128 tokens
slot      release: id  1 | task 24600 | stop processing: n_tokens = 702, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.891 (> 0.100 thold), f_keep = 0.806
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24664 | processing task, is_child = 0
slot update_slots: id  1 | task 24664 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 635
slot update_slots: id  1 | task 24664 | n_tokens = 566, memory_seq_rm [566, end)
slot update_slots: id  1 | task 24664 | prompt processing progress, n_tokens = 571, batch.n_tokens = 5, progress = 0.899213
slot update_slots: id  1 | task 24664 | n_tokens = 571, memory_seq_rm [571, end)
slot update_slots: id  1 | task 24664 | prompt processing progress, n_tokens = 635, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24664 | prompt done, n_tokens = 635, batch.n_tokens = 64
slot init_sampler: id  1 | task 24664 | init sampler, took 0.13 ms, tokens: text = 635, total = 635
slot print_timing: id  1 | task 24664 | 
prompt eval time =     306.23 ms /    69 tokens (    4.44 ms per token,   225.32 tokens per second)
       eval time =    7130.89 ms /   298 tokens (   23.93 ms per token,    41.79 tokens per second)
      total time =    7437.12 ms /   367 tokens
slot      release: id  1 | task 24664 | stop processing: n_tokens = 932, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.479 (> 0.100 thold), f_keep = 0.681
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24964 | processing task, is_child = 0
slot update_slots: id  1 | task 24964 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1325
slot update_slots: id  1 | task 24964 | n_tokens = 635, memory_seq_rm [635, end)
slot update_slots: id  1 | task 24964 | prompt processing progress, n_tokens = 1261, batch.n_tokens = 626, progress = 0.951698
slot update_slots: id  1 | task 24964 | n_tokens = 1261, memory_seq_rm [1261, end)
slot update_slots: id  1 | task 24964 | prompt processing progress, n_tokens = 1325, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24964 | prompt done, n_tokens = 1325, batch.n_tokens = 64
slot init_sampler: id  1 | task 24964 | init sampler, took 0.24 ms, tokens: text = 1325, total = 1325
slot update_slots: id  1 | task 24964 | created context checkpoint 3 of 8 (pos_min = 508, pos_max = 1260, size = 17.657 MiB)
slot print_timing: id  1 | task 24964 | 
prompt eval time =    1039.57 ms /   690 tokens (    1.51 ms per token,   663.74 tokens per second)
       eval time =   14969.98 ms /   624 tokens (   23.99 ms per token,    41.68 tokens per second)
      total time =   16009.56 ms /  1314 tokens
slot      release: id  1 | task 24964 | stop processing: n_tokens = 1948, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.517 (> 0.100 thold), f_keep = 0.320
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1948, total state size = 63.735 MiB
srv          load:  - looking for better prompt, base f_keep = 0.320, sim = 0.517
srv        update:  - cache state: 7 prompts, 2315.244 MiB (limits: 8192.000 MiB, 64000 tokens, 248352 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv        update:    - prompt 0x5a234dc80090:    2123 tokens, checkpoints:  1,    68.612 MiB
srv        update:    - prompt 0x5a234ccbfee0:   15206 tokens, checkpoints:  3,   424.099 MiB
srv        update:    - prompt 0x5a234dbdca00:    2924 tokens, checkpoints:  2,   117.715 MiB
srv        update:    - prompt 0x5a234cfa37d0:    1948 tokens, checkpoints:  3,   106.905 MiB
srv  get_availabl: prompt cache update took 84.72 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25590 | processing task, is_child = 0
slot update_slots: id  1 | task 25590 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1205
slot update_slots: id  1 | task 25590 | n_past = 623, slot.prompt.tokens.size() = 1948, seq_id = 1, pos_min = 1178, n_swa = 128
slot update_slots: id  1 | task 25590 | restored context checkpoint (pos_min = 0, pos_max = 576, size = 13.530 MiB)
slot update_slots: id  1 | task 25590 | erased invalidated context checkpoint (pos_min = 508, pos_max = 1260, n_swa = 128, size = 17.657 MiB)
slot update_slots: id  1 | task 25590 | n_tokens = 576, memory_seq_rm [576, end)
slot update_slots: id  1 | task 25590 | prompt processing progress, n_tokens = 1141, batch.n_tokens = 565, progress = 0.946888
slot update_slots: id  1 | task 25590 | n_tokens = 1141, memory_seq_rm [1141, end)
slot update_slots: id  1 | task 25590 | prompt processing progress, n_tokens = 1205, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25590 | prompt done, n_tokens = 1205, batch.n_tokens = 64
slot init_sampler: id  1 | task 25590 | init sampler, took 0.24 ms, tokens: text = 1205, total = 1205
slot update_slots: id  1 | task 25590 | created context checkpoint 3 of 8 (pos_min = 371, pos_max = 1140, size = 18.056 MiB)
slot print_timing: id  1 | task 25590 | 
prompt eval time =    1136.77 ms /   629 tokens (    1.81 ms per token,   553.32 tokens per second)
       eval time =    3591.24 ms /   149 tokens (   24.10 ms per token,    41.49 tokens per second)
      total time =    4728.01 ms /   778 tokens
slot      release: id  1 | task 25590 | stop processing: n_tokens = 1353, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.940 (> 0.100 thold), f_keep = 0.891
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25741 | processing task, is_child = 0
slot update_slots: id  1 | task 25741 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1282
slot update_slots: id  1 | task 25741 | n_tokens = 1205, memory_seq_rm [1205, end)
slot update_slots: id  1 | task 25741 | prompt processing progress, n_tokens = 1218, batch.n_tokens = 13, progress = 0.950078
slot update_slots: id  1 | task 25741 | n_tokens = 1218, memory_seq_rm [1218, end)
slot update_slots: id  1 | task 25741 | prompt processing progress, n_tokens = 1282, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25741 | prompt done, n_tokens = 1282, batch.n_tokens = 64
slot init_sampler: id  1 | task 25741 | init sampler, took 0.24 ms, tokens: text = 1282, total = 1282
slot update_slots: id  1 | task 25741 | created context checkpoint 4 of 8 (pos_min = 583, pos_max = 1217, size = 14.890 MiB)
slot print_timing: id  1 | task 25741 | 
prompt eval time =     260.91 ms /    77 tokens (    3.39 ms per token,   295.12 tokens per second)
       eval time =    4656.64 ms /   196 tokens (   23.76 ms per token,    42.09 tokens per second)
      total time =    4917.55 ms /   273 tokens
slot      release: id  1 | task 25741 | stop processing: n_tokens = 1477, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.913 (> 0.100 thold), f_keep = 0.868
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25939 | processing task, is_child = 0
slot update_slots: id  1 | task 25939 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1404
slot update_slots: id  1 | task 25939 | n_tokens = 1282, memory_seq_rm [1282, end)
slot update_slots: id  1 | task 25939 | prompt processing progress, n_tokens = 1340, batch.n_tokens = 58, progress = 0.954416
slot update_slots: id  1 | task 25939 | n_tokens = 1340, memory_seq_rm [1340, end)
slot update_slots: id  1 | task 25939 | prompt processing progress, n_tokens = 1404, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25939 | prompt done, n_tokens = 1404, batch.n_tokens = 64
slot init_sampler: id  1 | task 25939 | init sampler, took 0.23 ms, tokens: text = 1404, total = 1404
slot update_slots: id  1 | task 25939 | created context checkpoint 5 of 8 (pos_min = 707, pos_max = 1339, size = 14.843 MiB)
slot print_timing: id  1 | task 25939 | 
prompt eval time =     349.29 ms /   122 tokens (    2.86 ms per token,   349.28 tokens per second)
       eval time =    4098.16 ms /   171 tokens (   23.97 ms per token,    41.73 tokens per second)
      total time =    4447.45 ms /   293 tokens
slot      release: id  1 | task 25939 | stop processing: n_tokens = 1574, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.892
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26112 | processing task, is_child = 0
slot update_slots: id  1 | task 26112 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1477
slot update_slots: id  1 | task 26112 | n_tokens = 1404, memory_seq_rm [1404, end)
slot update_slots: id  1 | task 26112 | prompt processing progress, n_tokens = 1413, batch.n_tokens = 9, progress = 0.956669
slot update_slots: id  1 | task 26112 | n_tokens = 1413, memory_seq_rm [1413, end)
slot update_slots: id  1 | task 26112 | prompt processing progress, n_tokens = 1477, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26112 | prompt done, n_tokens = 1477, batch.n_tokens = 64
slot init_sampler: id  1 | task 26112 | init sampler, took 0.27 ms, tokens: text = 1477, total = 1477
slot update_slots: id  1 | task 26112 | created context checkpoint 6 of 8 (pos_min = 804, pos_max = 1412, size = 14.281 MiB)
slot print_timing: id  1 | task 26112 | 
prompt eval time =     248.26 ms /    73 tokens (    3.40 ms per token,   294.05 tokens per second)
       eval time =    2831.06 ms /   117 tokens (   24.20 ms per token,    41.33 tokens per second)
      total time =    3079.32 ms /   190 tokens
slot      release: id  1 | task 26112 | stop processing: n_tokens = 1593, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.834 (> 0.100 thold), f_keep = 0.927
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26231 | processing task, is_child = 0
slot update_slots: id  1 | task 26231 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1770
slot update_slots: id  1 | task 26231 | n_tokens = 1477, memory_seq_rm [1477, end)
slot update_slots: id  1 | task 26231 | prompt processing progress, n_tokens = 1706, batch.n_tokens = 229, progress = 0.963842
slot update_slots: id  1 | task 26231 | n_tokens = 1706, memory_seq_rm [1706, end)
slot update_slots: id  1 | task 26231 | prompt processing progress, n_tokens = 1770, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26231 | prompt done, n_tokens = 1770, batch.n_tokens = 64
slot init_sampler: id  1 | task 26231 | init sampler, took 0.28 ms, tokens: text = 1770, total = 1770
slot update_slots: id  1 | task 26231 | created context checkpoint 7 of 8 (pos_min = 1072, pos_max = 1705, size = 14.867 MiB)
slot print_timing: id  1 | task 26231 | 
prompt eval time =     552.86 ms /   293 tokens (    1.89 ms per token,   529.98 tokens per second)
       eval time =    6046.28 ms /   253 tokens (   23.90 ms per token,    41.84 tokens per second)
      total time =    6599.13 ms /   546 tokens
slot      release: id  1 | task 26231 | stop processing: n_tokens = 2022, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.707 (> 0.100 thold), f_keep = 0.587
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26486 | processing task, is_child = 0
slot update_slots: id  1 | task 26486 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1678
slot update_slots: id  1 | task 26486 | n_past = 1186, slot.prompt.tokens.size() = 2022, seq_id = 1, pos_min = 1388, n_swa = 128
slot update_slots: id  1 | task 26486 | restored context checkpoint (pos_min = 804, pos_max = 1412, size = 14.281 MiB)
slot update_slots: id  1 | task 26486 | erased invalidated context checkpoint (pos_min = 1072, pos_max = 1705, n_swa = 128, size = 14.867 MiB)
slot update_slots: id  1 | task 26486 | n_tokens = 1186, memory_seq_rm [1186, end)
slot update_slots: id  1 | task 26486 | prompt processing progress, n_tokens = 1614, batch.n_tokens = 428, progress = 0.961859
slot update_slots: id  1 | task 26486 | n_tokens = 1614, memory_seq_rm [1614, end)
slot update_slots: id  1 | task 26486 | prompt processing progress, n_tokens = 1678, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26486 | prompt done, n_tokens = 1678, batch.n_tokens = 64
slot init_sampler: id  1 | task 26486 | init sampler, took 0.39 ms, tokens: text = 1678, total = 1678
slot update_slots: id  1 | task 26486 | created context checkpoint 7 of 8 (pos_min = 1059, pos_max = 1613, size = 13.014 MiB)
slot print_timing: id  1 | task 26486 | 
prompt eval time =     883.20 ms /   492 tokens (    1.80 ms per token,   557.06 tokens per second)
       eval time =    2068.00 ms /    83 tokens (   24.92 ms per token,    40.14 tokens per second)
      total time =    2951.20 ms /   575 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 26486 | stop processing: n_tokens = 1760, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26571 | processing task, is_child = 0
slot update_slots: id  1 | task 26571 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1740
slot update_slots: id  1 | task 26571 | n_tokens = 1678, memory_seq_rm [1678, end)
slot update_slots: id  1 | task 26571 | prompt processing progress, n_tokens = 1740, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  1 | task 26571 | prompt done, n_tokens = 1740, batch.n_tokens = 62
slot init_sampler: id  1 | task 26571 | init sampler, took 0.31 ms, tokens: text = 1740, total = 1740
slot print_timing: id  1 | task 26571 | 
prompt eval time =     262.66 ms /    62 tokens (    4.24 ms per token,   236.05 tokens per second)
       eval time =    2008.29 ms /    81 tokens (   24.79 ms per token,    40.33 tokens per second)
      total time =    2270.95 ms /   143 tokens
slot      release: id  1 | task 26571 | stop processing: n_tokens = 1820, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.956
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26653 | processing task, is_child = 0
slot update_slots: id  1 | task 26653 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1819
slot update_slots: id  1 | task 26653 | n_tokens = 1740, memory_seq_rm [1740, end)
slot update_slots: id  1 | task 26653 | prompt processing progress, n_tokens = 1755, batch.n_tokens = 15, progress = 0.964816
slot update_slots: id  1 | task 26653 | n_tokens = 1755, memory_seq_rm [1755, end)
slot update_slots: id  1 | task 26653 | prompt processing progress, n_tokens = 1819, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26653 | prompt done, n_tokens = 1819, batch.n_tokens = 64
slot init_sampler: id  1 | task 26653 | init sampler, took 0.34 ms, tokens: text = 1819, total = 1819
slot update_slots: id  1 | task 26653 | created context checkpoint 8 of 8 (pos_min = 1059, pos_max = 1754, size = 16.321 MiB)
slot print_timing: id  1 | task 26653 | 
prompt eval time =     265.31 ms /    79 tokens (    3.36 ms per token,   297.76 tokens per second)
       eval time =    2201.75 ms /    89 tokens (   24.74 ms per token,    40.42 tokens per second)
      total time =    2467.06 ms /   168 tokens
slot      release: id  1 | task 26653 | stop processing: n_tokens = 1907, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.284 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26744 | processing task, is_child = 0
slot update_slots: id  1 | task 26744 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6398
slot update_slots: id  1 | task 26744 | n_tokens = 1819, memory_seq_rm [1819, end)
slot update_slots: id  1 | task 26744 | prompt processing progress, n_tokens = 3867, batch.n_tokens = 2048, progress = 0.604408
slot update_slots: id  1 | task 26744 | n_tokens = 3867, memory_seq_rm [3867, end)
slot update_slots: id  1 | task 26744 | prompt processing progress, n_tokens = 5915, batch.n_tokens = 2048, progress = 0.924508
slot update_slots: id  1 | task 26744 | n_tokens = 5915, memory_seq_rm [5915, end)
slot update_slots: id  1 | task 26744 | prompt processing progress, n_tokens = 6334, batch.n_tokens = 419, progress = 0.989997
slot update_slots: id  1 | task 26744 | n_tokens = 6334, memory_seq_rm [6334, end)
slot update_slots: id  1 | task 26744 | prompt processing progress, n_tokens = 6398, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26744 | prompt done, n_tokens = 6398, batch.n_tokens = 64
slot init_sampler: id  1 | task 26744 | init sampler, took 1.00 ms, tokens: text = 6398, total = 6398
slot update_slots: id  1 | task 26744 | erasing old context checkpoint (pos_min = 0, pos_max = 510, size = 11.983 MiB)
slot update_slots: id  1 | task 26744 | created context checkpoint 8 of 8 (pos_min = 5564, pos_max = 6333, size = 18.056 MiB)
slot print_timing: id  1 | task 26744 | 
prompt eval time =    5211.52 ms /  4579 tokens (    1.14 ms per token,   878.63 tokens per second)
       eval time =    2628.88 ms /   106 tokens (   24.80 ms per token,    40.32 tokens per second)
      total time =    7840.40 ms /  4685 tokens
slot      release: id  1 | task 26744 | stop processing: n_tokens = 6503, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26854 | processing task, is_child = 0
slot update_slots: id  1 | task 26854 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6490
slot update_slots: id  1 | task 26854 | n_tokens = 6398, memory_seq_rm [6398, end)
slot update_slots: id  1 | task 26854 | prompt processing progress, n_tokens = 6426, batch.n_tokens = 28, progress = 0.990139
slot update_slots: id  1 | task 26854 | n_tokens = 6426, memory_seq_rm [6426, end)
slot update_slots: id  1 | task 26854 | prompt processing progress, n_tokens = 6490, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26854 | prompt done, n_tokens = 6490, batch.n_tokens = 64
slot init_sampler: id  1 | task 26854 | init sampler, took 1.31 ms, tokens: text = 6490, total = 6490
slot update_slots: id  1 | task 26854 | erasing old context checkpoint (pos_min = 0, pos_max = 576, size = 13.530 MiB)
slot update_slots: id  1 | task 26854 | created context checkpoint 8 of 8 (pos_min = 5733, pos_max = 6425, size = 16.250 MiB)
slot print_timing: id  1 | task 26854 | 
prompt eval time =     309.11 ms /    92 tokens (    3.36 ms per token,   297.63 tokens per second)
       eval time =    3370.19 ms /   135 tokens (   24.96 ms per token,    40.06 tokens per second)
      total time =    3679.30 ms /   227 tokens
slot      release: id  1 | task 26854 | stop processing: n_tokens = 6624, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.967 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26991 | processing task, is_child = 0
slot update_slots: id  1 | task 26991 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6711
slot update_slots: id  1 | task 26991 | n_tokens = 6490, memory_seq_rm [6490, end)
slot update_slots: id  1 | task 26991 | prompt processing progress, n_tokens = 6647, batch.n_tokens = 157, progress = 0.990463
slot update_slots: id  1 | task 26991 | n_tokens = 6647, memory_seq_rm [6647, end)
slot update_slots: id  1 | task 26991 | prompt processing progress, n_tokens = 6711, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26991 | prompt done, n_tokens = 6711, batch.n_tokens = 64
slot init_sampler: id  1 | task 26991 | init sampler, took 1.27 ms, tokens: text = 6711, total = 6711
slot update_slots: id  1 | task 26991 | erasing old context checkpoint (pos_min = 371, pos_max = 1140, size = 18.056 MiB)
slot update_slots: id  1 | task 26991 | created context checkpoint 8 of 8 (pos_min = 5918, pos_max = 6646, size = 17.095 MiB)
slot print_timing: id  1 | task 26991 | 
prompt eval time =     509.24 ms /   221 tokens (    2.30 ms per token,   433.98 tokens per second)
       eval time =    2951.53 ms /   113 tokens (   26.12 ms per token,    38.29 tokens per second)
      total time =    3460.77 ms /   334 tokens
slot      release: id  1 | task 26991 | stop processing: n_tokens = 6823, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 27106 | processing task, is_child = 0
slot update_slots: id  0 | task 27106 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 117555
srv    send_error: task id = 27106, error: request (117555 tokens) exceeds the available context size (64000 tokens), try increasing it
slot      release: id  0 | task 27106 | stop processing: n_tokens = 0, truncated = 0
srv  update_slots: no tokens to decode
srv  update_slots: all slots are idle
srv          stop: cancel task, id_task = 27106
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 400
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.083
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6823, total state size = 177.087 MiB
srv          load:  - looking for better prompt, base f_keep = 0.083, sim = 0.953
srv        update:  - cache state: 8 prompts, 2617.082 MiB (limits: 8192.000 MiB, 64000 tokens, 241066 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv        update:    - prompt 0x5a234dc80090:    2123 tokens, checkpoints:  1,    68.612 MiB
srv        update:    - prompt 0x5a234ccbfee0:   15206 tokens, checkpoints:  3,   424.099 MiB
srv        update:    - prompt 0x5a234dbdca00:    2924 tokens, checkpoints:  2,   117.715 MiB
srv        update:    - prompt 0x5a234cfa37d0:    1948 tokens, checkpoints:  3,   106.905 MiB
srv        update:    - prompt 0x5a234cce4d20:    6823 tokens, checkpoints:  8,   301.838 MiB
srv  get_availabl: prompt cache update took 281.27 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27109 | processing task, is_child = 0
slot update_slots: id  1 | task 27109 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 594
slot update_slots: id  1 | task 27109 | n_past = 566, slot.prompt.tokens.size() = 6823, seq_id = 1, pos_min = 6094, n_swa = 128
slot update_slots: id  1 | task 27109 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 27109 | erased invalidated context checkpoint (pos_min = 583, pos_max = 1217, n_swa = 128, size = 14.890 MiB)
slot update_slots: id  1 | task 27109 | erased invalidated context checkpoint (pos_min = 707, pos_max = 1339, n_swa = 128, size = 14.843 MiB)
slot update_slots: id  1 | task 27109 | erased invalidated context checkpoint (pos_min = 804, pos_max = 1412, n_swa = 128, size = 14.281 MiB)
slot update_slots: id  1 | task 27109 | erased invalidated context checkpoint (pos_min = 1059, pos_max = 1613, n_swa = 128, size = 13.014 MiB)
slot update_slots: id  1 | task 27109 | erased invalidated context checkpoint (pos_min = 1059, pos_max = 1754, n_swa = 128, size = 16.321 MiB)
slot update_slots: id  1 | task 27109 | erased invalidated context checkpoint (pos_min = 5564, pos_max = 6333, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 27109 | erased invalidated context checkpoint (pos_min = 5733, pos_max = 6425, n_swa = 128, size = 16.250 MiB)
slot update_slots: id  1 | task 27109 | erased invalidated context checkpoint (pos_min = 5918, pos_max = 6646, n_swa = 128, size = 17.095 MiB)
slot update_slots: id  1 | task 27109 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 27109 | prompt processing progress, n_tokens = 530, batch.n_tokens = 530, progress = 0.892256
slot update_slots: id  1 | task 27109 | n_tokens = 530, memory_seq_rm [530, end)
slot update_slots: id  1 | task 27109 | prompt processing progress, n_tokens = 594, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27109 | prompt done, n_tokens = 594, batch.n_tokens = 64
slot init_sampler: id  1 | task 27109 | init sampler, took 0.12 ms, tokens: text = 594, total = 594
slot update_slots: id  1 | task 27109 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 529, size = 12.428 MiB)
slot print_timing: id  1 | task 27109 | 
prompt eval time =     931.31 ms /   594 tokens (    1.57 ms per token,   637.81 tokens per second)
       eval time =    1202.50 ms /    51 tokens (   23.58 ms per token,    42.41 tokens per second)
      total time =    2133.81 ms /   645 tokens
slot      release: id  1 | task 27109 | stop processing: n_tokens = 644, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.464 (> 0.100 thold), f_keep = 0.922
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27162 | processing task, is_child = 0
slot update_slots: id  1 | task 27162 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1281
slot update_slots: id  1 | task 27162 | n_tokens = 594, memory_seq_rm [594, end)
slot update_slots: id  1 | task 27162 | prompt processing progress, n_tokens = 1217, batch.n_tokens = 623, progress = 0.950039
slot update_slots: id  1 | task 27162 | n_tokens = 1217, memory_seq_rm [1217, end)
slot update_slots: id  1 | task 27162 | prompt processing progress, n_tokens = 1281, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27162 | prompt done, n_tokens = 1281, batch.n_tokens = 64
slot init_sampler: id  1 | task 27162 | init sampler, took 0.25 ms, tokens: text = 1281, total = 1281
slot update_slots: id  1 | task 27162 | created context checkpoint 2 of 8 (pos_min = 447, pos_max = 1216, size = 18.056 MiB)
slot print_timing: id  1 | task 27162 | 
prompt eval time =     983.22 ms /   687 tokens (    1.43 ms per token,   698.72 tokens per second)
       eval time =    1520.56 ms /    63 tokens (   24.14 ms per token,    41.43 tokens per second)
      total time =    2503.78 ms /   750 tokens
slot      release: id  1 | task 27162 | stop processing: n_tokens = 1343, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.398 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27227 | processing task, is_child = 0
slot update_slots: id  1 | task 27227 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3217
slot update_slots: id  1 | task 27227 | n_tokens = 1281, memory_seq_rm [1281, end)
slot update_slots: id  1 | task 27227 | prompt processing progress, n_tokens = 3153, batch.n_tokens = 1872, progress = 0.980106
slot update_slots: id  1 | task 27227 | n_tokens = 3153, memory_seq_rm [3153, end)
slot update_slots: id  1 | task 27227 | prompt processing progress, n_tokens = 3217, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27227 | prompt done, n_tokens = 3217, batch.n_tokens = 64
slot init_sampler: id  1 | task 27227 | init sampler, took 0.60 ms, tokens: text = 3217, total = 3217
slot update_slots: id  1 | task 27227 | created context checkpoint 3 of 8 (pos_min = 2383, pos_max = 3152, size = 18.056 MiB)
slot print_timing: id  1 | task 27227 | 
prompt eval time =    2338.07 ms /  1936 tokens (    1.21 ms per token,   828.03 tokens per second)
       eval time =    2347.43 ms /    94 tokens (   24.97 ms per token,    40.04 tokens per second)
      total time =    4685.50 ms /  2030 tokens
slot      release: id  1 | task 27227 | stop processing: n_tokens = 3310, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.785 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27323 | processing task, is_child = 0
slot update_slots: id  1 | task 27323 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4099
slot update_slots: id  1 | task 27323 | n_tokens = 3217, memory_seq_rm [3217, end)
slot update_slots: id  1 | task 27323 | prompt processing progress, n_tokens = 4035, batch.n_tokens = 818, progress = 0.984386
slot update_slots: id  1 | task 27323 | n_tokens = 4035, memory_seq_rm [4035, end)
slot update_slots: id  1 | task 27323 | prompt processing progress, n_tokens = 4099, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27323 | prompt done, n_tokens = 4099, batch.n_tokens = 64
slot init_sampler: id  1 | task 27323 | init sampler, took 0.79 ms, tokens: text = 4099, total = 4099
slot update_slots: id  1 | task 27323 | created context checkpoint 4 of 8 (pos_min = 3265, pos_max = 4034, size = 18.056 MiB)
slot print_timing: id  1 | task 27323 | 
prompt eval time =    1198.56 ms /   882 tokens (    1.36 ms per token,   735.88 tokens per second)
       eval time =    1743.50 ms /    70 tokens (   24.91 ms per token,    40.15 tokens per second)
      total time =    2942.06 ms /   952 tokens
slot      release: id  1 | task 27323 | stop processing: n_tokens = 4168, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.814 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27395 | processing task, is_child = 0
slot update_slots: id  1 | task 27395 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5038
slot update_slots: id  1 | task 27395 | n_tokens = 4099, memory_seq_rm [4099, end)
slot update_slots: id  1 | task 27395 | prompt processing progress, n_tokens = 4974, batch.n_tokens = 875, progress = 0.987297
slot update_slots: id  1 | task 27395 | n_tokens = 4974, memory_seq_rm [4974, end)
slot update_slots: id  1 | task 27395 | prompt processing progress, n_tokens = 5038, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27395 | prompt done, n_tokens = 5038, batch.n_tokens = 64
slot init_sampler: id  1 | task 27395 | init sampler, took 0.94 ms, tokens: text = 5038, total = 5038
slot update_slots: id  1 | task 27395 | created context checkpoint 5 of 8 (pos_min = 4204, pos_max = 4973, size = 18.056 MiB)
slot print_timing: id  1 | task 27395 | 
prompt eval time =    1287.38 ms /   939 tokens (    1.37 ms per token,   729.39 tokens per second)
       eval time =    8592.19 ms /   336 tokens (   25.57 ms per token,    39.11 tokens per second)
      total time =    9879.57 ms /  1275 tokens
slot      release: id  1 | task 27395 | stop processing: n_tokens = 5373, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.835 (> 0.100 thold), f_keep = 0.938
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27733 | processing task, is_child = 0
slot update_slots: id  1 | task 27733 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6035
slot update_slots: id  1 | task 27733 | n_tokens = 5038, memory_seq_rm [5038, end)
slot update_slots: id  1 | task 27733 | prompt processing progress, n_tokens = 5971, batch.n_tokens = 933, progress = 0.989395
slot update_slots: id  1 | task 27733 | n_tokens = 5971, memory_seq_rm [5971, end)
slot update_slots: id  1 | task 27733 | prompt processing progress, n_tokens = 6035, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27733 | prompt done, n_tokens = 6035, batch.n_tokens = 64
slot init_sampler: id  1 | task 27733 | init sampler, took 1.41 ms, tokens: text = 6035, total = 6035
slot update_slots: id  1 | task 27733 | created context checkpoint 6 of 8 (pos_min = 5201, pos_max = 5970, size = 18.056 MiB)
slot print_timing: id  1 | task 27733 | 
prompt eval time =    1365.31 ms /   997 tokens (    1.37 ms per token,   730.24 tokens per second)
       eval time =    8825.69 ms /   347 tokens (   25.43 ms per token,    39.32 tokens per second)
      total time =   10191.00 ms /  1344 tokens
slot      release: id  1 | task 27733 | stop processing: n_tokens = 6381, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.946
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28082 | processing task, is_child = 0
slot update_slots: id  1 | task 28082 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6101
slot update_slots: id  1 | task 28082 | n_tokens = 6035, memory_seq_rm [6035, end)
slot update_slots: id  1 | task 28082 | prompt processing progress, n_tokens = 6037, batch.n_tokens = 2, progress = 0.989510
slot update_slots: id  1 | task 28082 | n_tokens = 6037, memory_seq_rm [6037, end)
slot update_slots: id  1 | task 28082 | prompt processing progress, n_tokens = 6101, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28082 | prompt done, n_tokens = 6101, batch.n_tokens = 64
slot init_sampler: id  1 | task 28082 | init sampler, took 1.16 ms, tokens: text = 6101, total = 6101
slot update_slots: id  1 | task 28082 | created context checkpoint 7 of 8 (pos_min = 5611, pos_max = 6036, size = 9.990 MiB)
slot print_timing: id  1 | task 28082 | 
prompt eval time =     324.78 ms /    66 tokens (    4.92 ms per token,   203.21 tokens per second)
       eval time =    2588.89 ms /   105 tokens (   24.66 ms per token,    40.56 tokens per second)
      total time =    2913.68 ms /   171 tokens
slot      release: id  1 | task 28082 | stop processing: n_tokens = 6205, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.868 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28189 | processing task, is_child = 0
slot update_slots: id  1 | task 28189 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7028
slot update_slots: id  1 | task 28189 | n_tokens = 6101, memory_seq_rm [6101, end)
slot update_slots: id  1 | task 28189 | prompt processing progress, n_tokens = 6964, batch.n_tokens = 863, progress = 0.990894
slot update_slots: id  1 | task 28189 | n_tokens = 6964, memory_seq_rm [6964, end)
slot update_slots: id  1 | task 28189 | prompt processing progress, n_tokens = 7028, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28189 | prompt done, n_tokens = 7028, batch.n_tokens = 64
slot init_sampler: id  1 | task 28189 | init sampler, took 1.73 ms, tokens: text = 7028, total = 7028
slot update_slots: id  1 | task 28189 | created context checkpoint 8 of 8 (pos_min = 6194, pos_max = 6963, size = 18.056 MiB)
slot print_timing: id  1 | task 28189 | 
prompt eval time =    1272.19 ms /   927 tokens (    1.37 ms per token,   728.66 tokens per second)
       eval time =    9689.23 ms /   380 tokens (   25.50 ms per token,    39.22 tokens per second)
      total time =   10961.42 ms /  1307 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 28189 | stop processing: n_tokens = 7407, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.949
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28571 | processing task, is_child = 0
slot update_slots: id  1 | task 28571 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7367
slot update_slots: id  1 | task 28571 | n_tokens = 7028, memory_seq_rm [7028, end)
slot update_slots: id  1 | task 28571 | prompt processing progress, n_tokens = 7303, batch.n_tokens = 275, progress = 0.991313
slot update_slots: id  1 | task 28571 | n_tokens = 7303, memory_seq_rm [7303, end)
slot update_slots: id  1 | task 28571 | prompt processing progress, n_tokens = 7367, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28571 | prompt done, n_tokens = 7367, batch.n_tokens = 64
slot init_sampler: id  1 | task 28571 | init sampler, took 1.25 ms, tokens: text = 7367, total = 7367
slot update_slots: id  1 | task 28571 | erasing old context checkpoint (pos_min = 0, pos_max = 529, size = 12.428 MiB)
slot update_slots: id  1 | task 28571 | created context checkpoint 8 of 8 (pos_min = 6637, pos_max = 7302, size = 15.617 MiB)
slot print_timing: id  1 | task 28571 | 
prompt eval time =     580.96 ms /   339 tokens (    1.71 ms per token,   583.51 tokens per second)
       eval time =    5001.51 ms /   199 tokens (   25.13 ms per token,    39.79 tokens per second)
      total time =    5582.47 ms /   538 tokens
slot      release: id  1 | task 28571 | stop processing: n_tokens = 7565, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.887 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28772 | processing task, is_child = 0
slot update_slots: id  1 | task 28772 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8309
slot update_slots: id  1 | task 28772 | n_tokens = 7367, memory_seq_rm [7367, end)
slot update_slots: id  1 | task 28772 | prompt processing progress, n_tokens = 8245, batch.n_tokens = 878, progress = 0.992298
slot update_slots: id  1 | task 28772 | n_tokens = 8245, memory_seq_rm [8245, end)
slot update_slots: id  1 | task 28772 | prompt processing progress, n_tokens = 8309, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28772 | prompt done, n_tokens = 8309, batch.n_tokens = 64
slot init_sampler: id  1 | task 28772 | init sampler, took 1.59 ms, tokens: text = 8309, total = 8309
slot update_slots: id  1 | task 28772 | erasing old context checkpoint (pos_min = 447, pos_max = 1216, size = 18.056 MiB)
slot update_slots: id  1 | task 28772 | created context checkpoint 8 of 8 (pos_min = 7475, pos_max = 8244, size = 18.056 MiB)
slot print_timing: id  1 | task 28772 | 
prompt eval time =    1329.07 ms /   942 tokens (    1.41 ms per token,   708.77 tokens per second)
       eval time =   29747.57 ms /  1129 tokens (   26.35 ms per token,    37.95 tokens per second)
      total time =   31076.64 ms /  2071 tokens
slot      release: id  1 | task 28772 | stop processing: n_tokens = 9437, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.880
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29903 | processing task, is_child = 0
slot update_slots: id  1 | task 29903 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8600
slot update_slots: id  1 | task 29903 | n_past = 8309, slot.prompt.tokens.size() = 9437, seq_id = 1, pos_min = 8667, n_swa = 128
slot update_slots: id  1 | task 29903 | restored context checkpoint (pos_min = 7475, pos_max = 8244, size = 18.056 MiB)
slot update_slots: id  1 | task 29903 | n_tokens = 8244, memory_seq_rm [8244, end)
slot update_slots: id  1 | task 29903 | prompt processing progress, n_tokens = 8536, batch.n_tokens = 292, progress = 0.992558
slot update_slots: id  1 | task 29903 | n_tokens = 8536, memory_seq_rm [8536, end)
slot update_slots: id  1 | task 29903 | prompt processing progress, n_tokens = 8600, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29903 | prompt done, n_tokens = 8600, batch.n_tokens = 64
slot init_sampler: id  1 | task 29903 | init sampler, took 1.61 ms, tokens: text = 8600, total = 8600
slot update_slots: id  1 | task 29903 | erasing old context checkpoint (pos_min = 2383, pos_max = 3152, size = 18.056 MiB)
slot update_slots: id  1 | task 29903 | created context checkpoint 8 of 8 (pos_min = 7766, pos_max = 8535, size = 18.056 MiB)
slot print_timing: id  1 | task 29903 | 
prompt eval time =     780.53 ms /   356 tokens (    2.19 ms per token,   456.10 tokens per second)
       eval time =    2958.89 ms /   105 tokens (   28.18 ms per token,    35.49 tokens per second)
      total time =    3739.42 ms /   461 tokens
slot      release: id  1 | task 29903 | stop processing: n_tokens = 8704, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30010 | processing task, is_child = 0
slot update_slots: id  1 | task 30010 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8712
slot update_slots: id  1 | task 30010 | n_tokens = 8600, memory_seq_rm [8600, end)
slot update_slots: id  1 | task 30010 | prompt processing progress, n_tokens = 8648, batch.n_tokens = 48, progress = 0.992654
slot update_slots: id  1 | task 30010 | n_tokens = 8648, memory_seq_rm [8648, end)
slot update_slots: id  1 | task 30010 | prompt processing progress, n_tokens = 8712, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30010 | prompt done, n_tokens = 8712, batch.n_tokens = 64
slot init_sampler: id  1 | task 30010 | init sampler, took 1.88 ms, tokens: text = 8712, total = 8712
slot update_slots: id  1 | task 30010 | erasing old context checkpoint (pos_min = 3265, pos_max = 4034, size = 18.056 MiB)
slot update_slots: id  1 | task 30010 | created context checkpoint 8 of 8 (pos_min = 7934, pos_max = 8647, size = 16.743 MiB)
slot print_timing: id  1 | task 30010 | 
prompt eval time =     532.36 ms /   112 tokens (    4.75 ms per token,   210.38 tokens per second)
       eval time =   17111.10 ms /   638 tokens (   26.82 ms per token,    37.29 tokens per second)
      total time =   17643.46 ms /   750 tokens
slot      release: id  1 | task 30010 | stop processing: n_tokens = 9349, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30650 | processing task, is_child = 0
slot update_slots: id  1 | task 30650 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9033
slot update_slots: id  1 | task 30650 | n_tokens = 8712, memory_seq_rm [8712, end)
slot update_slots: id  1 | task 30650 | prompt processing progress, n_tokens = 8969, batch.n_tokens = 257, progress = 0.992915
slot update_slots: id  1 | task 30650 | n_tokens = 8969, memory_seq_rm [8969, end)
slot update_slots: id  1 | task 30650 | prompt processing progress, n_tokens = 9033, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30650 | prompt done, n_tokens = 9033, batch.n_tokens = 64
slot init_sampler: id  1 | task 30650 | init sampler, took 1.87 ms, tokens: text = 9033, total = 9033
slot update_slots: id  1 | task 30650 | erasing old context checkpoint (pos_min = 4204, pos_max = 4973, size = 18.056 MiB)
slot update_slots: id  1 | task 30650 | created context checkpoint 8 of 8 (pos_min = 8585, pos_max = 8968, size = 9.005 MiB)
slot print_timing: id  1 | task 30650 | 
prompt eval time =     598.47 ms /   321 tokens (    1.86 ms per token,   536.36 tokens per second)
       eval time =   72462.25 ms /  2687 tokens (   26.97 ms per token,    37.08 tokens per second)
      total time =   73060.72 ms /  3008 tokens
slot      release: id  1 | task 30650 | stop processing: n_tokens = 11719, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.796 (> 0.100 thold), f_keep = 0.771
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 33339 | processing task, is_child = 0
slot update_slots: id  1 | task 33339 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11349
slot update_slots: id  1 | task 33339 | n_past = 9033, slot.prompt.tokens.size() = 11719, seq_id = 1, pos_min = 10949, n_swa = 128
slot update_slots: id  1 | task 33339 | restored context checkpoint (pos_min = 8585, pos_max = 8968, size = 9.005 MiB)
slot update_slots: id  1 | task 33339 | n_tokens = 8968, memory_seq_rm [8968, end)
slot update_slots: id  1 | task 33339 | prompt processing progress, n_tokens = 11016, batch.n_tokens = 2048, progress = 0.970658
slot update_slots: id  1 | task 33339 | n_tokens = 11016, memory_seq_rm [11016, end)
slot update_slots: id  1 | task 33339 | prompt processing progress, n_tokens = 11285, batch.n_tokens = 269, progress = 0.994361
slot update_slots: id  1 | task 33339 | n_tokens = 11285, memory_seq_rm [11285, end)
slot update_slots: id  1 | task 33339 | prompt processing progress, n_tokens = 11349, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 33339 | prompt done, n_tokens = 11349, batch.n_tokens = 64
slot init_sampler: id  1 | task 33339 | init sampler, took 1.65 ms, tokens: text = 11349, total = 11349
slot update_slots: id  1 | task 33339 | erasing old context checkpoint (pos_min = 5201, pos_max = 5970, size = 18.056 MiB)
slot update_slots: id  1 | task 33339 | created context checkpoint 8 of 8 (pos_min = 10515, pos_max = 11284, size = 18.056 MiB)
slot print_timing: id  1 | task 33339 | 
prompt eval time =    3379.30 ms /  2381 tokens (    1.42 ms per token,   704.58 tokens per second)
       eval time =    9548.94 ms /   357 tokens (   26.75 ms per token,    37.39 tokens per second)
      total time =   12928.24 ms /  2738 tokens
slot      release: id  1 | task 33339 | stop processing: n_tokens = 11705, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 33699 | processing task, is_child = 0
slot update_slots: id  1 | task 33699 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11791
slot update_slots: id  1 | task 33699 | n_tokens = 11349, memory_seq_rm [11349, end)
slot update_slots: id  1 | task 33699 | prompt processing progress, n_tokens = 11727, batch.n_tokens = 378, progress = 0.994572
slot update_slots: id  1 | task 33699 | n_tokens = 11727, memory_seq_rm [11727, end)
slot update_slots: id  1 | task 33699 | prompt processing progress, n_tokens = 11791, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 33699 | prompt done, n_tokens = 11791, batch.n_tokens = 64
slot init_sampler: id  1 | task 33699 | init sampler, took 1.74 ms, tokens: text = 11791, total = 11791
slot update_slots: id  1 | task 33699 | erasing old context checkpoint (pos_min = 5611, pos_max = 6036, size = 9.990 MiB)
slot update_slots: id  1 | task 33699 | created context checkpoint 8 of 8 (pos_min = 11222, pos_max = 11726, size = 11.842 MiB)
slot print_timing: id  1 | task 33699 | 
prompt eval time =     785.55 ms /   442 tokens (    1.78 ms per token,   562.66 tokens per second)
       eval time =    2591.16 ms /    96 tokens (   26.99 ms per token,    37.05 tokens per second)
      total time =    3376.71 ms /   538 tokens
slot      release: id  1 | task 33699 | stop processing: n_tokens = 11886, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 33797 | processing task, is_child = 0
slot update_slots: id  1 | task 33797 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11857
slot update_slots: id  1 | task 33797 | n_tokens = 11791, memory_seq_rm [11791, end)
slot update_slots: id  1 | task 33797 | prompt processing progress, n_tokens = 11793, batch.n_tokens = 2, progress = 0.994602
slot update_slots: id  1 | task 33797 | n_tokens = 11793, memory_seq_rm [11793, end)
slot update_slots: id  1 | task 33797 | prompt processing progress, n_tokens = 11857, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 33797 | prompt done, n_tokens = 11857, batch.n_tokens = 64
slot init_sampler: id  1 | task 33797 | init sampler, took 1.66 ms, tokens: text = 11857, total = 11857
slot update_slots: id  1 | task 33797 | erasing old context checkpoint (pos_min = 6194, pos_max = 6963, size = 18.056 MiB)
slot update_slots: id  1 | task 33797 | created context checkpoint 8 of 8 (pos_min = 11222, pos_max = 11792, size = 13.390 MiB)
slot print_timing: id  1 | task 33797 | 
prompt eval time =     338.72 ms /    66 tokens (    5.13 ms per token,   194.85 tokens per second)
       eval time =    2966.64 ms /   114 tokens (   26.02 ms per token,    38.43 tokens per second)
      total time =    3305.36 ms /   180 tokens
slot      release: id  1 | task 33797 | stop processing: n_tokens = 11970, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.419 (> 0.100 thold), f_keep = 0.022
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 11970, total state size = 298.224 MiB
srv          load:  - looking for better prompt, base f_keep = 0.022, sim = 0.419
srv        update:  - cache state: 9 prompts, 3036.070 MiB (limits: 8192.000 MiB, 64000 tokens, 240096 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv        update:    - prompt 0x5a234dc80090:    2123 tokens, checkpoints:  1,    68.612 MiB
srv        update:    - prompt 0x5a234ccbfee0:   15206 tokens, checkpoints:  3,   424.099 MiB
srv        update:    - prompt 0x5a234dbdca00:    2924 tokens, checkpoints:  2,   117.715 MiB
srv        update:    - prompt 0x5a234cfa37d0:    1948 tokens, checkpoints:  3,   106.905 MiB
srv        update:    - prompt 0x5a234cce4d20:    6823 tokens, checkpoints:  8,   301.838 MiB
srv        update:    - prompt 0x5a237670b520:   11970 tokens, checkpoints:  8,   418.989 MiB
srv  get_availabl: prompt cache update took 336.12 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 33913 | processing task, is_child = 0
slot update_slots: id  1 | task 33913 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 639
slot update_slots: id  1 | task 33913 | n_past = 268, slot.prompt.tokens.size() = 11970, seq_id = 1, pos_min = 11222, n_swa = 128
slot update_slots: id  1 | task 33913 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 33913 | erased invalidated context checkpoint (pos_min = 6637, pos_max = 7302, n_swa = 128, size = 15.617 MiB)
slot update_slots: id  1 | task 33913 | erased invalidated context checkpoint (pos_min = 7475, pos_max = 8244, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 33913 | erased invalidated context checkpoint (pos_min = 7766, pos_max = 8535, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 33913 | erased invalidated context checkpoint (pos_min = 7934, pos_max = 8647, n_swa = 128, size = 16.743 MiB)
slot update_slots: id  1 | task 33913 | erased invalidated context checkpoint (pos_min = 8585, pos_max = 8968, n_swa = 128, size = 9.005 MiB)
slot update_slots: id  1 | task 33913 | erased invalidated context checkpoint (pos_min = 10515, pos_max = 11284, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 33913 | erased invalidated context checkpoint (pos_min = 11222, pos_max = 11726, n_swa = 128, size = 11.842 MiB)
slot update_slots: id  1 | task 33913 | erased invalidated context checkpoint (pos_min = 11222, pos_max = 11792, n_swa = 128, size = 13.390 MiB)
slot update_slots: id  1 | task 33913 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 33913 | prompt processing progress, n_tokens = 575, batch.n_tokens = 575, progress = 0.899844
slot update_slots: id  1 | task 33913 | n_tokens = 575, memory_seq_rm [575, end)
slot update_slots: id  1 | task 33913 | prompt processing progress, n_tokens = 639, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 33913 | prompt done, n_tokens = 639, batch.n_tokens = 64
slot init_sampler: id  1 | task 33913 | init sampler, took 0.12 ms, tokens: text = 639, total = 639
slot update_slots: id  1 | task 33913 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 574, size = 13.483 MiB)
slot print_timing: id  1 | task 33913 | 
prompt eval time =    1078.69 ms /   639 tokens (    1.69 ms per token,   592.38 tokens per second)
       eval time =    8986.34 ms /   350 tokens (   25.68 ms per token,    38.95 tokens per second)
      total time =   10065.03 ms /   989 tokens
slot      release: id  1 | task 33913 | stop processing: n_tokens = 988, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.889 (> 0.100 thold), f_keep = 0.647
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 34265 | processing task, is_child = 0
slot update_slots: id  1 | task 34265 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 719
slot update_slots: id  1 | task 34265 | n_tokens = 639, memory_seq_rm [639, end)
slot update_slots: id  1 | task 34265 | prompt processing progress, n_tokens = 655, batch.n_tokens = 16, progress = 0.910987
slot update_slots: id  1 | task 34265 | n_tokens = 655, memory_seq_rm [655, end)
slot update_slots: id  1 | task 34265 | prompt processing progress, n_tokens = 719, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 34265 | prompt done, n_tokens = 719, batch.n_tokens = 64
slot init_sampler: id  1 | task 34265 | init sampler, took 0.14 ms, tokens: text = 719, total = 719
slot update_slots: id  1 | task 34265 | created context checkpoint 2 of 8 (pos_min = 218, pos_max = 654, size = 10.247 MiB)
slot print_timing: id  1 | task 34265 | 
prompt eval time =     274.80 ms /    80 tokens (    3.43 ms per token,   291.12 tokens per second)
       eval time =    5052.90 ms /   197 tokens (   25.65 ms per token,    38.99 tokens per second)
      total time =    5327.69 ms /   277 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 34265 | stop processing: n_tokens = 915, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.901 (> 0.100 thold), f_keep = 0.786
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 34464 | processing task, is_child = 0
slot update_slots: id  1 | task 34464 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 798
slot update_slots: id  1 | task 34464 | n_tokens = 719, memory_seq_rm [719, end)
slot update_slots: id  1 | task 34464 | prompt processing progress, n_tokens = 734, batch.n_tokens = 15, progress = 0.919800
slot update_slots: id  1 | task 34464 | n_tokens = 734, memory_seq_rm [734, end)
slot update_slots: id  1 | task 34464 | prompt processing progress, n_tokens = 798, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 34464 | prompt done, n_tokens = 798, batch.n_tokens = 64
slot init_sampler: id  1 | task 34464 | init sampler, took 0.14 ms, tokens: text = 798, total = 798
slot update_slots: id  1 | task 34464 | created context checkpoint 3 of 8 (pos_min = 276, pos_max = 733, size = 10.740 MiB)
slot print_timing: id  1 | task 34464 | 
prompt eval time =     281.91 ms /    79 tokens (    3.57 ms per token,   280.23 tokens per second)
       eval time =    4470.50 ms /   171 tokens (   26.14 ms per token,    38.25 tokens per second)
      total time =    4752.41 ms /   250 tokens
slot      release: id  1 | task 34464 | stop processing: n_tokens = 968, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.891 (> 0.100 thold), f_keep = 0.824
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 34637 | processing task, is_child = 0
slot update_slots: id  1 | task 34637 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 896
slot update_slots: id  1 | task 34637 | n_tokens = 798, memory_seq_rm [798, end)
slot update_slots: id  1 | task 34637 | prompt processing progress, n_tokens = 832, batch.n_tokens = 34, progress = 0.928571
slot update_slots: id  1 | task 34637 | n_tokens = 832, memory_seq_rm [832, end)
slot update_slots: id  1 | task 34637 | prompt processing progress, n_tokens = 896, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 34637 | prompt done, n_tokens = 896, batch.n_tokens = 64
slot init_sampler: id  1 | task 34637 | init sampler, took 0.17 ms, tokens: text = 896, total = 896
slot update_slots: id  1 | task 34637 | created context checkpoint 4 of 8 (pos_min = 329, pos_max = 831, size = 11.795 MiB)
slot print_timing: id  1 | task 34637 | 
prompt eval time =     339.99 ms /    98 tokens (    3.47 ms per token,   288.24 tokens per second)
       eval time =    4423.11 ms /   169 tokens (   26.17 ms per token,    38.21 tokens per second)
      total time =    4763.10 ms /   267 tokens
slot      release: id  1 | task 34637 | stop processing: n_tokens = 1064, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.802 (> 0.100 thold), f_keep = 0.842
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 34808 | processing task, is_child = 0
slot update_slots: id  1 | task 34808 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1117
slot update_slots: id  1 | task 34808 | n_tokens = 896, memory_seq_rm [896, end)
slot update_slots: id  1 | task 34808 | prompt processing progress, n_tokens = 1053, batch.n_tokens = 157, progress = 0.942704
slot update_slots: id  1 | task 34808 | n_tokens = 1053, memory_seq_rm [1053, end)
slot update_slots: id  1 | task 34808 | prompt processing progress, n_tokens = 1117, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 34808 | prompt done, n_tokens = 1117, batch.n_tokens = 64
slot init_sampler: id  1 | task 34808 | init sampler, took 0.20 ms, tokens: text = 1117, total = 1117
slot update_slots: id  1 | task 34808 | created context checkpoint 5 of 8 (pos_min = 425, pos_max = 1052, size = 14.726 MiB)
slot print_timing: id  1 | task 34808 | 
prompt eval time =     499.56 ms /   221 tokens (    2.26 ms per token,   442.39 tokens per second)
       eval time =    7515.61 ms /   290 tokens (   25.92 ms per token,    38.59 tokens per second)
      total time =    8015.17 ms /   511 tokens
slot      release: id  1 | task 34808 | stop processing: n_tokens = 1406, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.550 (> 0.100 thold), f_keep = 0.442
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1406, total state size = 50.955 MiB
srv          load:  - looking for better prompt, base f_keep = 0.442, sim = 0.550
srv        update:  - cache state: 10 prompts, 3148.018 MiB (limits: 8192.000 MiB, 64000 tokens, 235216 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv        update:    - prompt 0x5a234dc80090:    2123 tokens, checkpoints:  1,    68.612 MiB
srv        update:    - prompt 0x5a234ccbfee0:   15206 tokens, checkpoints:  3,   424.099 MiB
srv        update:    - prompt 0x5a234dbdca00:    2924 tokens, checkpoints:  2,   117.715 MiB
srv        update:    - prompt 0x5a234cfa37d0:    1948 tokens, checkpoints:  3,   106.905 MiB
srv        update:    - prompt 0x5a234cce4d20:    6823 tokens, checkpoints:  8,   301.838 MiB
srv        update:    - prompt 0x5a237670b520:   11970 tokens, checkpoints:  8,   418.989 MiB
srv        update:    - prompt 0x5a234d356c00:    1406 tokens, checkpoints:  5,   111.947 MiB
srv  get_availabl: prompt cache update took 81.62 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35100 | processing task, is_child = 0
slot update_slots: id  1 | task 35100 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1130
slot update_slots: id  1 | task 35100 | n_past = 621, slot.prompt.tokens.size() = 1406, seq_id = 1, pos_min = 639, n_swa = 128
slot update_slots: id  1 | task 35100 | restored context checkpoint (pos_min = 425, pos_max = 1052, size = 14.726 MiB)
slot update_slots: id  1 | task 35100 | n_tokens = 621, memory_seq_rm [621, end)
slot update_slots: id  1 | task 35100 | prompt processing progress, n_tokens = 1066, batch.n_tokens = 445, progress = 0.943363
slot update_slots: id  1 | task 35100 | n_tokens = 1066, memory_seq_rm [1066, end)
slot update_slots: id  1 | task 35100 | prompt processing progress, n_tokens = 1130, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35100 | prompt done, n_tokens = 1130, batch.n_tokens = 64
slot init_sampler: id  1 | task 35100 | init sampler, took 0.20 ms, tokens: text = 1130, total = 1130
slot print_timing: id  1 | task 35100 | 
prompt eval time =     939.33 ms /   509 tokens (    1.85 ms per token,   541.88 tokens per second)
       eval time =    2143.66 ms /    83 tokens (   25.83 ms per token,    38.72 tokens per second)
      total time =    3082.99 ms /   592 tokens
slot      release: id  1 | task 35100 | stop processing: n_tokens = 1212, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35185 | processing task, is_child = 0
slot update_slots: id  1 | task 35185 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1211
slot update_slots: id  1 | task 35185 | n_tokens = 1130, memory_seq_rm [1130, end)
slot update_slots: id  1 | task 35185 | prompt processing progress, n_tokens = 1147, batch.n_tokens = 17, progress = 0.947151
slot update_slots: id  1 | task 35185 | n_tokens = 1147, memory_seq_rm [1147, end)
slot update_slots: id  1 | task 35185 | prompt processing progress, n_tokens = 1211, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35185 | prompt done, n_tokens = 1211, batch.n_tokens = 64
slot init_sampler: id  1 | task 35185 | init sampler, took 0.24 ms, tokens: text = 1211, total = 1211
slot update_slots: id  1 | task 35185 | created context checkpoint 6 of 8 (pos_min = 602, pos_max = 1146, size = 12.780 MiB)
slot print_timing: id  1 | task 35185 | 
prompt eval time =     289.87 ms /    81 tokens (    3.58 ms per token,   279.44 tokens per second)
       eval time =    1386.88 ms /    56 tokens (   24.77 ms per token,    40.38 tokens per second)
      total time =    1676.75 ms /   137 tokens
slot      release: id  1 | task 35185 | stop processing: n_tokens = 1266, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = 1663905208
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1921, total state size = 48.024 MiB
srv          load:  - looking for better prompt, base f_keep = 0.140, sim = 0.021
srv          load:  - found better prompt with f_keep = 0.442, sim = 0.049
srv        update:  - cache state: 10 prompts, 3139.411 MiB (limits: 8192.000 MiB, 64000 tokens, 237205 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv        update:    - prompt 0x5a234dc80090:    2123 tokens, checkpoints:  1,    68.612 MiB
srv        update:    - prompt 0x5a234ccbfee0:   15206 tokens, checkpoints:  3,   424.099 MiB
srv        update:    - prompt 0x5a234dbdca00:    2924 tokens, checkpoints:  2,   117.715 MiB
srv        update:    - prompt 0x5a234cfa37d0:    1948 tokens, checkpoints:  3,   106.905 MiB
srv        update:    - prompt 0x5a234cce4d20:    6823 tokens, checkpoints:  8,   301.838 MiB
srv        update:    - prompt 0x5a237670b520:   11970 tokens, checkpoints:  8,   418.989 MiB
srv        update:    - prompt 0x5a234f21f8f0:    1921 tokens, checkpoints:  3,   103.341 MiB
srv  get_availabl: prompt cache update took 456.77 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 35243 | processing task, is_child = 0
slot update_slots: id  3 | task 35243 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12546
slot update_slots: id  3 | task 35243 | n_past = 621, slot.prompt.tokens.size() = 1406, seq_id = 3, pos_min = 639, n_swa = 128
slot update_slots: id  3 | task 35243 | restored context checkpoint (pos_min = 425, pos_max = 1052, size = 14.726 MiB)
slot update_slots: id  3 | task 35243 | n_tokens = 621, memory_seq_rm [621, end)
slot update_slots: id  3 | task 35243 | prompt processing progress, n_tokens = 2669, batch.n_tokens = 2048, progress = 0.212737
slot update_slots: id  3 | task 35243 | n_tokens = 2669, memory_seq_rm [2669, end)
slot update_slots: id  3 | task 35243 | prompt processing progress, n_tokens = 4717, batch.n_tokens = 2048, progress = 0.375976
slot update_slots: id  3 | task 35243 | n_tokens = 4717, memory_seq_rm [4717, end)
slot update_slots: id  3 | task 35243 | prompt processing progress, n_tokens = 6765, batch.n_tokens = 2048, progress = 0.539216
slot update_slots: id  3 | task 35243 | n_tokens = 6765, memory_seq_rm [6765, end)
slot update_slots: id  3 | task 35243 | prompt processing progress, n_tokens = 8813, batch.n_tokens = 2048, progress = 0.702455
slot update_slots: id  3 | task 35243 | n_tokens = 8813, memory_seq_rm [8813, end)
slot update_slots: id  3 | task 35243 | prompt processing progress, n_tokens = 10861, batch.n_tokens = 2048, progress = 0.865694
slot update_slots: id  3 | task 35243 | n_tokens = 10861, memory_seq_rm [10861, end)
slot update_slots: id  3 | task 35243 | prompt processing progress, n_tokens = 12482, batch.n_tokens = 1621, progress = 0.994899
slot update_slots: id  3 | task 35243 | n_tokens = 12482, memory_seq_rm [12482, end)
slot update_slots: id  3 | task 35243 | prompt processing progress, n_tokens = 12546, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 35243 | prompt done, n_tokens = 12546, batch.n_tokens = 64
slot init_sampler: id  3 | task 35243 | init sampler, took 1.84 ms, tokens: text = 12546, total = 12546
slot update_slots: id  3 | task 35243 | created context checkpoint 6 of 8 (pos_min = 11712, pos_max = 12481, size = 18.056 MiB)
slot print_timing: id  3 | task 35243 | 
prompt eval time =   14804.22 ms / 11925 tokens (    1.24 ms per token,   805.51 tokens per second)
       eval time =    3124.12 ms /   120 tokens (   26.03 ms per token,    38.41 tokens per second)
      total time =   17928.35 ms / 12045 tokens
slot      release: id  3 | task 35243 | stop processing: n_tokens = 12665, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.823 (> 0.100 thold), f_keep = 0.876
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35370 | processing task, is_child = 0
slot update_slots: id  1 | task 35370 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1347
slot update_slots: id  1 | task 35370 | n_past = 1109, slot.prompt.tokens.size() = 1266, seq_id = 1, pos_min = 1139, n_swa = 128
slot update_slots: id  1 | task 35370 | restored context checkpoint (pos_min = 602, pos_max = 1146, size = 12.780 MiB)
slot update_slots: id  1 | task 35370 | n_tokens = 1109, memory_seq_rm [1109, end)
slot update_slots: id  1 | task 35370 | prompt processing progress, n_tokens = 1283, batch.n_tokens = 174, progress = 0.952487
slot update_slots: id  1 | task 35370 | n_tokens = 1283, memory_seq_rm [1283, end)
slot update_slots: id  1 | task 35370 | prompt processing progress, n_tokens = 1347, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35370 | prompt done, n_tokens = 1347, batch.n_tokens = 64
slot init_sampler: id  1 | task 35370 | init sampler, took 0.26 ms, tokens: text = 1347, total = 1347
slot update_slots: id  1 | task 35370 | created context checkpoint 7 of 8 (pos_min = 621, pos_max = 1282, size = 15.523 MiB)
slot print_timing: id  1 | task 35370 | 
prompt eval time =     713.38 ms /   238 tokens (    3.00 ms per token,   333.62 tokens per second)
       eval time =    3072.67 ms /   119 tokens (   25.82 ms per token,    38.73 tokens per second)
      total time =    3786.04 ms /   357 tokens
slot      release: id  1 | task 35370 | stop processing: n_tokens = 1465, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.395 (> 0.100 thold), f_keep = 0.919
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35491 | processing task, is_child = 0
slot update_slots: id  1 | task 35491 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3410
slot update_slots: id  1 | task 35491 | n_tokens = 1347, memory_seq_rm [1347, end)
slot update_slots: id  1 | task 35491 | prompt processing progress, n_tokens = 3346, batch.n_tokens = 1999, progress = 0.981232
slot update_slots: id  1 | task 35491 | n_tokens = 3346, memory_seq_rm [3346, end)
slot update_slots: id  1 | task 35491 | prompt processing progress, n_tokens = 3410, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35491 | prompt done, n_tokens = 3410, batch.n_tokens = 64
slot init_sampler: id  1 | task 35491 | init sampler, took 0.62 ms, tokens: text = 3410, total = 3410
slot update_slots: id  1 | task 35491 | created context checkpoint 8 of 8 (pos_min = 2576, pos_max = 3345, size = 18.056 MiB)
slot print_timing: id  1 | task 35491 | 
prompt eval time =    2734.03 ms /  2063 tokens (    1.33 ms per token,   754.56 tokens per second)
       eval time =    1888.96 ms /    70 tokens (   26.99 ms per token,    37.06 tokens per second)
      total time =    4622.99 ms /  2133 tokens
slot      release: id  1 | task 35491 | stop processing: n_tokens = 3479, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.942 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35563 | processing task, is_child = 0
slot update_slots: id  1 | task 35563 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3621
slot update_slots: id  1 | task 35563 | n_tokens = 3410, memory_seq_rm [3410, end)
slot update_slots: id  1 | task 35563 | prompt processing progress, n_tokens = 3557, batch.n_tokens = 147, progress = 0.982325
slot update_slots: id  1 | task 35563 | n_tokens = 3557, memory_seq_rm [3557, end)
slot update_slots: id  1 | task 35563 | prompt processing progress, n_tokens = 3621, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35563 | prompt done, n_tokens = 3621, batch.n_tokens = 64
slot init_sampler: id  1 | task 35563 | init sampler, took 0.83 ms, tokens: text = 3621, total = 3621
slot update_slots: id  1 | task 35563 | erasing old context checkpoint (pos_min = 0, pos_max = 574, size = 13.483 MiB)
slot update_slots: id  1 | task 35563 | created context checkpoint 8 of 8 (pos_min = 2787, pos_max = 3556, size = 18.056 MiB)
slot print_timing: id  1 | task 35563 | 
prompt eval time =     515.30 ms /   211 tokens (    2.44 ms per token,   409.47 tokens per second)
       eval time =    1241.10 ms /    45 tokens (   27.58 ms per token,    36.26 tokens per second)
      total time =    1756.41 ms /   256 tokens
slot      release: id  1 | task 35563 | stop processing: n_tokens = 3665, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35610 | processing task, is_child = 0
slot update_slots: id  1 | task 35610 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3824
slot update_slots: id  1 | task 35610 | n_tokens = 3621, memory_seq_rm [3621, end)
slot update_slots: id  1 | task 35610 | prompt processing progress, n_tokens = 3760, batch.n_tokens = 139, progress = 0.983264
slot update_slots: id  1 | task 35610 | n_tokens = 3760, memory_seq_rm [3760, end)
slot update_slots: id  1 | task 35610 | prompt processing progress, n_tokens = 3824, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35610 | prompt done, n_tokens = 3824, batch.n_tokens = 64
slot init_sampler: id  1 | task 35610 | init sampler, took 0.61 ms, tokens: text = 3824, total = 3824
slot update_slots: id  1 | task 35610 | erasing old context checkpoint (pos_min = 218, pos_max = 654, size = 10.247 MiB)
slot update_slots: id  1 | task 35610 | created context checkpoint 8 of 8 (pos_min = 2990, pos_max = 3759, size = 18.056 MiB)
slot print_timing: id  1 | task 35610 | 
prompt eval time =     507.14 ms /   203 tokens (    2.50 ms per token,   400.29 tokens per second)
       eval time =    8998.63 ms /   335 tokens (   26.86 ms per token,    37.23 tokens per second)
      total time =    9505.77 ms /   538 tokens
slot      release: id  1 | task 35610 | stop processing: n_tokens = 4158, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.883 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35947 | processing task, is_child = 0
slot update_slots: id  1 | task 35947 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4331
slot update_slots: id  1 | task 35947 | n_tokens = 3824, memory_seq_rm [3824, end)
slot update_slots: id  1 | task 35947 | prompt processing progress, n_tokens = 4267, batch.n_tokens = 443, progress = 0.985223
slot update_slots: id  1 | task 35947 | n_tokens = 4267, memory_seq_rm [4267, end)
slot update_slots: id  1 | task 35947 | prompt processing progress, n_tokens = 4331, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35947 | prompt done, n_tokens = 4331, batch.n_tokens = 64
slot init_sampler: id  1 | task 35947 | init sampler, took 0.67 ms, tokens: text = 4331, total = 4331
slot update_slots: id  1 | task 35947 | erasing old context checkpoint (pos_min = 276, pos_max = 733, size = 10.740 MiB)
slot update_slots: id  1 | task 35947 | created context checkpoint 8 of 8 (pos_min = 3650, pos_max = 4266, size = 14.468 MiB)
slot print_timing: id  1 | task 35947 | 
prompt eval time =     936.67 ms /   507 tokens (    1.85 ms per token,   541.28 tokens per second)
       eval time =    6933.85 ms /   255 tokens (   27.19 ms per token,    36.78 tokens per second)
      total time =    7870.52 ms /   762 tokens
slot      release: id  1 | task 35947 | stop processing: n_tokens = 4585, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.536 (> 0.100 thold), f_keep = 0.945
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 36204 | processing task, is_child = 0
slot update_slots: id  1 | task 36204 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8080
slot update_slots: id  1 | task 36204 | n_tokens = 4331, memory_seq_rm [4331, end)
slot update_slots: id  1 | task 36204 | prompt processing progress, n_tokens = 6379, batch.n_tokens = 2048, progress = 0.789480
slot update_slots: id  1 | task 36204 | n_tokens = 6379, memory_seq_rm [6379, end)
slot update_slots: id  1 | task 36204 | prompt processing progress, n_tokens = 8016, batch.n_tokens = 1637, progress = 0.992079
slot update_slots: id  1 | task 36204 | n_tokens = 8016, memory_seq_rm [8016, end)
slot update_slots: id  1 | task 36204 | prompt processing progress, n_tokens = 8080, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 36204 | prompt done, n_tokens = 8080, batch.n_tokens = 64
slot init_sampler: id  1 | task 36204 | init sampler, took 1.30 ms, tokens: text = 8080, total = 8080
slot update_slots: id  1 | task 36204 | erasing old context checkpoint (pos_min = 329, pos_max = 831, size = 11.795 MiB)
slot update_slots: id  1 | task 36204 | created context checkpoint 8 of 8 (pos_min = 7246, pos_max = 8015, size = 18.056 MiB)
slot print_timing: id  1 | task 36204 | 
prompt eval time =    5581.26 ms /  3749 tokens (    1.49 ms per token,   671.71 tokens per second)
       eval time =   29249.21 ms /  1052 tokens (   27.80 ms per token,    35.97 tokens per second)
      total time =   34830.48 ms /  4801 tokens
slot      release: id  1 | task 36204 | stop processing: n_tokens = 9131, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.440 (> 0.100 thold), f_keep = 0.143
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9131, total state size = 232.169 MiB
srv          load:  - looking for better prompt, base f_keep = 0.143, sim = 0.440
srv        update:  - cache state: 11 prompts, 3501.302 MiB (limits: 8192.000 MiB, 64000 tokens, 234052 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv        update:    - prompt 0x5a234dc80090:    2123 tokens, checkpoints:  1,    68.612 MiB
srv        update:    - prompt 0x5a234ccbfee0:   15206 tokens, checkpoints:  3,   424.099 MiB
srv        update:    - prompt 0x5a234dbdca00:    2924 tokens, checkpoints:  2,   117.715 MiB
srv        update:    - prompt 0x5a234cfa37d0:    1948 tokens, checkpoints:  3,   106.905 MiB
srv        update:    - prompt 0x5a234cce4d20:    6823 tokens, checkpoints:  8,   301.838 MiB
srv        update:    - prompt 0x5a237670b520:   11970 tokens, checkpoints:  8,   418.989 MiB
srv        update:    - prompt 0x5a234f21f8f0:    1921 tokens, checkpoints:  3,   103.341 MiB
srv        update:    - prompt 0x5a237670b330:    9131 tokens, checkpoints:  8,   361.890 MiB
srv  get_availabl: prompt cache update took 282.90 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 37259 | processing task, is_child = 0
slot update_slots: id  1 | task 37259 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2974
slot update_slots: id  1 | task 37259 | n_past = 1308, slot.prompt.tokens.size() = 9131, seq_id = 1, pos_min = 8361, n_swa = 128
slot update_slots: id  1 | task 37259 | restored context checkpoint (pos_min = 621, pos_max = 1282, size = 15.523 MiB)
slot update_slots: id  1 | task 37259 | erased invalidated context checkpoint (pos_min = 2576, pos_max = 3345, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 37259 | erased invalidated context checkpoint (pos_min = 2787, pos_max = 3556, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 37259 | erased invalidated context checkpoint (pos_min = 2990, pos_max = 3759, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 37259 | erased invalidated context checkpoint (pos_min = 3650, pos_max = 4266, n_swa = 128, size = 14.468 MiB)
slot update_slots: id  1 | task 37259 | erased invalidated context checkpoint (pos_min = 7246, pos_max = 8015, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 37259 | n_tokens = 1282, memory_seq_rm [1282, end)
slot update_slots: id  1 | task 37259 | prompt processing progress, n_tokens = 2910, batch.n_tokens = 1628, progress = 0.978480
slot update_slots: id  1 | task 37259 | n_tokens = 2910, memory_seq_rm [2910, end)
slot update_slots: id  1 | task 37259 | prompt processing progress, n_tokens = 2974, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 37259 | prompt done, n_tokens = 2974, batch.n_tokens = 64
slot init_sampler: id  1 | task 37259 | init sampler, took 0.51 ms, tokens: text = 2974, total = 2974
slot update_slots: id  1 | task 37259 | created context checkpoint 4 of 8 (pos_min = 2140, pos_max = 2909, size = 18.056 MiB)
slot print_timing: id  1 | task 37259 | 
prompt eval time =    2670.42 ms /  1692 tokens (    1.58 ms per token,   633.61 tokens per second)
       eval time =    4318.15 ms /   163 tokens (   26.49 ms per token,    37.75 tokens per second)
      total time =    6988.57 ms /  1855 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 37259 | stop processing: n_tokens = 3136, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.948
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 37424 | processing task, is_child = 0
slot update_slots: id  1 | task 37424 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3055
slot update_slots: id  1 | task 37424 | n_tokens = 2974, memory_seq_rm [2974, end)
slot update_slots: id  1 | task 37424 | prompt processing progress, n_tokens = 2991, batch.n_tokens = 17, progress = 0.979051
slot update_slots: id  1 | task 37424 | n_tokens = 2991, memory_seq_rm [2991, end)
slot update_slots: id  1 | task 37424 | prompt processing progress, n_tokens = 3055, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 37424 | prompt done, n_tokens = 3055, batch.n_tokens = 64
slot init_sampler: id  1 | task 37424 | init sampler, took 0.59 ms, tokens: text = 3055, total = 3055
slot update_slots: id  1 | task 37424 | created context checkpoint 5 of 8 (pos_min = 2366, pos_max = 2990, size = 14.656 MiB)
slot print_timing: id  1 | task 37424 | 
prompt eval time =     313.16 ms /    81 tokens (    3.87 ms per token,   258.65 tokens per second)
       eval time =    1541.60 ms /    58 tokens (   26.58 ms per token,    37.62 tokens per second)
      total time =    1854.76 ms /   139 tokens
slot      release: id  1 | task 37424 | stop processing: n_tokens = 3112, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.212 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 37484 | processing task, is_child = 0
slot update_slots: id  1 | task 37484 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14396
slot update_slots: id  1 | task 37484 | n_tokens = 3055, memory_seq_rm [3055, end)
slot update_slots: id  1 | task 37484 | prompt processing progress, n_tokens = 5103, batch.n_tokens = 2048, progress = 0.354473
slot update_slots: id  1 | task 37484 | n_tokens = 5103, memory_seq_rm [5103, end)
slot update_slots: id  1 | task 37484 | prompt processing progress, n_tokens = 7151, batch.n_tokens = 2048, progress = 0.496735
slot update_slots: id  1 | task 37484 | n_tokens = 7151, memory_seq_rm [7151, end)
slot update_slots: id  1 | task 37484 | prompt processing progress, n_tokens = 9199, batch.n_tokens = 2048, progress = 0.638997
slot update_slots: id  1 | task 37484 | n_tokens = 9199, memory_seq_rm [9199, end)
slot update_slots: id  1 | task 37484 | prompt processing progress, n_tokens = 11247, batch.n_tokens = 2048, progress = 0.781259
slot update_slots: id  1 | task 37484 | n_tokens = 11247, memory_seq_rm [11247, end)
slot update_slots: id  1 | task 37484 | prompt processing progress, n_tokens = 13295, batch.n_tokens = 2048, progress = 0.923520
slot update_slots: id  1 | task 37484 | n_tokens = 13295, memory_seq_rm [13295, end)
slot update_slots: id  1 | task 37484 | prompt processing progress, n_tokens = 14332, batch.n_tokens = 1037, progress = 0.995554
slot update_slots: id  1 | task 37484 | n_tokens = 14332, memory_seq_rm [14332, end)
slot update_slots: id  1 | task 37484 | prompt processing progress, n_tokens = 14396, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 37484 | prompt done, n_tokens = 14396, batch.n_tokens = 64
slot init_sampler: id  1 | task 37484 | init sampler, took 2.36 ms, tokens: text = 14396, total = 14396
slot update_slots: id  1 | task 37484 | created context checkpoint 6 of 8 (pos_min = 13562, pos_max = 14331, size = 18.056 MiB)
slot print_timing: id  1 | task 37484 | 
prompt eval time =   16490.37 ms / 11341 tokens (    1.45 ms per token,   687.73 tokens per second)
       eval time =    1889.40 ms /    67 tokens (   28.20 ms per token,    35.46 tokens per second)
      total time =   18379.77 ms / 11408 tokens
slot      release: id  1 | task 37484 | stop processing: n_tokens = 14462, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.886 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 37558 | processing task, is_child = 0
slot update_slots: id  1 | task 37558 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16241
slot update_slots: id  1 | task 37558 | n_tokens = 14396, memory_seq_rm [14396, end)
slot update_slots: id  1 | task 37558 | prompt processing progress, n_tokens = 16177, batch.n_tokens = 1781, progress = 0.996059
slot update_slots: id  1 | task 37558 | n_tokens = 16177, memory_seq_rm [16177, end)
slot update_slots: id  1 | task 37558 | prompt processing progress, n_tokens = 16241, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 37558 | prompt done, n_tokens = 16241, batch.n_tokens = 64
slot init_sampler: id  1 | task 37558 | init sampler, took 2.35 ms, tokens: text = 16241, total = 16241
slot update_slots: id  1 | task 37558 | created context checkpoint 7 of 8 (pos_min = 15407, pos_max = 16176, size = 18.056 MiB)
slot print_timing: id  1 | task 37558 | 
prompt eval time =    3122.32 ms /  1845 tokens (    1.69 ms per token,   590.91 tokens per second)
       eval time =    9406.12 ms /   323 tokens (   29.12 ms per token,    34.34 tokens per second)
      total time =   12528.44 ms /  2168 tokens
slot      release: id  1 | task 37558 | stop processing: n_tokens = 16563, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.894 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 37883 | processing task, is_child = 0
slot update_slots: id  1 | task 37883 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18173
slot update_slots: id  1 | task 37883 | n_tokens = 16241, memory_seq_rm [16241, end)
slot update_slots: id  1 | task 37883 | prompt processing progress, n_tokens = 18109, batch.n_tokens = 1868, progress = 0.996478
slot update_slots: id  1 | task 37883 | n_tokens = 18109, memory_seq_rm [18109, end)
slot update_slots: id  1 | task 37883 | prompt processing progress, n_tokens = 18173, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 37883 | prompt done, n_tokens = 18173, batch.n_tokens = 64
slot init_sampler: id  1 | task 37883 | init sampler, took 3.89 ms, tokens: text = 18173, total = 18173
slot update_slots: id  1 | task 37883 | created context checkpoint 8 of 8 (pos_min = 17339, pos_max = 18108, size = 18.056 MiB)
slot print_timing: id  1 | task 37883 | 
prompt eval time =    3322.57 ms /  1932 tokens (    1.72 ms per token,   581.48 tokens per second)
       eval time =   21214.81 ms /   735 tokens (   28.86 ms per token,    34.65 tokens per second)
      total time =   24537.38 ms /  2667 tokens
slot      release: id  1 | task 37883 | stop processing: n_tokens = 18907, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.730 (> 0.100 thold), f_keep = 0.156
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 18907, total state size = 461.405 MiB
srv          load:  - looking for better prompt, base f_keep = 0.156, sim = 0.730
srv        update:  - cache state: 12 prompts, 4092.617 MiB (limits: 8192.000 MiB, 64000 tokens, 238080 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv        update:    - prompt 0x5a234dc80090:    2123 tokens, checkpoints:  1,    68.612 MiB
srv        update:    - prompt 0x5a234ccbfee0:   15206 tokens, checkpoints:  3,   424.099 MiB
srv        update:    - prompt 0x5a234dbdca00:    2924 tokens, checkpoints:  2,   117.715 MiB
srv        update:    - prompt 0x5a234cfa37d0:    1948 tokens, checkpoints:  3,   106.905 MiB
srv        update:    - prompt 0x5a234cce4d20:    6823 tokens, checkpoints:  8,   301.838 MiB
srv        update:    - prompt 0x5a237670b520:   11970 tokens, checkpoints:  8,   418.989 MiB
srv        update:    - prompt 0x5a234f21f8f0:    1921 tokens, checkpoints:  3,   103.341 MiB
srv        update:    - prompt 0x5a237670b330:    9131 tokens, checkpoints:  8,   361.890 MiB
srv        update:    - prompt 0x5a23757875b0:   18907 tokens, checkpoints:  8,   591.315 MiB
srv  get_availabl: prompt cache update took 509.03 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 38620 | processing task, is_child = 0
slot update_slots: id  1 | task 38620 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4033
slot update_slots: id  1 | task 38620 | n_past = 2943, slot.prompt.tokens.size() = 18907, seq_id = 1, pos_min = 18137, n_swa = 128
slot update_slots: id  1 | task 38620 | restored context checkpoint (pos_min = 2366, pos_max = 2990, size = 14.656 MiB)
slot update_slots: id  1 | task 38620 | erased invalidated context checkpoint (pos_min = 13562, pos_max = 14331, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 38620 | erased invalidated context checkpoint (pos_min = 15407, pos_max = 16176, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 38620 | erased invalidated context checkpoint (pos_min = 17339, pos_max = 18108, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 38620 | n_tokens = 2943, memory_seq_rm [2943, end)
slot update_slots: id  1 | task 38620 | prompt processing progress, n_tokens = 3969, batch.n_tokens = 1026, progress = 0.984131
slot update_slots: id  1 | task 38620 | n_tokens = 3969, memory_seq_rm [3969, end)
slot update_slots: id  1 | task 38620 | prompt processing progress, n_tokens = 4033, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 38620 | prompt done, n_tokens = 4033, batch.n_tokens = 64
slot init_sampler: id  1 | task 38620 | init sampler, took 0.65 ms, tokens: text = 4033, total = 4033
slot update_slots: id  1 | task 38620 | created context checkpoint 6 of 8 (pos_min = 3199, pos_max = 3968, size = 18.056 MiB)
slot print_timing: id  1 | task 38620 | 
prompt eval time =    1834.62 ms /  1090 tokens (    1.68 ms per token,   594.13 tokens per second)
       eval time =   12571.49 ms /   469 tokens (   26.80 ms per token,    37.31 tokens per second)
      total time =   14406.11 ms /  1559 tokens
slot      release: id  1 | task 38620 | stop processing: n_tokens = 4501, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.921 (> 0.100 thold), f_keep = 0.896
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 39091 | processing task, is_child = 0
slot update_slots: id  1 | task 39091 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4380
slot update_slots: id  1 | task 39091 | n_tokens = 4033, memory_seq_rm [4033, end)
slot update_slots: id  1 | task 39091 | prompt processing progress, n_tokens = 4316, batch.n_tokens = 283, progress = 0.985388
slot update_slots: id  1 | task 39091 | n_tokens = 4316, memory_seq_rm [4316, end)
slot update_slots: id  1 | task 39091 | prompt processing progress, n_tokens = 4380, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 39091 | prompt done, n_tokens = 4380, batch.n_tokens = 64
slot init_sampler: id  1 | task 39091 | init sampler, took 0.95 ms, tokens: text = 4380, total = 4380
slot update_slots: id  1 | task 39091 | created context checkpoint 7 of 8 (pos_min = 3731, pos_max = 4315, size = 13.718 MiB)
slot print_timing: id  1 | task 39091 | 
prompt eval time =     755.52 ms /   347 tokens (    2.18 ms per token,   459.29 tokens per second)
       eval time =   12321.45 ms /   454 tokens (   27.14 ms per token,    36.85 tokens per second)
      total time =   13076.97 ms /   801 tokens
slot      release: id  1 | task 39091 | stop processing: n_tokens = 4833, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.867 (> 0.100 thold), f_keep = 0.829
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 39547 | processing task, is_child = 0
slot update_slots: id  1 | task 39547 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4624
slot update_slots: id  1 | task 39547 | n_past = 4008, slot.prompt.tokens.size() = 4833, seq_id = 1, pos_min = 4063, n_swa = 128
slot update_slots: id  1 | task 39547 | restored context checkpoint (pos_min = 3731, pos_max = 4315, size = 13.718 MiB)
slot update_slots: id  1 | task 39547 | n_tokens = 4008, memory_seq_rm [4008, end)
slot update_slots: id  1 | task 39547 | prompt processing progress, n_tokens = 4560, batch.n_tokens = 552, progress = 0.986159
slot update_slots: id  1 | task 39547 | n_tokens = 4560, memory_seq_rm [4560, end)
slot update_slots: id  1 | task 39547 | prompt processing progress, n_tokens = 4624, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 39547 | prompt done, n_tokens = 4624, batch.n_tokens = 64
slot init_sampler: id  1 | task 39547 | init sampler, took 0.71 ms, tokens: text = 4624, total = 4624
slot update_slots: id  1 | task 39547 | created context checkpoint 8 of 8 (pos_min = 3881, pos_max = 4559, size = 15.922 MiB)
slot print_timing: id  1 | task 39547 | 
prompt eval time =    1288.40 ms /   616 tokens (    2.09 ms per token,   478.11 tokens per second)
       eval time =    7780.96 ms /   283 tokens (   27.49 ms per token,    36.37 tokens per second)
      total time =    9069.36 ms /   899 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 39547 | stop processing: n_tokens = 4906, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.945 (> 0.100 thold), f_keep = 0.943
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 39832 | processing task, is_child = 0
slot update_slots: id  1 | task 39832 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4895
slot update_slots: id  1 | task 39832 | n_tokens = 4624, memory_seq_rm [4624, end)
slot update_slots: id  1 | task 39832 | prompt processing progress, n_tokens = 4831, batch.n_tokens = 207, progress = 0.986925
slot update_slots: id  1 | task 39832 | n_tokens = 4831, memory_seq_rm [4831, end)
slot update_slots: id  1 | task 39832 | prompt processing progress, n_tokens = 4895, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 39832 | prompt done, n_tokens = 4895, batch.n_tokens = 64
slot init_sampler: id  1 | task 39832 | init sampler, took 0.74 ms, tokens: text = 4895, total = 4895
slot update_slots: id  1 | task 39832 | erasing old context checkpoint (pos_min = 425, pos_max = 1052, size = 14.726 MiB)
slot update_slots: id  1 | task 39832 | created context checkpoint 8 of 8 (pos_min = 4232, pos_max = 4830, size = 14.046 MiB)
slot print_timing: id  1 | task 39832 | 
prompt eval time =     711.25 ms /   271 tokens (    2.62 ms per token,   381.02 tokens per second)
       eval time =    9582.28 ms /   356 tokens (   26.92 ms per token,    37.15 tokens per second)
      total time =   10293.52 ms /   627 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 39832 | stop processing: n_tokens = 5250, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.946 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 40190 | processing task, is_child = 0
slot update_slots: id  1 | task 40190 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5176
slot update_slots: id  1 | task 40190 | n_tokens = 4895, memory_seq_rm [4895, end)
slot update_slots: id  1 | task 40190 | prompt processing progress, n_tokens = 5112, batch.n_tokens = 217, progress = 0.987635
slot update_slots: id  1 | task 40190 | n_tokens = 5112, memory_seq_rm [5112, end)
slot update_slots: id  1 | task 40190 | prompt processing progress, n_tokens = 5176, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 40190 | prompt done, n_tokens = 5176, batch.n_tokens = 64
slot init_sampler: id  1 | task 40190 | init sampler, took 0.81 ms, tokens: text = 5176, total = 5176
slot update_slots: id  1 | task 40190 | erasing old context checkpoint (pos_min = 602, pos_max = 1146, size = 12.780 MiB)
slot update_slots: id  1 | task 40190 | created context checkpoint 8 of 8 (pos_min = 4507, pos_max = 5111, size = 14.187 MiB)
slot print_timing: id  1 | task 40190 | 
prompt eval time =     706.59 ms /   281 tokens (    2.51 ms per token,   397.69 tokens per second)
       eval time =    8046.58 ms /   297 tokens (   27.09 ms per token,    36.91 tokens per second)
      total time =    8753.17 ms /   578 tokens
slot      release: id  1 | task 40190 | stop processing: n_tokens = 5472, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.946
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 40489 | processing task, is_child = 0
slot update_slots: id  1 | task 40489 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5456
slot update_slots: id  1 | task 40489 | n_tokens = 5176, memory_seq_rm [5176, end)
slot update_slots: id  1 | task 40489 | prompt processing progress, n_tokens = 5392, batch.n_tokens = 216, progress = 0.988270
slot update_slots: id  1 | task 40489 | n_tokens = 5392, memory_seq_rm [5392, end)
slot update_slots: id  1 | task 40489 | prompt processing progress, n_tokens = 5456, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 40489 | prompt done, n_tokens = 5456, batch.n_tokens = 64
slot init_sampler: id  1 | task 40489 | init sampler, took 1.10 ms, tokens: text = 5456, total = 5456
slot update_slots: id  1 | task 40489 | erasing old context checkpoint (pos_min = 621, pos_max = 1282, size = 15.523 MiB)
slot update_slots: id  1 | task 40489 | created context checkpoint 8 of 8 (pos_min = 4840, pos_max = 5391, size = 12.944 MiB)
slot print_timing: id  1 | task 40489 | 
prompt eval time =     737.65 ms /   280 tokens (    2.63 ms per token,   379.59 tokens per second)
       eval time =   12360.91 ms /   452 tokens (   27.35 ms per token,    36.57 tokens per second)
      total time =   13098.56 ms /   732 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 40489 | stop processing: n_tokens = 5907, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.941 (> 0.100 thold), f_keep = 0.924
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 40943 | processing task, is_child = 0
slot update_slots: id  1 | task 40943 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5796
slot update_slots: id  1 | task 40943 | n_tokens = 5456, memory_seq_rm [5456, end)
slot update_slots: id  1 | task 40943 | prompt processing progress, n_tokens = 5732, batch.n_tokens = 276, progress = 0.988958
slot update_slots: id  1 | task 40943 | n_tokens = 5732, memory_seq_rm [5732, end)
slot update_slots: id  1 | task 40943 | prompt processing progress, n_tokens = 5796, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 40943 | prompt done, n_tokens = 5796, batch.n_tokens = 64
slot init_sampler: id  1 | task 40943 | init sampler, took 1.00 ms, tokens: text = 5796, total = 5796
slot update_slots: id  1 | task 40943 | erasing old context checkpoint (pos_min = 2140, pos_max = 2909, size = 18.056 MiB)
slot update_slots: id  1 | task 40943 | created context checkpoint 8 of 8 (pos_min = 5176, pos_max = 5731, size = 13.038 MiB)
slot print_timing: id  1 | task 40943 | 
prompt eval time =     795.34 ms /   340 tokens (    2.34 ms per token,   427.49 tokens per second)
       eval time =    4472.38 ms /   162 tokens (   27.61 ms per token,    36.22 tokens per second)
      total time =    5267.72 ms /   502 tokens
slot      release: id  1 | task 40943 | stop processing: n_tokens = 5957, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 41107 | processing task, is_child = 0
slot update_slots: id  1 | task 41107 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5930
slot update_slots: id  1 | task 41107 | n_tokens = 5796, memory_seq_rm [5796, end)
slot update_slots: id  1 | task 41107 | prompt processing progress, n_tokens = 5866, batch.n_tokens = 70, progress = 0.989207
slot update_slots: id  1 | task 41107 | n_tokens = 5866, memory_seq_rm [5866, end)
slot update_slots: id  1 | task 41107 | prompt processing progress, n_tokens = 5930, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 41107 | prompt done, n_tokens = 5930, batch.n_tokens = 64
slot init_sampler: id  1 | task 41107 | init sampler, took 0.91 ms, tokens: text = 5930, total = 5930
slot update_slots: id  1 | task 41107 | erasing old context checkpoint (pos_min = 2366, pos_max = 2990, size = 14.656 MiB)
slot update_slots: id  1 | task 41107 | created context checkpoint 8 of 8 (pos_min = 5246, pos_max = 5865, size = 14.539 MiB)
slot print_timing: id  1 | task 41107 | 
prompt eval time =     448.35 ms /   134 tokens (    3.35 ms per token,   298.88 tokens per second)
       eval time =   13165.90 ms /   476 tokens (   27.66 ms per token,    36.15 tokens per second)
      total time =   13614.25 ms /   610 tokens
slot      release: id  1 | task 41107 | stop processing: n_tokens = 6405, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.926
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 41585 | processing task, is_child = 0
slot update_slots: id  1 | task 41585 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6361
slot update_slots: id  1 | task 41585 | n_tokens = 5930, memory_seq_rm [5930, end)
slot update_slots: id  1 | task 41585 | prompt processing progress, n_tokens = 6297, batch.n_tokens = 367, progress = 0.989939
slot update_slots: id  1 | task 41585 | n_tokens = 6297, memory_seq_rm [6297, end)
slot update_slots: id  1 | task 41585 | prompt processing progress, n_tokens = 6361, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 41585 | prompt done, n_tokens = 6361, batch.n_tokens = 64
slot init_sampler: id  1 | task 41585 | init sampler, took 1.41 ms, tokens: text = 6361, total = 6361
slot update_slots: id  1 | task 41585 | erasing old context checkpoint (pos_min = 3199, pos_max = 3968, size = 18.056 MiB)
slot update_slots: id  1 | task 41585 | created context checkpoint 8 of 8 (pos_min = 5785, pos_max = 6296, size = 12.006 MiB)
slot print_timing: id  1 | task 41585 | 
prompt eval time =     920.36 ms /   431 tokens (    2.14 ms per token,   468.29 tokens per second)
       eval time =    5928.71 ms /   215 tokens (   27.58 ms per token,    36.26 tokens per second)
      total time =    6849.07 ms /   646 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 41585 | stop processing: n_tokens = 6575, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.773 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 41802 | processing task, is_child = 0
slot update_slots: id  1 | task 41802 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8230
slot update_slots: id  1 | task 41802 | n_tokens = 6361, memory_seq_rm [6361, end)
slot update_slots: id  1 | task 41802 | prompt processing progress, n_tokens = 8166, batch.n_tokens = 1805, progress = 0.992224
slot update_slots: id  1 | task 41802 | n_tokens = 8166, memory_seq_rm [8166, end)
slot update_slots: id  1 | task 41802 | prompt processing progress, n_tokens = 8230, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 41802 | prompt done, n_tokens = 8230, batch.n_tokens = 64
slot init_sampler: id  1 | task 41802 | init sampler, took 1.35 ms, tokens: text = 8230, total = 8230
slot update_slots: id  1 | task 41802 | erasing old context checkpoint (pos_min = 3731, pos_max = 4315, size = 13.718 MiB)
slot update_slots: id  1 | task 41802 | created context checkpoint 8 of 8 (pos_min = 7396, pos_max = 8165, size = 18.056 MiB)
slot print_timing: id  1 | task 41802 | 
prompt eval time =    2864.50 ms /  1869 tokens (    1.53 ms per token,   652.47 tokens per second)
       eval time =   11033.46 ms /   400 tokens (   27.58 ms per token,    36.25 tokens per second)
      total time =   13897.96 ms /  2269 tokens
slot      release: id  1 | task 41802 | stop processing: n_tokens = 8629, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.967 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42204 | processing task, is_child = 0
slot update_slots: id  1 | task 42204 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8513
slot update_slots: id  1 | task 42204 | n_tokens = 8230, memory_seq_rm [8230, end)
slot update_slots: id  1 | task 42204 | prompt processing progress, n_tokens = 8449, batch.n_tokens = 219, progress = 0.992482
slot update_slots: id  1 | task 42204 | n_tokens = 8449, memory_seq_rm [8449, end)
slot update_slots: id  1 | task 42204 | prompt processing progress, n_tokens = 8513, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42204 | prompt done, n_tokens = 8513, batch.n_tokens = 64
slot init_sampler: id  1 | task 42204 | init sampler, took 1.69 ms, tokens: text = 8513, total = 8513
slot update_slots: id  1 | task 42204 | erasing old context checkpoint (pos_min = 3881, pos_max = 4559, size = 15.922 MiB)
slot update_slots: id  1 | task 42204 | created context checkpoint 8 of 8 (pos_min = 7993, pos_max = 8448, size = 10.693 MiB)
slot print_timing: id  1 | task 42204 | 
prompt eval time =     748.16 ms /   283 tokens (    2.64 ms per token,   378.26 tokens per second)
       eval time =    6327.47 ms /   232 tokens (   27.27 ms per token,    36.67 tokens per second)
      total time =    7075.63 ms /   515 tokens
slot      release: id  1 | task 42204 | stop processing: n_tokens = 8744, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42438 | processing task, is_child = 0
slot update_slots: id  1 | task 42438 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8600
slot update_slots: id  1 | task 42438 | n_tokens = 8513, memory_seq_rm [8513, end)
slot update_slots: id  1 | task 42438 | prompt processing progress, n_tokens = 8536, batch.n_tokens = 23, progress = 0.992558
slot update_slots: id  1 | task 42438 | n_tokens = 8536, memory_seq_rm [8536, end)
slot update_slots: id  1 | task 42438 | prompt processing progress, n_tokens = 8600, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42438 | prompt done, n_tokens = 8600, batch.n_tokens = 64
slot init_sampler: id  1 | task 42438 | init sampler, took 1.53 ms, tokens: text = 8600, total = 8600
slot update_slots: id  1 | task 42438 | erasing old context checkpoint (pos_min = 4232, pos_max = 4830, size = 14.046 MiB)
slot update_slots: id  1 | task 42438 | created context checkpoint 8 of 8 (pos_min = 8230, pos_max = 8535, size = 7.176 MiB)
slot print_timing: id  1 | task 42438 | 
prompt eval time =     337.28 ms /    87 tokens (    3.88 ms per token,   257.94 tokens per second)
       eval time =    5473.45 ms /   200 tokens (   27.37 ms per token,    36.54 tokens per second)
      total time =    5810.74 ms /   287 tokens
slot      release: id  1 | task 42438 | stop processing: n_tokens = 8799, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42640 | processing task, is_child = 0
slot update_slots: id  1 | task 42640 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8874
slot update_slots: id  1 | task 42640 | n_tokens = 8600, memory_seq_rm [8600, end)
slot update_slots: id  1 | task 42640 | prompt processing progress, n_tokens = 8810, batch.n_tokens = 210, progress = 0.992788
slot update_slots: id  1 | task 42640 | n_tokens = 8810, memory_seq_rm [8810, end)
slot update_slots: id  1 | task 42640 | prompt processing progress, n_tokens = 8874, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42640 | prompt done, n_tokens = 8874, batch.n_tokens = 64
slot init_sampler: id  1 | task 42640 | init sampler, took 1.65 ms, tokens: text = 8874, total = 8874
slot update_slots: id  1 | task 42640 | erasing old context checkpoint (pos_min = 4507, pos_max = 5111, size = 14.187 MiB)
slot update_slots: id  1 | task 42640 | created context checkpoint 8 of 8 (pos_min = 8230, pos_max = 8809, size = 13.601 MiB)
slot print_timing: id  1 | task 42640 | 
prompt eval time =     590.67 ms /   274 tokens (    2.16 ms per token,   463.88 tokens per second)
       eval time =    6122.56 ms /   221 tokens (   27.70 ms per token,    36.10 tokens per second)
      total time =    6713.22 ms /   495 tokens
slot      release: id  1 | task 42640 | stop processing: n_tokens = 9094, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.654 (> 0.100 thold), f_keep = 0.507
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42863 | processing task, is_child = 0
slot update_slots: id  1 | task 42863 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7049
slot update_slots: id  1 | task 42863 | n_past = 4613, slot.prompt.tokens.size() = 9094, seq_id = 1, pos_min = 8324, n_swa = 128
slot update_slots: id  1 | task 42863 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 42863 | erased invalidated context checkpoint (pos_min = 4840, pos_max = 5391, n_swa = 128, size = 12.944 MiB)
slot update_slots: id  1 | task 42863 | erased invalidated context checkpoint (pos_min = 5176, pos_max = 5731, n_swa = 128, size = 13.038 MiB)
slot update_slots: id  1 | task 42863 | erased invalidated context checkpoint (pos_min = 5246, pos_max = 5865, n_swa = 128, size = 14.539 MiB)
slot update_slots: id  1 | task 42863 | erased invalidated context checkpoint (pos_min = 5785, pos_max = 6296, n_swa = 128, size = 12.006 MiB)
slot update_slots: id  1 | task 42863 | erased invalidated context checkpoint (pos_min = 7396, pos_max = 8165, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 42863 | erased invalidated context checkpoint (pos_min = 7993, pos_max = 8448, n_swa = 128, size = 10.693 MiB)
slot update_slots: id  1 | task 42863 | erased invalidated context checkpoint (pos_min = 8230, pos_max = 8535, n_swa = 128, size = 7.176 MiB)
slot update_slots: id  1 | task 42863 | erased invalidated context checkpoint (pos_min = 8230, pos_max = 8809, n_swa = 128, size = 13.601 MiB)
slot update_slots: id  1 | task 42863 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 42863 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.290538
slot update_slots: id  1 | task 42863 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 42863 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.581075
slot update_slots: id  1 | task 42863 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 42863 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.871613
slot update_slots: id  1 | task 42863 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 42863 | prompt processing progress, n_tokens = 6985, batch.n_tokens = 841, progress = 0.990921
slot update_slots: id  1 | task 42863 | n_tokens = 6985, memory_seq_rm [6985, end)
slot update_slots: id  1 | task 42863 | prompt processing progress, n_tokens = 7049, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42863 | prompt done, n_tokens = 7049, batch.n_tokens = 64
slot init_sampler: id  1 | task 42863 | init sampler, took 1.43 ms, tokens: text = 7049, total = 7049
slot update_slots: id  1 | task 42863 | created context checkpoint 1 of 8 (pos_min = 6215, pos_max = 6984, size = 18.056 MiB)
slot print_timing: id  1 | task 42863 | 
prompt eval time =    9772.84 ms /  7049 tokens (    1.39 ms per token,   721.28 tokens per second)
       eval time =    3311.66 ms /   123 tokens (   26.92 ms per token,    37.14 tokens per second)
      total time =   13084.50 ms /  7172 tokens
slot      release: id  1 | task 42863 | stop processing: n_tokens = 7171, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.168 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42991 | processing task, is_child = 0
slot update_slots: id  1 | task 42991 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 41983
slot update_slots: id  1 | task 42991 | n_tokens = 7049, memory_seq_rm [7049, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 9097, batch.n_tokens = 2048, progress = 0.216683
slot update_slots: id  1 | task 42991 | n_tokens = 9097, memory_seq_rm [9097, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 11145, batch.n_tokens = 2048, progress = 0.265465
slot update_slots: id  1 | task 42991 | n_tokens = 11145, memory_seq_rm [11145, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 13193, batch.n_tokens = 2048, progress = 0.314246
slot update_slots: id  1 | task 42991 | n_tokens = 13193, memory_seq_rm [13193, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 15241, batch.n_tokens = 2048, progress = 0.363028
slot update_slots: id  1 | task 42991 | n_tokens = 15241, memory_seq_rm [15241, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 17289, batch.n_tokens = 2048, progress = 0.411810
slot update_slots: id  1 | task 42991 | n_tokens = 17289, memory_seq_rm [17289, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 19337, batch.n_tokens = 2048, progress = 0.460591
slot update_slots: id  1 | task 42991 | n_tokens = 19337, memory_seq_rm [19337, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 21385, batch.n_tokens = 2048, progress = 0.509373
slot update_slots: id  1 | task 42991 | n_tokens = 21385, memory_seq_rm [21385, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 23433, batch.n_tokens = 2048, progress = 0.558154
slot update_slots: id  1 | task 42991 | n_tokens = 23433, memory_seq_rm [23433, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 25481, batch.n_tokens = 2048, progress = 0.606936
slot update_slots: id  1 | task 42991 | n_tokens = 25481, memory_seq_rm [25481, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 27529, batch.n_tokens = 2048, progress = 0.655718
slot update_slots: id  1 | task 42991 | n_tokens = 27529, memory_seq_rm [27529, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 29577, batch.n_tokens = 2048, progress = 0.704499
slot update_slots: id  1 | task 42991 | n_tokens = 29577, memory_seq_rm [29577, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 31625, batch.n_tokens = 2048, progress = 0.753281
slot update_slots: id  1 | task 42991 | n_tokens = 31625, memory_seq_rm [31625, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 33673, batch.n_tokens = 2048, progress = 0.802063
slot update_slots: id  1 | task 42991 | n_tokens = 33673, memory_seq_rm [33673, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 35721, batch.n_tokens = 2048, progress = 0.850844
slot update_slots: id  1 | task 42991 | n_tokens = 35721, memory_seq_rm [35721, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 37769, batch.n_tokens = 2048, progress = 0.899626
decode: failed to find a memory slot for batch of size 2048
srv  try_clear_id: purging slot 2 with 14930 tokens
slot prompt_clear: id  2 | task -1 | clearing prompt with 14930 tokens
srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 2048, ret = 1
slot update_slots: id  1 | task 42991 | n_tokens = 37769, memory_seq_rm [37769, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 39817, batch.n_tokens = 2048, progress = 0.948408
slot update_slots: id  1 | task 42991 | n_tokens = 39817, memory_seq_rm [39817, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 41865, batch.n_tokens = 2048, progress = 0.997189
slot update_slots: id  1 | task 42991 | n_tokens = 41865, memory_seq_rm [41865, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 41919, batch.n_tokens = 54, progress = 0.998476
slot update_slots: id  1 | task 42991 | n_tokens = 41919, memory_seq_rm [41919, end)
slot update_slots: id  1 | task 42991 | prompt processing progress, n_tokens = 41983, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42991 | prompt done, n_tokens = 41983, batch.n_tokens = 64
slot init_sampler: id  1 | task 42991 | init sampler, took 5.98 ms, tokens: text = 41983, total = 41983
slot update_slots: id  1 | task 42991 | created context checkpoint 2 of 8 (pos_min = 41022, pos_max = 41918, size = 21.034 MiB)
slot print_timing: id  1 | task 42991 | 
prompt eval time =   56026.96 ms / 34934 tokens (    1.60 ms per token,   623.52 tokens per second)
       eval time =    2821.36 ms /    94 tokens (   30.01 ms per token,    33.32 tokens per second)
      total time =   58848.32 ms / 35028 tokens
slot      release: id  1 | task 42991 | stop processing: n_tokens = 42076, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.968 (> 0.100 thold), f_keep = 0.165
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 42076, total state size = 1007.672 MiB
srv          load:  - looking for better prompt, base f_keep = 0.165, sim = 0.968
srv        update:  - cache state: 13 prompts, 5139.379 MiB (limits: 8192.000 MiB, 64000 tokens, 256657 est)
srv        update:    - prompt 0x5a234b577500:    4873 tokens, checkpoints:  5,   235.945 MiB
srv        update:    - prompt 0x5a234d243b70:    4947 tokens, checkpoints:  7,   261.904 MiB
srv        update:    - prompt 0x5a234cfbeaa0:   38169 tokens, checkpoints:  8,  1100.063 MiB
srv        update:    - prompt 0x5a234dc80090:    2123 tokens, checkpoints:  1,    68.612 MiB
srv        update:    - prompt 0x5a234ccbfee0:   15206 tokens, checkpoints:  3,   424.099 MiB
srv        update:    - prompt 0x5a234dbdca00:    2924 tokens, checkpoints:  2,   117.715 MiB
srv        update:    - prompt 0x5a234cfa37d0:    1948 tokens, checkpoints:  3,   106.905 MiB
srv        update:    - prompt 0x5a234cce4d20:    6823 tokens, checkpoints:  8,   301.838 MiB
srv        update:    - prompt 0x5a237670b520:   11970 tokens, checkpoints:  8,   418.989 MiB
srv        update:    - prompt 0x5a234f21f8f0:    1921 tokens, checkpoints:  3,   103.341 MiB
srv        update:    - prompt 0x5a237670b330:    9131 tokens, checkpoints:  8,   361.890 MiB
srv        update:    - prompt 0x5a23757875b0:   18907 tokens, checkpoints:  8,   591.315 MiB
srv        update:    - prompt 0x5a2351b39e00:   42076 tokens, checkpoints:  2,  1046.762 MiB
srv  get_availabl: prompt cache update took 1238.40 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 43104 | processing task, is_child = 0
slot update_slots: id  1 | task 43104 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7184
slot update_slots: id  1 | task 43104 | n_past = 6955, slot.prompt.tokens.size() = 42076, seq_id = 1, pos_min = 41179, n_swa = 128
slot update_slots: id  1 | task 43104 | restored context checkpoint (pos_min = 6215, pos_max = 6984, size = 18.056 MiB)
slot update_slots: id  1 | task 43104 | erased invalidated context checkpoint (pos_min = 41022, pos_max = 41918, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 43104 | n_tokens = 6955, memory_seq_rm [6955, end)
slot update_slots: id  1 | task 43104 | prompt processing progress, n_tokens = 7120, batch.n_tokens = 165, progress = 0.991091
slot update_slots: id  1 | task 43104 | n_tokens = 7120, memory_seq_rm [7120, end)
slot update_slots: id  1 | task 43104 | prompt processing progress, n_tokens = 7184, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 43104 | prompt done, n_tokens = 7184, batch.n_tokens = 64
slot init_sampler: id  1 | task 43104 | init sampler, took 1.86 ms, tokens: text = 7184, total = 7184
slot update_slots: id  1 | task 43104 | created context checkpoint 2 of 8 (pos_min = 6350, pos_max = 7119, size = 18.056 MiB)
slot print_timing: id  1 | task 43104 | 
prompt eval time =     822.43 ms /   229 tokens (    3.59 ms per token,   278.44 tokens per second)
       eval time =    2651.41 ms /   101 tokens (   26.25 ms per token,    38.09 tokens per second)
      total time =    3473.84 ms /   330 tokens
slot      release: id  1 | task 43104 | stop processing: n_tokens = 7284, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 43207 | processing task, is_child = 0
slot update_slots: id  1 | task 43207 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7226
slot update_slots: id  1 | task 43207 | n_tokens = 7099, memory_seq_rm [7099, end)
slot update_slots: id  1 | task 43207 | prompt processing progress, n_tokens = 7162, batch.n_tokens = 63, progress = 0.991143
slot update_slots: id  1 | task 43207 | n_tokens = 7162, memory_seq_rm [7162, end)
slot update_slots: id  1 | task 43207 | prompt processing progress, n_tokens = 7226, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 43207 | prompt done, n_tokens = 7226, batch.n_tokens = 64
slot init_sampler: id  1 | task 43207 | init sampler, took 1.03 ms, tokens: text = 7226, total = 7226
slot print_timing: id  1 | task 43207 | 
prompt eval time =     511.55 ms /   127 tokens (    4.03 ms per token,   248.27 tokens per second)
       eval time =    4084.77 ms /   154 tokens (   26.52 ms per token,    37.70 tokens per second)
      total time =    4596.31 ms /   281 tokens
slot      release: id  1 | task 43207 | stop processing: n_tokens = 7379, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 43363 | processing task, is_child = 0
slot update_slots: id  1 | task 43363 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7150
slot update_slots: id  1 | task 43363 | n_tokens = 7117, memory_seq_rm [7117, end)
slot update_slots: id  1 | task 43363 | prompt processing progress, n_tokens = 7150, batch.n_tokens = 33, progress = 1.000000
slot update_slots: id  1 | task 43363 | prompt done, n_tokens = 7150, batch.n_tokens = 33
slot init_sampler: id  1 | task 43363 | init sampler, took 1.18 ms, tokens: text = 7150, total = 7150
slot print_timing: id  1 | task 43363 | 
prompt eval time =     226.63 ms /    33 tokens (    6.87 ms per token,   145.61 tokens per second)
       eval time =    2126.81 ms /    80 tokens (   26.59 ms per token,    37.62 tokens per second)
      total time =    2353.44 ms /   113 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 43363 | stop processing: n_tokens = 7229, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 43444 | processing task, is_child = 0
slot update_slots: id  1 | task 43444 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7199
slot update_slots: id  1 | task 43444 | n_tokens = 7135, memory_seq_rm [7135, end)
slot update_slots: id  1 | task 43444 | prompt processing progress, n_tokens = 7199, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 43444 | prompt done, n_tokens = 7199, batch.n_tokens = 64
slot init_sampler: id  1 | task 43444 | init sampler, took 1.09 ms, tokens: text = 7199, total = 7199
slot print_timing: id  1 | task 43444 | 
prompt eval time =     303.36 ms /    64 tokens (    4.74 ms per token,   210.97 tokens per second)
       eval time =     933.10 ms /    36 tokens (   25.92 ms per token,    38.58 tokens per second)
      total time =    1236.46 ms /   100 tokens
slot      release: id  1 | task 43444 | stop processing: n_tokens = 7234, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 43481 | processing task, is_child = 0
slot update_slots: id  1 | task 43481 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7185
slot update_slots: id  1 | task 43481 | n_tokens = 7153, memory_seq_rm [7153, end)
slot update_slots: id  1 | task 43481 | prompt processing progress, n_tokens = 7185, batch.n_tokens = 32, progress = 1.000000
slot update_slots: id  1 | task 43481 | prompt done, n_tokens = 7185, batch.n_tokens = 32
slot init_sampler: id  1 | task 43481 | init sampler, took 1.07 ms, tokens: text = 7185, total = 7185
slot print_timing: id  1 | task 43481 | 
prompt eval time =     212.14 ms /    32 tokens (    6.63 ms per token,   150.84 tokens per second)
       eval time =    1034.04 ms /    39 tokens (   26.51 ms per token,    37.72 tokens per second)
      total time =    1246.18 ms /    71 tokens
slot      release: id  1 | task 43481 | stop processing: n_tokens = 7223, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 43521 | processing task, is_child = 0
slot update_slots: id  1 | task 43521 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7203
slot update_slots: id  1 | task 43521 | n_tokens = 7171, memory_seq_rm [7171, end)
slot update_slots: id  1 | task 43521 | prompt processing progress, n_tokens = 7203, batch.n_tokens = 32, progress = 1.000000
slot update_slots: id  1 | task 43521 | prompt done, n_tokens = 7203, batch.n_tokens = 32
slot init_sampler: id  1 | task 43521 | init sampler, took 1.04 ms, tokens: text = 7203, total = 7203
slot print_timing: id  1 | task 43521 | 
prompt eval time =     193.76 ms /    32 tokens (    6.06 ms per token,   165.15 tokens per second)
       eval time =    2054.74 ms /    77 tokens (   26.68 ms per token,    37.47 tokens per second)
      total time =    2248.51 ms /   109 tokens
slot      release: id  1 | task 43521 | stop processing: n_tokens = 7279, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 43599 | processing task, is_child = 0
slot update_slots: id  1 | task 43599 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7221
slot update_slots: id  1 | task 43599 | n_tokens = 7189, memory_seq_rm [7189, end)
slot update_slots: id  1 | task 43599 | prompt processing progress, n_tokens = 7221, batch.n_tokens = 32, progress = 1.000000
slot update_slots: id  1 | task 43599 | prompt done, n_tokens = 7221, batch.n_tokens = 32
slot init_sampler: id  1 | task 43599 | init sampler, took 1.41 ms, tokens: text = 7221, total = 7221
slot update_slots: id  1 | task 43599 | created context checkpoint 3 of 8 (pos_min = 6609, pos_max = 7188, size = 13.601 MiB)
slot print_timing: id  1 | task 43599 | 
prompt eval time =     222.13 ms /    32 tokens (    6.94 ms per token,   144.06 tokens per second)
       eval time =    1331.13 ms /    50 tokens (   26.62 ms per token,    37.56 tokens per second)
      total time =    1553.26 ms /    82 tokens
slot      release: id  1 | task 43599 | stop processing: n_tokens = 7270, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 43650 | processing task, is_child = 0
slot update_slots: id  1 | task 43650 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7239
slot update_slots: id  1 | task 43650 | n_tokens = 7207, memory_seq_rm [7207, end)
slot update_slots: id  1 | task 43650 | prompt processing progress, n_tokens = 7239, batch.n_tokens = 32, progress = 1.000000
slot update_slots: id  1 | task 43650 | prompt done, n_tokens = 7239, batch.n_tokens = 32
slot init_sampler: id  1 | task 43650 | init sampler, took 1.11 ms, tokens: text = 7239, total = 7239
slot print_timing: id  1 | task 43650 | 
prompt eval time =     142.53 ms /    32 tokens (    4.45 ms per token,   224.52 tokens per second)
       eval time =     779.10 ms /    30 tokens (   25.97 ms per token,    38.51 tokens per second)
      total time =     921.63 ms /    62 tokens
slot      release: id  1 | task 43650 | stop processing: n_tokens = 7268, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 43681 | processing task, is_child = 0
slot update_slots: id  1 | task 43681 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7263
slot update_slots: id  1 | task 43681 | n_tokens = 7225, memory_seq_rm [7225, end)
slot update_slots: id  1 | task 43681 | prompt processing progress, n_tokens = 7263, batch.n_tokens = 38, progress = 1.000000
slot update_slots: id  1 | task 43681 | prompt done, n_tokens = 7263, batch.n_tokens = 38
slot init_sampler: id  1 | task 43681 | init sampler, took 1.08 ms, tokens: text = 7263, total = 7263
slot print_timing: id  1 | task 43681 | 
prompt eval time =     235.52 ms /    38 tokens (    6.20 ms per token,   161.35 tokens per second)
       eval time =    3950.38 ms /   145 tokens (   27.24 ms per token,    36.71 tokens per second)
      total time =    4185.89 ms /   183 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 43681 | stop processing: n_tokens = 7407, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 43827 | processing task, is_child = 0
slot update_slots: id  1 | task 43827 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7473
slot update_slots: id  1 | task 43827 | n_tokens = 7243, memory_seq_rm [7243, end)
slot update_slots: id  1 | task 43827 | prompt processing progress, n_tokens = 7409, batch.n_tokens = 166, progress = 0.991436
slot update_slots: id  1 | task 43827 | n_tokens = 7409, memory_seq_rm [7409, end)
slot update_slots: id  1 | task 43827 | prompt processing progress, n_tokens = 7473, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 43827 | prompt done, n_tokens = 7473, batch.n_tokens = 64
slot init_sampler: id  1 | task 43827 | init sampler, took 1.14 ms, tokens: text = 7473, total = 7473
slot update_slots: id  1 | task 43827 | created context checkpoint 4 of 8 (pos_min = 6639, pos_max = 7408, size = 18.056 MiB)
slot print_timing: id  1 | task 43827 | 
prompt eval time =     633.10 ms /   230 tokens (    2.75 ms per token,   363.29 tokens per second)
       eval time =   17731.20 ms /   643 tokens (   27.58 ms per token,    36.26 tokens per second)
      total time =   18364.30 ms /   873 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 43827 | stop processing: n_tokens = 8115, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.891 (> 0.100 thold), f_keep = 0.895
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 44472 | processing task, is_child = 0
slot update_slots: id  1 | task 44472 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8148
slot update_slots: id  1 | task 44472 | n_past = 7261, slot.prompt.tokens.size() = 8115, seq_id = 1, pos_min = 7218, n_swa = 128
slot update_slots: id  1 | task 44472 | restored context checkpoint (pos_min = 6639, pos_max = 7408, size = 18.056 MiB)
slot update_slots: id  1 | task 44472 | n_tokens = 7261, memory_seq_rm [7261, end)
slot update_slots: id  1 | task 44472 | prompt processing progress, n_tokens = 8084, batch.n_tokens = 823, progress = 0.992145
slot update_slots: id  1 | task 44472 | n_tokens = 8084, memory_seq_rm [8084, end)
slot update_slots: id  1 | task 44472 | prompt processing progress, n_tokens = 8148, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 44472 | prompt done, n_tokens = 8148, batch.n_tokens = 64
slot init_sampler: id  1 | task 44472 | init sampler, took 1.23 ms, tokens: text = 8148, total = 8148
slot update_slots: id  1 | task 44472 | created context checkpoint 5 of 8 (pos_min = 7187, pos_max = 8083, size = 21.034 MiB)
slot print_timing: id  1 | task 44472 | 
prompt eval time =    1654.67 ms /   887 tokens (    1.87 ms per token,   536.06 tokens per second)
       eval time =   24079.04 ms /   872 tokens (   27.61 ms per token,    36.21 tokens per second)
      total time =   25733.71 ms /  1759 tokens
slot      release: id  1 | task 44472 | stop processing: n_tokens = 9019, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.875 (> 0.100 thold), f_keep = 0.877
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 45346 | processing task, is_child = 0
slot update_slots: id  1 | task 45346 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9036
slot update_slots: id  1 | task 45346 | n_past = 7906, slot.prompt.tokens.size() = 9019, seq_id = 1, pos_min = 8122, n_swa = 128
slot update_slots: id  1 | task 45346 | restored context checkpoint (pos_min = 7187, pos_max = 8083, size = 21.034 MiB)
slot update_slots: id  1 | task 45346 | n_tokens = 7906, memory_seq_rm [7906, end)
slot update_slots: id  1 | task 45346 | prompt processing progress, n_tokens = 8972, batch.n_tokens = 1066, progress = 0.992917
slot update_slots: id  1 | task 45346 | n_tokens = 8972, memory_seq_rm [8972, end)
slot update_slots: id  1 | task 45346 | prompt processing progress, n_tokens = 9036, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 45346 | prompt done, n_tokens = 9036, batch.n_tokens = 64
slot init_sampler: id  1 | task 45346 | init sampler, took 1.31 ms, tokens: text = 9036, total = 9036
slot update_slots: id  1 | task 45346 | created context checkpoint 6 of 8 (pos_min = 8075, pos_max = 8971, size = 21.034 MiB)
slot print_timing: id  1 | task 45346 | 
prompt eval time =    2032.71 ms /  1130 tokens (    1.80 ms per token,   555.91 tokens per second)
       eval time =   31729.87 ms /  1149 tokens (   27.62 ms per token,    36.21 tokens per second)
      total time =   33762.57 ms /  2279 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 45346 | stop processing: n_tokens = 10184, truncated = 0
srv  update_slots: all slots are idle
