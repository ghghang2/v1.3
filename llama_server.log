ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14992 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1578 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 64000 -> need 1580 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.75 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
srv  log_server_r: request: GET /health 127.0.0.1 503
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 64000
llama_context: n_ctx_seq     = 64000
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (64000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 64000 cells
llama_kv_cache:      CUDA0 KV buffer size =  1500.00 MiB
llama_kv_cache: size = 1500.00 MiB ( 64000 cells,  12 layers,  4/1 seqs), K (f16):  750.00 MiB, V (f16):  750.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   132.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 63.11 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv  log_server_r: request: GET /health 127.0.0.1 503
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  1 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  2 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  3 | task -1 | new slot, n_ctx = 64000
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-07

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 460
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 396, batch.n_tokens = 396, progress = 0.860870
slot update_slots: id  3 | task 0 | n_tokens = 396, memory_seq_rm [396, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 460, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 460, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.08 ms, tokens: text = 460, total = 460
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 395, size = 9.286 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =     824.00 ms /   460 tokens (    1.79 ms per token,   558.25 tokens per second)
       eval time =     690.71 ms /    30 tokens (   23.02 ms per token,    43.43 tokens per second)
      total time =    1514.71 ms /   490 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 489, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.876 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32 | processing task, is_child = 0
slot update_slots: id  3 | task 32 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 525
slot update_slots: id  3 | task 32 | n_tokens = 460, memory_seq_rm [460, end)
slot update_slots: id  3 | task 32 | prompt processing progress, n_tokens = 461, batch.n_tokens = 1, progress = 0.878095
slot update_slots: id  3 | task 32 | n_tokens = 461, memory_seq_rm [461, end)
slot update_slots: id  3 | task 32 | prompt processing progress, n_tokens = 525, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 32 | prompt done, n_tokens = 525, batch.n_tokens = 64
slot init_sampler: id  3 | task 32 | init sampler, took 0.10 ms, tokens: text = 525, total = 525
slot update_slots: id  3 | task 32 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 460, size = 10.810 MiB)
slot print_timing: id  3 | task 32 | 
prompt eval time =     254.45 ms /    65 tokens (    3.91 ms per token,   255.46 tokens per second)
       eval time =     927.37 ms /    42 tokens (   22.08 ms per token,    45.29 tokens per second)
      total time =    1181.82 ms /   107 tokens
slot      release: id  3 | task 32 | stop processing: n_tokens = 566, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.503 (> 0.100 thold), f_keep = 0.928
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 76 | processing task, is_child = 0
slot update_slots: id  3 | task 76 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1043
slot update_slots: id  3 | task 76 | n_tokens = 525, memory_seq_rm [525, end)
slot update_slots: id  3 | task 76 | prompt processing progress, n_tokens = 979, batch.n_tokens = 454, progress = 0.938639
slot update_slots: id  3 | task 76 | n_tokens = 979, memory_seq_rm [979, end)
slot update_slots: id  3 | task 76 | prompt processing progress, n_tokens = 1043, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 76 | prompt done, n_tokens = 1043, batch.n_tokens = 64
slot init_sampler: id  3 | task 76 | init sampler, took 0.23 ms, tokens: text = 1043, total = 1043
slot update_slots: id  3 | task 76 | created context checkpoint 3 of 8 (pos_min = 0, pos_max = 978, size = 22.957 MiB)
slot print_timing: id  3 | task 76 | 
prompt eval time =     538.28 ms /   518 tokens (    1.04 ms per token,   962.33 tokens per second)
       eval time =    1620.13 ms /    74 tokens (   21.89 ms per token,    45.68 tokens per second)
      total time =    2158.41 ms /   592 tokens
slot      release: id  3 | task 76 | stop processing: n_tokens = 1116, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.896 (> 0.100 thold), f_keep = 0.408
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1116, total state size = 50.181 MiB
srv          load:  - looking for better prompt, base f_keep = 0.408, sim = 0.896
srv        update:  - cache state: 1 prompts, 93.234 MiB (limits: 8192.000 MiB, 64000 tokens, 98056 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv  get_availabl: prompt cache update took 75.13 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 152 | processing task, is_child = 0
slot update_slots: id  3 | task 152 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 508
slot update_slots: id  3 | task 152 | n_tokens = 455, memory_seq_rm [455, end)
slot update_slots: id  3 | task 152 | prompt processing progress, n_tokens = 508, batch.n_tokens = 53, progress = 1.000000
slot update_slots: id  3 | task 152 | prompt done, n_tokens = 508, batch.n_tokens = 53
slot init_sampler: id  3 | task 152 | init sampler, took 0.09 ms, tokens: text = 508, total = 508
slot print_timing: id  3 | task 152 | 
prompt eval time =     256.70 ms /    53 tokens (    4.84 ms per token,   206.47 tokens per second)
       eval time =     827.91 ms /    38 tokens (   21.79 ms per token,    45.90 tokens per second)
      total time =    1084.61 ms /    91 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 152 | stop processing: n_tokens = 545, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.493 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 191 | processing task, is_child = 0
slot update_slots: id  3 | task 191 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1031
slot update_slots: id  3 | task 191 | n_tokens = 508, memory_seq_rm [508, end)
slot update_slots: id  3 | task 191 | prompt processing progress, n_tokens = 967, batch.n_tokens = 459, progress = 0.937924
slot update_slots: id  3 | task 191 | n_tokens = 967, memory_seq_rm [967, end)
slot update_slots: id  3 | task 191 | prompt processing progress, n_tokens = 1031, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 191 | prompt done, n_tokens = 1031, batch.n_tokens = 64
slot init_sampler: id  3 | task 191 | init sampler, took 0.19 ms, tokens: text = 1031, total = 1031
slot print_timing: id  3 | task 191 | 
prompt eval time =     551.78 ms /   523 tokens (    1.06 ms per token,   947.84 tokens per second)
       eval time =    1019.97 ms /    43 tokens (   23.72 ms per token,    42.16 tokens per second)
      total time =    1571.76 ms /   566 tokens
slot      release: id  3 | task 191 | stop processing: n_tokens = 1073, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.797 (> 0.100 thold), f_keep = 0.961
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 236 | processing task, is_child = 0
slot update_slots: id  3 | task 236 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1294
slot update_slots: id  3 | task 236 | n_tokens = 1031, memory_seq_rm [1031, end)
slot update_slots: id  3 | task 236 | prompt processing progress, n_tokens = 1230, batch.n_tokens = 199, progress = 0.950541
slot update_slots: id  3 | task 236 | n_tokens = 1230, memory_seq_rm [1230, end)
slot update_slots: id  3 | task 236 | prompt processing progress, n_tokens = 1294, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 236 | prompt done, n_tokens = 1294, batch.n_tokens = 64
slot init_sampler: id  3 | task 236 | init sampler, took 0.23 ms, tokens: text = 1294, total = 1294
slot update_slots: id  3 | task 236 | created context checkpoint 4 of 8 (pos_min = 381, pos_max = 1229, size = 19.908 MiB)
slot print_timing: id  3 | task 236 | 
prompt eval time =     418.01 ms /   263 tokens (    1.59 ms per token,   629.16 tokens per second)
       eval time =    1087.11 ms /    48 tokens (   22.65 ms per token,    44.15 tokens per second)
      total time =    1505.12 ms /   311 tokens
slot      release: id  3 | task 236 | stop processing: n_tokens = 1341, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 286 | processing task, is_child = 0
slot update_slots: id  3 | task 286 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1358
slot update_slots: id  3 | task 286 | n_tokens = 1294, memory_seq_rm [1294, end)
slot update_slots: id  3 | task 286 | prompt processing progress, n_tokens = 1358, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 286 | prompt done, n_tokens = 1358, batch.n_tokens = 64
slot init_sampler: id  3 | task 286 | init sampler, took 0.22 ms, tokens: text = 1358, total = 1358
slot print_timing: id  3 | task 286 | 
prompt eval time =     150.15 ms /    64 tokens (    2.35 ms per token,   426.23 tokens per second)
       eval time =    1270.87 ms /    56 tokens (   22.69 ms per token,    44.06 tokens per second)
      total time =    1421.02 ms /   120 tokens
slot      release: id  3 | task 286 | stop processing: n_tokens = 1413, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.392 (> 0.100 thold), f_keep = 0.961
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 343 | processing task, is_child = 0
slot update_slots: id  3 | task 343 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3466
slot update_slots: id  3 | task 343 | n_tokens = 1358, memory_seq_rm [1358, end)
slot update_slots: id  3 | task 343 | prompt processing progress, n_tokens = 3402, batch.n_tokens = 2044, progress = 0.981535
slot update_slots: id  3 | task 343 | n_tokens = 3402, memory_seq_rm [3402, end)
slot update_slots: id  3 | task 343 | prompt processing progress, n_tokens = 3466, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 343 | prompt done, n_tokens = 3466, batch.n_tokens = 64
slot init_sampler: id  3 | task 343 | init sampler, took 0.60 ms, tokens: text = 3466, total = 3466
slot update_slots: id  3 | task 343 | created context checkpoint 5 of 8 (pos_min = 2378, pos_max = 3401, size = 24.012 MiB)
slot print_timing: id  3 | task 343 | 
prompt eval time =    2003.19 ms /  2108 tokens (    0.95 ms per token,  1052.32 tokens per second)
       eval time =    1030.66 ms /    46 tokens (   22.41 ms per token,    44.63 tokens per second)
      total time =    3033.84 ms /  2154 tokens
slot      release: id  3 | task 343 | stop processing: n_tokens = 3511, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.774 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 391 | processing task, is_child = 0
slot update_slots: id  3 | task 391 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4476
slot update_slots: id  3 | task 391 | n_tokens = 3466, memory_seq_rm [3466, end)
slot update_slots: id  3 | task 391 | prompt processing progress, n_tokens = 4412, batch.n_tokens = 946, progress = 0.985702
slot update_slots: id  3 | task 391 | n_tokens = 4412, memory_seq_rm [4412, end)
slot update_slots: id  3 | task 391 | prompt processing progress, n_tokens = 4476, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 391 | prompt done, n_tokens = 4476, batch.n_tokens = 64
slot init_sampler: id  3 | task 391 | init sampler, took 0.76 ms, tokens: text = 4476, total = 4476
slot update_slots: id  3 | task 391 | created context checkpoint 6 of 8 (pos_min = 3388, pos_max = 4411, size = 24.012 MiB)
slot print_timing: id  3 | task 391 | 
prompt eval time =    1052.99 ms /  1010 tokens (    1.04 ms per token,   959.18 tokens per second)
       eval time =    1998.69 ms /    86 tokens (   23.24 ms per token,    43.03 tokens per second)
      total time =    3051.68 ms /  1096 tokens
slot      release: id  3 | task 391 | stop processing: n_tokens = 4561, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.929 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 479 | processing task, is_child = 0
slot update_slots: id  3 | task 479 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4820
slot update_slots: id  3 | task 479 | n_tokens = 4476, memory_seq_rm [4476, end)
slot update_slots: id  3 | task 479 | prompt processing progress, n_tokens = 4756, batch.n_tokens = 280, progress = 0.986722
slot update_slots: id  3 | task 479 | n_tokens = 4756, memory_seq_rm [4756, end)
slot update_slots: id  3 | task 479 | prompt processing progress, n_tokens = 4820, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 479 | prompt done, n_tokens = 4820, batch.n_tokens = 64
slot init_sampler: id  3 | task 479 | init sampler, took 0.74 ms, tokens: text = 4820, total = 4820
slot update_slots: id  3 | task 479 | created context checkpoint 7 of 8 (pos_min = 3732, pos_max = 4755, size = 24.012 MiB)
slot print_timing: id  3 | task 479 | 
prompt eval time =     485.01 ms /   344 tokens (    1.41 ms per token,   709.27 tokens per second)
       eval time =    2583.20 ms /   108 tokens (   23.92 ms per token,    41.81 tokens per second)
      total time =    3068.20 ms /   452 tokens
slot      release: id  3 | task 479 | stop processing: n_tokens = 4927, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.828 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 589 | processing task, is_child = 0
slot update_slots: id  3 | task 589 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5818
slot update_slots: id  3 | task 589 | n_tokens = 4820, memory_seq_rm [4820, end)
slot update_slots: id  3 | task 589 | prompt processing progress, n_tokens = 5754, batch.n_tokens = 934, progress = 0.989000
slot update_slots: id  3 | task 589 | n_tokens = 5754, memory_seq_rm [5754, end)
slot update_slots: id  3 | task 589 | prompt processing progress, n_tokens = 5818, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 589 | prompt done, n_tokens = 5818, batch.n_tokens = 64
slot init_sampler: id  3 | task 589 | init sampler, took 1.06 ms, tokens: text = 5818, total = 5818
slot update_slots: id  3 | task 589 | created context checkpoint 8 of 8 (pos_min = 4730, pos_max = 5753, size = 24.012 MiB)
slot print_timing: id  3 | task 589 | 
prompt eval time =    1113.22 ms /   998 tokens (    1.12 ms per token,   896.50 tokens per second)
       eval time =   57127.66 ms /  2373 tokens (   24.07 ms per token,    41.54 tokens per second)
      total time =   58240.88 ms /  3371 tokens
slot      release: id  3 | task 589 | stop processing: n_tokens = 8190, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.835 (> 0.100 thold), f_keep = 0.710
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2964 | processing task, is_child = 0
slot update_slots: id  3 | task 2964 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6971
slot update_slots: id  3 | task 2964 | n_past = 5818, slot.prompt.tokens.size() = 8190, seq_id = 3, pos_min = 7166, n_swa = 128
slot update_slots: id  3 | task 2964 | restored context checkpoint (pos_min = 4730, pos_max = 5753, size = 24.012 MiB)
slot update_slots: id  3 | task 2964 | n_tokens = 5753, memory_seq_rm [5753, end)
slot update_slots: id  3 | task 2964 | prompt processing progress, n_tokens = 6907, batch.n_tokens = 1154, progress = 0.990819
slot update_slots: id  3 | task 2964 | n_tokens = 6907, memory_seq_rm [6907, end)
slot update_slots: id  3 | task 2964 | prompt processing progress, n_tokens = 6971, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2964 | prompt done, n_tokens = 6971, batch.n_tokens = 64
slot init_sampler: id  3 | task 2964 | init sampler, took 1.14 ms, tokens: text = 6971, total = 6971
slot update_slots: id  3 | task 2964 | erasing old context checkpoint (pos_min = 0, pos_max = 395, size = 9.286 MiB)
slot update_slots: id  3 | task 2964 | created context checkpoint 8 of 8 (pos_min = 5883, pos_max = 6906, size = 24.012 MiB)
slot print_timing: id  3 | task 2964 | 
prompt eval time =    1449.23 ms /  1218 tokens (    1.19 ms per token,   840.45 tokens per second)
       eval time =   30866.62 ms /  1294 tokens (   23.85 ms per token,    41.92 tokens per second)
      total time =   32315.85 ms /  2512 tokens
slot      release: id  3 | task 2964 | stop processing: n_tokens = 8264, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.847 (> 0.100 thold), f_keep = 0.844
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4260 | processing task, is_child = 0
slot update_slots: id  3 | task 4260 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8228
slot update_slots: id  3 | task 4260 | n_past = 6971, slot.prompt.tokens.size() = 8264, seq_id = 3, pos_min = 7240, n_swa = 128
slot update_slots: id  3 | task 4260 | restored context checkpoint (pos_min = 5883, pos_max = 6906, size = 24.012 MiB)
slot update_slots: id  3 | task 4260 | n_tokens = 6906, memory_seq_rm [6906, end)
slot update_slots: id  3 | task 4260 | prompt processing progress, n_tokens = 8164, batch.n_tokens = 1258, progress = 0.992222
slot update_slots: id  3 | task 4260 | n_tokens = 8164, memory_seq_rm [8164, end)
slot update_slots: id  3 | task 4260 | prompt processing progress, n_tokens = 8228, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4260 | prompt done, n_tokens = 8228, batch.n_tokens = 64
slot init_sampler: id  3 | task 4260 | init sampler, took 2.35 ms, tokens: text = 8228, total = 8228
slot update_slots: id  3 | task 4260 | erasing old context checkpoint (pos_min = 0, pos_max = 460, size = 10.810 MiB)
slot update_slots: id  3 | task 4260 | created context checkpoint 8 of 8 (pos_min = 7140, pos_max = 8163, size = 24.012 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 4260
slot      release: id  3 | task 4260 | stop processing: n_tokens = 9398, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.054
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9398, total state size = 244.385 MiB
srv          load:  - looking for better prompt, base f_keep = 0.054, sim = 1.000
srv        update:  - cache state: 2 prompts, 524.557 MiB (limits: 8192.000 MiB, 64000 tokens, 164196 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv  get_availabl: prompt cache update took 317.50 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5434 | processing task, is_child = 0
slot update_slots: id  3 | task 5434 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 508
slot update_slots: id  3 | task 5434 | n_past = 508, slot.prompt.tokens.size() = 9398, seq_id = 3, pos_min = 8374, n_swa = 128
slot update_slots: id  3 | task 5434 | restored context checkpoint (pos_min = 0, pos_max = 978, size = 22.957 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 381, pos_max = 1229, n_swa = 128, size = 19.908 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 2378, pos_max = 3401, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 3388, pos_max = 4411, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 3732, pos_max = 4755, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 4730, pos_max = 5753, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 5883, pos_max = 6906, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 7140, pos_max = 8163, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | need to evaluate at least 1 token for each active slot (n_past = 508, task.n_tokens() = 508)
slot update_slots: id  3 | task 5434 | n_past was set to 507
slot update_slots: id  3 | task 5434 | n_tokens = 507, memory_seq_rm [507, end)
slot update_slots: id  3 | task 5434 | prompt processing progress, n_tokens = 508, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  3 | task 5434 | prompt done, n_tokens = 508, batch.n_tokens = 1
slot init_sampler: id  3 | task 5434 | init sampler, took 0.08 ms, tokens: text = 508, total = 508
slot print_timing: id  3 | task 5434 | 
prompt eval time =      50.10 ms /     1 tokens (   50.10 ms per token,    19.96 tokens per second)
       eval time =    1271.03 ms /    53 tokens (   23.98 ms per token,    41.70 tokens per second)
      total time =    1321.13 ms /    54 tokens
slot      release: id  3 | task 5434 | stop processing: n_tokens = 560, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.887 (> 0.100 thold), f_keep = 0.907
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5488 | processing task, is_child = 0
slot update_slots: id  3 | task 5488 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 573
slot update_slots: id  3 | task 5488 | n_tokens = 508, memory_seq_rm [508, end)
slot update_slots: id  3 | task 5488 | prompt processing progress, n_tokens = 509, batch.n_tokens = 1, progress = 0.888307
slot update_slots: id  3 | task 5488 | n_tokens = 509, memory_seq_rm [509, end)
slot update_slots: id  3 | task 5488 | prompt processing progress, n_tokens = 573, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5488 | prompt done, n_tokens = 573, batch.n_tokens = 64
slot init_sampler: id  3 | task 5488 | init sampler, took 0.14 ms, tokens: text = 573, total = 573
slot print_timing: id  3 | task 5488 | 
prompt eval time =     284.62 ms /    65 tokens (    4.38 ms per token,   228.38 tokens per second)
       eval time =     956.40 ms /    43 tokens (   22.24 ms per token,    44.96 tokens per second)
      total time =    1241.01 ms /   108 tokens
slot      release: id  3 | task 5488 | stop processing: n_tokens = 615, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.526 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5533 | processing task, is_child = 0
slot update_slots: id  3 | task 5533 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1090
slot update_slots: id  3 | task 5533 | n_tokens = 573, memory_seq_rm [573, end)
slot update_slots: id  3 | task 5533 | prompt processing progress, n_tokens = 1026, batch.n_tokens = 453, progress = 0.941284
slot update_slots: id  3 | task 5533 | n_tokens = 1026, memory_seq_rm [1026, end)
slot update_slots: id  3 | task 5533 | prompt processing progress, n_tokens = 1090, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5533 | prompt done, n_tokens = 1090, batch.n_tokens = 64
slot init_sampler: id  3 | task 5533 | init sampler, took 0.18 ms, tokens: text = 1090, total = 1090
slot print_timing: id  3 | task 5533 | 
prompt eval time =     559.43 ms /   517 tokens (    1.08 ms per token,   924.15 tokens per second)
       eval time =    1261.06 ms /    54 tokens (   23.35 ms per token,    42.82 tokens per second)
      total time =    1820.49 ms /   571 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5533 | stop processing: n_tokens = 1143, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.809 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5589 | processing task, is_child = 0
slot update_slots: id  3 | task 5589 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1348
slot update_slots: id  3 | task 5589 | n_tokens = 1090, memory_seq_rm [1090, end)
slot update_slots: id  3 | task 5589 | prompt processing progress, n_tokens = 1284, batch.n_tokens = 194, progress = 0.952522
slot update_slots: id  3 | task 5589 | n_tokens = 1284, memory_seq_rm [1284, end)
slot update_slots: id  3 | task 5589 | prompt processing progress, n_tokens = 1348, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5589 | prompt done, n_tokens = 1348, batch.n_tokens = 64
slot init_sampler: id  3 | task 5589 | init sampler, took 0.25 ms, tokens: text = 1348, total = 1348
slot update_slots: id  3 | task 5589 | created context checkpoint 2 of 8 (pos_min = 260, pos_max = 1283, size = 24.012 MiB)
slot print_timing: id  3 | task 5589 | 
prompt eval time =     431.68 ms /   258 tokens (    1.67 ms per token,   597.66 tokens per second)
       eval time =     690.81 ms /    29 tokens (   23.82 ms per token,    41.98 tokens per second)
      total time =    1122.49 ms /   287 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5589 | stop processing: n_tokens = 1376, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5620 | processing task, is_child = 0
slot update_slots: id  3 | task 5620 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1407
slot update_slots: id  3 | task 5620 | n_tokens = 1348, memory_seq_rm [1348, end)
slot update_slots: id  3 | task 5620 | prompt processing progress, n_tokens = 1407, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 5620 | prompt done, n_tokens = 1407, batch.n_tokens = 59
slot init_sampler: id  3 | task 5620 | init sampler, took 0.27 ms, tokens: text = 1407, total = 1407
slot print_timing: id  3 | task 5620 | 
prompt eval time =     152.63 ms /    59 tokens (    2.59 ms per token,   386.56 tokens per second)
       eval time =    1010.99 ms /    42 tokens (   24.07 ms per token,    41.54 tokens per second)
      total time =    1163.62 ms /   101 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5620 | stop processing: n_tokens = 1448, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.401 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5663 | processing task, is_child = 0
slot update_slots: id  3 | task 5663 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3510
slot update_slots: id  3 | task 5663 | n_tokens = 1407, memory_seq_rm [1407, end)
slot update_slots: id  3 | task 5663 | prompt processing progress, n_tokens = 3446, batch.n_tokens = 2039, progress = 0.981766
slot update_slots: id  3 | task 5663 | n_tokens = 3446, memory_seq_rm [3446, end)
slot update_slots: id  3 | task 5663 | prompt processing progress, n_tokens = 3510, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5663 | prompt done, n_tokens = 3510, batch.n_tokens = 64
slot init_sampler: id  3 | task 5663 | init sampler, took 0.57 ms, tokens: text = 3510, total = 3510
slot update_slots: id  3 | task 5663 | created context checkpoint 3 of 8 (pos_min = 2422, pos_max = 3445, size = 24.012 MiB)
slot print_timing: id  3 | task 5663 | 
prompt eval time =    2082.99 ms /  2103 tokens (    0.99 ms per token,  1009.61 tokens per second)
       eval time =     848.18 ms /    36 tokens (   23.56 ms per token,    42.44 tokens per second)
      total time =    2931.17 ms /  2139 tokens
slot      release: id  3 | task 5663 | stop processing: n_tokens = 3545, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.777 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5701 | processing task, is_child = 0
slot update_slots: id  3 | task 5701 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4515
slot update_slots: id  3 | task 5701 | n_tokens = 3510, memory_seq_rm [3510, end)
slot update_slots: id  3 | task 5701 | prompt processing progress, n_tokens = 4451, batch.n_tokens = 941, progress = 0.985825
slot update_slots: id  3 | task 5701 | n_tokens = 4451, memory_seq_rm [4451, end)
slot update_slots: id  3 | task 5701 | prompt processing progress, n_tokens = 4515, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5701 | prompt done, n_tokens = 4515, batch.n_tokens = 64
slot init_sampler: id  3 | task 5701 | init sampler, took 0.69 ms, tokens: text = 4515, total = 4515
slot update_slots: id  3 | task 5701 | created context checkpoint 4 of 8 (pos_min = 3427, pos_max = 4450, size = 24.012 MiB)
slot print_timing: id  3 | task 5701 | 
prompt eval time =    1090.42 ms /  1005 tokens (    1.08 ms per token,   921.66 tokens per second)
       eval time =    2971.23 ms /   124 tokens (   23.96 ms per token,    41.73 tokens per second)
      total time =    4061.65 ms /  1129 tokens
slot      release: id  3 | task 5701 | stop processing: n_tokens = 4638, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.930 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5827 | processing task, is_child = 0
slot update_slots: id  3 | task 5827 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4854
slot update_slots: id  3 | task 5827 | n_tokens = 4515, memory_seq_rm [4515, end)
slot update_slots: id  3 | task 5827 | prompt processing progress, n_tokens = 4790, batch.n_tokens = 275, progress = 0.986815
slot update_slots: id  3 | task 5827 | n_tokens = 4790, memory_seq_rm [4790, end)
slot update_slots: id  3 | task 5827 | prompt processing progress, n_tokens = 4854, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5827 | prompt done, n_tokens = 4854, batch.n_tokens = 64
slot init_sampler: id  3 | task 5827 | init sampler, took 0.87 ms, tokens: text = 4854, total = 4854
slot update_slots: id  3 | task 5827 | created context checkpoint 5 of 8 (pos_min = 3766, pos_max = 4789, size = 24.012 MiB)
slot print_timing: id  3 | task 5827 | 
prompt eval time =     495.91 ms /   339 tokens (    1.46 ms per token,   683.59 tokens per second)
       eval time =   55053.84 ms /  2302 tokens (   23.92 ms per token,    41.81 tokens per second)
      total time =   55549.75 ms /  2641 tokens
slot      release: id  3 | task 5827 | stop processing: n_tokens = 7155, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.808 (> 0.100 thold), f_keep = 0.678
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8131 | processing task, is_child = 0
slot update_slots: id  3 | task 8131 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6004
slot update_slots: id  3 | task 8131 | n_past = 4854, slot.prompt.tokens.size() = 7155, seq_id = 3, pos_min = 6131, n_swa = 128
slot update_slots: id  3 | task 8131 | restored context checkpoint (pos_min = 3766, pos_max = 4789, size = 24.012 MiB)
slot update_slots: id  3 | task 8131 | n_tokens = 4789, memory_seq_rm [4789, end)
slot update_slots: id  3 | task 8131 | prompt processing progress, n_tokens = 5940, batch.n_tokens = 1151, progress = 0.989340
slot update_slots: id  3 | task 8131 | n_tokens = 5940, memory_seq_rm [5940, end)
slot update_slots: id  3 | task 8131 | prompt processing progress, n_tokens = 6004, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8131 | prompt done, n_tokens = 6004, batch.n_tokens = 64
slot init_sampler: id  3 | task 8131 | init sampler, took 0.94 ms, tokens: text = 6004, total = 6004
slot update_slots: id  3 | task 8131 | created context checkpoint 6 of 8 (pos_min = 4916, pos_max = 5939, size = 24.012 MiB)
slot print_timing: id  3 | task 8131 | 
prompt eval time =    1491.36 ms /  1215 tokens (    1.23 ms per token,   814.69 tokens per second)
       eval time =   35281.53 ms /  1483 tokens (   23.79 ms per token,    42.03 tokens per second)
      total time =   36772.89 ms /  2698 tokens
slot      release: id  3 | task 8131 | stop processing: n_tokens = 7486, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.827 (> 0.100 thold), f_keep = 0.802
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9616 | processing task, is_child = 0
slot update_slots: id  3 | task 9616 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7261
slot update_slots: id  3 | task 9616 | n_past = 6004, slot.prompt.tokens.size() = 7486, seq_id = 3, pos_min = 6462, n_swa = 128
slot update_slots: id  3 | task 9616 | restored context checkpoint (pos_min = 4916, pos_max = 5939, size = 24.012 MiB)
slot update_slots: id  3 | task 9616 | n_tokens = 5939, memory_seq_rm [5939, end)
slot update_slots: id  3 | task 9616 | prompt processing progress, n_tokens = 7197, batch.n_tokens = 1258, progress = 0.991186
slot update_slots: id  3 | task 9616 | n_tokens = 7197, memory_seq_rm [7197, end)
slot update_slots: id  3 | task 9616 | prompt processing progress, n_tokens = 7261, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9616 | prompt done, n_tokens = 7261, batch.n_tokens = 64
slot init_sampler: id  3 | task 9616 | init sampler, took 1.09 ms, tokens: text = 7261, total = 7261
slot update_slots: id  3 | task 9616 | created context checkpoint 7 of 8 (pos_min = 6173, pos_max = 7196, size = 24.012 MiB)
slot print_timing: id  3 | task 9616 | 
prompt eval time =    1556.33 ms /  1322 tokens (    1.18 ms per token,   849.43 tokens per second)
       eval time =   25539.62 ms /  1060 tokens (   24.09 ms per token,    41.50 tokens per second)
      total time =   27095.95 ms /  2382 tokens
slot      release: id  3 | task 9616 | stop processing: n_tokens = 8320, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.873
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10678 | processing task, is_child = 0
slot update_slots: id  3 | task 10678 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7340
slot update_slots: id  3 | task 10678 | n_past = 7261, slot.prompt.tokens.size() = 8320, seq_id = 3, pos_min = 7296, n_swa = 128
slot update_slots: id  3 | task 10678 | restored context checkpoint (pos_min = 6173, pos_max = 7196, size = 24.012 MiB)
slot update_slots: id  3 | task 10678 | n_tokens = 7196, memory_seq_rm [7196, end)
slot update_slots: id  3 | task 10678 | prompt processing progress, n_tokens = 7276, batch.n_tokens = 80, progress = 0.991281
slot update_slots: id  3 | task 10678 | n_tokens = 7276, memory_seq_rm [7276, end)
slot update_slots: id  3 | task 10678 | prompt processing progress, n_tokens = 7340, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10678 | prompt done, n_tokens = 7340, batch.n_tokens = 64
slot init_sampler: id  3 | task 10678 | init sampler, took 1.08 ms, tokens: text = 7340, total = 7340
slot update_slots: id  3 | task 10678 | created context checkpoint 8 of 8 (pos_min = 6252, pos_max = 7275, size = 24.012 MiB)
slot print_timing: id  3 | task 10678 | 
prompt eval time =     366.02 ms /   144 tokens (    2.54 ms per token,   393.42 tokens per second)
       eval time =  109835.82 ms /  4482 tokens (   24.51 ms per token,    40.81 tokens per second)
      total time =  110201.83 ms /  4626 tokens
slot      release: id  3 | task 10678 | stop processing: n_tokens = 11821, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.621
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15162 | processing task, is_child = 0
slot update_slots: id  3 | task 15162 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7598
slot update_slots: id  3 | task 15162 | n_past = 7340, slot.prompt.tokens.size() = 11821, seq_id = 3, pos_min = 10797, n_swa = 128
slot update_slots: id  3 | task 15162 | restored context checkpoint (pos_min = 6252, pos_max = 7275, size = 24.012 MiB)
slot update_slots: id  3 | task 15162 | n_tokens = 7275, memory_seq_rm [7275, end)
slot update_slots: id  3 | task 15162 | prompt processing progress, n_tokens = 7534, batch.n_tokens = 259, progress = 0.991577
slot update_slots: id  3 | task 15162 | n_tokens = 7534, memory_seq_rm [7534, end)
slot update_slots: id  3 | task 15162 | prompt processing progress, n_tokens = 7598, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15162 | prompt done, n_tokens = 7598, batch.n_tokens = 64
slot init_sampler: id  3 | task 15162 | init sampler, took 1.08 ms, tokens: text = 7598, total = 7598
slot update_slots: id  3 | task 15162 | erasing old context checkpoint (pos_min = 0, pos_max = 978, size = 22.957 MiB)
slot update_slots: id  3 | task 15162 | created context checkpoint 8 of 8 (pos_min = 6510, pos_max = 7533, size = 24.012 MiB)
slot print_timing: id  3 | task 15162 | 
prompt eval time =     488.43 ms /   323 tokens (    1.51 ms per token,   661.30 tokens per second)
       eval time =  152999.24 ms /  6204 tokens (   24.66 ms per token,    40.55 tokens per second)
      total time =  153487.68 ms /  6527 tokens
slot      release: id  3 | task 15162 | stop processing: n_tokens = 13801, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.869 (> 0.100 thold), f_keep = 0.551
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21368 | processing task, is_child = 0
slot update_slots: id  3 | task 21368 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8746
slot update_slots: id  3 | task 21368 | n_past = 7598, slot.prompt.tokens.size() = 13801, seq_id = 3, pos_min = 12777, n_swa = 128
slot update_slots: id  3 | task 21368 | restored context checkpoint (pos_min = 6510, pos_max = 7533, size = 24.012 MiB)
slot update_slots: id  3 | task 21368 | n_tokens = 7533, memory_seq_rm [7533, end)
slot update_slots: id  3 | task 21368 | prompt processing progress, n_tokens = 8682, batch.n_tokens = 1149, progress = 0.992682
slot update_slots: id  3 | task 21368 | n_tokens = 8682, memory_seq_rm [8682, end)
slot update_slots: id  3 | task 21368 | prompt processing progress, n_tokens = 8746, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21368 | prompt done, n_tokens = 8746, batch.n_tokens = 64
slot init_sampler: id  3 | task 21368 | init sampler, took 1.29 ms, tokens: text = 8746, total = 8746
slot update_slots: id  3 | task 21368 | erasing old context checkpoint (pos_min = 260, pos_max = 1283, size = 24.012 MiB)
slot update_slots: id  3 | task 21368 | created context checkpoint 8 of 8 (pos_min = 7658, pos_max = 8681, size = 24.012 MiB)
slot print_timing: id  3 | task 21368 | 
prompt eval time =    1497.03 ms /  1213 tokens (    1.23 ms per token,   810.27 tokens per second)
       eval time =     718.79 ms /    29 tokens (   24.79 ms per token,    40.35 tokens per second)
      total time =    2215.82 ms /  1242 tokens
slot      release: id  3 | task 21368 | stop processing: n_tokens = 8774, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21399 | processing task, is_child = 0
slot update_slots: id  3 | task 21399 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9085
slot update_slots: id  3 | task 21399 | n_tokens = 8746, memory_seq_rm [8746, end)
slot update_slots: id  3 | task 21399 | prompt processing progress, n_tokens = 9021, batch.n_tokens = 275, progress = 0.992955
slot update_slots: id  3 | task 21399 | n_tokens = 9021, memory_seq_rm [9021, end)
slot update_slots: id  3 | task 21399 | prompt processing progress, n_tokens = 9085, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21399 | prompt done, n_tokens = 9085, batch.n_tokens = 64
slot init_sampler: id  3 | task 21399 | init sampler, took 1.38 ms, tokens: text = 9085, total = 9085
slot update_slots: id  3 | task 21399 | erasing old context checkpoint (pos_min = 2422, pos_max = 3445, size = 24.012 MiB)
slot update_slots: id  3 | task 21399 | created context checkpoint 8 of 8 (pos_min = 7997, pos_max = 9020, size = 24.012 MiB)
slot print_timing: id  3 | task 21399 | 
prompt eval time =     564.44 ms /   339 tokens (    1.67 ms per token,   600.59 tokens per second)
       eval time =    7396.41 ms /   307 tokens (   24.09 ms per token,    41.51 tokens per second)
      total time =    7960.85 ms /   646 tokens
slot      release: id  3 | task 21399 | stop processing: n_tokens = 9391, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21708 | processing task, is_child = 0
slot update_slots: id  3 | task 21708 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9240
slot update_slots: id  3 | task 21708 | n_tokens = 9085, memory_seq_rm [9085, end)
slot update_slots: id  3 | task 21708 | prompt processing progress, n_tokens = 9176, batch.n_tokens = 91, progress = 0.993074
slot update_slots: id  3 | task 21708 | n_tokens = 9176, memory_seq_rm [9176, end)
slot update_slots: id  3 | task 21708 | prompt processing progress, n_tokens = 9240, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21708 | prompt done, n_tokens = 9240, batch.n_tokens = 64
slot init_sampler: id  3 | task 21708 | init sampler, took 1.37 ms, tokens: text = 9240, total = 9240
slot update_slots: id  3 | task 21708 | erasing old context checkpoint (pos_min = 3427, pos_max = 4450, size = 24.012 MiB)
slot update_slots: id  3 | task 21708 | created context checkpoint 8 of 8 (pos_min = 8367, pos_max = 9175, size = 18.970 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 21708
slot      release: id  3 | task 21708 | stop processing: n_tokens = 9556, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.948 (> 0.100 thold), f_keep = 0.048
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9556, total state size = 245.019 MiB
srv          load:  - looking for better prompt, base f_keep = 0.048, sim = 0.948
srv        update:  - cache state: 3 prompts, 956.630 MiB (limits: 8192.000 MiB, 64000 tokens, 171867 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv        update:    - prompt 0x5d0e7b4386e0:    9556 tokens, checkpoints:  8,   432.073 MiB
srv  get_availabl: prompt cache update took 328.24 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22028 | processing task, is_child = 0
slot update_slots: id  3 | task 22028 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 480
slot update_slots: id  3 | task 22028 | n_past = 455, slot.prompt.tokens.size() = 9556, seq_id = 3, pos_min = 8663, n_swa = 128
slot update_slots: id  3 | task 22028 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 3766, pos_max = 4789, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 4916, pos_max = 5939, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 6173, pos_max = 7196, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 6252, pos_max = 7275, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 6510, pos_max = 7533, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 7658, pos_max = 8681, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 7997, pos_max = 9020, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 8367, pos_max = 9175, n_swa = 128, size = 18.970 MiB)
slot update_slots: id  3 | task 22028 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 22028 | prompt processing progress, n_tokens = 416, batch.n_tokens = 416, progress = 0.866667
slot update_slots: id  3 | task 22028 | n_tokens = 416, memory_seq_rm [416, end)
slot update_slots: id  3 | task 22028 | prompt processing progress, n_tokens = 480, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22028 | prompt done, n_tokens = 480, batch.n_tokens = 64
slot init_sampler: id  3 | task 22028 | init sampler, took 0.08 ms, tokens: text = 480, total = 480
slot update_slots: id  3 | task 22028 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 415, size = 9.755 MiB)
slot print_timing: id  3 | task 22028 | 
prompt eval time =     592.86 ms /   480 tokens (    1.24 ms per token,   809.63 tokens per second)
       eval time =    2048.57 ms /    92 tokens (   22.27 ms per token,    44.91 tokens per second)
      total time =    2641.43 ms /   572 tokens
slot      release: id  3 | task 22028 | stop processing: n_tokens = 571, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.467 (> 0.100 thold), f_keep = 0.841
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22122 | processing task, is_child = 0
slot update_slots: id  3 | task 22122 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1027
slot update_slots: id  3 | task 22122 | n_tokens = 480, memory_seq_rm [480, end)
slot update_slots: id  3 | task 22122 | prompt processing progress, n_tokens = 963, batch.n_tokens = 483, progress = 0.937683
slot update_slots: id  3 | task 22122 | n_tokens = 963, memory_seq_rm [963, end)
slot update_slots: id  3 | task 22122 | prompt processing progress, n_tokens = 1027, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22122 | prompt done, n_tokens = 1027, batch.n_tokens = 64
slot init_sampler: id  3 | task 22122 | init sampler, took 0.18 ms, tokens: text = 1027, total = 1027
slot update_slots: id  3 | task 22122 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 962, size = 22.582 MiB)
slot print_timing: id  3 | task 22122 | 
prompt eval time =     576.50 ms /   547 tokens (    1.05 ms per token,   948.82 tokens per second)
       eval time =    1645.14 ms /    72 tokens (   22.85 ms per token,    43.77 tokens per second)
      total time =    2221.64 ms /   619 tokens
slot      release: id  3 | task 22122 | stop processing: n_tokens = 1098, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.491 (> 0.100 thold), f_keep = 0.935
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22196 | processing task, is_child = 0
slot update_slots: id  3 | task 22196 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2092
slot update_slots: id  3 | task 22196 | n_tokens = 1027, memory_seq_rm [1027, end)
slot update_slots: id  3 | task 22196 | prompt processing progress, n_tokens = 2028, batch.n_tokens = 1001, progress = 0.969407
slot update_slots: id  3 | task 22196 | n_tokens = 2028, memory_seq_rm [2028, end)
slot update_slots: id  3 | task 22196 | prompt processing progress, n_tokens = 2092, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22196 | prompt done, n_tokens = 2092, batch.n_tokens = 64
slot init_sampler: id  3 | task 22196 | init sampler, took 0.41 ms, tokens: text = 2092, total = 2092
slot update_slots: id  3 | task 22196 | created context checkpoint 3 of 8 (pos_min = 1004, pos_max = 2027, size = 24.012 MiB)
slot print_timing: id  3 | task 22196 | 
prompt eval time =    1078.40 ms /  1065 tokens (    1.01 ms per token,   987.57 tokens per second)
       eval time =    4087.54 ms /   176 tokens (   23.22 ms per token,    43.06 tokens per second)
      total time =    5165.94 ms /  1241 tokens
slot      release: id  3 | task 22196 | stop processing: n_tokens = 2267, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.499 (> 0.100 thold), f_keep = 0.923
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22374 | processing task, is_child = 0
slot update_slots: id  3 | task 22374 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4195
slot update_slots: id  3 | task 22374 | n_tokens = 2092, memory_seq_rm [2092, end)
slot update_slots: id  3 | task 22374 | prompt processing progress, n_tokens = 4131, batch.n_tokens = 2039, progress = 0.984744
slot update_slots: id  3 | task 22374 | n_tokens = 4131, memory_seq_rm [4131, end)
slot update_slots: id  3 | task 22374 | prompt processing progress, n_tokens = 4195, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22374 | prompt done, n_tokens = 4195, batch.n_tokens = 64
slot init_sampler: id  3 | task 22374 | init sampler, took 0.67 ms, tokens: text = 4195, total = 4195
slot update_slots: id  3 | task 22374 | created context checkpoint 4 of 8 (pos_min = 3107, pos_max = 4130, size = 24.012 MiB)
slot print_timing: id  3 | task 22374 | 
prompt eval time =    2074.30 ms /  2103 tokens (    0.99 ms per token,  1013.83 tokens per second)
       eval time =     934.42 ms /    40 tokens (   23.36 ms per token,    42.81 tokens per second)
      total time =    3008.72 ms /  2143 tokens
slot      release: id  3 | task 22374 | stop processing: n_tokens = 4234, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.807 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22416 | processing task, is_child = 0
slot update_slots: id  3 | task 22416 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5200
slot update_slots: id  3 | task 22416 | n_tokens = 4195, memory_seq_rm [4195, end)
slot update_slots: id  3 | task 22416 | prompt processing progress, n_tokens = 5136, batch.n_tokens = 941, progress = 0.987692
slot update_slots: id  3 | task 22416 | n_tokens = 5136, memory_seq_rm [5136, end)
slot update_slots: id  3 | task 22416 | prompt processing progress, n_tokens = 5200, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22416 | prompt done, n_tokens = 5200, batch.n_tokens = 64
slot init_sampler: id  3 | task 22416 | init sampler, took 0.81 ms, tokens: text = 5200, total = 5200
slot update_slots: id  3 | task 22416 | created context checkpoint 5 of 8 (pos_min = 4112, pos_max = 5135, size = 24.012 MiB)
slot print_timing: id  3 | task 22416 | 
prompt eval time =    1090.54 ms /  1005 tokens (    1.09 ms per token,   921.56 tokens per second)
       eval time =   47884.35 ms /  1997 tokens (   23.98 ms per token,    41.70 tokens per second)
      total time =   48974.89 ms /  3002 tokens
slot      release: id  3 | task 22416 | stop processing: n_tokens = 7196, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.806 (> 0.100 thold), f_keep = 0.723
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 24415 | processing task, is_child = 0
slot update_slots: id  3 | task 24415 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6455
slot update_slots: id  3 | task 24415 | n_past = 5200, slot.prompt.tokens.size() = 7196, seq_id = 3, pos_min = 6172, n_swa = 128
slot update_slots: id  3 | task 24415 | restored context checkpoint (pos_min = 4112, pos_max = 5135, size = 24.012 MiB)
slot update_slots: id  3 | task 24415 | n_tokens = 5135, memory_seq_rm [5135, end)
slot update_slots: id  3 | task 24415 | prompt processing progress, n_tokens = 6391, batch.n_tokens = 1256, progress = 0.990085
slot update_slots: id  3 | task 24415 | n_tokens = 6391, memory_seq_rm [6391, end)
slot update_slots: id  3 | task 24415 | prompt processing progress, n_tokens = 6455, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 24415 | prompt done, n_tokens = 6455, batch.n_tokens = 64
slot init_sampler: id  3 | task 24415 | init sampler, took 0.95 ms, tokens: text = 6455, total = 6455
slot update_slots: id  3 | task 24415 | created context checkpoint 6 of 8 (pos_min = 5367, pos_max = 6390, size = 24.012 MiB)
slot print_timing: id  3 | task 24415 | 
prompt eval time =    1548.48 ms /  1320 tokens (    1.17 ms per token,   852.45 tokens per second)
       eval time =   35894.03 ms /  1513 tokens (   23.72 ms per token,    42.15 tokens per second)
      total time =   37442.51 ms /  2833 tokens
slot      release: id  3 | task 24415 | stop processing: n_tokens = 7967, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.838 (> 0.100 thold), f_keep = 0.810
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25930 | processing task, is_child = 0
slot update_slots: id  3 | task 25930 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7703
slot update_slots: id  3 | task 25930 | n_past = 6455, slot.prompt.tokens.size() = 7967, seq_id = 3, pos_min = 6943, n_swa = 128
slot update_slots: id  3 | task 25930 | restored context checkpoint (pos_min = 5367, pos_max = 6390, size = 24.012 MiB)
slot update_slots: id  3 | task 25930 | n_tokens = 6390, memory_seq_rm [6390, end)
slot update_slots: id  3 | task 25930 | prompt processing progress, n_tokens = 7639, batch.n_tokens = 1249, progress = 0.991692
slot update_slots: id  3 | task 25930 | n_tokens = 7639, memory_seq_rm [7639, end)
slot update_slots: id  3 | task 25930 | prompt processing progress, n_tokens = 7703, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25930 | prompt done, n_tokens = 7703, batch.n_tokens = 64
slot init_sampler: id  3 | task 25930 | init sampler, took 1.46 ms, tokens: text = 7703, total = 7703
slot update_slots: id  3 | task 25930 | created context checkpoint 7 of 8 (pos_min = 6615, pos_max = 7638, size = 24.012 MiB)
slot print_timing: id  3 | task 25930 | 
prompt eval time =    1561.24 ms /  1313 tokens (    1.19 ms per token,   841.00 tokens per second)
       eval time =   39627.47 ms /  1645 tokens (   24.09 ms per token,    41.51 tokens per second)
      total time =   41188.71 ms /  2958 tokens
slot      release: id  3 | task 25930 | stop processing: n_tokens = 9347, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.860 (> 0.100 thold), f_keep = 0.824
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27577 | processing task, is_child = 0
slot update_slots: id  3 | task 27577 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8959
slot update_slots: id  3 | task 27577 | n_past = 7703, slot.prompt.tokens.size() = 9347, seq_id = 3, pos_min = 8323, n_swa = 128
slot update_slots: id  3 | task 27577 | restored context checkpoint (pos_min = 6615, pos_max = 7638, size = 24.012 MiB)
slot update_slots: id  3 | task 27577 | n_tokens = 7638, memory_seq_rm [7638, end)
slot update_slots: id  3 | task 27577 | prompt processing progress, n_tokens = 8895, batch.n_tokens = 1257, progress = 0.992856
slot update_slots: id  3 | task 27577 | n_tokens = 8895, memory_seq_rm [8895, end)
slot update_slots: id  3 | task 27577 | prompt processing progress, n_tokens = 8959, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 27577 | prompt done, n_tokens = 8959, batch.n_tokens = 64
slot init_sampler: id  3 | task 27577 | init sampler, took 1.33 ms, tokens: text = 8959, total = 8959
slot update_slots: id  3 | task 27577 | created context checkpoint 8 of 8 (pos_min = 7871, pos_max = 8894, size = 24.012 MiB)
slot print_timing: id  3 | task 27577 | 
prompt eval time =    1542.67 ms /  1321 tokens (    1.17 ms per token,   856.31 tokens per second)
       eval time =    1150.69 ms /    47 tokens (   24.48 ms per token,    40.84 tokens per second)
      total time =    2693.37 ms /  1368 tokens
slot      release: id  3 | task 27577 | stop processing: n_tokens = 9005, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27626 | processing task, is_child = 0
slot update_slots: id  3 | task 27626 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9421
slot update_slots: id  3 | task 27626 | n_tokens = 8959, memory_seq_rm [8959, end)
slot update_slots: id  3 | task 27626 | prompt processing progress, n_tokens = 9357, batch.n_tokens = 398, progress = 0.993207
slot update_slots: id  3 | task 27626 | n_tokens = 9357, memory_seq_rm [9357, end)
slot update_slots: id  3 | task 27626 | prompt processing progress, n_tokens = 9421, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 27626 | prompt done, n_tokens = 9421, batch.n_tokens = 64
slot init_sampler: id  3 | task 27626 | init sampler, took 1.37 ms, tokens: text = 9421, total = 9421
slot update_slots: id  3 | task 27626 | erasing old context checkpoint (pos_min = 0, pos_max = 415, size = 9.755 MiB)
slot update_slots: id  3 | task 27626 | created context checkpoint 8 of 8 (pos_min = 8333, pos_max = 9356, size = 24.012 MiB)
slot print_timing: id  3 | task 27626 | 
prompt eval time =     588.80 ms /   462 tokens (    1.27 ms per token,   784.65 tokens per second)
       eval time =   30882.86 ms /  1277 tokens (   24.18 ms per token,    41.35 tokens per second)
      total time =   31471.65 ms /  1739 tokens
slot      release: id  3 | task 27626 | stop processing: n_tokens = 10697, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.883 (> 0.100 thold), f_keep = 0.881
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28905 | processing task, is_child = 0
slot update_slots: id  3 | task 28905 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10674
slot update_slots: id  3 | task 28905 | n_past = 9421, slot.prompt.tokens.size() = 10697, seq_id = 3, pos_min = 9673, n_swa = 128
slot update_slots: id  3 | task 28905 | restored context checkpoint (pos_min = 8333, pos_max = 9356, size = 24.012 MiB)
slot update_slots: id  3 | task 28905 | n_tokens = 9356, memory_seq_rm [9356, end)
slot update_slots: id  3 | task 28905 | prompt processing progress, n_tokens = 10610, batch.n_tokens = 1254, progress = 0.994004
slot update_slots: id  3 | task 28905 | n_tokens = 10610, memory_seq_rm [10610, end)
slot update_slots: id  3 | task 28905 | prompt processing progress, n_tokens = 10674, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 28905 | prompt done, n_tokens = 10674, batch.n_tokens = 64
slot init_sampler: id  3 | task 28905 | init sampler, took 1.55 ms, tokens: text = 10674, total = 10674
slot update_slots: id  3 | task 28905 | erasing old context checkpoint (pos_min = 0, pos_max = 962, size = 22.582 MiB)
slot update_slots: id  3 | task 28905 | created context checkpoint 8 of 8 (pos_min = 9586, pos_max = 10609, size = 24.012 MiB)
slot print_timing: id  3 | task 28905 | 
prompt eval time =    1592.00 ms /  1318 tokens (    1.21 ms per token,   827.89 tokens per second)
       eval time =    1564.04 ms /    63 tokens (   24.83 ms per token,    40.28 tokens per second)
      total time =    3156.04 ms /  1381 tokens
slot      release: id  3 | task 28905 | stop processing: n_tokens = 10736, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.956 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28970 | processing task, is_child = 0
slot update_slots: id  3 | task 28970 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11164
slot update_slots: id  3 | task 28970 | n_tokens = 10674, memory_seq_rm [10674, end)
slot update_slots: id  3 | task 28970 | prompt processing progress, n_tokens = 11100, batch.n_tokens = 426, progress = 0.994267
slot update_slots: id  3 | task 28970 | n_tokens = 11100, memory_seq_rm [11100, end)
slot update_slots: id  3 | task 28970 | prompt processing progress, n_tokens = 11164, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 28970 | prompt done, n_tokens = 11164, batch.n_tokens = 64
slot init_sampler: id  3 | task 28970 | init sampler, took 1.65 ms, tokens: text = 11164, total = 11164
slot update_slots: id  3 | task 28970 | erasing old context checkpoint (pos_min = 1004, pos_max = 2027, size = 24.012 MiB)
slot update_slots: id  3 | task 28970 | created context checkpoint 8 of 8 (pos_min = 10076, pos_max = 11099, size = 24.012 MiB)
slot print_timing: id  3 | task 28970 | 
prompt eval time =     732.19 ms /   490 tokens (    1.49 ms per token,   669.23 tokens per second)
       eval time =    1762.71 ms /    73 tokens (   24.15 ms per token,    41.41 tokens per second)
      total time =    2494.90 ms /   563 tokens
slot      release: id  3 | task 28970 | stop processing: n_tokens = 11236, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29045 | processing task, is_child = 0
slot update_slots: id  3 | task 29045 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11467
slot update_slots: id  3 | task 29045 | n_tokens = 11164, memory_seq_rm [11164, end)
slot update_slots: id  3 | task 29045 | prompt processing progress, n_tokens = 11403, batch.n_tokens = 239, progress = 0.994419
slot update_slots: id  3 | task 29045 | n_tokens = 11403, memory_seq_rm [11403, end)
slot update_slots: id  3 | task 29045 | prompt processing progress, n_tokens = 11467, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29045 | prompt done, n_tokens = 11467, batch.n_tokens = 64
slot init_sampler: id  3 | task 29045 | init sampler, took 1.63 ms, tokens: text = 11467, total = 11467
slot update_slots: id  3 | task 29045 | erasing old context checkpoint (pos_min = 3107, pos_max = 4130, size = 24.012 MiB)
slot update_slots: id  3 | task 29045 | created context checkpoint 8 of 8 (pos_min = 10379, pos_max = 11402, size = 24.012 MiB)
slot print_timing: id  3 | task 29045 | 
prompt eval time =     538.83 ms /   303 tokens (    1.78 ms per token,   562.33 tokens per second)
       eval time =    2941.68 ms /   122 tokens (   24.11 ms per token,    41.47 tokens per second)
      total time =    3480.51 ms /   425 tokens
slot      release: id  3 | task 29045 | stop processing: n_tokens = 11588, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29169 | processing task, is_child = 0
slot update_slots: id  3 | task 29169 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11580
slot update_slots: id  3 | task 29169 | n_tokens = 11467, memory_seq_rm [11467, end)
slot update_slots: id  3 | task 29169 | prompt processing progress, n_tokens = 11516, batch.n_tokens = 49, progress = 0.994473
slot update_slots: id  3 | task 29169 | n_tokens = 11516, memory_seq_rm [11516, end)
slot update_slots: id  3 | task 29169 | prompt processing progress, n_tokens = 11580, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29169 | prompt done, n_tokens = 11580, batch.n_tokens = 64
slot init_sampler: id  3 | task 29169 | init sampler, took 1.64 ms, tokens: text = 11580, total = 11580
slot update_slots: id  3 | task 29169 | erasing old context checkpoint (pos_min = 4112, pos_max = 5135, size = 24.012 MiB)
slot update_slots: id  3 | task 29169 | created context checkpoint 8 of 8 (pos_min = 10564, pos_max = 11515, size = 22.324 MiB)
slot print_timing: id  3 | task 29169 | 
prompt eval time =     336.06 ms /   113 tokens (    2.97 ms per token,   336.25 tokens per second)
       eval time =    1398.35 ms /    57 tokens (   24.53 ms per token,    40.76 tokens per second)
      total time =    1734.40 ms /   170 tokens
slot      release: id  3 | task 29169 | stop processing: n_tokens = 11636, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.959 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29228 | processing task, is_child = 0
slot update_slots: id  3 | task 29228 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12070
slot update_slots: id  3 | task 29228 | n_tokens = 11580, memory_seq_rm [11580, end)
slot update_slots: id  3 | task 29228 | prompt processing progress, n_tokens = 12006, batch.n_tokens = 426, progress = 0.994698
slot update_slots: id  3 | task 29228 | n_tokens = 12006, memory_seq_rm [12006, end)
slot update_slots: id  3 | task 29228 | prompt processing progress, n_tokens = 12070, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29228 | prompt done, n_tokens = 12070, batch.n_tokens = 64
slot init_sampler: id  3 | task 29228 | init sampler, took 1.72 ms, tokens: text = 12070, total = 12070
slot update_slots: id  3 | task 29228 | erasing old context checkpoint (pos_min = 5367, pos_max = 6390, size = 24.012 MiB)
slot update_slots: id  3 | task 29228 | created context checkpoint 8 of 8 (pos_min = 10982, pos_max = 12005, size = 24.012 MiB)
slot print_timing: id  3 | task 29228 | 
prompt eval time =     637.00 ms /   490 tokens (    1.30 ms per token,   769.24 tokens per second)
       eval time =    1552.89 ms /    62 tokens (   25.05 ms per token,    39.93 tokens per second)
      total time =    2189.88 ms /   552 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 29228 | stop processing: n_tokens = 12131, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29292 | processing task, is_child = 0
slot update_slots: id  3 | task 29292 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12361
slot update_slots: id  3 | task 29292 | n_tokens = 12070, memory_seq_rm [12070, end)
slot update_slots: id  3 | task 29292 | prompt processing progress, n_tokens = 12297, batch.n_tokens = 227, progress = 0.994822
slot update_slots: id  3 | task 29292 | n_tokens = 12297, memory_seq_rm [12297, end)
slot update_slots: id  3 | task 29292 | prompt processing progress, n_tokens = 12361, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29292 | prompt done, n_tokens = 12361, batch.n_tokens = 64
slot init_sampler: id  3 | task 29292 | init sampler, took 2.31 ms, tokens: text = 12361, total = 12361
slot update_slots: id  3 | task 29292 | erasing old context checkpoint (pos_min = 6615, pos_max = 7638, size = 24.012 MiB)
slot update_slots: id  3 | task 29292 | created context checkpoint 8 of 8 (pos_min = 11273, pos_max = 12296, size = 24.012 MiB)
slot print_timing: id  3 | task 29292 | 
prompt eval time =     533.40 ms /   291 tokens (    1.83 ms per token,   545.55 tokens per second)
       eval time =    2226.17 ms /    90 tokens (   24.74 ms per token,    40.43 tokens per second)
      total time =    2759.58 ms /   381 tokens
slot      release: id  3 | task 29292 | stop processing: n_tokens = 12450, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29384 | processing task, is_child = 0
slot update_slots: id  3 | task 29384 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12463
slot update_slots: id  3 | task 29384 | n_tokens = 12361, memory_seq_rm [12361, end)
slot update_slots: id  3 | task 29384 | prompt processing progress, n_tokens = 12399, batch.n_tokens = 38, progress = 0.994865
slot update_slots: id  3 | task 29384 | n_tokens = 12399, memory_seq_rm [12399, end)
slot update_slots: id  3 | task 29384 | prompt processing progress, n_tokens = 12463, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29384 | prompt done, n_tokens = 12463, batch.n_tokens = 64
slot init_sampler: id  3 | task 29384 | init sampler, took 1.76 ms, tokens: text = 12463, total = 12463
slot update_slots: id  3 | task 29384 | erasing old context checkpoint (pos_min = 7871, pos_max = 8894, size = 24.012 MiB)
slot update_slots: id  3 | task 29384 | created context checkpoint 8 of 8 (pos_min = 11426, pos_max = 12398, size = 22.816 MiB)
slot print_timing: id  3 | task 29384 | 
prompt eval time =     315.50 ms /   102 tokens (    3.09 ms per token,   323.29 tokens per second)
       eval time =    1299.30 ms /    53 tokens (   24.52 ms per token,    40.79 tokens per second)
      total time =    1614.81 ms /   155 tokens
slot      release: id  3 | task 29384 | stop processing: n_tokens = 12515, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29439 | processing task, is_child = 0
slot update_slots: id  3 | task 29439 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12523
slot update_slots: id  3 | task 29439 | n_tokens = 12463, memory_seq_rm [12463, end)
slot update_slots: id  3 | task 29439 | prompt processing progress, n_tokens = 12523, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  3 | task 29439 | prompt done, n_tokens = 12523, batch.n_tokens = 60
slot init_sampler: id  3 | task 29439 | init sampler, took 1.80 ms, tokens: text = 12523, total = 12523
slot print_timing: id  3 | task 29439 | 
prompt eval time =     157.91 ms /    60 tokens (    2.63 ms per token,   379.97 tokens per second)
       eval time =    1361.61 ms /    56 tokens (   24.31 ms per token,    41.13 tokens per second)
      total time =    1519.51 ms /   116 tokens
slot      release: id  3 | task 29439 | stop processing: n_tokens = 12578, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29496 | processing task, is_child = 0
slot update_slots: id  3 | task 29496 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13014
slot update_slots: id  3 | task 29496 | n_tokens = 12523, memory_seq_rm [12523, end)
slot update_slots: id  3 | task 29496 | prompt processing progress, n_tokens = 12950, batch.n_tokens = 427, progress = 0.995082
slot update_slots: id  3 | task 29496 | n_tokens = 12950, memory_seq_rm [12950, end)
slot update_slots: id  3 | task 29496 | prompt processing progress, n_tokens = 13014, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29496 | prompt done, n_tokens = 13014, batch.n_tokens = 64
slot init_sampler: id  3 | task 29496 | init sampler, took 1.84 ms, tokens: text = 13014, total = 13014
slot update_slots: id  3 | task 29496 | erasing old context checkpoint (pos_min = 8333, pos_max = 9356, size = 24.012 MiB)
slot update_slots: id  3 | task 29496 | created context checkpoint 8 of 8 (pos_min = 11926, pos_max = 12949, size = 24.012 MiB)
slot print_timing: id  3 | task 29496 | 
prompt eval time =     648.23 ms /   491 tokens (    1.32 ms per token,   757.45 tokens per second)
       eval time =   59936.47 ms /  2428 tokens (   24.69 ms per token,    40.51 tokens per second)
      total time =   60584.70 ms /  2919 tokens
slot      release: id  3 | task 29496 | stop processing: n_tokens = 15441, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.843 (> 0.100 thold), f_keep = 0.843
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31926 | processing task, is_child = 0
slot update_slots: id  3 | task 31926 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15437
slot update_slots: id  3 | task 31926 | n_past = 13014, slot.prompt.tokens.size() = 15441, seq_id = 3, pos_min = 14417, n_swa = 128
slot update_slots: id  3 | task 31926 | restored context checkpoint (pos_min = 11926, pos_max = 12949, size = 24.012 MiB)
slot update_slots: id  3 | task 31926 | n_tokens = 12949, memory_seq_rm [12949, end)
slot update_slots: id  3 | task 31926 | prompt processing progress, n_tokens = 14997, batch.n_tokens = 2048, progress = 0.971497
slot update_slots: id  3 | task 31926 | n_tokens = 14997, memory_seq_rm [14997, end)
slot update_slots: id  3 | task 31926 | prompt processing progress, n_tokens = 15373, batch.n_tokens = 376, progress = 0.995854
slot update_slots: id  3 | task 31926 | n_tokens = 15373, memory_seq_rm [15373, end)
slot update_slots: id  3 | task 31926 | prompt processing progress, n_tokens = 15437, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31926 | prompt done, n_tokens = 15437, batch.n_tokens = 64
slot init_sampler: id  3 | task 31926 | init sampler, took 2.23 ms, tokens: text = 15437, total = 15437
slot update_slots: id  3 | task 31926 | erasing old context checkpoint (pos_min = 9586, pos_max = 10609, size = 24.012 MiB)
slot update_slots: id  3 | task 31926 | created context checkpoint 8 of 8 (pos_min = 14349, pos_max = 15372, size = 24.012 MiB)
slot print_timing: id  3 | task 31926 | 
prompt eval time =    2922.05 ms /  2488 tokens (    1.17 ms per token,   851.46 tokens per second)
       eval time =     810.52 ms /    33 tokens (   24.56 ms per token,    40.71 tokens per second)
      total time =    3732.57 ms /  2521 tokens
slot      release: id  3 | task 31926 | stop processing: n_tokens = 15469, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31962 | processing task, is_child = 0
slot update_slots: id  3 | task 31962 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15780
slot update_slots: id  3 | task 31962 | n_tokens = 15437, memory_seq_rm [15437, end)
slot update_slots: id  3 | task 31962 | prompt processing progress, n_tokens = 15716, batch.n_tokens = 279, progress = 0.995944
slot update_slots: id  3 | task 31962 | n_tokens = 15716, memory_seq_rm [15716, end)
slot update_slots: id  3 | task 31962 | prompt processing progress, n_tokens = 15780, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31962 | prompt done, n_tokens = 15780, batch.n_tokens = 64
slot init_sampler: id  3 | task 31962 | init sampler, took 2.40 ms, tokens: text = 15780, total = 15780
slot update_slots: id  3 | task 31962 | erasing old context checkpoint (pos_min = 10076, pos_max = 11099, size = 24.012 MiB)
slot update_slots: id  3 | task 31962 | created context checkpoint 8 of 8 (pos_min = 14692, pos_max = 15715, size = 24.012 MiB)
slot print_timing: id  3 | task 31962 | 
prompt eval time =     617.46 ms /   343 tokens (    1.80 ms per token,   555.51 tokens per second)
       eval time =   26877.93 ms /  1075 tokens (   25.00 ms per token,    40.00 tokens per second)
      total time =   27495.38 ms /  1418 tokens
slot      release: id  3 | task 31962 | stop processing: n_tokens = 16854, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.936
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33039 | processing task, is_child = 0
slot update_slots: id  3 | task 33039 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16100
slot update_slots: id  3 | task 33039 | n_past = 15780, slot.prompt.tokens.size() = 16854, seq_id = 3, pos_min = 15830, n_swa = 128
slot update_slots: id  3 | task 33039 | restored context checkpoint (pos_min = 14692, pos_max = 15715, size = 24.012 MiB)
slot update_slots: id  3 | task 33039 | n_tokens = 15715, memory_seq_rm [15715, end)
slot update_slots: id  3 | task 33039 | prompt processing progress, n_tokens = 16036, batch.n_tokens = 321, progress = 0.996025
slot update_slots: id  3 | task 33039 | n_tokens = 16036, memory_seq_rm [16036, end)
slot update_slots: id  3 | task 33039 | prompt processing progress, n_tokens = 16100, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33039 | prompt done, n_tokens = 16100, batch.n_tokens = 64
slot init_sampler: id  3 | task 33039 | init sampler, took 2.27 ms, tokens: text = 16100, total = 16100
slot update_slots: id  3 | task 33039 | erasing old context checkpoint (pos_min = 10379, pos_max = 11402, size = 24.012 MiB)
slot update_slots: id  3 | task 33039 | created context checkpoint 8 of 8 (pos_min = 15012, pos_max = 16035, size = 24.012 MiB)
slot print_timing: id  3 | task 33039 | 
prompt eval time =     623.17 ms /   385 tokens (    1.62 ms per token,   617.81 tokens per second)
       eval time =   11186.33 ms /   446 tokens (   25.08 ms per token,    39.87 tokens per second)
      total time =   11809.50 ms /   831 tokens
slot      release: id  3 | task 33039 | stop processing: n_tokens = 16545, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33487 | processing task, is_child = 0
slot update_slots: id  3 | task 33487 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16340
slot update_slots: id  3 | task 33487 | n_tokens = 16100, memory_seq_rm [16100, end)
slot update_slots: id  3 | task 33487 | prompt processing progress, n_tokens = 16276, batch.n_tokens = 176, progress = 0.996083
slot update_slots: id  3 | task 33487 | n_tokens = 16276, memory_seq_rm [16276, end)
slot update_slots: id  3 | task 33487 | prompt processing progress, n_tokens = 16340, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33487 | prompt done, n_tokens = 16340, batch.n_tokens = 64
slot init_sampler: id  3 | task 33487 | init sampler, took 2.27 ms, tokens: text = 16340, total = 16340
slot update_slots: id  3 | task 33487 | erasing old context checkpoint (pos_min = 10564, pos_max = 11515, size = 22.324 MiB)
slot update_slots: id  3 | task 33487 | created context checkpoint 8 of 8 (pos_min = 15536, pos_max = 16275, size = 17.353 MiB)
slot print_timing: id  3 | task 33487 | 
prompt eval time =     488.33 ms /   240 tokens (    2.03 ms per token,   491.47 tokens per second)
       eval time =     903.53 ms /    37 tokens (   24.42 ms per token,    40.95 tokens per second)
      total time =    1391.86 ms /   277 tokens
slot      release: id  3 | task 33487 | stop processing: n_tokens = 16376, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33526 | processing task, is_child = 0
slot update_slots: id  3 | task 33526 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16420
slot update_slots: id  3 | task 33526 | n_tokens = 16340, memory_seq_rm [16340, end)
slot update_slots: id  3 | task 33526 | prompt processing progress, n_tokens = 16356, batch.n_tokens = 16, progress = 0.996102
slot update_slots: id  3 | task 33526 | n_tokens = 16356, memory_seq_rm [16356, end)
slot update_slots: id  3 | task 33526 | prompt processing progress, n_tokens = 16420, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33526 | prompt done, n_tokens = 16420, batch.n_tokens = 64
slot init_sampler: id  3 | task 33526 | init sampler, took 3.14 ms, tokens: text = 16420, total = 16420
slot update_slots: id  3 | task 33526 | erasing old context checkpoint (pos_min = 10982, pos_max = 12005, size = 24.012 MiB)
slot update_slots: id  3 | task 33526 | created context checkpoint 8 of 8 (pos_min = 15636, pos_max = 16355, size = 16.884 MiB)
slot print_timing: id  3 | task 33526 | 
prompt eval time =     286.60 ms /    80 tokens (    3.58 ms per token,   279.14 tokens per second)
       eval time =    2032.77 ms /    81 tokens (   25.10 ms per token,    39.85 tokens per second)
      total time =    2319.36 ms /   161 tokens
slot      release: id  3 | task 33526 | stop processing: n_tokens = 16500, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 33609 | processing task, is_child = 0
slot update_slots: id  2 | task 33609 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9340
slot update_slots: id  2 | task 33609 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.219272
slot update_slots: id  2 | task 33609 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.438544
slot update_slots: id  2 | task 33609 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.657816
slot update_slots: id  2 | task 33609 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.877088
slot update_slots: id  2 | task 33609 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 9276, batch.n_tokens = 1084, progress = 0.993148
slot update_slots: id  2 | task 33609 | n_tokens = 9276, memory_seq_rm [9276, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 9340, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 33609 | prompt done, n_tokens = 9340, batch.n_tokens = 64
slot init_sampler: id  2 | task 33609 | init sampler, took 2.12 ms, tokens: text = 9340, total = 9340
slot update_slots: id  2 | task 33609 | created context checkpoint 1 of 8 (pos_min = 8379, pos_max = 9275, size = 21.034 MiB)
slot print_timing: id  2 | task 33609 | 
prompt eval time =   11372.96 ms /  9340 tokens (    1.22 ms per token,   821.25 tokens per second)
       eval time =    1352.88 ms /    52 tokens (   26.02 ms per token,    38.44 tokens per second)
      total time =   12725.84 ms /  9392 tokens
slot      release: id  2 | task 33609 | stop processing: n_tokens = 9391, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 33667 | processing task, is_child = 0
slot update_slots: id  2 | task 33667 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9446
slot update_slots: id  2 | task 33667 | n_tokens = 9340, memory_seq_rm [9340, end)
slot update_slots: id  2 | task 33667 | prompt processing progress, n_tokens = 9382, batch.n_tokens = 42, progress = 0.993225
slot update_slots: id  2 | task 33667 | n_tokens = 9382, memory_seq_rm [9382, end)
slot update_slots: id  2 | task 33667 | prompt processing progress, n_tokens = 9446, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 33667 | prompt done, n_tokens = 9446, batch.n_tokens = 64
slot init_sampler: id  2 | task 33667 | init sampler, took 1.35 ms, tokens: text = 9446, total = 9446
slot update_slots: id  2 | task 33667 | created context checkpoint 2 of 8 (pos_min = 8494, pos_max = 9381, size = 20.823 MiB)
slot print_timing: id  2 | task 33667 | 
prompt eval time =     349.37 ms /   106 tokens (    3.30 ms per token,   303.41 tokens per second)
       eval time =    2250.61 ms /    87 tokens (   25.87 ms per token,    38.66 tokens per second)
      total time =    2599.98 ms /   193 tokens
slot      release: id  2 | task 33667 | stop processing: n_tokens = 9532, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.811 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 33756 | processing task, is_child = 0
slot update_slots: id  2 | task 33756 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11653
slot update_slots: id  2 | task 33756 | n_tokens = 9446, memory_seq_rm [9446, end)
slot update_slots: id  2 | task 33756 | prompt processing progress, n_tokens = 11494, batch.n_tokens = 2048, progress = 0.986355
slot update_slots: id  2 | task 33756 | n_tokens = 11494, memory_seq_rm [11494, end)
slot update_slots: id  2 | task 33756 | prompt processing progress, n_tokens = 11589, batch.n_tokens = 95, progress = 0.994508
slot update_slots: id  2 | task 33756 | n_tokens = 11589, memory_seq_rm [11589, end)
slot update_slots: id  2 | task 33756 | prompt processing progress, n_tokens = 11653, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 33756 | prompt done, n_tokens = 11653, batch.n_tokens = 64
slot init_sampler: id  2 | task 33756 | init sampler, took 1.73 ms, tokens: text = 11653, total = 11653
slot update_slots: id  2 | task 33756 | created context checkpoint 3 of 8 (pos_min = 10692, pos_max = 11588, size = 21.034 MiB)
slot print_timing: id  2 | task 33756 | 
prompt eval time =    3075.22 ms /  2207 tokens (    1.39 ms per token,   717.67 tokens per second)
       eval time =   37055.03 ms /  1391 tokens (   26.64 ms per token,    37.54 tokens per second)
      total time =   40130.25 ms /  3598 tokens
slot      release: id  2 | task 33756 | stop processing: n_tokens = 13043, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.903 (> 0.100 thold), f_keep = 0.893
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 35150 | processing task, is_child = 0
slot update_slots: id  2 | task 35150 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12899
slot update_slots: id  2 | task 35150 | n_past = 11653, slot.prompt.tokens.size() = 13043, seq_id = 2, pos_min = 12146, n_swa = 128
slot update_slots: id  2 | task 35150 | restored context checkpoint (pos_min = 10692, pos_max = 11588, size = 21.034 MiB)
slot update_slots: id  2 | task 35150 | n_tokens = 11588, memory_seq_rm [11588, end)
slot update_slots: id  2 | task 35150 | prompt processing progress, n_tokens = 12835, batch.n_tokens = 1247, progress = 0.995038
slot update_slots: id  2 | task 35150 | n_tokens = 12835, memory_seq_rm [12835, end)
slot update_slots: id  2 | task 35150 | prompt processing progress, n_tokens = 12899, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 35150 | prompt done, n_tokens = 12899, batch.n_tokens = 64
slot init_sampler: id  2 | task 35150 | init sampler, took 2.05 ms, tokens: text = 12899, total = 12899
slot update_slots: id  2 | task 35150 | created context checkpoint 4 of 8 (pos_min = 11938, pos_max = 12834, size = 21.034 MiB)
slot print_timing: id  2 | task 35150 | 
prompt eval time =    2130.93 ms /  1311 tokens (    1.63 ms per token,   615.23 tokens per second)
       eval time =   34502.73 ms /  1311 tokens (   26.32 ms per token,    38.00 tokens per second)
      total time =   36633.66 ms /  2622 tokens
slot      release: id  2 | task 35150 | stop processing: n_tokens = 14209, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.912 (> 0.100 thold), f_keep = 0.908
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36463 | processing task, is_child = 0
slot update_slots: id  2 | task 36463 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14145
slot update_slots: id  2 | task 36463 | n_past = 12899, slot.prompt.tokens.size() = 14209, seq_id = 2, pos_min = 13312, n_swa = 128
slot update_slots: id  2 | task 36463 | restored context checkpoint (pos_min = 11938, pos_max = 12834, size = 21.034 MiB)
slot update_slots: id  2 | task 36463 | n_tokens = 12834, memory_seq_rm [12834, end)
slot update_slots: id  2 | task 36463 | prompt processing progress, n_tokens = 14081, batch.n_tokens = 1247, progress = 0.995475
slot update_slots: id  2 | task 36463 | n_tokens = 14081, memory_seq_rm [14081, end)
slot update_slots: id  2 | task 36463 | prompt processing progress, n_tokens = 14145, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36463 | prompt done, n_tokens = 14145, batch.n_tokens = 64
slot init_sampler: id  2 | task 36463 | init sampler, took 2.04 ms, tokens: text = 14145, total = 14145
slot update_slots: id  2 | task 36463 | created context checkpoint 5 of 8 (pos_min = 13184, pos_max = 14080, size = 21.034 MiB)
slot print_timing: id  2 | task 36463 | 
prompt eval time =    2119.39 ms /  1311 tokens (    1.62 ms per token,   618.57 tokens per second)
       eval time =    8487.38 ms /   320 tokens (   26.52 ms per token,    37.70 tokens per second)
      total time =   10606.77 ms /  1631 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 36463 | stop processing: n_tokens = 14464, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36785 | processing task, is_child = 0
slot update_slots: id  2 | task 36785 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14222
slot update_slots: id  2 | task 36785 | n_tokens = 14145, memory_seq_rm [14145, end)
slot update_slots: id  2 | task 36785 | prompt processing progress, n_tokens = 14158, batch.n_tokens = 13, progress = 0.995500
slot update_slots: id  2 | task 36785 | n_tokens = 14158, memory_seq_rm [14158, end)
slot update_slots: id  2 | task 36785 | prompt processing progress, n_tokens = 14222, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36785 | prompt done, n_tokens = 14222, batch.n_tokens = 64
slot init_sampler: id  2 | task 36785 | init sampler, took 2.68 ms, tokens: text = 14222, total = 14222
slot update_slots: id  2 | task 36785 | created context checkpoint 6 of 8 (pos_min = 13567, pos_max = 14157, size = 13.859 MiB)
slot print_timing: id  2 | task 36785 | 
prompt eval time =     298.55 ms /    77 tokens (    3.88 ms per token,   257.91 tokens per second)
       eval time =   33543.53 ms /  1255 tokens (   26.73 ms per token,    37.41 tokens per second)
      total time =   33842.08 ms /  1332 tokens
slot      release: id  2 | task 36785 | stop processing: n_tokens = 15476, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.919 (> 0.100 thold), f_keep = 0.919
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 38042 | processing task, is_child = 0
slot update_slots: id  2 | task 38042 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15468
slot update_slots: id  2 | task 38042 | n_past = 14222, slot.prompt.tokens.size() = 15476, seq_id = 2, pos_min = 14579, n_swa = 128
slot update_slots: id  2 | task 38042 | restored context checkpoint (pos_min = 13567, pos_max = 14157, size = 13.859 MiB)
slot update_slots: id  2 | task 38042 | n_tokens = 14157, memory_seq_rm [14157, end)
slot update_slots: id  2 | task 38042 | prompt processing progress, n_tokens = 15404, batch.n_tokens = 1247, progress = 0.995862
slot update_slots: id  2 | task 38042 | n_tokens = 15404, memory_seq_rm [15404, end)
slot update_slots: id  2 | task 38042 | prompt processing progress, n_tokens = 15468, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 38042 | prompt done, n_tokens = 15468, batch.n_tokens = 64
slot init_sampler: id  2 | task 38042 | init sampler, took 2.19 ms, tokens: text = 15468, total = 15468
slot update_slots: id  2 | task 38042 | created context checkpoint 7 of 8 (pos_min = 14556, pos_max = 15403, size = 19.885 MiB)
slot print_timing: id  2 | task 38042 | 
prompt eval time =    2107.21 ms /  1311 tokens (    1.61 ms per token,   622.15 tokens per second)
       eval time =   36164.97 ms /  1348 tokens (   26.83 ms per token,    37.27 tokens per second)
      total time =   38272.18 ms /  2659 tokens
slot      release: id  2 | task 38042 | stop processing: n_tokens = 16815, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 39392 | processing task, is_child = 0
slot update_slots: id  2 | task 39392 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15530
slot update_slots: id  2 | task 39392 | n_past = 15468, slot.prompt.tokens.size() = 16815, seq_id = 2, pos_min = 15918, n_swa = 128
slot update_slots: id  2 | task 39392 | restored context checkpoint (pos_min = 14556, pos_max = 15403, size = 19.885 MiB)
slot update_slots: id  2 | task 39392 | n_tokens = 15403, memory_seq_rm [15403, end)
slot update_slots: id  2 | task 39392 | prompt processing progress, n_tokens = 15466, batch.n_tokens = 63, progress = 0.995879
slot update_slots: id  2 | task 39392 | n_tokens = 15466, memory_seq_rm [15466, end)
slot update_slots: id  2 | task 39392 | prompt processing progress, n_tokens = 15530, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 39392 | prompt done, n_tokens = 15530, batch.n_tokens = 64
slot init_sampler: id  2 | task 39392 | init sampler, took 2.21 ms, tokens: text = 15530, total = 15530
slot print_timing: id  2 | task 39392 | 
prompt eval time =     570.52 ms /   127 tokens (    4.49 ms per token,   222.61 tokens per second)
       eval time =    7473.50 ms /   279 tokens (   26.79 ms per token,    37.33 tokens per second)
      total time =    8044.02 ms /   406 tokens
slot      release: id  2 | task 39392 | stop processing: n_tokens = 15808, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 39673 | processing task, is_child = 0
slot update_slots: id  2 | task 39673 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15626
slot update_slots: id  2 | task 39673 | n_tokens = 15530, memory_seq_rm [15530, end)
slot update_slots: id  2 | task 39673 | prompt processing progress, n_tokens = 15562, batch.n_tokens = 32, progress = 0.995904
slot update_slots: id  2 | task 39673 | n_tokens = 15562, memory_seq_rm [15562, end)
slot update_slots: id  2 | task 39673 | prompt processing progress, n_tokens = 15626, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 39673 | prompt done, n_tokens = 15626, batch.n_tokens = 64
slot init_sampler: id  2 | task 39673 | init sampler, took 2.50 ms, tokens: text = 15626, total = 15626
slot update_slots: id  2 | task 39673 | created context checkpoint 8 of 8 (pos_min = 14927, pos_max = 15561, size = 14.890 MiB)
slot print_timing: id  2 | task 39673 | 
prompt eval time =     410.59 ms /    96 tokens (    4.28 ms per token,   233.81 tokens per second)
       eval time =   78288.55 ms /  2888 tokens (   27.11 ms per token,    36.89 tokens per second)
      total time =   78699.15 ms /  2984 tokens
slot      release: id  2 | task 39673 | stop processing: n_tokens = 18513, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.844
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 42563 | processing task, is_child = 0
slot update_slots: id  2 | task 42563 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15725
slot update_slots: id  2 | task 42563 | n_past = 15626, slot.prompt.tokens.size() = 18513, seq_id = 2, pos_min = 17616, n_swa = 128
slot update_slots: id  2 | task 42563 | restored context checkpoint (pos_min = 14927, pos_max = 15561, size = 14.890 MiB)
slot update_slots: id  2 | task 42563 | n_tokens = 15561, memory_seq_rm [15561, end)
slot update_slots: id  2 | task 42563 | prompt processing progress, n_tokens = 15661, batch.n_tokens = 100, progress = 0.995930
slot update_slots: id  2 | task 42563 | n_tokens = 15661, memory_seq_rm [15661, end)
slot update_slots: id  2 | task 42563 | prompt processing progress, n_tokens = 15725, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 42563 | prompt done, n_tokens = 15725, batch.n_tokens = 64
slot init_sampler: id  2 | task 42563 | init sampler, took 3.05 ms, tokens: text = 15725, total = 15725
slot update_slots: id  2 | task 42563 | erasing old context checkpoint (pos_min = 8379, pos_max = 9275, size = 21.034 MiB)
slot update_slots: id  2 | task 42563 | created context checkpoint 8 of 8 (pos_min = 14927, pos_max = 15660, size = 17.212 MiB)
slot print_timing: id  2 | task 42563 | 
prompt eval time =     627.51 ms /   164 tokens (    3.83 ms per token,   261.35 tokens per second)
       eval time =    3131.90 ms /   118 tokens (   26.54 ms per token,    37.68 tokens per second)
      total time =    3759.41 ms /   282 tokens
slot      release: id  2 | task 42563 | stop processing: n_tokens = 15842, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.937 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 42683 | processing task, is_child = 0
slot update_slots: id  2 | task 42683 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16790
slot update_slots: id  2 | task 42683 | n_tokens = 15725, memory_seq_rm [15725, end)
slot update_slots: id  2 | task 42683 | prompt processing progress, n_tokens = 16726, batch.n_tokens = 1001, progress = 0.996188
slot update_slots: id  2 | task 42683 | n_tokens = 16726, memory_seq_rm [16726, end)
slot update_slots: id  2 | task 42683 | prompt processing progress, n_tokens = 16790, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 42683 | prompt done, n_tokens = 16790, batch.n_tokens = 64
slot init_sampler: id  2 | task 42683 | init sampler, took 2.42 ms, tokens: text = 16790, total = 16790
slot update_slots: id  2 | task 42683 | erasing old context checkpoint (pos_min = 8494, pos_max = 9381, size = 20.823 MiB)
slot update_slots: id  2 | task 42683 | created context checkpoint 8 of 8 (pos_min = 15829, pos_max = 16725, size = 21.034 MiB)
slot print_timing: id  2 | task 42683 | 
prompt eval time =    1664.54 ms /  1065 tokens (    1.56 ms per token,   639.81 tokens per second)
       eval time =    1971.96 ms /    74 tokens (   26.65 ms per token,    37.53 tokens per second)
      total time =    3636.50 ms /  1139 tokens
slot      release: id  2 | task 42683 | stop processing: n_tokens = 16863, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 42759 | processing task, is_child = 0
slot update_slots: id  2 | task 42759 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17048
slot update_slots: id  2 | task 42759 | n_tokens = 16790, memory_seq_rm [16790, end)
slot update_slots: id  2 | task 42759 | prompt processing progress, n_tokens = 16984, batch.n_tokens = 194, progress = 0.996246
slot update_slots: id  2 | task 42759 | n_tokens = 16984, memory_seq_rm [16984, end)
slot update_slots: id  2 | task 42759 | prompt processing progress, n_tokens = 17048, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 42759 | prompt done, n_tokens = 17048, batch.n_tokens = 64
slot init_sampler: id  2 | task 42759 | init sampler, took 2.98 ms, tokens: text = 17048, total = 17048
slot update_slots: id  2 | task 42759 | erasing old context checkpoint (pos_min = 10692, pos_max = 11588, size = 21.034 MiB)
slot update_slots: id  2 | task 42759 | created context checkpoint 8 of 8 (pos_min = 16087, pos_max = 16983, size = 21.034 MiB)
slot print_timing: id  2 | task 42759 | 
prompt eval time =     599.08 ms /   258 tokens (    2.32 ms per token,   430.66 tokens per second)
       eval time =    4095.80 ms /   152 tokens (   26.95 ms per token,    37.11 tokens per second)
      total time =    4694.89 ms /   410 tokens
slot      release: id  2 | task 42759 | stop processing: n_tokens = 17199, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 42913 | processing task, is_child = 0
slot update_slots: id  2 | task 42913 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17108
slot update_slots: id  2 | task 42913 | n_tokens = 17048, memory_seq_rm [17048, end)
slot update_slots: id  2 | task 42913 | prompt processing progress, n_tokens = 17108, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  2 | task 42913 | prompt done, n_tokens = 17108, batch.n_tokens = 60
slot init_sampler: id  2 | task 42913 | init sampler, took 3.26 ms, tokens: text = 17108, total = 17108
slot print_timing: id  2 | task 42913 | 
prompt eval time =     179.81 ms /    60 tokens (    3.00 ms per token,   333.69 tokens per second)
       eval time =    2665.48 ms /   100 tokens (   26.65 ms per token,    37.52 tokens per second)
      total time =    2845.29 ms /   160 tokens
slot      release: id  2 | task 42913 | stop processing: n_tokens = 17207, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43014 | processing task, is_child = 0
slot update_slots: id  2 | task 43014 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17189
slot update_slots: id  2 | task 43014 | n_tokens = 17108, memory_seq_rm [17108, end)
slot update_slots: id  2 | task 43014 | prompt processing progress, n_tokens = 17125, batch.n_tokens = 17, progress = 0.996277
slot update_slots: id  2 | task 43014 | n_tokens = 17125, memory_seq_rm [17125, end)
slot update_slots: id  2 | task 43014 | prompt processing progress, n_tokens = 17189, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 43014 | prompt done, n_tokens = 17189, batch.n_tokens = 64
slot init_sampler: id  2 | task 43014 | init sampler, took 2.51 ms, tokens: text = 17189, total = 17189
slot update_slots: id  2 | task 43014 | erasing old context checkpoint (pos_min = 11938, pos_max = 12834, size = 21.034 MiB)
slot update_slots: id  2 | task 43014 | created context checkpoint 8 of 8 (pos_min = 16310, pos_max = 17124, size = 19.111 MiB)
slot print_timing: id  2 | task 43014 | 
prompt eval time =     317.29 ms /    81 tokens (    3.92 ms per token,   255.29 tokens per second)
       eval time =    3365.90 ms /   126 tokens (   26.71 ms per token,    37.43 tokens per second)
      total time =    3683.19 ms /   207 tokens
slot      release: id  2 | task 43014 | stop processing: n_tokens = 17314, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43142 | processing task, is_child = 0
slot update_slots: id  2 | task 43142 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17251
slot update_slots: id  2 | task 43142 | n_tokens = 17189, memory_seq_rm [17189, end)
slot update_slots: id  2 | task 43142 | prompt processing progress, n_tokens = 17251, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  2 | task 43142 | prompt done, n_tokens = 17251, batch.n_tokens = 62
slot init_sampler: id  2 | task 43142 | init sampler, took 2.42 ms, tokens: text = 17251, total = 17251
slot print_timing: id  2 | task 43142 | 
prompt eval time =     217.17 ms /    62 tokens (    3.50 ms per token,   285.49 tokens per second)
       eval time =   31529.59 ms /  1169 tokens (   26.97 ms per token,    37.08 tokens per second)
      total time =   31746.76 ms /  1231 tokens
slot      release: id  2 | task 43142 | stop processing: n_tokens = 18419, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.938 (> 0.100 thold), f_keep = 0.937
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44312 | processing task, is_child = 0
slot update_slots: id  2 | task 44312 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18389
slot update_slots: id  2 | task 44312 | n_past = 17251, slot.prompt.tokens.size() = 18419, seq_id = 2, pos_min = 17522, n_swa = 128
slot update_slots: id  2 | task 44312 | restored context checkpoint (pos_min = 16310, pos_max = 17124, size = 19.111 MiB)
slot update_slots: id  2 | task 44312 | n_tokens = 17124, memory_seq_rm [17124, end)
slot update_slots: id  2 | task 44312 | prompt processing progress, n_tokens = 18325, batch.n_tokens = 1201, progress = 0.996520
slot update_slots: id  2 | task 44312 | n_tokens = 18325, memory_seq_rm [18325, end)
slot update_slots: id  2 | task 44312 | prompt processing progress, n_tokens = 18389, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44312 | prompt done, n_tokens = 18389, batch.n_tokens = 64
slot init_sampler: id  2 | task 44312 | init sampler, took 3.23 ms, tokens: text = 18389, total = 18389
slot update_slots: id  2 | task 44312 | erasing old context checkpoint (pos_min = 13184, pos_max = 14080, size = 21.034 MiB)
slot update_slots: id  2 | task 44312 | created context checkpoint 8 of 8 (pos_min = 17428, pos_max = 18324, size = 21.034 MiB)
slot print_timing: id  2 | task 44312 | 
prompt eval time =    2191.41 ms /  1265 tokens (    1.73 ms per token,   577.25 tokens per second)
       eval time =     946.51 ms /    36 tokens (   26.29 ms per token,    38.03 tokens per second)
      total time =    3137.92 ms /  1301 tokens
slot      release: id  2 | task 44312 | stop processing: n_tokens = 18424, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.626 (> 0.100 thold), f_keep = 0.505
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44350 | processing task, is_child = 0
slot update_slots: id  2 | task 44350 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14855
slot update_slots: id  2 | task 44350 | n_past = 9304, slot.prompt.tokens.size() = 18424, seq_id = 2, pos_min = 17527, n_swa = 128
slot update_slots: id  2 | task 44350 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 13567, pos_max = 14157, n_swa = 128, size = 13.859 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 14556, pos_max = 15403, n_swa = 128, size = 19.885 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 14927, pos_max = 15561, n_swa = 128, size = 14.890 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 14927, pos_max = 15660, n_swa = 128, size = 17.212 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 15829, pos_max = 16725, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 16087, pos_max = 16983, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 16310, pos_max = 17124, n_swa = 128, size = 19.111 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 17428, pos_max = 18324, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 44350 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.137866
slot update_slots: id  2 | task 44350 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.275732
slot update_slots: id  2 | task 44350 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.413598
slot update_slots: id  2 | task 44350 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.551464
slot update_slots: id  2 | task 44350 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.689330
slot update_slots: id  2 | task 44350 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.827196
slot update_slots: id  2 | task 44350 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.965062
slot update_slots: id  2 | task 44350 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 14791, batch.n_tokens = 455, progress = 0.995692
slot update_slots: id  2 | task 44350 | n_tokens = 14791, memory_seq_rm [14791, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 14855, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44350 | prompt done, n_tokens = 14855, batch.n_tokens = 64
slot init_sampler: id  2 | task 44350 | init sampler, took 2.10 ms, tokens: text = 14855, total = 14855
slot update_slots: id  2 | task 44350 | created context checkpoint 1 of 8 (pos_min = 13894, pos_max = 14790, size = 21.034 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 44350
slot      release: id  2 | task 44350 | stop processing: n_tokens = 14859, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44365 | processing task, is_child = 0
slot update_slots: id  2 | task 44365 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14853
slot update_slots: id  2 | task 44365 | n_tokens = 14845, memory_seq_rm [14845, end)
slot update_slots: id  2 | task 44365 | prompt processing progress, n_tokens = 14853, batch.n_tokens = 8, progress = 1.000000
slot update_slots: id  2 | task 44365 | prompt done, n_tokens = 14853, batch.n_tokens = 8
slot init_sampler: id  2 | task 44365 | init sampler, took 2.06 ms, tokens: text = 14853, total = 14853
slot print_timing: id  2 | task 44365 | 
prompt eval time =     118.85 ms /     8 tokens (   14.86 ms per token,    67.31 tokens per second)
       eval time =    2585.46 ms /    97 tokens (   26.65 ms per token,    37.52 tokens per second)
      total time =    2704.31 ms /   105 tokens
slot      release: id  2 | task 44365 | stop processing: n_tokens = 14949, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44463 | processing task, is_child = 0
slot update_slots: id  2 | task 44463 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14949
slot update_slots: id  2 | task 44463 | n_tokens = 14853, memory_seq_rm [14853, end)
slot update_slots: id  2 | task 44463 | prompt processing progress, n_tokens = 14885, batch.n_tokens = 32, progress = 0.995719
slot update_slots: id  2 | task 44463 | n_tokens = 14885, memory_seq_rm [14885, end)
slot update_slots: id  2 | task 44463 | prompt processing progress, n_tokens = 14949, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44463 | prompt done, n_tokens = 14949, batch.n_tokens = 64
slot init_sampler: id  2 | task 44463 | init sampler, took 2.32 ms, tokens: text = 14949, total = 14949
slot update_slots: id  2 | task 44463 | created context checkpoint 2 of 8 (pos_min = 14052, pos_max = 14884, size = 19.533 MiB)
slot print_timing: id  2 | task 44463 | 
prompt eval time =     339.12 ms /    96 tokens (    3.53 ms per token,   283.09 tokens per second)
       eval time =    1746.70 ms /    66 tokens (   26.47 ms per token,    37.79 tokens per second)
      total time =    2085.82 ms /   162 tokens
slot      release: id  2 | task 44463 | stop processing: n_tokens = 15014, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44531 | processing task, is_child = 0
slot update_slots: id  2 | task 44531 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15007
slot update_slots: id  2 | task 44531 | n_tokens = 14949, memory_seq_rm [14949, end)
slot update_slots: id  2 | task 44531 | prompt processing progress, n_tokens = 15007, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  2 | task 44531 | prompt done, n_tokens = 15007, batch.n_tokens = 58
slot init_sampler: id  2 | task 44531 | init sampler, took 2.14 ms, tokens: text = 15007, total = 15007
slot print_timing: id  2 | task 44531 | 
prompt eval time =     178.68 ms /    58 tokens (    3.08 ms per token,   324.60 tokens per second)
       eval time =     857.57 ms /    32 tokens (   26.80 ms per token,    37.31 tokens per second)
      total time =    1036.25 ms /    90 tokens
slot      release: id  2 | task 44531 | stop processing: n_tokens = 15038, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.929 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44564 | processing task, is_child = 0
slot update_slots: id  2 | task 44564 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16152
slot update_slots: id  2 | task 44564 | n_tokens = 15007, memory_seq_rm [15007, end)
slot update_slots: id  2 | task 44564 | prompt processing progress, n_tokens = 16088, batch.n_tokens = 1081, progress = 0.996038
slot update_slots: id  2 | task 44564 | n_tokens = 16088, memory_seq_rm [16088, end)
slot update_slots: id  2 | task 44564 | prompt processing progress, n_tokens = 16152, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44564 | prompt done, n_tokens = 16152, batch.n_tokens = 64
slot init_sampler: id  2 | task 44564 | init sampler, took 2.30 ms, tokens: text = 16152, total = 16152
slot update_slots: id  2 | task 44564 | created context checkpoint 3 of 8 (pos_min = 15191, pos_max = 16087, size = 21.034 MiB)
slot print_timing: id  2 | task 44564 | 
prompt eval time =    1812.48 ms /  1145 tokens (    1.58 ms per token,   631.73 tokens per second)
       eval time =    8096.06 ms /   298 tokens (   27.17 ms per token,    36.81 tokens per second)
      total time =    9908.54 ms /  1443 tokens
slot      release: id  2 | task 44564 | stop processing: n_tokens = 16449, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44864 | processing task, is_child = 0
slot update_slots: id  2 | task 44864 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16424
slot update_slots: id  2 | task 44864 | n_tokens = 16152, memory_seq_rm [16152, end)
slot update_slots: id  2 | task 44864 | prompt processing progress, n_tokens = 16360, batch.n_tokens = 208, progress = 0.996103
slot update_slots: id  2 | task 44864 | n_tokens = 16360, memory_seq_rm [16360, end)
slot update_slots: id  2 | task 44864 | prompt processing progress, n_tokens = 16424, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44864 | prompt done, n_tokens = 16424, batch.n_tokens = 64
slot init_sampler: id  2 | task 44864 | init sampler, took 2.37 ms, tokens: text = 16424, total = 16424
slot update_slots: id  2 | task 44864 | created context checkpoint 4 of 8 (pos_min = 15552, pos_max = 16359, size = 18.947 MiB)
slot print_timing: id  2 | task 44864 | 
prompt eval time =     622.74 ms /   272 tokens (    2.29 ms per token,   436.78 tokens per second)
       eval time =    1532.22 ms /    56 tokens (   27.36 ms per token,    36.55 tokens per second)
      total time =    2154.95 ms /   328 tokens
slot      release: id  2 | task 44864 | stop processing: n_tokens = 16479, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44922 | processing task, is_child = 0
slot update_slots: id  2 | task 44922 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16498
slot update_slots: id  2 | task 44922 | n_tokens = 16424, memory_seq_rm [16424, end)
slot update_slots: id  2 | task 44922 | prompt processing progress, n_tokens = 16434, batch.n_tokens = 10, progress = 0.996121
slot update_slots: id  2 | task 44922 | n_tokens = 16434, memory_seq_rm [16434, end)
slot update_slots: id  2 | task 44922 | prompt processing progress, n_tokens = 16498, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44922 | prompt done, n_tokens = 16498, batch.n_tokens = 64
slot init_sampler: id  2 | task 44922 | init sampler, took 2.41 ms, tokens: text = 16498, total = 16498
slot update_slots: id  2 | task 44922 | created context checkpoint 5 of 8 (pos_min = 15582, pos_max = 16433, size = 19.979 MiB)
slot print_timing: id  2 | task 44922 | 
prompt eval time =     293.73 ms /    74 tokens (    3.97 ms per token,   251.93 tokens per second)
       eval time =    1401.95 ms /    52 tokens (   26.96 ms per token,    37.09 tokens per second)
      total time =    1695.68 ms /   126 tokens
slot      release: id  2 | task 44922 | stop processing: n_tokens = 16549, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
