ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14807 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1763 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 56064 -> need 1767 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.80 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14807 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
srv  log_server_r: request: GET /health 127.0.0.1 503
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...........srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 56064
llama_context: n_ctx_seq     = 56064
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (56064) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 56064 cells
llama_kv_cache:      CUDA0 KV buffer size =  1314.00 MiB
llama_kv_cache: size = 1314.00 MiB ( 56064 cells,  12 layers,  4/1 seqs), K (f16):  657.00 MiB, V (f16):  657.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   117.15 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 78.82 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv  log_server_r: request: GET /health 127.0.0.1 503
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  1 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  2 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  3 | task -1 | new slot, n_ctx = 56064
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-23

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 890
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 826, batch.n_tokens = 826, progress = 0.928090
slot update_slots: id  3 | task 0 | n_tokens = 826, memory_seq_rm [826, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 890, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 890, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.14 ms, tokens: text = 890, total = 890
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 825, size = 19.369 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =    1222.76 ms /   890 tokens (    1.37 ms per token,   727.86 tokens per second)
       eval time =    4346.24 ms /   192 tokens (   22.64 ms per token,    44.18 tokens per second)
      total time =    5569.00 ms /  1082 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 1081, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.858 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 194 | processing task, is_child = 0
slot update_slots: id  3 | task 194 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1228
slot update_slots: id  3 | task 194 | n_tokens = 1054, memory_seq_rm [1054, end)
slot update_slots: id  3 | task 194 | prompt processing progress, n_tokens = 1164, batch.n_tokens = 110, progress = 0.947883
slot update_slots: id  3 | task 194 | n_tokens = 1164, memory_seq_rm [1164, end)
slot update_slots: id  3 | task 194 | prompt processing progress, n_tokens = 1228, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 194 | prompt done, n_tokens = 1228, batch.n_tokens = 64
slot init_sampler: id  3 | task 194 | init sampler, took 0.18 ms, tokens: text = 1228, total = 1228
slot update_slots: id  3 | task 194 | created context checkpoint 2 of 8 (pos_min = 140, pos_max = 1163, size = 24.012 MiB)
slot print_timing: id  3 | task 194 | 
prompt eval time =     379.03 ms /   174 tokens (    2.18 ms per token,   459.07 tokens per second)
       eval time =    2643.11 ms /   115 tokens (   22.98 ms per token,    43.51 tokens per second)
      total time =    3022.14 ms /   289 tokens
slot      release: id  3 | task 194 | stop processing: n_tokens = 1342, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.656
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 311 | processing task, is_child = 0
slot update_slots: id  3 | task 311 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 904
slot update_slots: id  3 | task 311 | n_tokens = 881, memory_seq_rm [881, end)
slot update_slots: id  3 | task 311 | prompt processing progress, n_tokens = 904, batch.n_tokens = 23, progress = 1.000000
slot update_slots: id  3 | task 311 | prompt done, n_tokens = 904, batch.n_tokens = 23
slot init_sampler: id  3 | task 311 | init sampler, took 0.14 ms, tokens: text = 904, total = 904
slot print_timing: id  3 | task 311 | 
prompt eval time =     162.84 ms /    23 tokens (    7.08 ms per token,   141.24 tokens per second)
       eval time =    1728.24 ms /    75 tokens (   23.04 ms per token,    43.40 tokens per second)
      total time =    1891.08 ms /    98 tokens
slot      release: id  3 | task 311 | stop processing: n_tokens = 978, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.659 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 387 | processing task, is_child = 0
slot update_slots: id  3 | task 387 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1450
slot update_slots: id  3 | task 387 | n_tokens = 955, memory_seq_rm [955, end)
slot update_slots: id  3 | task 387 | prompt processing progress, n_tokens = 1386, batch.n_tokens = 431, progress = 0.955862
slot update_slots: id  3 | task 387 | n_tokens = 1386, memory_seq_rm [1386, end)
slot update_slots: id  3 | task 387 | prompt processing progress, n_tokens = 1450, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 387 | prompt done, n_tokens = 1450, batch.n_tokens = 64
slot init_sampler: id  3 | task 387 | init sampler, took 0.21 ms, tokens: text = 1450, total = 1450
slot update_slots: id  3 | task 387 | created context checkpoint 3 of 8 (pos_min = 505, pos_max = 1385, size = 20.659 MiB)
slot print_timing: id  3 | task 387 | 
prompt eval time =     554.79 ms /   495 tokens (    1.12 ms per token,   892.23 tokens per second)
       eval time =    1095.36 ms /    45 tokens (   24.34 ms per token,    41.08 tokens per second)
      total time =    1650.15 ms /   540 tokens
slot      release: id  3 | task 387 | stop processing: n_tokens = 1494, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.904 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 434 | processing task, is_child = 0
slot update_slots: id  3 | task 434 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1615
slot update_slots: id  3 | task 434 | n_tokens = 1460, memory_seq_rm [1460, end)
slot update_slots: id  3 | task 434 | prompt processing progress, n_tokens = 1551, batch.n_tokens = 91, progress = 0.960371
slot update_slots: id  3 | task 434 | n_tokens = 1551, memory_seq_rm [1551, end)
slot update_slots: id  3 | task 434 | prompt processing progress, n_tokens = 1615, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 434 | prompt done, n_tokens = 1615, batch.n_tokens = 64
slot init_sampler: id  3 | task 434 | init sampler, took 0.31 ms, tokens: text = 1615, total = 1615
slot update_slots: id  3 | task 434 | created context checkpoint 4 of 8 (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot print_timing: id  3 | task 434 | 
prompt eval time =     376.57 ms /   155 tokens (    2.43 ms per token,   411.61 tokens per second)
       eval time =     752.15 ms /    31 tokens (   24.26 ms per token,    41.22 tokens per second)
      total time =    1128.72 ms /   186 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 434 | stop processing: n_tokens = 1645, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.660 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 467 | processing task, is_child = 0
slot update_slots: id  3 | task 467 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2448
slot update_slots: id  3 | task 467 | n_tokens = 1615, memory_seq_rm [1615, end)
slot update_slots: id  3 | task 467 | prompt processing progress, n_tokens = 2384, batch.n_tokens = 769, progress = 0.973856
slot update_slots: id  3 | task 467 | n_tokens = 2384, memory_seq_rm [2384, end)
slot update_slots: id  3 | task 467 | prompt processing progress, n_tokens = 2448, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 467 | prompt done, n_tokens = 2448, batch.n_tokens = 64
slot init_sampler: id  3 | task 467 | init sampler, took 0.37 ms, tokens: text = 2448, total = 2448
slot update_slots: id  3 | task 467 | created context checkpoint 5 of 8 (pos_min = 1360, pos_max = 2383, size = 24.012 MiB)
slot print_timing: id  3 | task 467 | 
prompt eval time =     952.34 ms /   833 tokens (    1.14 ms per token,   874.69 tokens per second)
       eval time =    1052.17 ms /    45 tokens (   23.38 ms per token,    42.77 tokens per second)
      total time =    2004.51 ms /   878 tokens
slot      release: id  3 | task 467 | stop processing: n_tokens = 2492, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.670 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 514 | processing task, is_child = 0
slot update_slots: id  3 | task 514 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3668
slot update_slots: id  3 | task 514 | n_tokens = 2458, memory_seq_rm [2458, end)
slot update_slots: id  3 | task 514 | prompt processing progress, n_tokens = 3604, batch.n_tokens = 1146, progress = 0.982552
slot update_slots: id  3 | task 514 | n_tokens = 3604, memory_seq_rm [3604, end)
slot update_slots: id  3 | task 514 | prompt processing progress, n_tokens = 3668, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 514 | prompt done, n_tokens = 3668, batch.n_tokens = 64
slot init_sampler: id  3 | task 514 | init sampler, took 0.71 ms, tokens: text = 3668, total = 3668
slot update_slots: id  3 | task 514 | created context checkpoint 6 of 8 (pos_min = 2580, pos_max = 3603, size = 24.012 MiB)
slot print_timing: id  3 | task 514 | 
prompt eval time =    1445.86 ms /  1210 tokens (    1.19 ms per token,   836.87 tokens per second)
       eval time =    1065.04 ms /    45 tokens (   23.67 ms per token,    42.25 tokens per second)
      total time =    2510.90 ms /  1255 tokens
slot      release: id  3 | task 514 | stop processing: n_tokens = 3712, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.902 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 561 | processing task, is_child = 0
slot update_slots: id  3 | task 561 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4077
slot update_slots: id  3 | task 561 | n_tokens = 3679, memory_seq_rm [3679, end)
slot update_slots: id  3 | task 561 | prompt processing progress, n_tokens = 4013, batch.n_tokens = 334, progress = 0.984302
slot update_slots: id  3 | task 561 | n_tokens = 4013, memory_seq_rm [4013, end)
slot update_slots: id  3 | task 561 | prompt processing progress, n_tokens = 4077, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 561 | prompt done, n_tokens = 4077, batch.n_tokens = 64
slot init_sampler: id  3 | task 561 | init sampler, took 0.57 ms, tokens: text = 4077, total = 4077
slot update_slots: id  3 | task 561 | created context checkpoint 7 of 8 (pos_min = 2989, pos_max = 4012, size = 24.012 MiB)
slot print_timing: id  3 | task 561 | 
prompt eval time =     533.11 ms /   398 tokens (    1.34 ms per token,   746.57 tokens per second)
       eval time =    1025.77 ms /    43 tokens (   23.86 ms per token,    41.92 tokens per second)
      total time =    1558.88 ms /   441 tokens
slot      release: id  3 | task 561 | stop processing: n_tokens = 4119, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.872 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 606 | processing task, is_child = 0
slot update_slots: id  3 | task 606 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4686
slot update_slots: id  3 | task 606 | n_tokens = 4088, memory_seq_rm [4088, end)
slot update_slots: id  3 | task 606 | prompt processing progress, n_tokens = 4622, batch.n_tokens = 534, progress = 0.986342
slot update_slots: id  3 | task 606 | n_tokens = 4622, memory_seq_rm [4622, end)
slot update_slots: id  3 | task 606 | prompt processing progress, n_tokens = 4686, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 606 | prompt done, n_tokens = 4686, batch.n_tokens = 64
slot init_sampler: id  3 | task 606 | init sampler, took 0.74 ms, tokens: text = 4686, total = 4686
slot update_slots: id  3 | task 606 | created context checkpoint 8 of 8 (pos_min = 3598, pos_max = 4621, size = 24.012 MiB)
slot print_timing: id  3 | task 606 | 
prompt eval time =     771.51 ms /   598 tokens (    1.29 ms per token,   775.11 tokens per second)
       eval time =    1007.67 ms /    42 tokens (   23.99 ms per token,    41.68 tokens per second)
      total time =    1779.18 ms /   640 tokens
slot      release: id  3 | task 606 | stop processing: n_tokens = 4727, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.887 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 650 | processing task, is_child = 0
slot update_slots: id  3 | task 650 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5296
slot update_slots: id  3 | task 650 | n_tokens = 4696, memory_seq_rm [4696, end)
slot update_slots: id  3 | task 650 | prompt processing progress, n_tokens = 5232, batch.n_tokens = 536, progress = 0.987915
slot update_slots: id  3 | task 650 | n_tokens = 5232, memory_seq_rm [5232, end)
slot update_slots: id  3 | task 650 | prompt processing progress, n_tokens = 5296, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 650 | prompt done, n_tokens = 5296, batch.n_tokens = 64
slot init_sampler: id  3 | task 650 | init sampler, took 0.74 ms, tokens: text = 5296, total = 5296
slot update_slots: id  3 | task 650 | erasing old context checkpoint (pos_min = 0, pos_max = 825, size = 19.369 MiB)
slot update_slots: id  3 | task 650 | created context checkpoint 8 of 8 (pos_min = 4208, pos_max = 5231, size = 24.012 MiB)
slot print_timing: id  3 | task 650 | 
prompt eval time =     787.65 ms /   600 tokens (    1.31 ms per token,   761.76 tokens per second)
       eval time =    1090.35 ms /    45 tokens (   24.23 ms per token,    41.27 tokens per second)
      total time =    1878.00 ms /   645 tokens
slot      release: id  3 | task 650 | stop processing: n_tokens = 5340, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 697 | processing task, is_child = 0
slot update_slots: id  2 | task 697 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4294
slot update_slots: id  2 | task 697 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 697 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.476945
slot update_slots: id  2 | task 697 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 697 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.953889
slot update_slots: id  2 | task 697 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 697 | prompt processing progress, n_tokens = 4230, batch.n_tokens = 134, progress = 0.985096
slot update_slots: id  2 | task 697 | n_tokens = 4230, memory_seq_rm [4230, end)
slot update_slots: id  2 | task 697 | prompt processing progress, n_tokens = 4294, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 697 | prompt done, n_tokens = 4294, batch.n_tokens = 64
slot init_sampler: id  2 | task 697 | init sampler, took 0.82 ms, tokens: text = 4294, total = 4294
slot update_slots: id  2 | task 697 | created context checkpoint 1 of 8 (pos_min = 3333, pos_max = 4229, size = 21.034 MiB)
slot print_timing: id  2 | task 697 | 
prompt eval time =    4667.91 ms /  4294 tokens (    1.09 ms per token,   919.90 tokens per second)
       eval time =    9237.92 ms /   372 tokens (   24.83 ms per token,    40.27 tokens per second)
      total time =   13905.83 ms /  4666 tokens
slot      release: id  2 | task 697 | stop processing: n_tokens = 4665, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.161 (> 0.100 thold), f_keep = 0.165
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5340, total state size = 128.196 MiB
srv          load:  - looking for better prompt, base f_keep = 0.165, sim = 0.161
srv        update:  - cache state: 1 prompts, 313.586 MiB (limits: 8192.000 MiB, 56064 tokens, 139500 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv  get_availabl: prompt cache update took 269.00 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1073 | processing task, is_child = 0
slot update_slots: id  3 | task 1073 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5452
slot update_slots: id  3 | task 1073 | n_past = 879, slot.prompt.tokens.size() = 5340, seq_id = 3, pos_min = 5213, n_swa = 128
slot update_slots: id  3 | task 1073 | restored context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 1073 | erased invalidated context checkpoint (pos_min = 1360, pos_max = 2383, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1073 | erased invalidated context checkpoint (pos_min = 2580, pos_max = 3603, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1073 | erased invalidated context checkpoint (pos_min = 2989, pos_max = 4012, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1073 | erased invalidated context checkpoint (pos_min = 3598, pos_max = 4621, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1073 | erased invalidated context checkpoint (pos_min = 4208, pos_max = 5231, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1073 | n_tokens = 879, memory_seq_rm [879, end)
slot update_slots: id  3 | task 1073 | prompt processing progress, n_tokens = 2927, batch.n_tokens = 2048, progress = 0.536867
slot update_slots: id  3 | task 1073 | n_tokens = 2927, memory_seq_rm [2927, end)
slot update_slots: id  3 | task 1073 | prompt processing progress, n_tokens = 4975, batch.n_tokens = 2048, progress = 0.912509
slot update_slots: id  3 | task 1073 | n_tokens = 4975, memory_seq_rm [4975, end)
slot update_slots: id  3 | task 1073 | prompt processing progress, n_tokens = 5388, batch.n_tokens = 413, progress = 0.988261
slot update_slots: id  3 | task 1073 | n_tokens = 5388, memory_seq_rm [5388, end)
slot update_slots: id  3 | task 1073 | prompt processing progress, n_tokens = 5452, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1073 | prompt done, n_tokens = 5452, batch.n_tokens = 64
slot init_sampler: id  3 | task 1073 | init sampler, took 0.80 ms, tokens: text = 5452, total = 5452
slot update_slots: id  3 | task 1073 | created context checkpoint 4 of 8 (pos_min = 4491, pos_max = 5387, size = 21.034 MiB)
slot print_timing: id  3 | task 1073 | 
prompt eval time =    5247.36 ms /  4573 tokens (    1.15 ms per token,   871.49 tokens per second)
       eval time =    7690.71 ms /   298 tokens (   25.81 ms per token,    38.75 tokens per second)
      total time =   12938.07 ms /  4871 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 1073 | stop processing: n_tokens = 5749, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1375 | processing task, is_child = 0
slot update_slots: id  3 | task 1375 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5830
slot update_slots: id  3 | task 1375 | n_tokens = 5714, memory_seq_rm [5714, end)
slot update_slots: id  3 | task 1375 | prompt processing progress, n_tokens = 5766, batch.n_tokens = 52, progress = 0.989022
slot update_slots: id  3 | task 1375 | n_tokens = 5766, memory_seq_rm [5766, end)
slot update_slots: id  3 | task 1375 | prompt processing progress, n_tokens = 5830, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1375 | prompt done, n_tokens = 5830, batch.n_tokens = 64
slot init_sampler: id  3 | task 1375 | init sampler, took 1.09 ms, tokens: text = 5830, total = 5830
slot update_slots: id  3 | task 1375 | created context checkpoint 5 of 8 (pos_min = 4869, pos_max = 5765, size = 21.034 MiB)
slot print_timing: id  3 | task 1375 | 
prompt eval time =     355.49 ms /   116 tokens (    3.06 ms per token,   326.31 tokens per second)
       eval time =     926.59 ms /    35 tokens (   26.47 ms per token,    37.77 tokens per second)
      total time =    1282.08 ms /   151 tokens
slot      release: id  3 | task 1375 | stop processing: n_tokens = 5864, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.930 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1412 | processing task, is_child = 0
slot update_slots: id  3 | task 1412 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6271
slot update_slots: id  3 | task 1412 | n_tokens = 5830, memory_seq_rm [5830, end)
slot update_slots: id  3 | task 1412 | prompt processing progress, n_tokens = 6207, batch.n_tokens = 377, progress = 0.989794
slot update_slots: id  3 | task 1412 | n_tokens = 6207, memory_seq_rm [6207, end)
slot update_slots: id  3 | task 1412 | prompt processing progress, n_tokens = 6271, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1412 | prompt done, n_tokens = 6271, batch.n_tokens = 64
slot init_sampler: id  3 | task 1412 | init sampler, took 1.16 ms, tokens: text = 6271, total = 6271
slot update_slots: id  3 | task 1412 | created context checkpoint 6 of 8 (pos_min = 5310, pos_max = 6206, size = 21.034 MiB)
slot print_timing: id  3 | task 1412 | 
prompt eval time =     693.83 ms /   441 tokens (    1.57 ms per token,   635.60 tokens per second)
       eval time =    8480.73 ms /   327 tokens (   25.93 ms per token,    38.56 tokens per second)
      total time =    9174.56 ms /   768 tokens
slot      release: id  3 | task 1412 | stop processing: n_tokens = 6597, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.122 (> 0.100 thold), f_keep = 0.091
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4665, total state size = 112.368 MiB
srv          load:  - looking for better prompt, base f_keep = 0.091, sim = 0.122
srv        update:  - cache state: 2 prompts, 446.988 MiB (limits: 8192.000 MiB, 56064 tokens, 183362 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv  get_availabl: prompt cache update took 101.36 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1741 | processing task, is_child = 0
slot update_slots: id  2 | task 1741 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3480
slot update_slots: id  2 | task 1741 | n_past = 425, slot.prompt.tokens.size() = 4665, seq_id = 2, pos_min = 4538, n_swa = 128
slot update_slots: id  2 | task 1741 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 1741 | erased invalidated context checkpoint (pos_min = 3333, pos_max = 4229, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 1741 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 1741 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.588506
slot update_slots: id  2 | task 1741 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 1741 | prompt processing progress, n_tokens = 3416, batch.n_tokens = 1368, progress = 0.981609
slot update_slots: id  2 | task 1741 | n_tokens = 3416, memory_seq_rm [3416, end)
slot update_slots: id  2 | task 1741 | prompt processing progress, n_tokens = 3480, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1741 | prompt done, n_tokens = 3480, batch.n_tokens = 64
slot init_sampler: id  2 | task 1741 | init sampler, took 0.62 ms, tokens: text = 3480, total = 3480
slot update_slots: id  2 | task 1741 | created context checkpoint 1 of 8 (pos_min = 2519, pos_max = 3415, size = 21.034 MiB)
slot print_timing: id  2 | task 1741 | 
prompt eval time =    4101.80 ms /  3480 tokens (    1.18 ms per token,   848.41 tokens per second)
       eval time =    9468.64 ms /   372 tokens (   25.45 ms per token,    39.29 tokens per second)
      total time =   13570.43 ms /  3852 tokens
slot      release: id  2 | task 1741 | stop processing: n_tokens = 3851, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.509 (> 0.100 thold), f_keep = 0.133
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6597, total state size = 157.671 MiB
srv          load:  - looking for better prompt, base f_keep = 0.133, sim = 0.509
srv        update:  - cache state: 3 prompts, 733.090 MiB (limits: 8192.000 MiB, 56064 tokens, 185520 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv  get_availabl: prompt cache update took 268.50 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2116 | processing task, is_child = 0
slot update_slots: id  3 | task 2116 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1730
slot update_slots: id  3 | task 2116 | n_past = 880, slot.prompt.tokens.size() = 6597, seq_id = 3, pos_min = 6470, n_swa = 128
slot update_slots: id  3 | task 2116 | restored context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 2116 | erased invalidated context checkpoint (pos_min = 4491, pos_max = 5387, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 2116 | erased invalidated context checkpoint (pos_min = 4869, pos_max = 5765, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 2116 | erased invalidated context checkpoint (pos_min = 5310, pos_max = 6206, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 2116 | n_tokens = 880, memory_seq_rm [880, end)
slot update_slots: id  3 | task 2116 | prompt processing progress, n_tokens = 1666, batch.n_tokens = 786, progress = 0.963006
slot update_slots: id  3 | task 2116 | n_tokens = 1666, memory_seq_rm [1666, end)
slot update_slots: id  3 | task 2116 | prompt processing progress, n_tokens = 1730, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2116 | prompt done, n_tokens = 1730, batch.n_tokens = 64
slot init_sampler: id  3 | task 2116 | init sampler, took 0.34 ms, tokens: text = 1730, total = 1730
slot update_slots: id  3 | task 2116 | created context checkpoint 4 of 8 (pos_min = 896, pos_max = 1665, size = 18.056 MiB)
slot print_timing: id  3 | task 2116 | 
prompt eval time =    1294.22 ms /   850 tokens (    1.52 ms per token,   656.77 tokens per second)
       eval time =    3456.49 ms /   137 tokens (   25.23 ms per token,    39.64 tokens per second)
      total time =    4750.71 ms /   987 tokens
slot      release: id  3 | task 2116 | stop processing: n_tokens = 1866, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.878 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2255 | processing task, is_child = 0
slot update_slots: id  3 | task 2255 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2090
slot update_slots: id  3 | task 2255 | n_tokens = 1835, memory_seq_rm [1835, end)
slot update_slots: id  3 | task 2255 | prompt processing progress, n_tokens = 2026, batch.n_tokens = 191, progress = 0.969378
slot update_slots: id  3 | task 2255 | n_tokens = 2026, memory_seq_rm [2026, end)
slot update_slots: id  3 | task 2255 | prompt processing progress, n_tokens = 2090, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2255 | prompt done, n_tokens = 2090, batch.n_tokens = 64
slot init_sampler: id  3 | task 2255 | init sampler, took 0.29 ms, tokens: text = 2090, total = 2090
slot update_slots: id  3 | task 2255 | created context checkpoint 5 of 8 (pos_min = 1129, pos_max = 2025, size = 21.034 MiB)
slot print_timing: id  3 | task 2255 | 
prompt eval time =     469.38 ms /   255 tokens (    1.84 ms per token,   543.27 tokens per second)
       eval time =    1025.51 ms /    41 tokens (   25.01 ms per token,    39.98 tokens per second)
      total time =    1494.89 ms /   296 tokens
slot      release: id  3 | task 2255 | stop processing: n_tokens = 2130, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.779 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2298 | processing task, is_child = 0
slot update_slots: id  3 | task 2298 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2695
slot update_slots: id  3 | task 2298 | n_tokens = 2099, memory_seq_rm [2099, end)
slot update_slots: id  3 | task 2298 | prompt processing progress, n_tokens = 2631, batch.n_tokens = 532, progress = 0.976252
slot update_slots: id  3 | task 2298 | n_tokens = 2631, memory_seq_rm [2631, end)
slot update_slots: id  3 | task 2298 | prompt processing progress, n_tokens = 2695, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2298 | prompt done, n_tokens = 2695, batch.n_tokens = 64
slot init_sampler: id  3 | task 2298 | init sampler, took 0.41 ms, tokens: text = 2695, total = 2695
slot update_slots: id  3 | task 2298 | created context checkpoint 6 of 8 (pos_min = 1734, pos_max = 2630, size = 21.034 MiB)
slot print_timing: id  3 | task 2298 | 
prompt eval time =     847.25 ms /   596 tokens (    1.42 ms per token,   703.45 tokens per second)
       eval time =   19808.58 ms /   789 tokens (   25.11 ms per token,    39.83 tokens per second)
      total time =   20655.83 ms /  1385 tokens
slot      release: id  3 | task 2298 | stop processing: n_tokens = 3483, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.832 (> 0.100 thold), f_keep = 0.840
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3089 | processing task, is_child = 0
slot update_slots: id  3 | task 3089 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3513
slot update_slots: id  3 | task 3089 | n_tokens = 2924, memory_seq_rm [2924, end)
slot update_slots: id  3 | task 3089 | prompt processing progress, n_tokens = 3449, batch.n_tokens = 525, progress = 0.981782
slot update_slots: id  3 | task 3089 | n_tokens = 3449, memory_seq_rm [3449, end)
slot update_slots: id  3 | task 3089 | prompt processing progress, n_tokens = 3513, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3089 | prompt done, n_tokens = 3513, batch.n_tokens = 64
slot init_sampler: id  3 | task 3089 | init sampler, took 0.66 ms, tokens: text = 3513, total = 3513
slot update_slots: id  3 | task 3089 | created context checkpoint 7 of 8 (pos_min = 2797, pos_max = 3448, size = 15.289 MiB)
slot print_timing: id  3 | task 3089 | 
prompt eval time =     845.43 ms /   589 tokens (    1.44 ms per token,   696.69 tokens per second)
       eval time =    4147.35 ms /   164 tokens (   25.29 ms per token,    39.54 tokens per second)
      total time =    4992.78 ms /   753 tokens
slot      release: id  3 | task 3089 | stop processing: n_tokens = 3676, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.188 (> 0.100 thold), f_keep = 0.111
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3851, total state size = 93.280 MiB
srv          load:  - looking for better prompt, base f_keep = 0.111, sim = 0.188
srv        update:  - cache state: 4 prompts, 847.405 MiB (limits: 8192.000 MiB, 56064 tokens, 197722 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv  get_availabl: prompt cache update took 85.72 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3255 | processing task, is_child = 0
slot update_slots: id  2 | task 3255 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2262
slot update_slots: id  2 | task 3255 | n_past = 426, slot.prompt.tokens.size() = 3851, seq_id = 2, pos_min = 3724, n_swa = 128
slot update_slots: id  2 | task 3255 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 3255 | erased invalidated context checkpoint (pos_min = 2519, pos_max = 3415, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 3255 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 3255 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.905393
slot update_slots: id  2 | task 3255 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 3255 | prompt processing progress, n_tokens = 2198, batch.n_tokens = 150, progress = 0.971706
slot update_slots: id  2 | task 3255 | n_tokens = 2198, memory_seq_rm [2198, end)
slot update_slots: id  2 | task 3255 | prompt processing progress, n_tokens = 2262, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3255 | prompt done, n_tokens = 2262, batch.n_tokens = 64
slot init_sampler: id  2 | task 3255 | init sampler, took 0.32 ms, tokens: text = 2262, total = 2262
slot update_slots: id  2 | task 3255 | created context checkpoint 1 of 8 (pos_min = 1301, pos_max = 2197, size = 21.034 MiB)
slot print_timing: id  2 | task 3255 | 
prompt eval time =    2497.36 ms /  2262 tokens (    1.10 ms per token,   905.76 tokens per second)
       eval time =    9918.22 ms /   403 tokens (   24.61 ms per token,    40.63 tokens per second)
      total time =   12415.58 ms /  2665 tokens
slot      release: id  2 | task 3255 | stop processing: n_tokens = 2664, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.440 (> 0.100 thold), f_keep = 0.239
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3676, total state size = 89.177 MiB
srv          load:  - looking for better prompt, base f_keep = 0.239, sim = 0.440
srv        update:  - cache state: 5 prompts, 1077.324 MiB (limits: 8192.000 MiB, 56064 tokens, 183477 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv  get_availabl: prompt cache update took 173.07 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3661 | processing task, is_child = 0
slot update_slots: id  3 | task 3661 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2002
slot update_slots: id  3 | task 3661 | n_past = 880, slot.prompt.tokens.size() = 3676, seq_id = 3, pos_min = 3549, n_swa = 128
slot update_slots: id  3 | task 3661 | restored context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 3661 | erased invalidated context checkpoint (pos_min = 896, pos_max = 1665, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 3661 | erased invalidated context checkpoint (pos_min = 1129, pos_max = 2025, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 3661 | erased invalidated context checkpoint (pos_min = 1734, pos_max = 2630, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 3661 | erased invalidated context checkpoint (pos_min = 2797, pos_max = 3448, n_swa = 128, size = 15.289 MiB)
slot update_slots: id  3 | task 3661 | n_tokens = 880, memory_seq_rm [880, end)
slot update_slots: id  3 | task 3661 | prompt processing progress, n_tokens = 1938, batch.n_tokens = 1058, progress = 0.968032
slot update_slots: id  3 | task 3661 | n_tokens = 1938, memory_seq_rm [1938, end)
slot update_slots: id  3 | task 3661 | prompt processing progress, n_tokens = 2002, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3661 | prompt done, n_tokens = 2002, batch.n_tokens = 64
slot init_sampler: id  3 | task 3661 | init sampler, took 0.28 ms, tokens: text = 2002, total = 2002
slot update_slots: id  3 | task 3661 | created context checkpoint 4 of 8 (pos_min = 1168, pos_max = 1937, size = 18.056 MiB)
slot print_timing: id  3 | task 3661 | 
prompt eval time =    1513.90 ms /  1122 tokens (    1.35 ms per token,   741.13 tokens per second)
       eval time =     866.19 ms /    31 tokens (   27.94 ms per token,    35.79 tokens per second)
      total time =    2380.09 ms /  1153 tokens
slot      release: id  3 | task 3661 | stop processing: n_tokens = 2032, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3694 | processing task, is_child = 0
slot update_slots: id  3 | task 3694 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2180
slot update_slots: id  3 | task 3694 | n_tokens = 2015, memory_seq_rm [2015, end)
slot update_slots: id  3 | task 3694 | prompt processing progress, n_tokens = 2116, batch.n_tokens = 101, progress = 0.970642
slot update_slots: id  3 | task 3694 | n_tokens = 2116, memory_seq_rm [2116, end)
slot update_slots: id  3 | task 3694 | prompt processing progress, n_tokens = 2180, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3694 | prompt done, n_tokens = 2180, batch.n_tokens = 64
slot init_sampler: id  3 | task 3694 | init sampler, took 0.30 ms, tokens: text = 2180, total = 2180
slot update_slots: id  3 | task 3694 | created context checkpoint 5 of 8 (pos_min = 1219, pos_max = 2115, size = 21.034 MiB)
slot print_timing: id  3 | task 3694 | 
prompt eval time =     500.01 ms /   165 tokens (    3.03 ms per token,   329.99 tokens per second)
       eval time =    1033.07 ms /    42 tokens (   24.60 ms per token,    40.66 tokens per second)
      total time =    1533.08 ms /   207 tokens
slot      release: id  3 | task 3694 | stop processing: n_tokens = 2221, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.815 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3738 | processing task, is_child = 0
slot update_slots: id  3 | task 3738 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2691
slot update_slots: id  3 | task 3738 | n_tokens = 2194, memory_seq_rm [2194, end)
slot update_slots: id  3 | task 3738 | prompt processing progress, n_tokens = 2627, batch.n_tokens = 433, progress = 0.976217
slot update_slots: id  3 | task 3738 | n_tokens = 2627, memory_seq_rm [2627, end)
slot update_slots: id  3 | task 3738 | prompt processing progress, n_tokens = 2691, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3738 | prompt done, n_tokens = 2691, batch.n_tokens = 64
slot init_sampler: id  3 | task 3738 | init sampler, took 0.38 ms, tokens: text = 2691, total = 2691
slot update_slots: id  3 | task 3738 | created context checkpoint 6 of 8 (pos_min = 1730, pos_max = 2626, size = 21.034 MiB)
slot print_timing: id  3 | task 3738 | 
prompt eval time =     644.30 ms /   497 tokens (    1.30 ms per token,   771.38 tokens per second)
       eval time =    2087.71 ms /    83 tokens (   25.15 ms per token,    39.76 tokens per second)
      total time =    2732.01 ms /   580 tokens
slot      release: id  3 | task 3738 | stop processing: n_tokens = 2773, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3823 | processing task, is_child = 0
slot update_slots: id  3 | task 3823 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2818
slot update_slots: id  3 | task 3823 | n_tokens = 2722, memory_seq_rm [2722, end)
slot update_slots: id  3 | task 3823 | prompt processing progress, n_tokens = 2754, batch.n_tokens = 32, progress = 0.977289
slot update_slots: id  3 | task 3823 | n_tokens = 2754, memory_seq_rm [2754, end)
slot update_slots: id  3 | task 3823 | prompt processing progress, n_tokens = 2818, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3823 | prompt done, n_tokens = 2818, batch.n_tokens = 64
slot init_sampler: id  3 | task 3823 | init sampler, took 0.53 ms, tokens: text = 2818, total = 2818
slot update_slots: id  3 | task 3823 | created context checkpoint 7 of 8 (pos_min = 1876, pos_max = 2753, size = 20.588 MiB)
slot print_timing: id  3 | task 3823 | 
prompt eval time =     287.48 ms /    96 tokens (    2.99 ms per token,   333.94 tokens per second)
       eval time =    1012.13 ms /    40 tokens (   25.30 ms per token,    39.52 tokens per second)
      total time =    1299.61 ms /   136 tokens
slot      release: id  3 | task 3823 | stop processing: n_tokens = 2857, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.945 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3865 | processing task, is_child = 0
slot update_slots: id  3 | task 3865 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3007
slot update_slots: id  3 | task 3865 | n_tokens = 2842, memory_seq_rm [2842, end)
slot update_slots: id  3 | task 3865 | prompt processing progress, n_tokens = 2943, batch.n_tokens = 101, progress = 0.978716
slot update_slots: id  3 | task 3865 | n_tokens = 2943, memory_seq_rm [2943, end)
slot update_slots: id  3 | task 3865 | prompt processing progress, n_tokens = 3007, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3865 | prompt done, n_tokens = 3007, batch.n_tokens = 64
slot init_sampler: id  3 | task 3865 | init sampler, took 0.50 ms, tokens: text = 3007, total = 3007
slot update_slots: id  3 | task 3865 | created context checkpoint 8 of 8 (pos_min = 2046, pos_max = 2942, size = 21.034 MiB)
slot print_timing: id  3 | task 3865 | 
prompt eval time =     425.12 ms /   165 tokens (    2.58 ms per token,   388.13 tokens per second)
       eval time =     890.48 ms /    36 tokens (   24.74 ms per token,    40.43 tokens per second)
      total time =    1315.59 ms /   201 tokens
slot      release: id  3 | task 3865 | stop processing: n_tokens = 3042, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.935 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3903 | processing task, is_child = 0
slot update_slots: id  3 | task 3903 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3229
slot update_slots: id  3 | task 3903 | n_tokens = 3018, memory_seq_rm [3018, end)
slot update_slots: id  3 | task 3903 | prompt processing progress, n_tokens = 3165, batch.n_tokens = 147, progress = 0.980180
slot update_slots: id  3 | task 3903 | n_tokens = 3165, memory_seq_rm [3165, end)
slot update_slots: id  3 | task 3903 | prompt processing progress, n_tokens = 3229, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3903 | prompt done, n_tokens = 3229, batch.n_tokens = 64
slot init_sampler: id  3 | task 3903 | init sampler, took 0.45 ms, tokens: text = 3229, total = 3229
slot update_slots: id  3 | task 3903 | erasing old context checkpoint (pos_min = 140, pos_max = 1163, size = 24.012 MiB)
slot update_slots: id  3 | task 3903 | created context checkpoint 8 of 8 (pos_min = 2268, pos_max = 3164, size = 21.034 MiB)
slot print_timing: id  3 | task 3903 | 
prompt eval time =     426.39 ms /   211 tokens (    2.02 ms per token,   494.86 tokens per second)
       eval time =    1515.67 ms /    61 tokens (   24.85 ms per token,    40.25 tokens per second)
      total time =    1942.06 ms /   272 tokens
slot      release: id  3 | task 3903 | stop processing: n_tokens = 3289, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.302 (> 0.100 thold), f_keep = 0.268
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3289, total state size = 98.158 MiB
srv          load:  - looking for better prompt, base f_keep = 0.268, sim = 0.302
srv        update:  - cache state: 6 prompts, 1339.580 MiB (limits: 8192.000 MiB, 56064 tokens, 167670 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fc4fbf0:    3289 tokens, checkpoints:  8,   262.256 MiB
srv  get_availabl: prompt cache update took 205.69 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3966 | processing task, is_child = 0
slot update_slots: id  3 | task 3966 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2912
slot update_slots: id  3 | task 3966 | n_past = 880, slot.prompt.tokens.size() = 3289, seq_id = 3, pos_min = 2392, n_swa = 128
slot update_slots: id  3 | task 3966 | restored context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 3966 | erased invalidated context checkpoint (pos_min = 1168, pos_max = 1937, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 3966 | erased invalidated context checkpoint (pos_min = 1219, pos_max = 2115, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 3966 | erased invalidated context checkpoint (pos_min = 1730, pos_max = 2626, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 3966 | erased invalidated context checkpoint (pos_min = 1876, pos_max = 2753, n_swa = 128, size = 20.588 MiB)
slot update_slots: id  3 | task 3966 | erased invalidated context checkpoint (pos_min = 2046, pos_max = 2942, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 3966 | erased invalidated context checkpoint (pos_min = 2268, pos_max = 3164, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 3966 | n_tokens = 880, memory_seq_rm [880, end)
slot update_slots: id  3 | task 3966 | prompt processing progress, n_tokens = 2848, batch.n_tokens = 1968, progress = 0.978022
slot update_slots: id  3 | task 3966 | n_tokens = 2848, memory_seq_rm [2848, end)
slot update_slots: id  3 | task 3966 | prompt processing progress, n_tokens = 2912, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3966 | prompt done, n_tokens = 2912, batch.n_tokens = 64
slot init_sampler: id  3 | task 3966 | init sampler, took 0.41 ms, tokens: text = 2912, total = 2912
slot update_slots: id  3 | task 3966 | created context checkpoint 3 of 8 (pos_min = 1951, pos_max = 2847, size = 21.034 MiB)
slot print_timing: id  3 | task 3966 | 
prompt eval time =    2356.20 ms /  2032 tokens (    1.16 ms per token,   862.40 tokens per second)
       eval time =    1067.60 ms /    44 tokens (   24.26 ms per token,    41.21 tokens per second)
      total time =    3423.80 ms /  2076 tokens
slot      release: id  3 | task 3966 | stop processing: n_tokens = 2955, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.151 (> 0.100 thold), f_keep = 0.160
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2664, total state size = 65.447 MiB
srv          load:  - looking for better prompt, base f_keep = 0.160, sim = 0.151
srv        update:  - cache state: 7 prompts, 1426.061 MiB (limits: 8192.000 MiB, 56064 tokens, 172805 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fc4fbf0:    3289 tokens, checkpoints:  8,   262.256 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv  get_availabl: prompt cache update took 60.90 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4012 | processing task, is_child = 0
slot update_slots: id  2 | task 4012 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2828
slot update_slots: id  2 | task 4012 | n_past = 426, slot.prompt.tokens.size() = 2664, seq_id = 2, pos_min = 2537, n_swa = 128
slot update_slots: id  2 | task 4012 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 4012 | erased invalidated context checkpoint (pos_min = 1301, pos_max = 2197, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 4012 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 4012 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.724187
slot update_slots: id  2 | task 4012 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 4012 | prompt processing progress, n_tokens = 2764, batch.n_tokens = 716, progress = 0.977369
slot update_slots: id  2 | task 4012 | n_tokens = 2764, memory_seq_rm [2764, end)
slot update_slots: id  2 | task 4012 | prompt processing progress, n_tokens = 2828, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4012 | prompt done, n_tokens = 2828, batch.n_tokens = 64
slot init_sampler: id  2 | task 4012 | init sampler, took 0.40 ms, tokens: text = 2828, total = 2828
slot update_slots: id  2 | task 4012 | created context checkpoint 1 of 8 (pos_min = 1867, pos_max = 2763, size = 21.034 MiB)
slot print_timing: id  2 | task 4012 | 
prompt eval time =    3034.32 ms /  2828 tokens (    1.07 ms per token,   932.00 tokens per second)
       eval time =    7567.17 ms /   309 tokens (   24.49 ms per token,    40.83 tokens per second)
      total time =   10601.49 ms /  3137 tokens
slot      release: id  2 | task 4012 | stop processing: n_tokens = 3136, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.297
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2955, total state size = 72.270 MiB
srv          load:  - looking for better prompt, base f_keep = 0.297, sim = 0.963
srv        update:  - cache state: 8 prompts, 1560.683 MiB (limits: 8192.000 MiB, 56064 tokens, 173410 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fc4fbf0:    3289 tokens, checkpoints:  8,   262.256 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8fd36650:    2955 tokens, checkpoints:  3,   134.622 MiB
srv  get_availabl: prompt cache update took 76.66 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4324 | processing task, is_child = 0
slot update_slots: id  3 | task 4324 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 913
slot update_slots: id  3 | task 4324 | n_past = 879, slot.prompt.tokens.size() = 2955, seq_id = 3, pos_min = 2828, n_swa = 128
slot update_slots: id  3 | task 4324 | restored context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 4324 | erased invalidated context checkpoint (pos_min = 1951, pos_max = 2847, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 4324 | n_tokens = 879, memory_seq_rm [879, end)
slot update_slots: id  3 | task 4324 | prompt processing progress, n_tokens = 913, batch.n_tokens = 34, progress = 1.000000
slot update_slots: id  3 | task 4324 | prompt done, n_tokens = 913, batch.n_tokens = 34
slot init_sampler: id  3 | task 4324 | init sampler, took 0.15 ms, tokens: text = 913, total = 913
slot print_timing: id  3 | task 4324 | 
prompt eval time =     364.82 ms /    34 tokens (   10.73 ms per token,    93.20 tokens per second)
       eval time =    1561.15 ms /    65 tokens (   24.02 ms per token,    41.64 tokens per second)
      total time =    1925.98 ms /    99 tokens
slot      release: id  3 | task 4324 | stop processing: n_tokens = 977, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.937 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4390 | processing task, is_child = 0
slot update_slots: id  3 | task 4390 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1011
slot update_slots: id  3 | task 4390 | n_tokens = 947, memory_seq_rm [947, end)
slot update_slots: id  3 | task 4390 | prompt processing progress, n_tokens = 1011, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4390 | prompt done, n_tokens = 1011, batch.n_tokens = 64
slot init_sampler: id  3 | task 4390 | init sampler, took 0.16 ms, tokens: text = 1011, total = 1011
slot print_timing: id  3 | task 4390 | 
prompt eval time =     167.29 ms /    64 tokens (    2.61 ms per token,   382.56 tokens per second)
       eval time =    1122.43 ms /    46 tokens (   24.40 ms per token,    40.98 tokens per second)
      total time =    1289.72 ms /   110 tokens
slot      release: id  3 | task 4390 | stop processing: n_tokens = 1056, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.472 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4437 | processing task, is_child = 0
slot update_slots: id  3 | task 4437 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2178
slot update_slots: id  3 | task 4437 | n_tokens = 1029, memory_seq_rm [1029, end)
slot update_slots: id  3 | task 4437 | prompt processing progress, n_tokens = 2114, batch.n_tokens = 1085, progress = 0.970615
slot update_slots: id  3 | task 4437 | n_tokens = 2114, memory_seq_rm [2114, end)
slot update_slots: id  3 | task 4437 | prompt processing progress, n_tokens = 2178, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4437 | prompt done, n_tokens = 2178, batch.n_tokens = 64
slot init_sampler: id  3 | task 4437 | init sampler, took 0.39 ms, tokens: text = 2178, total = 2178
slot update_slots: id  3 | task 4437 | created context checkpoint 3 of 8 (pos_min = 1217, pos_max = 2113, size = 21.034 MiB)
slot print_timing: id  3 | task 4437 | 
prompt eval time =    1430.44 ms /  1149 tokens (    1.24 ms per token,   803.25 tokens per second)
       eval time =    1115.25 ms /    44 tokens (   25.35 ms per token,    39.45 tokens per second)
      total time =    2545.69 ms /  1193 tokens
slot      release: id  3 | task 4437 | stop processing: n_tokens = 2221, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.816 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4483 | processing task, is_child = 0
slot update_slots: id  3 | task 4483 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2696
slot update_slots: id  3 | task 4483 | n_tokens = 2201, memory_seq_rm [2201, end)
slot update_slots: id  3 | task 4483 | prompt processing progress, n_tokens = 2632, batch.n_tokens = 431, progress = 0.976261
slot update_slots: id  3 | task 4483 | n_tokens = 2632, memory_seq_rm [2632, end)
slot update_slots: id  3 | task 4483 | prompt processing progress, n_tokens = 2696, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4483 | prompt done, n_tokens = 2696, batch.n_tokens = 64
slot init_sampler: id  3 | task 4483 | init sampler, took 0.49 ms, tokens: text = 2696, total = 2696
slot update_slots: id  3 | task 4483 | created context checkpoint 4 of 8 (pos_min = 1735, pos_max = 2631, size = 21.034 MiB)
slot print_timing: id  3 | task 4483 | 
prompt eval time =     617.37 ms /   495 tokens (    1.25 ms per token,   801.79 tokens per second)
       eval time =    1040.26 ms /    42 tokens (   24.77 ms per token,    40.37 tokens per second)
      total time =    1657.63 ms /   537 tokens
slot      release: id  3 | task 4483 | stop processing: n_tokens = 2737, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.728 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4527 | processing task, is_child = 0
slot update_slots: id  3 | task 4527 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3720
slot update_slots: id  3 | task 4527 | n_tokens = 2710, memory_seq_rm [2710, end)
slot update_slots: id  3 | task 4527 | prompt processing progress, n_tokens = 3656, batch.n_tokens = 946, progress = 0.982796
slot update_slots: id  3 | task 4527 | n_tokens = 3656, memory_seq_rm [3656, end)
slot update_slots: id  3 | task 4527 | prompt processing progress, n_tokens = 3720, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4527 | prompt done, n_tokens = 3720, batch.n_tokens = 64
slot init_sampler: id  3 | task 4527 | init sampler, took 0.52 ms, tokens: text = 3720, total = 3720
slot update_slots: id  3 | task 4527 | created context checkpoint 5 of 8 (pos_min = 2759, pos_max = 3655, size = 21.034 MiB)
slot print_timing: id  3 | task 4527 | 
prompt eval time =    1212.48 ms /  1010 tokens (    1.20 ms per token,   833.00 tokens per second)
       eval time =     924.33 ms /    37 tokens (   24.98 ms per token,    40.03 tokens per second)
      total time =    2136.82 ms /  1047 tokens
slot      release: id  3 | task 4527 | stop processing: n_tokens = 3756, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.818 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4566 | processing task, is_child = 0
slot update_slots: id  3 | task 4566 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4559
slot update_slots: id  3 | task 4566 | n_tokens = 3730, memory_seq_rm [3730, end)
slot update_slots: id  3 | task 4566 | prompt processing progress, n_tokens = 4495, batch.n_tokens = 765, progress = 0.985962
slot update_slots: id  3 | task 4566 | n_tokens = 4495, memory_seq_rm [4495, end)
slot update_slots: id  3 | task 4566 | prompt processing progress, n_tokens = 4559, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4566 | prompt done, n_tokens = 4559, batch.n_tokens = 64
slot init_sampler: id  3 | task 4566 | init sampler, took 0.67 ms, tokens: text = 4559, total = 4559
slot update_slots: id  3 | task 4566 | created context checkpoint 6 of 8 (pos_min = 3598, pos_max = 4494, size = 21.034 MiB)
slot print_timing: id  3 | task 4566 | 
prompt eval time =    1128.70 ms /   829 tokens (    1.36 ms per token,   734.47 tokens per second)
       eval time =    1060.56 ms /    42 tokens (   25.25 ms per token,    39.60 tokens per second)
      total time =    2189.26 ms /   871 tokens
slot      release: id  3 | task 4566 | stop processing: n_tokens = 4600, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.128 (> 0.100 thold), f_keep = 0.136
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3136, total state size = 76.514 MiB
srv          load:  - looking for better prompt, base f_keep = 0.136, sim = 0.128
srv        update:  - cache state: 9 prompts, 1658.231 MiB (limits: 8192.000 MiB, 56064 tokens, 178701 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fc4fbf0:    3289 tokens, checkpoints:  8,   262.256 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8fd36650:    2955 tokens, checkpoints:  3,   134.622 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv  get_availabl: prompt cache update took 80.92 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4610 | processing task, is_child = 0
slot update_slots: id  2 | task 4610 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3328
slot update_slots: id  2 | task 4610 | n_past = 425, slot.prompt.tokens.size() = 3136, seq_id = 2, pos_min = 3009, n_swa = 128
slot update_slots: id  2 | task 4610 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 4610 | erased invalidated context checkpoint (pos_min = 1867, pos_max = 2763, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 4610 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 4610 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.615385
slot update_slots: id  2 | task 4610 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 4610 | prompt processing progress, n_tokens = 3264, batch.n_tokens = 1216, progress = 0.980769
slot update_slots: id  2 | task 4610 | n_tokens = 3264, memory_seq_rm [3264, end)
slot update_slots: id  2 | task 4610 | prompt processing progress, n_tokens = 3328, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4610 | prompt done, n_tokens = 3328, batch.n_tokens = 64
slot init_sampler: id  2 | task 4610 | init sampler, took 0.49 ms, tokens: text = 3328, total = 3328
slot update_slots: id  2 | task 4610 | created context checkpoint 1 of 8 (pos_min = 2367, pos_max = 3263, size = 21.034 MiB)
slot print_timing: id  2 | task 4610 | 
prompt eval time =    3806.86 ms /  3328 tokens (    1.14 ms per token,   874.21 tokens per second)
       eval time =   10074.05 ms /   394 tokens (   25.57 ms per token,    39.11 tokens per second)
      total time =   13880.91 ms /  3722 tokens
slot      release: id  2 | task 4610 | stop processing: n_tokens = 3721, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.155 (> 0.100 thold), f_keep = 0.191
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4600, total state size = 110.844 MiB
srv          load:  - looking for better prompt, base f_keep = 0.191, sim = 0.155
srv          load:  - found better prompt with f_keep = 0.268, sim = 0.155
srv        update:  - cache state: 9 prompts, 1632.273 MiB (limits: 8192.000 MiB, 56064 tokens, 188123 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8fd36650:    2955 tokens, checkpoints:  3,   134.622 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv  get_availabl: prompt cache update took 898.71 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5007 | processing task, is_child = 0
slot update_slots: id  3 | task 5007 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5681
slot update_slots: id  3 | task 5007 | n_past = 883, slot.prompt.tokens.size() = 3289, seq_id = 3, pos_min = 2392, n_swa = 128
slot update_slots: id  3 | task 5007 | restored context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 5007 | erased invalidated context checkpoint (pos_min = 1168, pos_max = 1937, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  3 | task 5007 | erased invalidated context checkpoint (pos_min = 1219, pos_max = 2115, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 5007 | erased invalidated context checkpoint (pos_min = 1730, pos_max = 2626, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 5007 | erased invalidated context checkpoint (pos_min = 1876, pos_max = 2753, n_swa = 128, size = 20.588 MiB)
slot update_slots: id  3 | task 5007 | erased invalidated context checkpoint (pos_min = 2046, pos_max = 2942, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 5007 | erased invalidated context checkpoint (pos_min = 2268, pos_max = 3164, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 5007 | n_tokens = 883, memory_seq_rm [883, end)
slot update_slots: id  3 | task 5007 | prompt processing progress, n_tokens = 2931, batch.n_tokens = 2048, progress = 0.515930
slot update_slots: id  3 | task 5007 | n_tokens = 2931, memory_seq_rm [2931, end)
slot update_slots: id  3 | task 5007 | prompt processing progress, n_tokens = 4979, batch.n_tokens = 2048, progress = 0.876430
slot update_slots: id  3 | task 5007 | n_tokens = 4979, memory_seq_rm [4979, end)
slot update_slots: id  3 | task 5007 | prompt processing progress, n_tokens = 5617, batch.n_tokens = 638, progress = 0.988734
slot update_slots: id  3 | task 5007 | n_tokens = 5617, memory_seq_rm [5617, end)
slot update_slots: id  3 | task 5007 | prompt processing progress, n_tokens = 5681, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5007 | prompt done, n_tokens = 5681, batch.n_tokens = 64
slot init_sampler: id  3 | task 5007 | init sampler, took 1.18 ms, tokens: text = 5681, total = 5681
slot update_slots: id  3 | task 5007 | created context checkpoint 3 of 8 (pos_min = 4720, pos_max = 5616, size = 21.034 MiB)
slot print_timing: id  3 | task 5007 | 
prompt eval time =    5561.86 ms /  4798 tokens (    1.16 ms per token,   862.66 tokens per second)
       eval time =     998.54 ms /    40 tokens (   24.96 ms per token,    40.06 tokens per second)
      total time =    6560.41 ms /  4838 tokens
slot      release: id  3 | task 5007 | stop processing: n_tokens = 5720, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.320 (> 0.100 thold), f_keep = 0.114
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3721, total state size = 90.232 MiB
srv          load:  - looking for better prompt, base f_keep = 0.114, sim = 0.320
srv        update:  - cache state: 10 prompts, 1743.539 MiB (limits: 8192.000 MiB, 56064 tokens, 193601 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8fd36650:    2955 tokens, checkpoints:  3,   134.622 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv  get_availabl: prompt cache update took 44.31 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 5051 | processing task, is_child = 0
slot update_slots: id  2 | task 5051 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1327
slot update_slots: id  2 | task 5051 | n_past = 425, slot.prompt.tokens.size() = 3721, seq_id = 2, pos_min = 3594, n_swa = 128
slot update_slots: id  2 | task 5051 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 5051 | erased invalidated context checkpoint (pos_min = 2367, pos_max = 3263, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 5051 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 5051 | prompt processing progress, n_tokens = 1263, batch.n_tokens = 1263, progress = 0.951771
slot update_slots: id  2 | task 5051 | n_tokens = 1263, memory_seq_rm [1263, end)
slot update_slots: id  2 | task 5051 | prompt processing progress, n_tokens = 1327, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 5051 | prompt done, n_tokens = 1327, batch.n_tokens = 64
slot init_sampler: id  2 | task 5051 | init sampler, took 0.20 ms, tokens: text = 1327, total = 1327
slot update_slots: id  2 | task 5051 | created context checkpoint 1 of 8 (pos_min = 366, pos_max = 1262, size = 21.034 MiB)
slot print_timing: id  2 | task 5051 | 
prompt eval time =    1700.22 ms /  1327 tokens (    1.28 ms per token,   780.49 tokens per second)
       eval time =    9288.05 ms /   368 tokens (   25.24 ms per token,    39.62 tokens per second)
      total time =   10988.27 ms /  1695 tokens
slot      release: id  2 | task 5051 | stop processing: n_tokens = 1694, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.132 (> 0.100 thold), f_keep = 0.154
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5720, total state size = 137.107 MiB
srv          load:  - looking for better prompt, base f_keep = 0.154, sim = 0.132
srv        update:  - cache state: 11 prompts, 1942.997 MiB (limits: 8192.000 MiB, 56064 tokens, 197843 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8fd36650:    2955 tokens, checkpoints:  3,   134.622 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv  get_availabl: prompt cache update took 152.96 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5421 | processing task, is_child = 0
slot update_slots: id  3 | task 5421 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6670
slot update_slots: id  3 | task 5421 | n_past = 883, slot.prompt.tokens.size() = 5720, seq_id = 3, pos_min = 5593, n_swa = 128
slot update_slots: id  3 | task 5421 | restored context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 5421 | erased invalidated context checkpoint (pos_min = 4720, pos_max = 5616, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 5421 | n_tokens = 883, memory_seq_rm [883, end)
slot update_slots: id  3 | task 5421 | prompt processing progress, n_tokens = 2931, batch.n_tokens = 2048, progress = 0.439430
slot update_slots: id  3 | task 5421 | n_tokens = 2931, memory_seq_rm [2931, end)
slot update_slots: id  3 | task 5421 | prompt processing progress, n_tokens = 4979, batch.n_tokens = 2048, progress = 0.746477
slot update_slots: id  3 | task 5421 | n_tokens = 4979, memory_seq_rm [4979, end)
slot update_slots: id  3 | task 5421 | prompt processing progress, n_tokens = 6606, batch.n_tokens = 1627, progress = 0.990405
slot update_slots: id  3 | task 5421 | n_tokens = 6606, memory_seq_rm [6606, end)
slot update_slots: id  3 | task 5421 | prompt processing progress, n_tokens = 6670, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5421 | prompt done, n_tokens = 6670, batch.n_tokens = 64
slot init_sampler: id  3 | task 5421 | init sampler, took 1.02 ms, tokens: text = 6670, total = 6670
slot update_slots: id  3 | task 5421 | created context checkpoint 3 of 8 (pos_min = 5709, pos_max = 6605, size = 21.034 MiB)
slot print_timing: id  3 | task 5421 | 
prompt eval time =    6094.64 ms /  5787 tokens (    1.05 ms per token,   949.52 tokens per second)
       eval time =    1128.40 ms /    43 tokens (   26.24 ms per token,    38.11 tokens per second)
      total time =    7223.04 ms /  5830 tokens
slot      release: id  3 | task 5421 | stop processing: n_tokens = 6712, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.172 (> 0.100 thold), f_keep = 0.253
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1694, total state size = 42.701 MiB
srv          load:  - looking for better prompt, base f_keep = 0.253, sim = 0.172
srv        update:  - cache state: 12 prompts, 2006.732 MiB (limits: 8192.000 MiB, 56064 tokens, 198475 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8fd36650:    2955 tokens, checkpoints:  3,   134.622 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv  get_availabl: prompt cache update took 51.60 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 5468 | processing task, is_child = 0
slot update_slots: id  2 | task 5468 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2492
slot update_slots: id  2 | task 5468 | n_past = 429, slot.prompt.tokens.size() = 1694, seq_id = 2, pos_min = 1567, n_swa = 128
slot update_slots: id  2 | task 5468 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 5468 | erased invalidated context checkpoint (pos_min = 366, pos_max = 1262, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 5468 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 5468 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.821830
slot update_slots: id  2 | task 5468 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 5468 | prompt processing progress, n_tokens = 2428, batch.n_tokens = 380, progress = 0.974318
slot update_slots: id  2 | task 5468 | n_tokens = 2428, memory_seq_rm [2428, end)
slot update_slots: id  2 | task 5468 | prompt processing progress, n_tokens = 2492, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 5468 | prompt done, n_tokens = 2492, batch.n_tokens = 64
slot init_sampler: id  2 | task 5468 | init sampler, took 0.35 ms, tokens: text = 2492, total = 2492
slot update_slots: id  2 | task 5468 | created context checkpoint 1 of 8 (pos_min = 1531, pos_max = 2427, size = 21.034 MiB)
slot print_timing: id  2 | task 5468 | 
prompt eval time =    2779.56 ms /  2492 tokens (    1.12 ms per token,   896.55 tokens per second)
       eval time =    6424.43 ms /   258 tokens (   24.90 ms per token,    40.16 tokens per second)
      total time =    9203.98 ms /  2750 tokens
slot      release: id  2 | task 5468 | stop processing: n_tokens = 2749, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.296 (> 0.100 thold), f_keep = 0.132
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6712, total state size = 160.368 MiB
srv          load:  - looking for better prompt, base f_keep = 0.132, sim = 0.296
srv        update:  - cache state: 13 prompts, 2229.452 MiB (limits: 8192.000 MiB, 56064 tokens, 203310 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8fd36650:    2955 tokens, checkpoints:  3,   134.622 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv  get_availabl: prompt cache update took 186.81 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5729 | processing task, is_child = 0
slot update_slots: id  3 | task 5729 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2983
slot update_slots: id  3 | task 5729 | n_past = 883, slot.prompt.tokens.size() = 6712, seq_id = 3, pos_min = 6585, n_swa = 128
slot update_slots: id  3 | task 5729 | restored context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 5729 | erased invalidated context checkpoint (pos_min = 5709, pos_max = 6605, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 5729 | n_tokens = 883, memory_seq_rm [883, end)
slot update_slots: id  3 | task 5729 | prompt processing progress, n_tokens = 2919, batch.n_tokens = 2036, progress = 0.978545
slot update_slots: id  3 | task 5729 | n_tokens = 2919, memory_seq_rm [2919, end)
slot update_slots: id  3 | task 5729 | prompt processing progress, n_tokens = 2983, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5729 | prompt done, n_tokens = 2983, batch.n_tokens = 64
slot init_sampler: id  3 | task 5729 | init sampler, took 0.44 ms, tokens: text = 2983, total = 2983
slot update_slots: id  3 | task 5729 | created context checkpoint 3 of 8 (pos_min = 2022, pos_max = 2918, size = 21.034 MiB)
slot print_timing: id  3 | task 5729 | 
prompt eval time =    2414.09 ms /  2100 tokens (    1.15 ms per token,   869.89 tokens per second)
       eval time =    1452.66 ms /    58 tokens (   25.05 ms per token,    39.93 tokens per second)
      total time =    3866.75 ms /  2158 tokens
slot      release: id  3 | task 5729 | stop processing: n_tokens = 3040, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.179 (> 0.100 thold), f_keep = 0.156
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2749, total state size = 67.440 MiB
srv          load:  - looking for better prompt, base f_keep = 0.156, sim = 0.179
srv        update:  - cache state: 14 prompts, 2317.925 MiB (limits: 8192.000 MiB, 56064 tokens, 205266 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8fd36650:    2955 tokens, checkpoints:  3,   134.622 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv  get_availabl: prompt cache update took 74.63 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 5789 | processing task, is_child = 0
slot update_slots: id  2 | task 5789 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2401
slot update_slots: id  2 | task 5789 | n_past = 429, slot.prompt.tokens.size() = 2749, seq_id = 2, pos_min = 2622, n_swa = 128
slot update_slots: id  2 | task 5789 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 5789 | erased invalidated context checkpoint (pos_min = 1531, pos_max = 2427, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 5789 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 5789 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.852978
slot update_slots: id  2 | task 5789 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 5789 | prompt processing progress, n_tokens = 2337, batch.n_tokens = 289, progress = 0.973344
slot update_slots: id  2 | task 5789 | n_tokens = 2337, memory_seq_rm [2337, end)
slot update_slots: id  2 | task 5789 | prompt processing progress, n_tokens = 2401, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 5789 | prompt done, n_tokens = 2401, batch.n_tokens = 64
slot init_sampler: id  2 | task 5789 | init sampler, took 0.45 ms, tokens: text = 2401, total = 2401
slot update_slots: id  2 | task 5789 | created context checkpoint 1 of 8 (pos_min = 1440, pos_max = 2336, size = 21.034 MiB)
slot print_timing: id  2 | task 5789 | 
prompt eval time =    2535.59 ms /  2401 tokens (    1.06 ms per token,   946.92 tokens per second)
       eval time =    2062.79 ms /    85 tokens (   24.27 ms per token,    41.21 tokens per second)
      total time =    4598.38 ms /  2486 tokens
slot      release: id  2 | task 5789 | stop processing: n_tokens = 2485, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.175 (> 0.100 thold), f_keep = 0.290
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3040, total state size = 74.263 MiB
srv          load:  - looking for better prompt, base f_keep = 0.290, sim = 0.175
srv        update:  - cache state: 15 prompts, 2454.540 MiB (limits: 8192.000 MiB, 56064 tokens, 203987 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8fd36650:    2955 tokens, checkpoints:  3,   134.622 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8fdda790:    3040 tokens, checkpoints:  3,   136.615 MiB
srv  get_availabl: prompt cache update took 105.96 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5877 | processing task, is_child = 0
slot update_slots: id  3 | task 5877 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5048
slot update_slots: id  3 | task 5877 | n_past = 883, slot.prompt.tokens.size() = 3040, seq_id = 3, pos_min = 2913, n_swa = 128
slot update_slots: id  3 | task 5877 | restored context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 5877 | erased invalidated context checkpoint (pos_min = 2022, pos_max = 2918, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 5877 | n_tokens = 883, memory_seq_rm [883, end)
slot update_slots: id  3 | task 5877 | prompt processing progress, n_tokens = 2931, batch.n_tokens = 2048, progress = 0.580626
slot update_slots: id  3 | task 5877 | n_tokens = 2931, memory_seq_rm [2931, end)
slot update_slots: id  3 | task 5877 | prompt processing progress, n_tokens = 4979, batch.n_tokens = 2048, progress = 0.986331
slot update_slots: id  3 | task 5877 | n_tokens = 4979, memory_seq_rm [4979, end)
slot update_slots: id  3 | task 5877 | prompt processing progress, n_tokens = 4984, batch.n_tokens = 5, progress = 0.987322
slot update_slots: id  3 | task 5877 | n_tokens = 4984, memory_seq_rm [4984, end)
slot update_slots: id  3 | task 5877 | prompt processing progress, n_tokens = 5048, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5877 | prompt done, n_tokens = 5048, batch.n_tokens = 64
slot init_sampler: id  3 | task 5877 | init sampler, took 0.72 ms, tokens: text = 5048, total = 5048
slot update_slots: id  3 | task 5877 | created context checkpoint 3 of 8 (pos_min = 4087, pos_max = 4983, size = 21.034 MiB)
slot print_timing: id  3 | task 5877 | 
prompt eval time =    4476.65 ms /  4165 tokens (    1.07 ms per token,   930.38 tokens per second)
       eval time =    1064.76 ms /    43 tokens (   24.76 ms per token,    40.38 tokens per second)
      total time =    5541.41 ms /  4208 tokens
slot      release: id  3 | task 5877 | stop processing: n_tokens = 5090, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.726 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5924 | processing task, is_child = 0
slot update_slots: id  3 | task 5924 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6964
slot update_slots: id  3 | task 5924 | n_tokens = 5059, memory_seq_rm [5059, end)
slot update_slots: id  3 | task 5924 | prompt processing progress, n_tokens = 6900, batch.n_tokens = 1841, progress = 0.990810
slot update_slots: id  3 | task 5924 | n_tokens = 6900, memory_seq_rm [6900, end)
slot update_slots: id  3 | task 5924 | prompt processing progress, n_tokens = 6964, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5924 | prompt done, n_tokens = 6964, batch.n_tokens = 64
slot init_sampler: id  3 | task 5924 | init sampler, took 0.96 ms, tokens: text = 6964, total = 6964
slot update_slots: id  3 | task 5924 | created context checkpoint 4 of 8 (pos_min = 6003, pos_max = 6899, size = 21.034 MiB)
slot print_timing: id  3 | task 5924 | 
prompt eval time =    2213.60 ms /  1905 tokens (    1.16 ms per token,   860.59 tokens per second)
       eval time =    1048.88 ms /    42 tokens (   24.97 ms per token,    40.04 tokens per second)
      total time =    3262.48 ms /  1947 tokens
slot      release: id  3 | task 5924 | stop processing: n_tokens = 7005, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5968 | processing task, is_child = 0
slot update_slots: id  3 | task 5968 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7033
slot update_slots: id  3 | task 5968 | n_tokens = 6974, memory_seq_rm [6974, end)
slot update_slots: id  3 | task 5968 | prompt processing progress, n_tokens = 7033, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 5968 | prompt done, n_tokens = 7033, batch.n_tokens = 59
slot init_sampler: id  3 | task 5968 | init sampler, took 1.00 ms, tokens: text = 7033, total = 7033
slot update_slots: id  3 | task 5968 | created context checkpoint 5 of 8 (pos_min = 6108, pos_max = 6973, size = 20.307 MiB)
slot print_timing: id  3 | task 5968 | 
prompt eval time =     179.85 ms /    59 tokens (    3.05 ms per token,   328.06 tokens per second)
       eval time =    1440.18 ms /    54 tokens (   26.67 ms per token,    37.50 tokens per second)
      total time =    1620.02 ms /   113 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5968 | stop processing: n_tokens = 7086, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 6023 | processing task, is_child = 0
slot update_slots: id  1 | task 6023 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4614
slot update_slots: id  1 | task 6023 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 6023 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.443866
slot update_slots: id  1 | task 6023 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 6023 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.887733
slot update_slots: id  1 | task 6023 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 6023 | prompt processing progress, n_tokens = 4550, batch.n_tokens = 454, progress = 0.986129
slot update_slots: id  1 | task 6023 | n_tokens = 4550, memory_seq_rm [4550, end)
slot update_slots: id  1 | task 6023 | prompt processing progress, n_tokens = 4614, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 6023 | prompt done, n_tokens = 4614, batch.n_tokens = 64
slot init_sampler: id  1 | task 6023 | init sampler, took 0.67 ms, tokens: text = 4614, total = 4614
slot update_slots: id  1 | task 6023 | created context checkpoint 1 of 8 (pos_min = 3780, pos_max = 4549, size = 18.056 MiB)
srv          stop: cancel task, id_task = 6023
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 6023 | stop processing: n_tokens = 4834, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.952 (> 0.100 thold), f_keep = 0.124
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7086, total state size = 169.138 MiB
srv          load:  - looking for better prompt, base f_keep = 0.124, sim = 0.952
srv        update:  - cache state: 16 prompts, 2727.371 MiB (limits: 8192.000 MiB, 56064 tokens, 204865 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8fd36650:    2955 tokens, checkpoints:  3,   134.622 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8fdda790:    3040 tokens, checkpoints:  3,   136.615 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv  get_availabl: prompt cache update took 218.40 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6249 | processing task, is_child = 0
slot update_slots: id  3 | task 6249 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 923
slot update_slots: id  3 | task 6249 | n_past = 879, slot.prompt.tokens.size() = 7086, seq_id = 3, pos_min = 6959, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 6249 | failed to restore context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 6249 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 6249 | erased invalidated context checkpoint (pos_min = 4087, pos_max = 4983, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 6249 | erased invalidated context checkpoint (pos_min = 6003, pos_max = 6899, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 6249 | erased invalidated context checkpoint (pos_min = 6108, pos_max = 6973, n_swa = 128, size = 20.307 MiB)
slot update_slots: id  3 | task 6249 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 6249 | prompt processing progress, n_tokens = 859, batch.n_tokens = 859, progress = 0.930661
slot update_slots: id  3 | task 6249 | n_tokens = 859, memory_seq_rm [859, end)
slot update_slots: id  3 | task 6249 | prompt processing progress, n_tokens = 923, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6249 | prompt done, n_tokens = 923, batch.n_tokens = 64
slot init_sampler: id  3 | task 6249 | init sampler, took 0.18 ms, tokens: text = 923, total = 923
slot print_timing: id  3 | task 6249 | 
prompt eval time =    1275.18 ms /   923 tokens (    1.38 ms per token,   723.82 tokens per second)
       eval time =    1323.42 ms /    53 tokens (   24.97 ms per token,    40.05 tokens per second)
      total time =    2598.60 ms /   976 tokens
slot      release: id  3 | task 6249 | stop processing: n_tokens = 975, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.452 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6304 | processing task, is_child = 0
slot update_slots: id  3 | task 6304 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2098
slot update_slots: id  3 | task 6304 | n_tokens = 949, memory_seq_rm [949, end)
slot update_slots: id  3 | task 6304 | prompt processing progress, n_tokens = 2034, batch.n_tokens = 1085, progress = 0.969495
slot update_slots: id  3 | task 6304 | n_tokens = 2034, memory_seq_rm [2034, end)
slot update_slots: id  3 | task 6304 | prompt processing progress, n_tokens = 2098, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6304 | prompt done, n_tokens = 2098, batch.n_tokens = 64
slot init_sampler: id  3 | task 6304 | init sampler, took 0.29 ms, tokens: text = 2098, total = 2098
slot update_slots: id  3 | task 6304 | created context checkpoint 3 of 8 (pos_min = 1264, pos_max = 2033, size = 18.056 MiB)
slot print_timing: id  3 | task 6304 | 
prompt eval time =    1570.83 ms /  1149 tokens (    1.37 ms per token,   731.46 tokens per second)
       eval time =    1038.60 ms /    41 tokens (   25.33 ms per token,    39.48 tokens per second)
      total time =    2609.43 ms /  1190 tokens
slot      release: id  3 | task 6304 | stop processing: n_tokens = 2138, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.830 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6347 | processing task, is_child = 0
slot update_slots: id  3 | task 6347 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2552
slot update_slots: id  3 | task 6347 | n_tokens = 2118, memory_seq_rm [2118, end)
slot update_slots: id  3 | task 6347 | prompt processing progress, n_tokens = 2488, batch.n_tokens = 370, progress = 0.974922
slot update_slots: id  3 | task 6347 | n_tokens = 2488, memory_seq_rm [2488, end)
slot update_slots: id  3 | task 6347 | prompt processing progress, n_tokens = 2552, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6347 | prompt done, n_tokens = 2552, batch.n_tokens = 64
slot init_sampler: id  3 | task 6347 | init sampler, took 0.37 ms, tokens: text = 2552, total = 2552
slot update_slots: id  3 | task 6347 | created context checkpoint 4 of 8 (pos_min = 1718, pos_max = 2487, size = 18.056 MiB)
slot print_timing: id  3 | task 6347 | 
prompt eval time =     658.79 ms /   434 tokens (    1.52 ms per token,   658.79 tokens per second)
       eval time =    1198.56 ms /    46 tokens (   26.06 ms per token,    38.38 tokens per second)
      total time =    1857.35 ms /   480 tokens
slot      release: id  3 | task 6347 | stop processing: n_tokens = 2597, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.718 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6395 | processing task, is_child = 0
slot update_slots: id  3 | task 6395 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3576
slot update_slots: id  3 | task 6395 | n_tokens = 2566, memory_seq_rm [2566, end)
slot update_slots: id  3 | task 6395 | prompt processing progress, n_tokens = 3512, batch.n_tokens = 946, progress = 0.982103
slot update_slots: id  3 | task 6395 | n_tokens = 3512, memory_seq_rm [3512, end)
slot update_slots: id  3 | task 6395 | prompt processing progress, n_tokens = 3576, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6395 | prompt done, n_tokens = 3576, batch.n_tokens = 64
slot init_sampler: id  3 | task 6395 | init sampler, took 0.67 ms, tokens: text = 3576, total = 3576
slot update_slots: id  3 | task 6395 | created context checkpoint 5 of 8 (pos_min = 2742, pos_max = 3511, size = 18.056 MiB)
slot print_timing: id  3 | task 6395 | 
prompt eval time =    1340.56 ms /  1010 tokens (    1.33 ms per token,   753.42 tokens per second)
       eval time =     973.46 ms /    37 tokens (   26.31 ms per token,    38.01 tokens per second)
      total time =    2314.01 ms /  1047 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 6395 | stop processing: n_tokens = 3612, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.812 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6434 | processing task, is_child = 0
slot update_slots: id  3 | task 6434 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4415
slot update_slots: id  3 | task 6434 | n_tokens = 3586, memory_seq_rm [3586, end)
slot update_slots: id  3 | task 6434 | prompt processing progress, n_tokens = 4351, batch.n_tokens = 765, progress = 0.985504
slot update_slots: id  3 | task 6434 | n_tokens = 4351, memory_seq_rm [4351, end)
slot update_slots: id  3 | task 6434 | prompt processing progress, n_tokens = 4415, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6434 | prompt done, n_tokens = 4415, batch.n_tokens = 64
slot init_sampler: id  3 | task 6434 | init sampler, took 0.61 ms, tokens: text = 4415, total = 4415
slot update_slots: id  3 | task 6434 | created context checkpoint 6 of 8 (pos_min = 3581, pos_max = 4350, size = 18.056 MiB)
slot print_timing: id  3 | task 6434 | 
prompt eval time =    1223.62 ms /   829 tokens (    1.48 ms per token,   677.50 tokens per second)
       eval time =    1046.15 ms /    41 tokens (   25.52 ms per token,    39.19 tokens per second)
      total time =    2269.78 ms /   870 tokens
slot      release: id  3 | task 6434 | stop processing: n_tokens = 4455, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.690 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6477 | processing task, is_child = 0
slot update_slots: id  3 | task 6477 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6418
slot update_slots: id  3 | task 6477 | n_tokens = 4428, memory_seq_rm [4428, end)
slot update_slots: id  3 | task 6477 | prompt processing progress, n_tokens = 6354, batch.n_tokens = 1926, progress = 0.990028
slot update_slots: id  3 | task 6477 | n_tokens = 6354, memory_seq_rm [6354, end)
slot update_slots: id  3 | task 6477 | prompt processing progress, n_tokens = 6418, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6477 | prompt done, n_tokens = 6418, batch.n_tokens = 64
slot init_sampler: id  3 | task 6477 | init sampler, took 1.13 ms, tokens: text = 6418, total = 6418
slot update_slots: id  3 | task 6477 | created context checkpoint 7 of 8 (pos_min = 5584, pos_max = 6353, size = 18.056 MiB)
slot print_timing: id  3 | task 6477 | 
prompt eval time =    2453.48 ms /  1990 tokens (    1.23 ms per token,   811.09 tokens per second)
       eval time =    1038.17 ms /    40 tokens (   25.95 ms per token,    38.53 tokens per second)
      total time =    3491.65 ms /  2030 tokens
slot      release: id  3 | task 6477 | stop processing: n_tokens = 6457, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.771 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6519 | processing task, is_child = 0
slot update_slots: id  3 | task 6519 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8331
slot update_slots: id  3 | task 6519 | n_tokens = 6426, memory_seq_rm [6426, end)
slot update_slots: id  3 | task 6519 | prompt processing progress, n_tokens = 8267, batch.n_tokens = 1841, progress = 0.992318
slot update_slots: id  3 | task 6519 | n_tokens = 8267, memory_seq_rm [8267, end)
slot update_slots: id  3 | task 6519 | prompt processing progress, n_tokens = 8331, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6519 | prompt done, n_tokens = 8331, batch.n_tokens = 64
slot init_sampler: id  3 | task 6519 | init sampler, took 1.17 ms, tokens: text = 8331, total = 8331
slot update_slots: id  3 | task 6519 | created context checkpoint 8 of 8 (pos_min = 7497, pos_max = 8266, size = 18.056 MiB)
slot print_timing: id  3 | task 6519 | 
prompt eval time =    2436.30 ms /  1905 tokens (    1.28 ms per token,   781.92 tokens per second)
       eval time =    7344.76 ms /   276 tokens (   26.61 ms per token,    37.58 tokens per second)
      total time =    9781.06 ms /  2181 tokens
slot      release: id  3 | task 6519 | stop processing: n_tokens = 8606, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.935 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6797 | processing task, is_child = 0
slot update_slots: id  3 | task 6797 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9172
slot update_slots: id  3 | task 6797 | n_tokens = 8576, memory_seq_rm [8576, end)
slot update_slots: id  3 | task 6797 | prompt processing progress, n_tokens = 9108, batch.n_tokens = 532, progress = 0.993022
slot update_slots: id  3 | task 6797 | n_tokens = 9108, memory_seq_rm [9108, end)
slot update_slots: id  3 | task 6797 | prompt processing progress, n_tokens = 9172, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6797 | prompt done, n_tokens = 9172, batch.n_tokens = 64
slot init_sampler: id  3 | task 6797 | init sampler, took 1.83 ms, tokens: text = 9172, total = 9172
slot update_slots: id  3 | task 6797 | erasing old context checkpoint (pos_min = 505, pos_max = 1385, size = 20.659 MiB)
slot update_slots: id  3 | task 6797 | created context checkpoint 8 of 8 (pos_min = 8338, pos_max = 9107, size = 18.056 MiB)
slot print_timing: id  3 | task 6797 | 
prompt eval time =     954.18 ms /   596 tokens (    1.60 ms per token,   624.62 tokens per second)
       eval time =    4738.66 ms /   178 tokens (   26.62 ms per token,    37.56 tokens per second)
      total time =    5692.84 ms /   774 tokens
slot      release: id  3 | task 6797 | stop processing: n_tokens = 9349, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 6977 | processing task, is_child = 0
slot update_slots: id  0 | task 6977 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7939
slot update_slots: id  0 | task 6977 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 6977 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.257967
slot update_slots: id  0 | task 6977 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 6977 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.515934
slot update_slots: id  0 | task 6977 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 6977 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.773901
slot update_slots: id  0 | task 6977 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 6977 | prompt processing progress, n_tokens = 7875, batch.n_tokens = 1731, progress = 0.991939
slot update_slots: id  0 | task 6977 | n_tokens = 7875, memory_seq_rm [7875, end)
slot update_slots: id  0 | task 6977 | prompt processing progress, n_tokens = 7939, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 6977 | prompt done, n_tokens = 7939, batch.n_tokens = 64
slot init_sampler: id  0 | task 6977 | init sampler, took 1.12 ms, tokens: text = 7939, total = 7939
slot update_slots: id  0 | task 6977 | created context checkpoint 1 of 8 (pos_min = 7232, pos_max = 7874, size = 15.078 MiB)
slot print_timing: id  0 | task 6977 | 
prompt eval time =   10482.45 ms /  7939 tokens (    1.32 ms per token,   757.36 tokens per second)
       eval time =    3027.37 ms /   113 tokens (   26.79 ms per token,    37.33 tokens per second)
      total time =   13509.82 ms /  8052 tokens
slot      release: id  0 | task 6977 | stop processing: n_tokens = 8051, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.291 (> 0.100 thold), f_keep = 0.094
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9349, total state size = 222.203 MiB
srv          load:  - looking for better prompt, base f_keep = 0.094, sim = 0.291
srv          load:  - found better prompt with f_keep = 0.298, sim = 0.291
srv        update:  - cache state: 16 prompts, 2962.002 MiB (limits: 8192.000 MiB, 56064 tokens, 206320 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8fdda790:    3040 tokens, checkpoints:  3,   136.615 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv  get_availabl: prompt cache update took 319.84 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7095 | processing task, is_child = 0
slot update_slots: id  3 | task 7095 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3024
slot update_slots: id  3 | task 7095 | n_past = 880, slot.prompt.tokens.size() = 2955, seq_id = 3, pos_min = 2828, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 7095 | failed to restore context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 7095 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 7095 | erased invalidated context checkpoint (pos_min = 1951, pos_max = 2847, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 7095 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 7095 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.677249
slot update_slots: id  3 | task 7095 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 7095 | prompt processing progress, n_tokens = 2960, batch.n_tokens = 912, progress = 0.978836
slot update_slots: id  3 | task 7095 | n_tokens = 2960, memory_seq_rm [2960, end)
slot update_slots: id  3 | task 7095 | prompt processing progress, n_tokens = 3024, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7095 | prompt done, n_tokens = 3024, batch.n_tokens = 64
slot init_sampler: id  3 | task 7095 | init sampler, took 0.59 ms, tokens: text = 3024, total = 3024
slot update_slots: id  3 | task 7095 | created context checkpoint 3 of 8 (pos_min = 2317, pos_max = 2959, size = 15.078 MiB)
slot print_timing: id  3 | task 7095 | 
prompt eval time =    4190.92 ms /  3024 tokens (    1.39 ms per token,   721.56 tokens per second)
       eval time =    1494.19 ms /    55 tokens (   27.17 ms per token,    36.81 tokens per second)
      total time =    5685.11 ms /  3079 tokens
slot      release: id  3 | task 7095 | stop processing: n_tokens = 3078, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.927 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7153 | processing task, is_child = 0
slot update_slots: id  3 | task 7153 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3294
slot update_slots: id  3 | task 7153 | n_tokens = 3053, memory_seq_rm [3053, end)
slot update_slots: id  3 | task 7153 | prompt processing progress, n_tokens = 3230, batch.n_tokens = 177, progress = 0.980571
slot update_slots: id  3 | task 7153 | n_tokens = 3230, memory_seq_rm [3230, end)
slot update_slots: id  3 | task 7153 | prompt processing progress, n_tokens = 3294, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7153 | prompt done, n_tokens = 3294, batch.n_tokens = 64
slot init_sampler: id  3 | task 7153 | init sampler, took 0.46 ms, tokens: text = 3294, total = 3294
slot update_slots: id  3 | task 7153 | created context checkpoint 4 of 8 (pos_min = 2587, pos_max = 3229, size = 15.078 MiB)
slot print_timing: id  3 | task 7153 | 
prompt eval time =     569.36 ms /   241 tokens (    2.36 ms per token,   423.28 tokens per second)
       eval time =    1102.04 ms /    41 tokens (   26.88 ms per token,    37.20 tokens per second)
      total time =    1671.40 ms /   282 tokens
slot      release: id  3 | task 7153 | stop processing: n_tokens = 3334, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7196 | processing task, is_child = 0
slot update_slots: id  3 | task 7196 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3363
slot update_slots: id  3 | task 7196 | n_tokens = 3307, memory_seq_rm [3307, end)
slot update_slots: id  3 | task 7196 | prompt processing progress, n_tokens = 3363, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  3 | task 7196 | prompt done, n_tokens = 3363, batch.n_tokens = 56
slot init_sampler: id  3 | task 7196 | init sampler, took 0.47 ms, tokens: text = 3363, total = 3363
slot update_slots: id  3 | task 7196 | created context checkpoint 5 of 8 (pos_min = 2691, pos_max = 3306, size = 14.445 MiB)
slot print_timing: id  3 | task 7196 | 
prompt eval time =     302.59 ms /    56 tokens (    5.40 ms per token,   185.07 tokens per second)
       eval time =    1026.11 ms /    39 tokens (   26.31 ms per token,    38.01 tokens per second)
      total time =    1328.69 ms /    95 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 7196 | stop processing: n_tokens = 3401, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7236 | processing task, is_child = 0
slot update_slots: id  3 | task 7236 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3444
slot update_slots: id  3 | task 7236 | n_tokens = 3374, memory_seq_rm [3374, end)
slot update_slots: id  3 | task 7236 | prompt processing progress, n_tokens = 3380, batch.n_tokens = 6, progress = 0.981417
slot update_slots: id  3 | task 7236 | n_tokens = 3380, memory_seq_rm [3380, end)
slot update_slots: id  3 | task 7236 | prompt processing progress, n_tokens = 3444, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7236 | prompt done, n_tokens = 3444, batch.n_tokens = 64
slot init_sampler: id  3 | task 7236 | init sampler, took 0.65 ms, tokens: text = 3444, total = 3444
slot update_slots: id  3 | task 7236 | created context checkpoint 6 of 8 (pos_min = 2758, pos_max = 3379, size = 14.586 MiB)
slot print_timing: id  3 | task 7236 | 
prompt eval time =     269.80 ms /    70 tokens (    3.85 ms per token,   259.45 tokens per second)
       eval time =    1455.49 ms /    54 tokens (   26.95 ms per token,    37.10 tokens per second)
      total time =    1725.29 ms /   124 tokens
slot      release: id  3 | task 7236 | stop processing: n_tokens = 3497, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7292 | processing task, is_child = 0
slot update_slots: id  3 | task 7292 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3540
slot update_slots: id  3 | task 7292 | n_tokens = 3473, memory_seq_rm [3473, end)
slot update_slots: id  3 | task 7292 | prompt processing progress, n_tokens = 3476, batch.n_tokens = 3, progress = 0.981921
slot update_slots: id  3 | task 7292 | n_tokens = 3476, memory_seq_rm [3476, end)
slot update_slots: id  3 | task 7292 | prompt processing progress, n_tokens = 3540, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7292 | prompt done, n_tokens = 3540, batch.n_tokens = 64
slot init_sampler: id  3 | task 7292 | init sampler, took 0.66 ms, tokens: text = 3540, total = 3540
slot update_slots: id  3 | task 7292 | created context checkpoint 7 of 8 (pos_min = 2854, pos_max = 3475, size = 14.586 MiB)
slot print_timing: id  3 | task 7292 | 
prompt eval time =     253.25 ms /    67 tokens (    3.78 ms per token,   264.56 tokens per second)
       eval time =     524.27 ms /    19 tokens (   27.59 ms per token,    36.24 tokens per second)
      total time =     777.52 ms /    86 tokens
slot      release: id  3 | task 7292 | stop processing: n_tokens = 3558, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7313 | processing task, is_child = 0
slot update_slots: id  3 | task 7313 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3609
slot update_slots: id  3 | task 7313 | n_tokens = 3549, memory_seq_rm [3549, end)
slot update_slots: id  3 | task 7313 | prompt processing progress, n_tokens = 3609, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  3 | task 7313 | prompt done, n_tokens = 3609, batch.n_tokens = 60
slot init_sampler: id  3 | task 7313 | init sampler, took 0.52 ms, tokens: text = 3609, total = 3609
slot update_slots: id  3 | task 7313 | created context checkpoint 8 of 8 (pos_min = 2915, pos_max = 3548, size = 14.867 MiB)
slot print_timing: id  3 | task 7313 | 
prompt eval time =     188.00 ms /    60 tokens (    3.13 ms per token,   319.15 tokens per second)
       eval time =   17009.37 ms /   626 tokens (   27.17 ms per token,    36.80 tokens per second)
      total time =   17197.37 ms /   686 tokens
slot      release: id  3 | task 7313 | stop processing: n_tokens = 4234, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.709 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7940 | processing task, is_child = 0
slot update_slots: id  3 | task 7940 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5939
slot update_slots: id  3 | task 7940 | n_tokens = 4208, memory_seq_rm [4208, end)
slot update_slots: id  3 | task 7940 | prompt processing progress, n_tokens = 5875, batch.n_tokens = 1667, progress = 0.989224
slot update_slots: id  3 | task 7940 | n_tokens = 5875, memory_seq_rm [5875, end)
slot update_slots: id  3 | task 7940 | prompt processing progress, n_tokens = 5939, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7940 | prompt done, n_tokens = 5939, batch.n_tokens = 64
slot init_sampler: id  3 | task 7940 | init sampler, took 0.96 ms, tokens: text = 5939, total = 5939
slot update_slots: id  3 | task 7940 | erasing old context checkpoint (pos_min = 505, pos_max = 1385, size = 20.659 MiB)
slot update_slots: id  3 | task 7940 | created context checkpoint 8 of 8 (pos_min = 5232, pos_max = 5874, size = 15.078 MiB)
slot print_timing: id  3 | task 7940 | 
prompt eval time =    2638.63 ms /  1731 tokens (    1.52 ms per token,   656.02 tokens per second)
       eval time =   22597.61 ms /   816 tokens (   27.69 ms per token,    36.11 tokens per second)
      total time =   25236.24 ms /  2547 tokens
slot      release: id  3 | task 7940 | stop processing: n_tokens = 6754, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8758 | processing task, is_child = 0
slot update_slots: id  3 | task 8758 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6798
slot update_slots: id  3 | task 8758 | n_tokens = 6726, memory_seq_rm [6726, end)
slot update_slots: id  3 | task 8758 | prompt processing progress, n_tokens = 6734, batch.n_tokens = 8, progress = 0.990585
slot update_slots: id  3 | task 8758 | n_tokens = 6734, memory_seq_rm [6734, end)
slot update_slots: id  3 | task 8758 | prompt processing progress, n_tokens = 6798, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8758 | prompt done, n_tokens = 6798, batch.n_tokens = 64
slot init_sampler: id  3 | task 8758 | init sampler, took 0.97 ms, tokens: text = 6798, total = 6798
slot update_slots: id  3 | task 8758 | erasing old context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 8758 | created context checkpoint 8 of 8 (pos_min = 6111, pos_max = 6733, size = 14.609 MiB)
slot print_timing: id  3 | task 8758 | 
prompt eval time =     299.81 ms /    72 tokens (    4.16 ms per token,   240.15 tokens per second)
       eval time =   23556.07 ms /   853 tokens (   27.62 ms per token,    36.21 tokens per second)
      total time =   23855.89 ms /   925 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 8758 | stop processing: n_tokens = 7650, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.179 (> 0.100 thold), f_keep = 0.115
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7650, total state size = 194.463 MiB
srv          load:  - looking for better prompt, base f_keep = 0.115, sim = 0.179
srv        update:  - cache state: 17 prompts, 3274.791 MiB (limits: 8192.000 MiB, 56064 tokens, 205751 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8fdda790:    3040 tokens, checkpoints:  3,   136.615 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv  get_availabl: prompt cache update took 265.18 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9613 | processing task, is_child = 0
slot update_slots: id  3 | task 9613 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4914
slot update_slots: id  3 | task 9613 | n_past = 880, slot.prompt.tokens.size() = 7650, seq_id = 3, pos_min = 7007, n_swa = 128
slot update_slots: id  3 | task 9613 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 9613 | erased invalidated context checkpoint (pos_min = 2317, pos_max = 2959, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 9613 | erased invalidated context checkpoint (pos_min = 2587, pos_max = 3229, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 9613 | erased invalidated context checkpoint (pos_min = 2691, pos_max = 3306, n_swa = 128, size = 14.445 MiB)
slot update_slots: id  3 | task 9613 | erased invalidated context checkpoint (pos_min = 2758, pos_max = 3379, n_swa = 128, size = 14.586 MiB)
slot update_slots: id  3 | task 9613 | erased invalidated context checkpoint (pos_min = 2854, pos_max = 3475, n_swa = 128, size = 14.586 MiB)
slot update_slots: id  3 | task 9613 | erased invalidated context checkpoint (pos_min = 2915, pos_max = 3548, n_swa = 128, size = 14.867 MiB)
slot update_slots: id  3 | task 9613 | erased invalidated context checkpoint (pos_min = 5232, pos_max = 5874, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 9613 | erased invalidated context checkpoint (pos_min = 6111, pos_max = 6733, n_swa = 128, size = 14.609 MiB)
slot update_slots: id  3 | task 9613 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 9613 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.416768
slot update_slots: id  3 | task 9613 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 9613 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.833537
slot update_slots: id  3 | task 9613 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 9613 | prompt processing progress, n_tokens = 4850, batch.n_tokens = 754, progress = 0.986976
slot update_slots: id  3 | task 9613 | n_tokens = 4850, memory_seq_rm [4850, end)
slot update_slots: id  3 | task 9613 | prompt processing progress, n_tokens = 4914, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9613 | prompt done, n_tokens = 4914, batch.n_tokens = 64
slot init_sampler: id  3 | task 9613 | init sampler, took 0.79 ms, tokens: text = 4914, total = 4914
slot update_slots: id  3 | task 9613 | created context checkpoint 1 of 8 (pos_min = 4207, pos_max = 4849, size = 15.078 MiB)
slot print_timing: id  3 | task 9613 | 
prompt eval time =    6590.06 ms /  4914 tokens (    1.34 ms per token,   745.67 tokens per second)
       eval time =    2481.68 ms /    94 tokens (   26.40 ms per token,    37.88 tokens per second)
      total time =    9071.74 ms /  5008 tokens
slot      release: id  3 | task 9613 | stop processing: n_tokens = 5007, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = 1174828381
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2485, total state size = 61.249 MiB
srv          load:  - looking for better prompt, base f_keep = 0.173, sim = 0.073
srv        update:  - cache state: 18 prompts, 3357.074 MiB (limits: 8192.000 MiB, 56064 tokens, 206772 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8fdda790:    3040 tokens, checkpoints:  3,   136.615 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv  get_availabl: prompt cache update took 33.84 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9711 | processing task, is_child = 0
slot update_slots: id  2 | task 9711 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5844
slot update_slots: id  2 | task 9711 | n_past = 429, slot.prompt.tokens.size() = 2485, seq_id = 2, pos_min = 2358, n_swa = 128
slot update_slots: id  2 | task 9711 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 9711 | erased invalidated context checkpoint (pos_min = 1440, pos_max = 2336, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 9711 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 9711 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.350445
slot update_slots: id  2 | task 9711 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 9711 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.700890
slot update_slots: id  2 | task 9711 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 9711 | prompt processing progress, n_tokens = 5780, batch.n_tokens = 1684, progress = 0.989049
slot update_slots: id  2 | task 9711 | n_tokens = 5780, memory_seq_rm [5780, end)
slot update_slots: id  2 | task 9711 | prompt processing progress, n_tokens = 5844, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9711 | prompt done, n_tokens = 5844, batch.n_tokens = 64
slot init_sampler: id  2 | task 9711 | init sampler, took 0.81 ms, tokens: text = 5844, total = 5844
slot update_slots: id  2 | task 9711 | created context checkpoint 1 of 8 (pos_min = 5137, pos_max = 5779, size = 15.078 MiB)
slot print_timing: id  2 | task 9711 | 
prompt eval time =    7917.10 ms /  5844 tokens (    1.35 ms per token,   738.15 tokens per second)
       eval time =   13846.65 ms /   512 tokens (   27.04 ms per token,    36.98 tokens per second)
      total time =   21763.75 ms /  6356 tokens
slot      release: id  2 | task 9711 | stop processing: n_tokens = 6355, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.162 (> 0.100 thold), f_keep = 0.176
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5007, total state size = 120.387 MiB
srv          load:  - looking for better prompt, base f_keep = 0.176, sim = 0.162
srv          load:  - found better prompt with f_keep = 0.290, sim = 0.162
srv        update:  - cache state: 18 prompts, 3355.924 MiB (limits: 8192.000 MiB, 56064 tokens, 211644 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv  get_availabl: prompt cache update took 675.76 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10227 | processing task, is_child = 0
slot update_slots: id  3 | task 10227 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5438
slot update_slots: id  3 | task 10227 | n_past = 881, slot.prompt.tokens.size() = 3040, seq_id = 3, pos_min = 2913, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 10227 | failed to restore context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 10227 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 10227 | erased invalidated context checkpoint (pos_min = 2022, pos_max = 2918, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 10227 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 10227 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.376609
slot update_slots: id  3 | task 10227 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 10227 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.753218
slot update_slots: id  3 | task 10227 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 10227 | prompt processing progress, n_tokens = 5374, batch.n_tokens = 1278, progress = 0.988231
slot update_slots: id  3 | task 10227 | n_tokens = 5374, memory_seq_rm [5374, end)
slot update_slots: id  3 | task 10227 | prompt processing progress, n_tokens = 5438, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10227 | prompt done, n_tokens = 5438, batch.n_tokens = 64
slot init_sampler: id  3 | task 10227 | init sampler, took 1.13 ms, tokens: text = 5438, total = 5438
slot update_slots: id  3 | task 10227 | created context checkpoint 3 of 8 (pos_min = 4731, pos_max = 5373, size = 15.078 MiB)
slot print_timing: id  3 | task 10227 | 
prompt eval time =    7564.61 ms /  5438 tokens (    1.39 ms per token,   718.87 tokens per second)
       eval time =    1346.76 ms /    48 tokens (   28.06 ms per token,    35.64 tokens per second)
      total time =    8911.37 ms /  5486 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 10227 | stop processing: n_tokens = 5485, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.741 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10279 | processing task, is_child = 0
slot update_slots: id  3 | task 10279 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7359
slot update_slots: id  3 | task 10279 | n_tokens = 5454, memory_seq_rm [5454, end)
slot update_slots: id  3 | task 10279 | prompt processing progress, n_tokens = 7295, batch.n_tokens = 1841, progress = 0.991303
slot update_slots: id  3 | task 10279 | n_tokens = 7295, memory_seq_rm [7295, end)
slot update_slots: id  3 | task 10279 | prompt processing progress, n_tokens = 7359, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10279 | prompt done, n_tokens = 7359, batch.n_tokens = 64
slot init_sampler: id  3 | task 10279 | init sampler, took 1.09 ms, tokens: text = 7359, total = 7359
slot update_slots: id  3 | task 10279 | created context checkpoint 4 of 8 (pos_min = 6652, pos_max = 7294, size = 15.078 MiB)
slot print_timing: id  3 | task 10279 | 
prompt eval time =    2824.37 ms /  1905 tokens (    1.48 ms per token,   674.49 tokens per second)
       eval time =    4511.20 ms /   165 tokens (   27.34 ms per token,    36.58 tokens per second)
      total time =    7335.57 ms /  2070 tokens
slot      release: id  3 | task 10279 | stop processing: n_tokens = 7523, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.926 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10446 | processing task, is_child = 0
slot update_slots: id  3 | task 10446 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8089
slot update_slots: id  3 | task 10446 | n_tokens = 7493, memory_seq_rm [7493, end)
slot update_slots: id  3 | task 10446 | prompt processing progress, n_tokens = 8025, batch.n_tokens = 532, progress = 0.992088
slot update_slots: id  3 | task 10446 | n_tokens = 8025, memory_seq_rm [8025, end)
slot update_slots: id  3 | task 10446 | prompt processing progress, n_tokens = 8089, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10446 | prompt done, n_tokens = 8089, batch.n_tokens = 64
slot init_sampler: id  3 | task 10446 | init sampler, took 1.12 ms, tokens: text = 8089, total = 8089
slot update_slots: id  3 | task 10446 | created context checkpoint 5 of 8 (pos_min = 7382, pos_max = 8024, size = 15.078 MiB)
slot print_timing: id  3 | task 10446 | 
prompt eval time =    1058.55 ms /   596 tokens (    1.78 ms per token,   563.03 tokens per second)
       eval time =    5647.60 ms /   205 tokens (   27.55 ms per token,    36.30 tokens per second)
      total time =    6706.16 ms /   801 tokens
slot      release: id  3 | task 10446 | stop processing: n_tokens = 8293, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.827 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10653 | processing task, is_child = 0
slot update_slots: id  3 | task 10653 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9995
slot update_slots: id  3 | task 10653 | n_tokens = 8264, memory_seq_rm [8264, end)
slot update_slots: id  3 | task 10653 | prompt processing progress, n_tokens = 9931, batch.n_tokens = 1667, progress = 0.993597
slot update_slots: id  3 | task 10653 | n_tokens = 9931, memory_seq_rm [9931, end)
slot update_slots: id  3 | task 10653 | prompt processing progress, n_tokens = 9995, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10653 | prompt done, n_tokens = 9995, batch.n_tokens = 64
slot init_sampler: id  3 | task 10653 | init sampler, took 1.37 ms, tokens: text = 9995, total = 9995
slot update_slots: id  3 | task 10653 | created context checkpoint 6 of 8 (pos_min = 9288, pos_max = 9930, size = 15.078 MiB)
slot print_timing: id  3 | task 10653 | 
prompt eval time =    2673.28 ms /  1731 tokens (    1.54 ms per token,   647.52 tokens per second)
       eval time =   35132.27 ms /  1277 tokens (   27.51 ms per token,    36.35 tokens per second)
      total time =   37805.56 ms /  3008 tokens
slot      release: id  3 | task 10653 | stop processing: n_tokens = 11271, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = 1196889439
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4834, total state size = 116.331 MiB
srv          load:  - looking for better prompt, base f_keep = 0.088, sim = 0.074
srv        update:  - cache state: 19 prompts, 3490.311 MiB (limits: 8192.000 MiB, 56064 tokens, 214841 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv        update:    - prompt 0x591f8fd107c0:    4834 tokens, checkpoints:  1,   134.387 MiB
srv  get_availabl: prompt cache update took 120.73 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 11932 | processing task, is_child = 0
slot update_slots: id  1 | task 11932 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5739
slot update_slots: id  1 | task 11932 | n_past = 427, slot.prompt.tokens.size() = 4834, seq_id = 1, pos_min = 4707, n_swa = 128
slot update_slots: id  1 | task 11932 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 11932 | erased invalidated context checkpoint (pos_min = 3780, pos_max = 4549, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 11932 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 11932 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.356857
slot update_slots: id  1 | task 11932 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 11932 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.713713
slot update_slots: id  1 | task 11932 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 11932 | prompt processing progress, n_tokens = 5675, batch.n_tokens = 1579, progress = 0.988848
slot update_slots: id  1 | task 11932 | n_tokens = 5675, memory_seq_rm [5675, end)
slot update_slots: id  1 | task 11932 | prompt processing progress, n_tokens = 5739, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 11932 | prompt done, n_tokens = 5739, batch.n_tokens = 64
slot init_sampler: id  1 | task 11932 | init sampler, took 1.06 ms, tokens: text = 5739, total = 5739
slot update_slots: id  1 | task 11932 | created context checkpoint 1 of 8 (pos_min = 5032, pos_max = 5674, size = 15.078 MiB)
slot print_timing: id  1 | task 11932 | 
prompt eval time =    8454.09 ms /  5739 tokens (    1.47 ms per token,   678.84 tokens per second)
       eval time =    8763.56 ms /   318 tokens (   27.56 ms per token,    36.29 tokens per second)
      total time =   17217.65 ms /  6057 tokens
slot      release: id  1 | task 11932 | stop processing: n_tokens = 6056, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.215 (> 0.100 thold), f_keep = 0.078
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 11271, total state size = 267.272 MiB
srv          load:  - looking for better prompt, base f_keep = 0.078, sim = 0.215
srv        update:  - cache state: 20 prompts, 3859.212 MiB (limits: 8192.000 MiB, 56064 tokens, 218229 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv        update:    - prompt 0x591f8fd107c0:    4834 tokens, checkpoints:  1,   134.387 MiB
srv        update:    - prompt 0x591f8ff1bd40:   11271 tokens, checkpoints:  6,   368.901 MiB
srv  get_availabl: prompt cache update took 285.84 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12254 | processing task, is_child = 0
slot update_slots: id  3 | task 12254 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4104
slot update_slots: id  3 | task 12254 | n_past = 881, slot.prompt.tokens.size() = 11271, seq_id = 3, pos_min = 11144, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 12254 | failed to restore context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 12254 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 12254 | erased invalidated context checkpoint (pos_min = 4731, pos_max = 5373, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 12254 | erased invalidated context checkpoint (pos_min = 6652, pos_max = 7294, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 12254 | erased invalidated context checkpoint (pos_min = 7382, pos_max = 8024, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 12254 | erased invalidated context checkpoint (pos_min = 9288, pos_max = 9930, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 12254 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 12254 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.499025
slot update_slots: id  3 | task 12254 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 12254 | prompt processing progress, n_tokens = 4040, batch.n_tokens = 1992, progress = 0.984405
slot update_slots: id  3 | task 12254 | n_tokens = 4040, memory_seq_rm [4040, end)
slot update_slots: id  3 | task 12254 | prompt processing progress, n_tokens = 4104, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12254 | prompt done, n_tokens = 4104, batch.n_tokens = 64
slot init_sampler: id  3 | task 12254 | init sampler, took 0.63 ms, tokens: text = 4104, total = 4104
slot update_slots: id  3 | task 12254 | created context checkpoint 3 of 8 (pos_min = 3397, pos_max = 4039, size = 15.078 MiB)
slot print_timing: id  3 | task 12254 | 
prompt eval time =    6049.27 ms /  4104 tokens (    1.47 ms per token,   678.43 tokens per second)
       eval time =    9877.64 ms /   354 tokens (   27.90 ms per token,    35.84 tokens per second)
      total time =   15926.91 ms /  4458 tokens
slot      release: id  3 | task 12254 | stop processing: n_tokens = 4457, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.914 (> 0.100 thold), f_keep = 0.923
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12611 | processing task, is_child = 0
slot update_slots: id  3 | task 12611 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4504
slot update_slots: id  3 | task 12611 | n_tokens = 4116, memory_seq_rm [4116, end)
slot update_slots: id  3 | task 12611 | prompt processing progress, n_tokens = 4440, batch.n_tokens = 324, progress = 0.985790
slot update_slots: id  3 | task 12611 | n_tokens = 4440, memory_seq_rm [4440, end)
slot update_slots: id  3 | task 12611 | prompt processing progress, n_tokens = 4504, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12611 | prompt done, n_tokens = 4504, batch.n_tokens = 64
slot init_sampler: id  3 | task 12611 | init sampler, took 0.62 ms, tokens: text = 4504, total = 4504
slot update_slots: id  3 | task 12611 | created context checkpoint 4 of 8 (pos_min = 3902, pos_max = 4439, size = 12.616 MiB)
slot print_timing: id  3 | task 12611 | 
prompt eval time =     740.06 ms /   388 tokens (    1.91 ms per token,   524.28 tokens per second)
       eval time =    1189.14 ms /    43 tokens (   27.65 ms per token,    36.16 tokens per second)
      total time =    1929.20 ms /   431 tokens
slot      release: id  3 | task 12611 | stop processing: n_tokens = 4546, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.694 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12656 | processing task, is_child = 0
slot update_slots: id  3 | task 12656 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6509
slot update_slots: id  3 | task 12656 | n_tokens = 4519, memory_seq_rm [4519, end)
slot update_slots: id  3 | task 12656 | prompt processing progress, n_tokens = 6445, batch.n_tokens = 1926, progress = 0.990167
slot update_slots: id  3 | task 12656 | n_tokens = 6445, memory_seq_rm [6445, end)
slot update_slots: id  3 | task 12656 | prompt processing progress, n_tokens = 6509, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12656 | prompt done, n_tokens = 6509, batch.n_tokens = 64
slot init_sampler: id  3 | task 12656 | init sampler, took 0.90 ms, tokens: text = 6509, total = 6509
slot update_slots: id  3 | task 12656 | created context checkpoint 5 of 8 (pos_min = 5802, pos_max = 6444, size = 15.078 MiB)
slot print_timing: id  3 | task 12656 | 
prompt eval time =    3057.21 ms /  1990 tokens (    1.54 ms per token,   650.92 tokens per second)
       eval time =    1074.07 ms /    39 tokens (   27.54 ms per token,    36.31 tokens per second)
      total time =    4131.27 ms /  2029 tokens
slot      release: id  3 | task 12656 | stop processing: n_tokens = 6547, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.104 (> 0.100 thold), f_keep = 0.068
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6355, total state size = 151.997 MiB
srv          load:  - looking for better prompt, base f_keep = 0.068, sim = 0.104
srv        update:  - cache state: 21 prompts, 4026.287 MiB (limits: 8192.000 MiB, 56064 tokens, 222104 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv        update:    - prompt 0x591f8fd107c0:    4834 tokens, checkpoints:  1,   134.387 MiB
srv        update:    - prompt 0x591f8ff1bd40:   11271 tokens, checkpoints:  6,   368.901 MiB
srv        update:    - prompt 0x591fad763900:    6355 tokens, checkpoints:  1,   167.075 MiB
srv  get_availabl: prompt cache update took 145.23 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12697 | processing task, is_child = 0
slot update_slots: id  2 | task 12697 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4112
slot update_slots: id  2 | task 12697 | n_past = 429, slot.prompt.tokens.size() = 6355, seq_id = 2, pos_min = 6228, n_swa = 128
slot update_slots: id  2 | task 12697 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 12697 | erased invalidated context checkpoint (pos_min = 5137, pos_max = 5779, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 12697 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 12697 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.498054
slot update_slots: id  2 | task 12697 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 12697 | prompt processing progress, n_tokens = 4048, batch.n_tokens = 2000, progress = 0.984436
slot update_slots: id  2 | task 12697 | n_tokens = 4048, memory_seq_rm [4048, end)
slot update_slots: id  2 | task 12697 | prompt processing progress, n_tokens = 4112, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12697 | prompt done, n_tokens = 4112, batch.n_tokens = 64
slot init_sampler: id  2 | task 12697 | init sampler, took 0.59 ms, tokens: text = 4112, total = 4112
slot update_slots: id  2 | task 12697 | created context checkpoint 1 of 8 (pos_min = 3405, pos_max = 4047, size = 15.078 MiB)
slot print_timing: id  2 | task 12697 | 
prompt eval time =    6071.85 ms /  4112 tokens (    1.48 ms per token,   677.22 tokens per second)
       eval time =    8374.34 ms /   305 tokens (   27.46 ms per token,    36.42 tokens per second)
      total time =   14446.18 ms /  4417 tokens
slot      release: id  2 | task 12697 | stop processing: n_tokens = 4416, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.132 (> 0.100 thold), f_keep = 0.135
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6547, total state size = 156.499 MiB
srv          load:  - looking for better prompt, base f_keep = 0.135, sim = 0.132
srv        update:  - cache state: 22 prompts, 4266.875 MiB (limits: 8192.000 MiB, 56064 tokens, 222150 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8e5f3b50:    1694 tokens, checkpoints:  1,    63.735 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv        update:    - prompt 0x591f8fd107c0:    4834 tokens, checkpoints:  1,   134.387 MiB
srv        update:    - prompt 0x591f8ff1bd40:   11271 tokens, checkpoints:  6,   368.901 MiB
srv        update:    - prompt 0x591fad763900:    6355 tokens, checkpoints:  1,   167.075 MiB
srv        update:    - prompt 0x591fae931b90:    6547 tokens, checkpoints:  5,   240.588 MiB
srv  get_availabl: prompt cache update took 260.32 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13005 | processing task, is_child = 0
slot update_slots: id  3 | task 13005 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6674
slot update_slots: id  3 | task 13005 | n_past = 883, slot.prompt.tokens.size() = 6547, seq_id = 3, pos_min = 6420, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 13005 | failed to restore context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 13005 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 13005 | erased invalidated context checkpoint (pos_min = 3397, pos_max = 4039, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 13005 | erased invalidated context checkpoint (pos_min = 3902, pos_max = 4439, n_swa = 128, size = 12.616 MiB)
slot update_slots: id  3 | task 13005 | erased invalidated context checkpoint (pos_min = 5802, pos_max = 6444, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 13005 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 13005 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.306862
slot update_slots: id  3 | task 13005 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 13005 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.613725
slot update_slots: id  3 | task 13005 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 13005 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.920587
slot update_slots: id  3 | task 13005 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  3 | task 13005 | prompt processing progress, n_tokens = 6610, batch.n_tokens = 466, progress = 0.990411
slot update_slots: id  3 | task 13005 | n_tokens = 6610, memory_seq_rm [6610, end)
slot update_slots: id  3 | task 13005 | prompt processing progress, n_tokens = 6674, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13005 | prompt done, n_tokens = 6674, batch.n_tokens = 64
slot init_sampler: id  3 | task 13005 | init sampler, took 0.91 ms, tokens: text = 6674, total = 6674
slot update_slots: id  3 | task 13005 | created context checkpoint 3 of 8 (pos_min = 5967, pos_max = 6609, size = 15.078 MiB)
slot print_timing: id  3 | task 13005 | 
prompt eval time =    9652.33 ms /  6674 tokens (    1.45 ms per token,   691.44 tokens per second)
       eval time =    1065.44 ms /    35 tokens (   30.44 ms per token,    32.85 tokens per second)
      total time =   10717.78 ms /  6709 tokens
slot      release: id  3 | task 13005 | stop processing: n_tokens = 6708, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13045 | processing task, is_child = 0
slot update_slots: id  3 | task 13045 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6757
slot update_slots: id  3 | task 13045 | n_tokens = 6697, memory_seq_rm [6697, end)
slot update_slots: id  3 | task 13045 | prompt processing progress, n_tokens = 6757, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  3 | task 13045 | prompt done, n_tokens = 6757, batch.n_tokens = 60
slot init_sampler: id  3 | task 13045 | init sampler, took 1.26 ms, tokens: text = 6757, total = 6757
slot update_slots: id  3 | task 13045 | created context checkpoint 4 of 8 (pos_min = 6065, pos_max = 6696, size = 14.820 MiB)
slot print_timing: id  3 | task 13045 | 
prompt eval time =     199.54 ms /    60 tokens (    3.33 ms per token,   300.69 tokens per second)
       eval time =    8138.06 ms /   293 tokens (   27.77 ms per token,    36.00 tokens per second)
      total time =    8337.59 ms /   353 tokens
slot      release: id  3 | task 13045 | stop processing: n_tokens = 7049, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13339 | processing task, is_child = 0
slot update_slots: id  3 | task 13339 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7528
slot update_slots: id  3 | task 13339 | n_tokens = 7023, memory_seq_rm [7023, end)
slot update_slots: id  3 | task 13339 | prompt processing progress, n_tokens = 7464, batch.n_tokens = 441, progress = 0.991498
slot update_slots: id  3 | task 13339 | n_tokens = 7464, memory_seq_rm [7464, end)
slot update_slots: id  3 | task 13339 | prompt processing progress, n_tokens = 7528, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13339 | prompt done, n_tokens = 7528, batch.n_tokens = 64
slot init_sampler: id  3 | task 13339 | init sampler, took 1.07 ms, tokens: text = 7528, total = 7528
slot update_slots: id  3 | task 13339 | created context checkpoint 5 of 8 (pos_min = 6821, pos_max = 7463, size = 15.078 MiB)
slot print_timing: id  3 | task 13339 | 
prompt eval time =     859.52 ms /   505 tokens (    1.70 ms per token,   587.54 tokens per second)
       eval time =    1182.07 ms /    43 tokens (   27.49 ms per token,    36.38 tokens per second)
      total time =    2041.59 ms /   548 tokens
slot      release: id  3 | task 13339 | stop processing: n_tokens = 7570, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.923 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13384 | processing task, is_child = 0
slot update_slots: id  3 | task 13384 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8170
slot update_slots: id  3 | task 13384 | n_tokens = 7539, memory_seq_rm [7539, end)
slot update_slots: id  3 | task 13384 | prompt processing progress, n_tokens = 8106, batch.n_tokens = 567, progress = 0.992166
slot update_slots: id  3 | task 13384 | n_tokens = 8106, memory_seq_rm [8106, end)
slot update_slots: id  3 | task 13384 | prompt processing progress, n_tokens = 8170, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13384 | prompt done, n_tokens = 8170, batch.n_tokens = 64
slot init_sampler: id  3 | task 13384 | init sampler, took 1.13 ms, tokens: text = 8170, total = 8170
slot update_slots: id  3 | task 13384 | created context checkpoint 6 of 8 (pos_min = 7463, pos_max = 8105, size = 15.078 MiB)
slot print_timing: id  3 | task 13384 | 
prompt eval time =    1184.48 ms /   631 tokens (    1.88 ms per token,   532.72 tokens per second)
       eval time =    5172.03 ms /   183 tokens (   28.26 ms per token,    35.38 tokens per second)
      total time =    6356.51 ms /   814 tokens
slot      release: id  3 | task 13384 | stop processing: n_tokens = 8352, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.955 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13569 | processing task, is_child = 0
slot update_slots: id  3 | task 13569 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8720
slot update_slots: id  3 | task 13569 | n_tokens = 8326, memory_seq_rm [8326, end)
slot update_slots: id  3 | task 13569 | prompt processing progress, n_tokens = 8656, batch.n_tokens = 330, progress = 0.992661
slot update_slots: id  3 | task 13569 | n_tokens = 8656, memory_seq_rm [8656, end)
slot update_slots: id  3 | task 13569 | prompt processing progress, n_tokens = 8720, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13569 | prompt done, n_tokens = 8720, batch.n_tokens = 64
slot init_sampler: id  3 | task 13569 | init sampler, took 1.25 ms, tokens: text = 8720, total = 8720
slot update_slots: id  3 | task 13569 | created context checkpoint 7 of 8 (pos_min = 8013, pos_max = 8655, size = 15.078 MiB)
slot print_timing: id  3 | task 13569 | 
prompt eval time =     770.27 ms /   394 tokens (    1.95 ms per token,   511.51 tokens per second)
       eval time =    1294.80 ms /    47 tokens (   27.55 ms per token,    36.30 tokens per second)
      total time =    2065.07 ms /   441 tokens
slot      release: id  3 | task 13569 | stop processing: n_tokens = 8766, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.809 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13618 | processing task, is_child = 0
slot update_slots: id  3 | task 13618 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10801
slot update_slots: id  3 | task 13618 | n_tokens = 8743, memory_seq_rm [8743, end)
slot update_slots: id  3 | task 13618 | prompt processing progress, n_tokens = 10737, batch.n_tokens = 1994, progress = 0.994075
slot update_slots: id  3 | task 13618 | n_tokens = 10737, memory_seq_rm [10737, end)
slot update_slots: id  3 | task 13618 | prompt processing progress, n_tokens = 10801, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13618 | prompt done, n_tokens = 10801, batch.n_tokens = 64
slot init_sampler: id  3 | task 13618 | init sampler, took 1.49 ms, tokens: text = 10801, total = 10801
slot update_slots: id  3 | task 13618 | created context checkpoint 8 of 8 (pos_min = 10094, pos_max = 10736, size = 15.078 MiB)
slot print_timing: id  3 | task 13618 | 
prompt eval time =    3171.83 ms /  2058 tokens (    1.54 ms per token,   648.84 tokens per second)
       eval time =   11291.53 ms /   401 tokens (   28.16 ms per token,    35.51 tokens per second)
      total time =   14463.37 ms /  2459 tokens
slot      release: id  3 | task 13618 | stop processing: n_tokens = 11201, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 1319408575
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8051, total state size = 191.766 MiB
srv          load:  - looking for better prompt, base f_keep = 0.053, sim = 0.066
srv          load:  - found better prompt with f_keep = 0.253, sim = 0.066
srv        update:  - cache state: 22 prompts, 4409.984 MiB (limits: 8192.000 MiB, 56064 tokens, 226750 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv        update:    - prompt 0x591f8fd107c0:    4834 tokens, checkpoints:  1,   134.387 MiB
srv        update:    - prompt 0x591f8ff1bd40:   11271 tokens, checkpoints:  6,   368.901 MiB
srv        update:    - prompt 0x591fad763900:    6355 tokens, checkpoints:  1,   167.075 MiB
srv        update:    - prompt 0x591fae931b90:    6547 tokens, checkpoints:  5,   240.588 MiB
srv        update:    - prompt 0x591f8f17a930:    8051 tokens, checkpoints:  1,   206.844 MiB
srv  get_availabl: prompt cache update took 183.72 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 14021 | processing task, is_child = 0
slot update_slots: id  0 | task 14021 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6455
slot update_slots: id  0 | task 14021 | n_past = 429, slot.prompt.tokens.size() = 1694, seq_id = 0, pos_min = 1567, n_swa = 128
slot update_slots: id  0 | task 14021 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 14021 | erased invalidated context checkpoint (pos_min = 366, pos_max = 1262, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  0 | task 14021 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 14021 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.317273
slot update_slots: id  0 | task 14021 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 14021 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.634547
slot update_slots: id  0 | task 14021 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 14021 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.951820
slot update_slots: id  0 | task 14021 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 14021 | prompt processing progress, n_tokens = 6391, batch.n_tokens = 247, progress = 0.990085
slot update_slots: id  0 | task 14021 | n_tokens = 6391, memory_seq_rm [6391, end)
slot update_slots: id  0 | task 14021 | prompt processing progress, n_tokens = 6455, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 14021 | prompt done, n_tokens = 6455, batch.n_tokens = 64
slot init_sampler: id  0 | task 14021 | init sampler, took 1.37 ms, tokens: text = 6455, total = 6455
slot update_slots: id  0 | task 14021 | created context checkpoint 1 of 8 (pos_min = 5748, pos_max = 6390, size = 15.078 MiB)
slot print_timing: id  0 | task 14021 | 
prompt eval time =    9470.69 ms /  6455 tokens (    1.47 ms per token,   681.58 tokens per second)
       eval time =    9633.61 ms /   350 tokens (   27.52 ms per token,    36.33 tokens per second)
      total time =   19104.30 ms /  6805 tokens
slot      release: id  0 | task 14021 | stop processing: n_tokens = 6804, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.195 (> 0.100 thold), f_keep = 0.079
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 11201, total state size = 265.630 MiB
srv          load:  - looking for better prompt, base f_keep = 0.079, sim = 0.195
srv        update:  - cache state: 23 prompts, 4807.142 MiB (limits: 8192.000 MiB, 56064 tokens, 227104 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv        update:    - prompt 0x591f8fd107c0:    4834 tokens, checkpoints:  1,   134.387 MiB
srv        update:    - prompt 0x591f8ff1bd40:   11271 tokens, checkpoints:  6,   368.901 MiB
srv        update:    - prompt 0x591fad763900:    6355 tokens, checkpoints:  1,   167.075 MiB
srv        update:    - prompt 0x591fae931b90:    6547 tokens, checkpoints:  5,   240.588 MiB
srv        update:    - prompt 0x591f8f17a930:    8051 tokens, checkpoints:  1,   206.844 MiB
srv        update:    - prompt 0x591f8e5f3b50:   11201 tokens, checkpoints:  8,   397.158 MiB
srv  get_availabl: prompt cache update took 321.58 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14376 | processing task, is_child = 0
slot update_slots: id  3 | task 14376 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4537
slot update_slots: id  3 | task 14376 | n_past = 883, slot.prompt.tokens.size() = 11201, seq_id = 3, pos_min = 11074, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 14376 | failed to restore context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 14376 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 14376 | erased invalidated context checkpoint (pos_min = 5967, pos_max = 6609, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 14376 | erased invalidated context checkpoint (pos_min = 6065, pos_max = 6696, n_swa = 128, size = 14.820 MiB)
slot update_slots: id  3 | task 14376 | erased invalidated context checkpoint (pos_min = 6821, pos_max = 7463, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 14376 | erased invalidated context checkpoint (pos_min = 7463, pos_max = 8105, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 14376 | erased invalidated context checkpoint (pos_min = 8013, pos_max = 8655, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 14376 | erased invalidated context checkpoint (pos_min = 10094, pos_max = 10736, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 14376 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 14376 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.451400
slot update_slots: id  3 | task 14376 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 14376 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.902799
slot update_slots: id  3 | task 14376 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 14376 | prompt processing progress, n_tokens = 4473, batch.n_tokens = 377, progress = 0.985894
slot update_slots: id  3 | task 14376 | n_tokens = 4473, memory_seq_rm [4473, end)
slot update_slots: id  3 | task 14376 | prompt processing progress, n_tokens = 4537, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 14376 | prompt done, n_tokens = 4537, batch.n_tokens = 64
slot init_sampler: id  3 | task 14376 | init sampler, took 0.64 ms, tokens: text = 4537, total = 4537
slot update_slots: id  3 | task 14376 | created context checkpoint 3 of 8 (pos_min = 3830, pos_max = 4472, size = 15.078 MiB)
slot print_timing: id  3 | task 14376 | 
prompt eval time =    6751.07 ms /  4537 tokens (    1.49 ms per token,   672.04 tokens per second)
       eval time =   22955.19 ms /   821 tokens (   27.96 ms per token,    35.77 tokens per second)
      total time =   29706.26 ms /  5358 tokens
slot      release: id  3 | task 14376 | stop processing: n_tokens = 5357, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15201 | processing task, is_child = 0
slot update_slots: id  3 | task 15201 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5471
slot update_slots: id  3 | task 15201 | n_tokens = 5329, memory_seq_rm [5329, end)
slot update_slots: id  3 | task 15201 | prompt processing progress, n_tokens = 5407, batch.n_tokens = 78, progress = 0.988302
slot update_slots: id  3 | task 15201 | n_tokens = 5407, memory_seq_rm [5407, end)
slot update_slots: id  3 | task 15201 | prompt processing progress, n_tokens = 5471, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15201 | prompt done, n_tokens = 5471, batch.n_tokens = 64
slot init_sampler: id  3 | task 15201 | init sampler, took 1.19 ms, tokens: text = 5471, total = 5471
slot update_slots: id  3 | task 15201 | created context checkpoint 4 of 8 (pos_min = 4764, pos_max = 5406, size = 15.078 MiB)
slot print_timing: id  3 | task 15201 | 
prompt eval time =     480.39 ms /   142 tokens (    3.38 ms per token,   295.59 tokens per second)
       eval time =    1536.47 ms /    55 tokens (   27.94 ms per token,    35.80 tokens per second)
      total time =    2016.86 ms /   197 tokens
slot      release: id  3 | task 15201 | stop processing: n_tokens = 5525, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15258 | processing task, is_child = 0
slot update_slots: id  3 | task 15258 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5574
slot update_slots: id  3 | task 15258 | n_tokens = 5499, memory_seq_rm [5499, end)
slot update_slots: id  3 | task 15258 | prompt processing progress, n_tokens = 5510, batch.n_tokens = 11, progress = 0.988518
slot update_slots: id  3 | task 15258 | n_tokens = 5510, memory_seq_rm [5510, end)
slot update_slots: id  3 | task 15258 | prompt processing progress, n_tokens = 5574, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15258 | prompt done, n_tokens = 5574, batch.n_tokens = 64
slot init_sampler: id  3 | task 15258 | init sampler, took 0.79 ms, tokens: text = 5574, total = 5574
slot update_slots: id  3 | task 15258 | created context checkpoint 5 of 8 (pos_min = 4882, pos_max = 5509, size = 14.726 MiB)
slot print_timing: id  3 | task 15258 | 
prompt eval time =     306.97 ms /    75 tokens (    4.09 ms per token,   244.32 tokens per second)
       eval time =    1199.61 ms /    43 tokens (   27.90 ms per token,    35.84 tokens per second)
      total time =    1506.58 ms /   118 tokens
slot      release: id  3 | task 15258 | stop processing: n_tokens = 5616, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15303 | processing task, is_child = 0
slot update_slots: id  3 | task 15303 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5648
slot update_slots: id  3 | task 15303 | n_tokens = 5593, memory_seq_rm [5593, end)
slot update_slots: id  3 | task 15303 | prompt processing progress, n_tokens = 5648, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 15303 | prompt done, n_tokens = 5648, batch.n_tokens = 55
slot init_sampler: id  3 | task 15303 | init sampler, took 0.85 ms, tokens: text = 5648, total = 5648
slot update_slots: id  3 | task 15303 | created context checkpoint 6 of 8 (pos_min = 4973, pos_max = 5592, size = 14.539 MiB)
slot print_timing: id  3 | task 15303 | 
prompt eval time =     196.05 ms /    55 tokens (    3.56 ms per token,   280.54 tokens per second)
       eval time =     832.75 ms /    30 tokens (   27.76 ms per token,    36.03 tokens per second)
      total time =    1028.80 ms /    85 tokens
slot      release: id  3 | task 15303 | stop processing: n_tokens = 5677, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.920 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15334 | processing task, is_child = 0
slot update_slots: id  3 | task 15334 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6153
slot update_slots: id  3 | task 15334 | n_tokens = 5658, memory_seq_rm [5658, end)
slot update_slots: id  3 | task 15334 | prompt processing progress, n_tokens = 6089, batch.n_tokens = 431, progress = 0.989599
slot update_slots: id  3 | task 15334 | n_tokens = 6089, memory_seq_rm [6089, end)
slot update_slots: id  3 | task 15334 | prompt processing progress, n_tokens = 6153, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15334 | prompt done, n_tokens = 6153, batch.n_tokens = 64
slot init_sampler: id  3 | task 15334 | init sampler, took 1.02 ms, tokens: text = 6153, total = 6153
slot update_slots: id  3 | task 15334 | created context checkpoint 7 of 8 (pos_min = 5446, pos_max = 6088, size = 15.078 MiB)
slot print_timing: id  3 | task 15334 | 
prompt eval time =     846.37 ms /   495 tokens (    1.71 ms per token,   584.85 tokens per second)
       eval time =    1161.77 ms /    42 tokens (   27.66 ms per token,    36.15 tokens per second)
      total time =    2008.13 ms /   537 tokens
slot      release: id  3 | task 15334 | stop processing: n_tokens = 6194, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.756 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15378 | processing task, is_child = 0
slot update_slots: id  3 | task 15378 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8153
slot update_slots: id  3 | task 15378 | n_tokens = 6163, memory_seq_rm [6163, end)
slot update_slots: id  3 | task 15378 | prompt processing progress, n_tokens = 8089, batch.n_tokens = 1926, progress = 0.992150
slot update_slots: id  3 | task 15378 | n_tokens = 8089, memory_seq_rm [8089, end)
slot update_slots: id  3 | task 15378 | prompt processing progress, n_tokens = 8153, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15378 | prompt done, n_tokens = 8153, batch.n_tokens = 64
slot init_sampler: id  3 | task 15378 | init sampler, took 1.14 ms, tokens: text = 8153, total = 8153
slot update_slots: id  3 | task 15378 | created context checkpoint 8 of 8 (pos_min = 7446, pos_max = 8088, size = 15.078 MiB)
slot print_timing: id  3 | task 15378 | 
prompt eval time =    3044.57 ms /  1990 tokens (    1.53 ms per token,   653.62 tokens per second)
       eval time =    1005.92 ms /    36 tokens (   27.94 ms per token,    35.79 tokens per second)
      total time =    4050.50 ms /  2026 tokens
slot      release: id  3 | task 15378 | stop processing: n_tokens = 8188, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = 1652178498
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6056, total state size = 144.985 MiB
srv          load:  - looking for better prompt, base f_keep = 0.071, sim = 0.074
srv        update:  - cache state: 24 prompts, 4967.205 MiB (limits: 8192.000 MiB, 56064 tokens, 229773 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv        update:    - prompt 0x591f8fd107c0:    4834 tokens, checkpoints:  1,   134.387 MiB
srv        update:    - prompt 0x591f8ff1bd40:   11271 tokens, checkpoints:  6,   368.901 MiB
srv        update:    - prompt 0x591fad763900:    6355 tokens, checkpoints:  1,   167.075 MiB
srv        update:    - prompt 0x591fae931b90:    6547 tokens, checkpoints:  5,   240.588 MiB
srv        update:    - prompt 0x591f8f17a930:    8051 tokens, checkpoints:  1,   206.844 MiB
srv        update:    - prompt 0x591f8e5f3b50:   11201 tokens, checkpoints:  8,   397.158 MiB
srv        update:    - prompt 0x591f8ddda2d0:    6056 tokens, checkpoints:  1,   160.063 MiB
srv  get_availabl: prompt cache update took 191.88 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 15416 | processing task, is_child = 0
slot update_slots: id  1 | task 15416 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5761
slot update_slots: id  1 | task 15416 | n_past = 427, slot.prompt.tokens.size() = 6056, seq_id = 1, pos_min = 5929, n_swa = 128
slot update_slots: id  1 | task 15416 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 15416 | erased invalidated context checkpoint (pos_min = 5032, pos_max = 5674, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 15416 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 15416 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.355494
slot update_slots: id  1 | task 15416 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 15416 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.710988
slot update_slots: id  1 | task 15416 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 15416 | prompt processing progress, n_tokens = 5697, batch.n_tokens = 1601, progress = 0.988891
slot update_slots: id  1 | task 15416 | n_tokens = 5697, memory_seq_rm [5697, end)
slot update_slots: id  1 | task 15416 | prompt processing progress, n_tokens = 5761, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 15416 | prompt done, n_tokens = 5761, batch.n_tokens = 64
slot init_sampler: id  1 | task 15416 | init sampler, took 0.80 ms, tokens: text = 5761, total = 5761
slot update_slots: id  1 | task 15416 | created context checkpoint 1 of 8 (pos_min = 5054, pos_max = 5696, size = 15.078 MiB)
slot print_timing: id  1 | task 15416 | 
prompt eval time =    7898.92 ms /  5761 tokens (    1.37 ms per token,   729.34 tokens per second)
       eval time =   13617.09 ms /   506 tokens (   26.91 ms per token,    37.16 tokens per second)
      total time =   21516.01 ms /  6267 tokens
slot      release: id  1 | task 15416 | stop processing: n_tokens = 6266, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.132 (> 0.100 thold), f_keep = 0.108
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8188, total state size = 194.979 MiB
srv          load:  - looking for better prompt, base f_keep = 0.108, sim = 0.132
srv        update:  - cache state: 25 prompts, 5293.078 MiB (limits: 8192.000 MiB, 56064 tokens, 228300 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv        update:    - prompt 0x591f8fd107c0:    4834 tokens, checkpoints:  1,   134.387 MiB
srv        update:    - prompt 0x591f8ff1bd40:   11271 tokens, checkpoints:  6,   368.901 MiB
srv        update:    - prompt 0x591fad763900:    6355 tokens, checkpoints:  1,   167.075 MiB
srv        update:    - prompt 0x591fae931b90:    6547 tokens, checkpoints:  5,   240.588 MiB
srv        update:    - prompt 0x591f8f17a930:    8051 tokens, checkpoints:  1,   206.844 MiB
srv        update:    - prompt 0x591f8e5f3b50:   11201 tokens, checkpoints:  8,   397.158 MiB
srv        update:    - prompt 0x591f8ddda2d0:    6056 tokens, checkpoints:  1,   160.063 MiB
srv        update:    - prompt 0x591fdbc437e0:    8188 tokens, checkpoints:  8,   325.873 MiB
srv  get_availabl: prompt cache update took 272.60 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15926 | processing task, is_child = 0
slot update_slots: id  3 | task 15926 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6666
slot update_slots: id  3 | task 15926 | n_past = 883, slot.prompt.tokens.size() = 8188, seq_id = 3, pos_min = 8061, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 15926 | failed to restore context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 15926 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 15926 | erased invalidated context checkpoint (pos_min = 3830, pos_max = 4472, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 15926 | erased invalidated context checkpoint (pos_min = 4764, pos_max = 5406, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 15926 | erased invalidated context checkpoint (pos_min = 4882, pos_max = 5509, n_swa = 128, size = 14.726 MiB)
slot update_slots: id  3 | task 15926 | erased invalidated context checkpoint (pos_min = 4973, pos_max = 5592, n_swa = 128, size = 14.539 MiB)
slot update_slots: id  3 | task 15926 | erased invalidated context checkpoint (pos_min = 5446, pos_max = 6088, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 15926 | erased invalidated context checkpoint (pos_min = 7446, pos_max = 8088, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 15926 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 15926 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.307231
slot update_slots: id  3 | task 15926 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 15926 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.614461
slot update_slots: id  3 | task 15926 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 15926 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.921692
slot update_slots: id  3 | task 15926 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  3 | task 15926 | prompt processing progress, n_tokens = 6602, batch.n_tokens = 458, progress = 0.990399
slot update_slots: id  3 | task 15926 | n_tokens = 6602, memory_seq_rm [6602, end)
slot update_slots: id  3 | task 15926 | prompt processing progress, n_tokens = 6666, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15926 | prompt done, n_tokens = 6666, batch.n_tokens = 64
slot init_sampler: id  3 | task 15926 | init sampler, took 1.13 ms, tokens: text = 6666, total = 6666
slot update_slots: id  3 | task 15926 | created context checkpoint 3 of 8 (pos_min = 5959, pos_max = 6601, size = 15.078 MiB)
slot print_timing: id  3 | task 15926 | 
prompt eval time =    9005.18 ms /  6666 tokens (    1.35 ms per token,   740.24 tokens per second)
       eval time =   14835.82 ms /   545 tokens (   27.22 ms per token,    36.74 tokens per second)
      total time =   23841.00 ms /  7211 tokens
slot      release: id  3 | task 15926 | stop processing: n_tokens = 7210, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 16476 | processing task, is_child = 0
slot update_slots: id  3 | task 16476 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7252
slot update_slots: id  3 | task 16476 | n_tokens = 6989, memory_seq_rm [6989, end)
slot update_slots: id  3 | task 16476 | prompt processing progress, n_tokens = 7188, batch.n_tokens = 199, progress = 0.991175
slot update_slots: id  3 | task 16476 | n_tokens = 7188, memory_seq_rm [7188, end)
slot update_slots: id  3 | task 16476 | prompt processing progress, n_tokens = 7252, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 16476 | prompt done, n_tokens = 7252, batch.n_tokens = 64
slot init_sampler: id  3 | task 16476 | init sampler, took 1.01 ms, tokens: text = 7252, total = 7252
slot update_slots: id  3 | task 16476 | created context checkpoint 4 of 8 (pos_min = 6629, pos_max = 7187, size = 13.108 MiB)
slot print_timing: id  3 | task 16476 | 
prompt eval time =     617.26 ms /   263 tokens (    2.35 ms per token,   426.08 tokens per second)
       eval time =   14557.76 ms /   534 tokens (   27.26 ms per token,    36.68 tokens per second)
      total time =   15175.01 ms /   797 tokens
slot      release: id  3 | task 16476 | stop processing: n_tokens = 7785, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.113
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7785, total state size = 197.628 MiB
srv          load:  - looking for better prompt, base f_keep = 0.113, sim = 0.988
srv        update:  - cache state: 26 prompts, 5560.210 MiB (limits: 8192.000 MiB, 56064 tokens, 228801 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv        update:    - prompt 0x591f8fd107c0:    4834 tokens, checkpoints:  1,   134.387 MiB
srv        update:    - prompt 0x591f8ff1bd40:   11271 tokens, checkpoints:  6,   368.901 MiB
srv        update:    - prompt 0x591fad763900:    6355 tokens, checkpoints:  1,   167.075 MiB
srv        update:    - prompt 0x591fae931b90:    6547 tokens, checkpoints:  5,   240.588 MiB
srv        update:    - prompt 0x591f8f17a930:    8051 tokens, checkpoints:  1,   206.844 MiB
srv        update:    - prompt 0x591f8e5f3b50:   11201 tokens, checkpoints:  8,   397.158 MiB
srv        update:    - prompt 0x591f8ddda2d0:    6056 tokens, checkpoints:  1,   160.063 MiB
srv        update:    - prompt 0x591fdbc437e0:    8188 tokens, checkpoints:  8,   325.873 MiB
srv        update:    - prompt 0x591f8f1597e0:    7785 tokens, checkpoints:  4,   267.132 MiB
srv  get_availabl: prompt cache update took 197.88 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17012 | processing task, is_child = 0
slot update_slots: id  3 | task 17012 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 890
slot update_slots: id  3 | task 17012 | n_past = 879, slot.prompt.tokens.size() = 7785, seq_id = 3, pos_min = 7142, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 17012 | failed to restore context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 17012 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 17012 | erased invalidated context checkpoint (pos_min = 5959, pos_max = 6601, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  3 | task 17012 | erased invalidated context checkpoint (pos_min = 6629, pos_max = 7187, n_swa = 128, size = 13.108 MiB)
slot update_slots: id  3 | task 17012 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 17012 | prompt processing progress, n_tokens = 826, batch.n_tokens = 826, progress = 0.928090
slot update_slots: id  3 | task 17012 | n_tokens = 826, memory_seq_rm [826, end)
slot update_slots: id  3 | task 17012 | prompt processing progress, n_tokens = 890, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17012 | prompt done, n_tokens = 890, batch.n_tokens = 64
slot init_sampler: id  3 | task 17012 | init sampler, took 0.17 ms, tokens: text = 890, total = 890
slot print_timing: id  3 | task 17012 | 
prompt eval time =    1389.89 ms /   890 tokens (    1.56 ms per token,   640.34 tokens per second)
       eval time =    1044.59 ms /    40 tokens (   26.11 ms per token,    38.29 tokens per second)
      total time =    2434.49 ms /   930 tokens
slot      release: id  3 | task 17012 | stop processing: n_tokens = 929, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.745 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17054 | processing task, is_child = 0
slot update_slots: id  3 | task 17054 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1222
slot update_slots: id  3 | task 17054 | n_tokens = 911, memory_seq_rm [911, end)
slot update_slots: id  3 | task 17054 | prompt processing progress, n_tokens = 1158, batch.n_tokens = 247, progress = 0.947627
slot update_slots: id  3 | task 17054 | n_tokens = 1158, memory_seq_rm [1158, end)
slot update_slots: id  3 | task 17054 | prompt processing progress, n_tokens = 1222, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17054 | prompt done, n_tokens = 1222, batch.n_tokens = 64
slot init_sampler: id  3 | task 17054 | init sampler, took 0.18 ms, tokens: text = 1222, total = 1222
slot print_timing: id  3 | task 17054 | 
prompt eval time =     689.87 ms /   311 tokens (    2.22 ms per token,   450.81 tokens per second)
       eval time =    3723.87 ms /   142 tokens (   26.22 ms per token,    38.13 tokens per second)
      total time =    4413.75 ms /   453 tokens
slot      release: id  3 | task 17054 | stop processing: n_tokens = 1363, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.853 (> 0.100 thold), f_keep = 0.062
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6804, total state size = 162.525 MiB
srv          load:  - looking for better prompt, base f_keep = 0.062, sim = 0.853
srv        update:  - cache state: 27 prompts, 5737.813 MiB (limits: 8192.000 MiB, 56064 tokens, 231433 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv        update:    - prompt 0x591f8fd107c0:    4834 tokens, checkpoints:  1,   134.387 MiB
srv        update:    - prompt 0x591f8ff1bd40:   11271 tokens, checkpoints:  6,   368.901 MiB
srv        update:    - prompt 0x591fad763900:    6355 tokens, checkpoints:  1,   167.075 MiB
srv        update:    - prompt 0x591fae931b90:    6547 tokens, checkpoints:  5,   240.588 MiB
srv        update:    - prompt 0x591f8f17a930:    8051 tokens, checkpoints:  1,   206.844 MiB
srv        update:    - prompt 0x591f8e5f3b50:   11201 tokens, checkpoints:  8,   397.158 MiB
srv        update:    - prompt 0x591f8ddda2d0:    6056 tokens, checkpoints:  1,   160.063 MiB
srv        update:    - prompt 0x591fdbc437e0:    8188 tokens, checkpoints:  8,   325.873 MiB
srv        update:    - prompt 0x591f8f1597e0:    7785 tokens, checkpoints:  4,   267.132 MiB
srv        update:    - prompt 0x591fd1d35c00:    6804 tokens, checkpoints:  1,   177.603 MiB
srv  get_availabl: prompt cache update took 163.33 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17198 | processing task, is_child = 0
slot update_slots: id  0 | task 17198 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 498
slot update_slots: id  0 | task 17198 | n_past = 425, slot.prompt.tokens.size() = 6804, seq_id = 0, pos_min = 6677, n_swa = 128
slot update_slots: id  0 | task 17198 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 17198 | erased invalidated context checkpoint (pos_min = 5748, pos_max = 6390, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 17198 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 17198 | prompt processing progress, n_tokens = 434, batch.n_tokens = 434, progress = 0.871486
slot update_slots: id  0 | task 17198 | n_tokens = 434, memory_seq_rm [434, end)
slot update_slots: id  0 | task 17198 | prompt processing progress, n_tokens = 498, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17198 | prompt done, n_tokens = 498, batch.n_tokens = 64
slot init_sampler: id  0 | task 17198 | init sampler, took 0.09 ms, tokens: text = 498, total = 498
slot update_slots: id  0 | task 17198 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 433, size = 10.177 MiB)
slot print_timing: id  0 | task 17198 | 
prompt eval time =     843.79 ms /   498 tokens (    1.69 ms per token,   590.19 tokens per second)
       eval time =    4928.55 ms /   187 tokens (   26.36 ms per token,    37.94 tokens per second)
      total time =    5772.34 ms /   685 tokens
slot      release: id  0 | task 17198 | stop processing: n_tokens = 684, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.734 (> 0.100 thold), f_keep = 0.645
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17387 | processing task, is_child = 0
slot update_slots: id  3 | task 17387 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1198
slot update_slots: id  3 | task 17387 | n_past = 879, slot.prompt.tokens.size() = 1363, seq_id = 3, pos_min = 1236, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 17387 | failed to restore context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 17387 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 17387 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 17387 | prompt processing progress, n_tokens = 1134, batch.n_tokens = 1134, progress = 0.946578
slot update_slots: id  3 | task 17387 | n_tokens = 1134, memory_seq_rm [1134, end)
slot update_slots: id  3 | task 17387 | prompt processing progress, n_tokens = 1198, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17387 | prompt done, n_tokens = 1198, batch.n_tokens = 64
slot init_sampler: id  3 | task 17387 | init sampler, took 0.18 ms, tokens: text = 1198, total = 1198
slot print_timing: id  3 | task 17387 | 
prompt eval time =    1890.31 ms /  1198 tokens (    1.58 ms per token,   633.76 tokens per second)
       eval time =    1070.01 ms /    41 tokens (   26.10 ms per token,    38.32 tokens per second)
      total time =    2960.32 ms /  1239 tokens
slot      release: id  3 | task 17387 | stop processing: n_tokens = 1238, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.875 (> 0.100 thold), f_keep = 0.624
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17430 | processing task, is_child = 0
slot update_slots: id  0 | task 17430 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 488
slot update_slots: id  0 | task 17430 | n_past = 427, slot.prompt.tokens.size() = 684, seq_id = 0, pos_min = 557, n_swa = 128
slot update_slots: id  0 | task 17430 | restored context checkpoint (pos_min = 0, pos_max = 433, size = 10.177 MiB)
slot update_slots: id  0 | task 17430 | n_tokens = 427, memory_seq_rm [427, end)
slot update_slots: id  0 | task 17430 | prompt processing progress, n_tokens = 488, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  0 | task 17430 | prompt done, n_tokens = 488, batch.n_tokens = 61
slot init_sampler: id  0 | task 17430 | init sampler, took 0.08 ms, tokens: text = 488, total = 488
slot print_timing: id  0 | task 17430 | 
prompt eval time =     281.75 ms /    61 tokens (    4.62 ms per token,   216.51 tokens per second)
       eval time =    5311.45 ms /   198 tokens (   26.83 ms per token,    37.28 tokens per second)
      total time =    5593.19 ms /   259 tokens
slot      release: id  0 | task 17430 | stop processing: n_tokens = 685, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.812 (> 0.100 thold), f_keep = 0.068
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6266, total state size = 149.910 MiB
srv          load:  - looking for better prompt, base f_keep = 0.068, sim = 0.812
srv        update:  - cache state: 28 prompts, 5902.801 MiB (limits: 8192.000 MiB, 56064 tokens, 233660 est)
srv        update:    - prompt 0x591f873ce950:    5340 tokens, checkpoints:  8,   313.586 MiB
srv        update:    - prompt 0x591f8d6dab10:    4665 tokens, checkpoints:  1,   133.402 MiB
srv        update:    - prompt 0x591f8d673750:    6597 tokens, checkpoints:  6,   286.103 MiB
srv        update:    - prompt 0x591f8fdffa20:    3851 tokens, checkpoints:  1,   114.314 MiB
srv        update:    - prompt 0x591f8fc606d0:    3676 tokens, checkpoints:  7,   229.920 MiB
srv        update:    - prompt 0x591f8fdc7900:    2664 tokens, checkpoints:  1,    86.481 MiB
srv        update:    - prompt 0x591f8d788040:    3136 tokens, checkpoints:  1,    97.548 MiB
srv        update:    - prompt 0x591f8d54a0d0:    4600 tokens, checkpoints:  6,   236.297 MiB
srv        update:    - prompt 0x591f8d604790:    3721 tokens, checkpoints:  1,   111.266 MiB
srv        update:    - prompt 0x591f8d7366a0:    5720 tokens, checkpoints:  3,   199.458 MiB
srv        update:    - prompt 0x591f8fcc2160:    6712 tokens, checkpoints:  3,   222.720 MiB
srv        update:    - prompt 0x591f8e5d7950:    2749 tokens, checkpoints:  1,    88.474 MiB
srv        update:    - prompt 0x591f8ebac590:    7086 tokens, checkpoints:  5,   272.830 MiB
srv        update:    - prompt 0x591f8cfdc990:    9349 tokens, checkpoints:  8,   369.253 MiB
srv        update:    - prompt 0x591f8d786160:    7650 tokens, checkpoints:  8,   312.788 MiB
srv        update:    - prompt 0x591fae9141e0:    2485 tokens, checkpoints:  1,    82.283 MiB
srv        update:    - prompt 0x591f8fd106e0:    5007 tokens, checkpoints:  1,   135.465 MiB
srv        update:    - prompt 0x591f8fd107c0:    4834 tokens, checkpoints:  1,   134.387 MiB
srv        update:    - prompt 0x591f8ff1bd40:   11271 tokens, checkpoints:  6,   368.901 MiB
srv        update:    - prompt 0x591fad763900:    6355 tokens, checkpoints:  1,   167.075 MiB
srv        update:    - prompt 0x591fae931b90:    6547 tokens, checkpoints:  5,   240.588 MiB
srv        update:    - prompt 0x591f8f17a930:    8051 tokens, checkpoints:  1,   206.844 MiB
srv        update:    - prompt 0x591f8e5f3b50:   11201 tokens, checkpoints:  8,   397.158 MiB
srv        update:    - prompt 0x591f8ddda2d0:    6056 tokens, checkpoints:  1,   160.063 MiB
srv        update:    - prompt 0x591fdbc437e0:    8188 tokens, checkpoints:  8,   325.873 MiB
srv        update:    - prompt 0x591f8f1597e0:    7785 tokens, checkpoints:  4,   267.132 MiB
srv        update:    - prompt 0x591fd1d35c00:    6804 tokens, checkpoints:  1,   177.603 MiB
srv        update:    - prompt 0x591f938b64d0:    6266 tokens, checkpoints:  1,   164.988 MiB
srv  get_availabl: prompt cache update took 160.71 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 17629 | processing task, is_child = 0
slot update_slots: id  1 | task 17629 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 528
slot update_slots: id  1 | task 17629 | n_past = 429, slot.prompt.tokens.size() = 6266, seq_id = 1, pos_min = 6139, n_swa = 128
slot update_slots: id  1 | task 17629 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 17629 | erased invalidated context checkpoint (pos_min = 5054, pos_max = 5696, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 17629 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 17629 | prompt processing progress, n_tokens = 464, batch.n_tokens = 464, progress = 0.878788
slot update_slots: id  1 | task 17629 | n_tokens = 464, memory_seq_rm [464, end)
slot update_slots: id  1 | task 17629 | prompt processing progress, n_tokens = 528, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 17629 | prompt done, n_tokens = 528, batch.n_tokens = 64
slot init_sampler: id  1 | task 17629 | init sampler, took 0.09 ms, tokens: text = 528, total = 528
slot update_slots: id  1 | task 17629 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 463, size = 10.881 MiB)
slot print_timing: id  1 | task 17629 | 
prompt eval time =     727.94 ms /   528 tokens (    1.38 ms per token,   725.33 tokens per second)
       eval time =    5753.98 ms /   229 tokens (   25.13 ms per token,    39.80 tokens per second)
      total time =    6481.92 ms /   757 tokens
slot      release: id  1 | task 17629 | stop processing: n_tokens = 756, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.710
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17860 | processing task, is_child = 0
slot update_slots: id  3 | task 17860 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 890
slot update_slots: id  3 | task 17860 | n_past = 879, slot.prompt.tokens.size() = 1238, seq_id = 3, pos_min = 1111, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 17860 | failed to restore context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 17860 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 17860 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 17860 | prompt processing progress, n_tokens = 826, batch.n_tokens = 826, progress = 0.928090
slot update_slots: id  3 | task 17860 | n_tokens = 826, memory_seq_rm [826, end)
slot update_slots: id  3 | task 17860 | prompt processing progress, n_tokens = 890, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17860 | prompt done, n_tokens = 890, batch.n_tokens = 64
slot init_sampler: id  3 | task 17860 | init sampler, took 0.14 ms, tokens: text = 890, total = 890
slot print_timing: id  3 | task 17860 | 
prompt eval time =    1154.81 ms /   890 tokens (    1.30 ms per token,   770.69 tokens per second)
       eval time =    1066.02 ms /    44 tokens (   24.23 ms per token,    41.28 tokens per second)
      total time =    2220.82 ms /   934 tokens
slot      release: id  3 | task 17860 | stop processing: n_tokens = 933, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.746 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17906 | processing task, is_child = 0
slot update_slots: id  3 | task 17906 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1226
slot update_slots: id  3 | task 17906 | n_tokens = 915, memory_seq_rm [915, end)
slot update_slots: id  3 | task 17906 | prompt processing progress, n_tokens = 1162, batch.n_tokens = 247, progress = 0.947798
slot update_slots: id  3 | task 17906 | n_tokens = 1162, memory_seq_rm [1162, end)
slot update_slots: id  3 | task 17906 | prompt processing progress, n_tokens = 1226, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17906 | prompt done, n_tokens = 1226, batch.n_tokens = 64
slot init_sampler: id  3 | task 17906 | init sampler, took 0.19 ms, tokens: text = 1226, total = 1226
slot print_timing: id  3 | task 17906 | 
prompt eval time =     537.42 ms /   311 tokens (    1.73 ms per token,   578.69 tokens per second)
       eval time =    3354.36 ms /   137 tokens (   24.48 ms per token,    40.84 tokens per second)
      total time =    3891.78 ms /   448 tokens
slot      release: id  3 | task 17906 | stop processing: n_tokens = 1362, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.857 (> 0.100 thold), f_keep = 0.623
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18045 | processing task, is_child = 0
slot update_slots: id  0 | task 18045 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 498
slot update_slots: id  0 | task 18045 | n_past = 427, slot.prompt.tokens.size() = 685, seq_id = 0, pos_min = 558, n_swa = 128
slot update_slots: id  0 | task 18045 | restored context checkpoint (pos_min = 0, pos_max = 433, size = 10.177 MiB)
slot update_slots: id  0 | task 18045 | n_tokens = 427, memory_seq_rm [427, end)
slot update_slots: id  0 | task 18045 | prompt processing progress, n_tokens = 434, batch.n_tokens = 7, progress = 0.871486
slot update_slots: id  0 | task 18045 | n_tokens = 434, memory_seq_rm [434, end)
slot update_slots: id  0 | task 18045 | prompt processing progress, n_tokens = 498, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18045 | prompt done, n_tokens = 498, batch.n_tokens = 64
slot init_sampler: id  0 | task 18045 | init sampler, took 0.12 ms, tokens: text = 498, total = 498
slot print_timing: id  0 | task 18045 | 
prompt eval time =     396.53 ms /    71 tokens (    5.58 ms per token,   179.05 tokens per second)
       eval time =    6023.66 ms /   245 tokens (   24.59 ms per token,    40.67 tokens per second)
      total time =    6420.19 ms /   316 tokens
slot      release: id  0 | task 18045 | stop processing: n_tokens = 742, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.737 (> 0.100 thold), f_keep = 0.645
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18292 | processing task, is_child = 0
slot update_slots: id  3 | task 18292 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1193
slot update_slots: id  3 | task 18292 | n_past = 879, slot.prompt.tokens.size() = 1362, seq_id = 3, pos_min = 1235, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 18292 | failed to restore context checkpoint (pos_min = 670, pos_max = 1550, size = 20.659 MiB)
slot update_slots: id  3 | task 18292 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 18292 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 18292 | prompt processing progress, n_tokens = 1129, batch.n_tokens = 1129, progress = 0.946354
slot update_slots: id  3 | task 18292 | n_tokens = 1129, memory_seq_rm [1129, end)
slot update_slots: id  3 | task 18292 | prompt processing progress, n_tokens = 1193, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18292 | prompt done, n_tokens = 1193, batch.n_tokens = 64
slot init_sampler: id  3 | task 18292 | init sampler, took 0.17 ms, tokens: text = 1193, total = 1193
slot print_timing: id  3 | task 18292 | 
prompt eval time =    1560.65 ms /  1193 tokens (    1.31 ms per token,   764.42 tokens per second)
       eval time =     874.45 ms /    36 tokens (   24.29 ms per token,    41.17 tokens per second)
      total time =    2435.10 ms /  1229 tokens
slot      release: id  3 | task 18292 | stop processing: n_tokens = 1228, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.875 (> 0.100 thold), f_keep = 0.575
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18330 | processing task, is_child = 0
slot update_slots: id  0 | task 18330 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 488
slot update_slots: id  0 | task 18330 | n_past = 427, slot.prompt.tokens.size() = 742, seq_id = 0, pos_min = 615, n_swa = 128
slot update_slots: id  0 | task 18330 | restored context checkpoint (pos_min = 0, pos_max = 433, size = 10.177 MiB)
slot update_slots: id  0 | task 18330 | n_tokens = 427, memory_seq_rm [427, end)
slot update_slots: id  0 | task 18330 | prompt processing progress, n_tokens = 488, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  0 | task 18330 | prompt done, n_tokens = 488, batch.n_tokens = 61
slot init_sampler: id  0 | task 18330 | init sampler, took 0.08 ms, tokens: text = 488, total = 488
slot print_timing: id  0 | task 18330 | 
prompt eval time =     248.28 ms /    61 tokens (    4.07 ms per token,   245.69 tokens per second)
       eval time =    4200.90 ms /   167 tokens (   25.16 ms per token,    39.75 tokens per second)
      total time =    4449.18 ms /   228 tokens
slot      release: id  0 | task 18330 | stop processing: n_tokens = 654, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.808 (> 0.100 thold), f_keep = 0.569
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18498 | processing task, is_child = 0
slot update_slots: id  1 | task 18498 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 532
slot update_slots: id  1 | task 18498 | n_past = 430, slot.prompt.tokens.size() = 756, seq_id = 1, pos_min = 629, n_swa = 128
slot update_slots: id  1 | task 18498 | restored context checkpoint (pos_min = 0, pos_max = 463, size = 10.881 MiB)
slot update_slots: id  1 | task 18498 | n_tokens = 430, memory_seq_rm [430, end)
slot update_slots: id  1 | task 18498 | prompt processing progress, n_tokens = 468, batch.n_tokens = 38, progress = 0.879699
slot update_slots: id  1 | task 18498 | n_tokens = 468, memory_seq_rm [468, end)
slot update_slots: id  1 | task 18498 | prompt processing progress, n_tokens = 532, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 18498 | prompt done, n_tokens = 532, batch.n_tokens = 64
slot init_sampler: id  1 | task 18498 | init sampler, took 0.09 ms, tokens: text = 532, total = 532
slot print_timing: id  1 | task 18498 | 
prompt eval time =     405.48 ms /   102 tokens (    3.98 ms per token,   251.55 tokens per second)
       eval time =    3956.08 ms /   157 tokens (   25.20 ms per token,    39.69 tokens per second)
      total time =    4361.56 ms /   259 tokens
slot      release: id  1 | task 18498 | stop processing: n_tokens = 688, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
