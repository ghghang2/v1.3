ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: using cached file: /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14992 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1578 MiB
llama_params_fit_impl: context size reduced from 131072 to 64000 -> need 1580 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.78 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 64000
llama_context: n_ctx_seq     = 64000
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (64000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 64000 cells
llama_kv_cache:      CUDA0 KV buffer size =  1500.00 MiB
llama_kv_cache: size = 1500.00 MiB ( 64000 cells,  12 layers,  4/1 seqs), K (f16):  750.00 MiB, V (f16):  750.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   132.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 88.57 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  1 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  2 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  3 | task -1 | new slot, n_ctx = 64000
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-10

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 678
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 614, batch.n_tokens = 614, progress = 0.905605
slot update_slots: id  3 | task 0 | n_tokens = 614, memory_seq_rm [614, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 678, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 678, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.13 ms, tokens: text = 678, total = 678
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 613, size = 14.398 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =    1158.77 ms /   678 tokens (    1.71 ms per token,   585.10 tokens per second)
       eval time =    1045.20 ms /    48 tokens (   21.78 ms per token,    45.92 tokens per second)
      total time =    2203.97 ms /   726 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 725, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.477 (> 0.100 thold), f_keep = 0.935
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 50 | processing task, is_child = 0
slot update_slots: id  3 | task 50 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1421
slot update_slots: id  3 | task 50 | n_tokens = 678, memory_seq_rm [678, end)
slot update_slots: id  3 | task 50 | prompt processing progress, n_tokens = 1357, batch.n_tokens = 679, progress = 0.954961
slot update_slots: id  3 | task 50 | n_tokens = 1357, memory_seq_rm [1357, end)
slot update_slots: id  3 | task 50 | prompt processing progress, n_tokens = 1421, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 50 | prompt done, n_tokens = 1421, batch.n_tokens = 64
slot init_sampler: id  3 | task 50 | init sampler, took 0.58 ms, tokens: text = 1421, total = 1421
slot update_slots: id  3 | task 50 | created context checkpoint 2 of 8 (pos_min = 333, pos_max = 1356, size = 24.012 MiB)
slot print_timing: id  3 | task 50 | 
prompt eval time =     806.80 ms /   743 tokens (    1.09 ms per token,   920.93 tokens per second)
       eval time =    1975.61 ms /    84 tokens (   23.52 ms per token,    42.52 tokens per second)
      total time =    2782.41 ms /   827 tokens
slot      release: id  3 | task 50 | stop processing: n_tokens = 1504, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.694 (> 0.100 thold), f_keep = 0.945
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 136 | processing task, is_child = 0
slot update_slots: id  3 | task 136 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2048
slot update_slots: id  3 | task 136 | n_tokens = 1421, memory_seq_rm [1421, end)
slot update_slots: id  3 | task 136 | prompt processing progress, n_tokens = 1984, batch.n_tokens = 563, progress = 0.968750
slot update_slots: id  3 | task 136 | n_tokens = 1984, memory_seq_rm [1984, end)
slot update_slots: id  3 | task 136 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 136 | prompt done, n_tokens = 2048, batch.n_tokens = 64
slot init_sampler: id  3 | task 136 | init sampler, took 0.35 ms, tokens: text = 2048, total = 2048
slot update_slots: id  3 | task 136 | created context checkpoint 3 of 8 (pos_min = 960, pos_max = 1983, size = 24.012 MiB)
slot print_timing: id  3 | task 136 | 
prompt eval time =     764.60 ms /   627 tokens (    1.22 ms per token,   820.04 tokens per second)
       eval time =    2305.68 ms /   100 tokens (   23.06 ms per token,    43.37 tokens per second)
      total time =    3070.28 ms /   727 tokens
slot      release: id  3 | task 136 | stop processing: n_tokens = 2147, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.967 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 238 | processing task, is_child = 0
slot update_slots: id  3 | task 238 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2118
slot update_slots: id  3 | task 238 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 238 | prompt processing progress, n_tokens = 2054, batch.n_tokens = 6, progress = 0.969783
slot update_slots: id  3 | task 238 | n_tokens = 2054, memory_seq_rm [2054, end)
slot update_slots: id  3 | task 238 | prompt processing progress, n_tokens = 2118, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 238 | prompt done, n_tokens = 2118, batch.n_tokens = 64
slot init_sampler: id  3 | task 238 | init sampler, took 0.35 ms, tokens: text = 2118, total = 2118
slot update_slots: id  3 | task 238 | created context checkpoint 4 of 8 (pos_min = 1123, pos_max = 2053, size = 21.831 MiB)
slot print_timing: id  3 | task 238 | 
prompt eval time =     228.37 ms /    70 tokens (    3.26 ms per token,   306.51 tokens per second)
       eval time =    2908.67 ms /   127 tokens (   22.90 ms per token,    43.66 tokens per second)
      total time =    3137.05 ms /   197 tokens
slot      release: id  3 | task 238 | stop processing: n_tokens = 2244, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.944
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 367 | processing task, is_child = 0
slot update_slots: id  3 | task 367 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2179
slot update_slots: id  3 | task 367 | n_tokens = 2118, memory_seq_rm [2118, end)
slot update_slots: id  3 | task 367 | prompt processing progress, n_tokens = 2179, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  3 | task 367 | prompt done, n_tokens = 2179, batch.n_tokens = 61
slot init_sampler: id  3 | task 367 | init sampler, took 2.30 ms, tokens: text = 2179, total = 2179
slot print_timing: id  3 | task 367 | 
prompt eval time =     220.73 ms /    61 tokens (    3.62 ms per token,   276.35 tokens per second)
       eval time =    1404.81 ms /    60 tokens (   23.41 ms per token,    42.71 tokens per second)
      total time =    1625.54 ms /   121 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 367 | stop processing: n_tokens = 2238, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 428 | processing task, is_child = 0
slot update_slots: id  3 | task 428 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2246
slot update_slots: id  3 | task 428 | n_tokens = 2179, memory_seq_rm [2179, end)
slot update_slots: id  3 | task 428 | prompt processing progress, n_tokens = 2182, batch.n_tokens = 3, progress = 0.971505
slot update_slots: id  3 | task 428 | n_tokens = 2182, memory_seq_rm [2182, end)
slot update_slots: id  3 | task 428 | prompt processing progress, n_tokens = 2246, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 428 | prompt done, n_tokens = 2246, batch.n_tokens = 64
slot init_sampler: id  3 | task 428 | init sampler, took 0.37 ms, tokens: text = 2246, total = 2246
slot update_slots: id  3 | task 428 | created context checkpoint 5 of 8 (pos_min = 1220, pos_max = 2181, size = 22.558 MiB)
slot print_timing: id  3 | task 428 | 
prompt eval time =     201.68 ms /    67 tokens (    3.01 ms per token,   332.21 tokens per second)
       eval time =    4002.17 ms /   167 tokens (   23.97 ms per token,    41.73 tokens per second)
      total time =    4203.85 ms /   234 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 428 | stop processing: n_tokens = 2412, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.931
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 597 | processing task, is_child = 0
slot update_slots: id  3 | task 597 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2315
slot update_slots: id  3 | task 597 | n_tokens = 2246, memory_seq_rm [2246, end)
slot update_slots: id  3 | task 597 | prompt processing progress, n_tokens = 2251, batch.n_tokens = 5, progress = 0.972354
slot update_slots: id  3 | task 597 | n_tokens = 2251, memory_seq_rm [2251, end)
slot update_slots: id  3 | task 597 | prompt processing progress, n_tokens = 2315, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 597 | prompt done, n_tokens = 2315, batch.n_tokens = 64
slot init_sampler: id  3 | task 597 | init sampler, took 0.44 ms, tokens: text = 2315, total = 2315
slot update_slots: id  3 | task 597 | created context checkpoint 6 of 8 (pos_min = 1388, pos_max = 2250, size = 20.237 MiB)
slot print_timing: id  3 | task 597 | 
prompt eval time =     213.29 ms /    69 tokens (    3.09 ms per token,   323.50 tokens per second)
       eval time =    3724.64 ms /   161 tokens (   23.13 ms per token,    43.23 tokens per second)
      total time =    3937.93 ms /   230 tokens
slot      release: id  3 | task 597 | stop processing: n_tokens = 2475, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.935
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 760 | processing task, is_child = 0
slot update_slots: id  3 | task 760 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2377
slot update_slots: id  3 | task 760 | n_tokens = 2315, memory_seq_rm [2315, end)
slot update_slots: id  3 | task 760 | prompt processing progress, n_tokens = 2377, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 760 | prompt done, n_tokens = 2377, batch.n_tokens = 62
slot init_sampler: id  3 | task 760 | init sampler, took 0.37 ms, tokens: text = 2377, total = 2377
slot print_timing: id  3 | task 760 | 
prompt eval time =     148.55 ms /    62 tokens (    2.40 ms per token,   417.38 tokens per second)
       eval time =    1883.50 ms /    82 tokens (   22.97 ms per token,    43.54 tokens per second)
      total time =    2032.04 ms /   144 tokens
slot      release: id  3 | task 760 | stop processing: n_tokens = 2458, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.718 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 843 | processing task, is_child = 0
slot update_slots: id  3 | task 843 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3312
slot update_slots: id  3 | task 843 | n_tokens = 2377, memory_seq_rm [2377, end)
slot update_slots: id  3 | task 843 | prompt processing progress, n_tokens = 3248, batch.n_tokens = 871, progress = 0.980676
slot update_slots: id  3 | task 843 | n_tokens = 3248, memory_seq_rm [3248, end)
slot update_slots: id  3 | task 843 | prompt processing progress, n_tokens = 3312, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 843 | prompt done, n_tokens = 3312, batch.n_tokens = 64
slot init_sampler: id  3 | task 843 | init sampler, took 0.54 ms, tokens: text = 3312, total = 3312
slot update_slots: id  3 | task 843 | created context checkpoint 7 of 8 (pos_min = 2224, pos_max = 3247, size = 24.012 MiB)
slot print_timing: id  3 | task 843 | 
prompt eval time =    1026.06 ms /   935 tokens (    1.10 ms per token,   911.26 tokens per second)
       eval time =    2165.88 ms /    92 tokens (   23.54 ms per token,    42.48 tokens per second)
      total time =    3191.93 ms /  1027 tokens
slot      release: id  3 | task 843 | stop processing: n_tokens = 3403, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.631 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 937 | processing task, is_child = 0
slot update_slots: id  3 | task 937 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5247
slot update_slots: id  3 | task 937 | n_tokens = 3312, memory_seq_rm [3312, end)
slot update_slots: id  3 | task 937 | prompt processing progress, n_tokens = 5183, batch.n_tokens = 1871, progress = 0.987803
slot update_slots: id  3 | task 937 | n_tokens = 5183, memory_seq_rm [5183, end)
slot update_slots: id  3 | task 937 | prompt processing progress, n_tokens = 5247, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 937 | prompt done, n_tokens = 5247, batch.n_tokens = 64
slot init_sampler: id  3 | task 937 | init sampler, took 0.99 ms, tokens: text = 5247, total = 5247
slot update_slots: id  3 | task 937 | created context checkpoint 8 of 8 (pos_min = 4159, pos_max = 5182, size = 24.012 MiB)
slot print_timing: id  3 | task 937 | 
prompt eval time =    1977.42 ms /  1935 tokens (    1.02 ms per token,   978.55 tokens per second)
       eval time =    4334.32 ms /   181 tokens (   23.95 ms per token,    41.76 tokens per second)
      total time =    6311.74 ms /  2116 tokens
slot      release: id  3 | task 937 | stop processing: n_tokens = 5427, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1120 | processing task, is_child = 0
slot update_slots: id  3 | task 1120 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5308
slot update_slots: id  3 | task 1120 | n_tokens = 5247, memory_seq_rm [5247, end)
slot update_slots: id  3 | task 1120 | prompt processing progress, n_tokens = 5308, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  3 | task 1120 | prompt done, n_tokens = 5308, batch.n_tokens = 61
slot init_sampler: id  3 | task 1120 | init sampler, took 0.78 ms, tokens: text = 5308, total = 5308
slot print_timing: id  3 | task 1120 | 
prompt eval time =     148.64 ms /    61 tokens (    2.44 ms per token,   410.38 tokens per second)
       eval time =    1543.99 ms /    66 tokens (   23.39 ms per token,    42.75 tokens per second)
      total time =    1692.63 ms /   127 tokens
slot      release: id  3 | task 1120 | stop processing: n_tokens = 5373, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1187 | processing task, is_child = 0
slot update_slots: id  3 | task 1187 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5370
slot update_slots: id  3 | task 1187 | n_tokens = 5308, memory_seq_rm [5308, end)
slot update_slots: id  3 | task 1187 | prompt processing progress, n_tokens = 5370, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 1187 | prompt done, n_tokens = 5370, batch.n_tokens = 62
slot init_sampler: id  3 | task 1187 | init sampler, took 0.83 ms, tokens: text = 5370, total = 5370
slot update_slots: id  3 | task 1187 | erasing old context checkpoint (pos_min = 0, pos_max = 613, size = 14.398 MiB)
slot update_slots: id  3 | task 1187 | created context checkpoint 8 of 8 (pos_min = 4403, pos_max = 5307, size = 21.222 MiB)
slot print_timing: id  3 | task 1187 | 
prompt eval time =     180.56 ms /    62 tokens (    2.91 ms per token,   343.37 tokens per second)
       eval time =    4415.64 ms /   188 tokens (   23.49 ms per token,    42.58 tokens per second)
      total time =    4596.20 ms /   250 tokens
slot      release: id  3 | task 1187 | stop processing: n_tokens = 5557, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1376 | processing task, is_child = 0
slot update_slots: id  3 | task 1376 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5432
slot update_slots: id  3 | task 1376 | n_tokens = 5370, memory_seq_rm [5370, end)
slot update_slots: id  3 | task 1376 | prompt processing progress, n_tokens = 5432, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 1376 | prompt done, n_tokens = 5432, batch.n_tokens = 62
slot init_sampler: id  3 | task 1376 | init sampler, took 0.83 ms, tokens: text = 5432, total = 5432
slot print_timing: id  3 | task 1376 | 
prompt eval time =     158.34 ms /    62 tokens (    2.55 ms per token,   391.55 tokens per second)
       eval time =   12330.87 ms /   516 tokens (   23.90 ms per token,    41.85 tokens per second)
      total time =   12489.22 ms /   578 tokens
slot      release: id  3 | task 1376 | stop processing: n_tokens = 5947, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.913
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1893 | processing task, is_child = 0
slot update_slots: id  3 | task 1893 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5501
slot update_slots: id  3 | task 1893 | n_tokens = 5432, memory_seq_rm [5432, end)
slot update_slots: id  3 | task 1893 | prompt processing progress, n_tokens = 5437, batch.n_tokens = 5, progress = 0.988366
slot update_slots: id  3 | task 1893 | n_tokens = 5437, memory_seq_rm [5437, end)
slot update_slots: id  3 | task 1893 | prompt processing progress, n_tokens = 5501, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1893 | prompt done, n_tokens = 5501, batch.n_tokens = 64
slot init_sampler: id  3 | task 1893 | init sampler, took 0.83 ms, tokens: text = 5501, total = 5501
slot update_slots: id  3 | task 1893 | erasing old context checkpoint (pos_min = 333, pos_max = 1356, size = 24.012 MiB)
slot update_slots: id  3 | task 1893 | created context checkpoint 8 of 8 (pos_min = 4923, pos_max = 5436, size = 12.053 MiB)
slot print_timing: id  3 | task 1893 | 
prompt eval time =     217.23 ms /    69 tokens (    3.15 ms per token,   317.64 tokens per second)
       eval time =    1307.21 ms /    55 tokens (   23.77 ms per token,    42.07 tokens per second)
      total time =    1524.44 ms /   124 tokens
slot      release: id  3 | task 1893 | stop processing: n_tokens = 5555, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.851 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1950 | processing task, is_child = 0
slot update_slots: id  3 | task 1950 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6461
slot update_slots: id  3 | task 1950 | n_tokens = 5501, memory_seq_rm [5501, end)
slot update_slots: id  3 | task 1950 | prompt processing progress, n_tokens = 6397, batch.n_tokens = 896, progress = 0.990094
slot update_slots: id  3 | task 1950 | n_tokens = 6397, memory_seq_rm [6397, end)
slot update_slots: id  3 | task 1950 | prompt processing progress, n_tokens = 6461, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1950 | prompt done, n_tokens = 6461, batch.n_tokens = 64
slot init_sampler: id  3 | task 1950 | init sampler, took 1.81 ms, tokens: text = 6461, total = 6461
slot update_slots: id  3 | task 1950 | erasing old context checkpoint (pos_min = 960, pos_max = 1983, size = 24.012 MiB)
slot update_slots: id  3 | task 1950 | created context checkpoint 8 of 8 (pos_min = 5373, pos_max = 6396, size = 24.012 MiB)
slot print_timing: id  3 | task 1950 | 
prompt eval time =    1080.00 ms /   960 tokens (    1.12 ms per token,   888.89 tokens per second)
       eval time =   18326.32 ms /   750 tokens (   24.44 ms per token,    40.92 tokens per second)
      total time =   19406.32 ms /  1710 tokens
slot      release: id  3 | task 1950 | stop processing: n_tokens = 7210, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.896
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2702 | processing task, is_child = 0
slot update_slots: id  3 | task 2702 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6523
slot update_slots: id  3 | task 2702 | n_tokens = 6461, memory_seq_rm [6461, end)
slot update_slots: id  3 | task 2702 | prompt processing progress, n_tokens = 6523, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 2702 | prompt done, n_tokens = 6523, batch.n_tokens = 62
slot init_sampler: id  3 | task 2702 | init sampler, took 1.11 ms, tokens: text = 6523, total = 6523
slot print_timing: id  3 | task 2702 | 
prompt eval time =     169.26 ms /    62 tokens (    2.73 ms per token,   366.31 tokens per second)
       eval time =    2447.93 ms /    99 tokens (   24.73 ms per token,    40.44 tokens per second)
      total time =    2617.19 ms /   161 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 2702 | stop processing: n_tokens = 6621, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2802 | processing task, is_child = 0
slot update_slots: id  3 | task 2802 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6585
slot update_slots: id  3 | task 2802 | n_tokens = 6523, memory_seq_rm [6523, end)
slot update_slots: id  3 | task 2802 | prompt processing progress, n_tokens = 6585, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 2802 | prompt done, n_tokens = 6585, batch.n_tokens = 62
slot init_sampler: id  3 | task 2802 | init sampler, took 1.42 ms, tokens: text = 6585, total = 6585
slot update_slots: id  3 | task 2802 | erasing old context checkpoint (pos_min = 1123, pos_max = 2053, size = 21.831 MiB)
slot update_slots: id  3 | task 2802 | created context checkpoint 8 of 8 (pos_min = 6304, pos_max = 6522, size = 5.136 MiB)
slot print_timing: id  3 | task 2802 | 
prompt eval time =     171.20 ms /    62 tokens (    2.76 ms per token,   362.14 tokens per second)
       eval time =   25845.51 ms /  1047 tokens (   24.69 ms per token,    40.51 tokens per second)
      total time =   26016.71 ms /  1109 tokens
slot      release: id  3 | task 2802 | stop processing: n_tokens = 7631, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.863
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3850 | processing task, is_child = 0
slot update_slots: id  3 | task 3850 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6647
slot update_slots: id  3 | task 3850 | n_past = 6585, slot.prompt.tokens.size() = 7631, seq_id = 3, pos_min = 6607, n_swa = 128
slot update_slots: id  3 | task 3850 | restored context checkpoint (pos_min = 6304, pos_max = 6522, size = 5.136 MiB)
slot update_slots: id  3 | task 3850 | n_tokens = 6522, memory_seq_rm [6522, end)
slot update_slots: id  3 | task 3850 | prompt processing progress, n_tokens = 6583, batch.n_tokens = 61, progress = 0.990372
slot update_slots: id  3 | task 3850 | n_tokens = 6583, memory_seq_rm [6583, end)
slot update_slots: id  3 | task 3850 | prompt processing progress, n_tokens = 6647, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3850 | prompt done, n_tokens = 6647, batch.n_tokens = 64
slot init_sampler: id  3 | task 3850 | init sampler, took 1.50 ms, tokens: text = 6647, total = 6647
slot print_timing: id  3 | task 3850 | 
prompt eval time =     338.33 ms /   125 tokens (    2.71 ms per token,   369.46 tokens per second)
       eval time =   31186.97 ms /  1270 tokens (   24.56 ms per token,    40.72 tokens per second)
      total time =   31525.30 ms /  1395 tokens
slot      release: id  3 | task 3850 | stop processing: n_tokens = 7916, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.395 (> 0.100 thold), f_keep = 0.078
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7916, total state size = 209.634 MiB
srv          load:  - looking for better prompt, base f_keep = 0.078, sim = 0.395
srv        update:  - cache state: 1 prompts, 362.875 MiB (limits: 8192.000 MiB, 64000 tokens, 178705 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv  get_availabl: prompt cache update took 278.00 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5122 | processing task, is_child = 0
slot update_slots: id  3 | task 5122 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1573
slot update_slots: id  3 | task 5122 | n_past = 621, slot.prompt.tokens.size() = 7916, seq_id = 3, pos_min = 6892, n_swa = 128
slot update_slots: id  3 | task 5122 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 1220, pos_max = 2181, n_swa = 128, size = 22.558 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 1388, pos_max = 2250, n_swa = 128, size = 20.237 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 2224, pos_max = 3247, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 4159, pos_max = 5182, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 4403, pos_max = 5307, n_swa = 128, size = 21.222 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 4923, pos_max = 5436, n_swa = 128, size = 12.053 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 5373, pos_max = 6396, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5122 | erased invalidated context checkpoint (pos_min = 6304, pos_max = 6522, n_swa = 128, size = 5.136 MiB)
slot update_slots: id  3 | task 5122 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 5122 | prompt processing progress, n_tokens = 1509, batch.n_tokens = 1509, progress = 0.959313
slot update_slots: id  3 | task 5122 | n_tokens = 1509, memory_seq_rm [1509, end)
slot update_slots: id  3 | task 5122 | prompt processing progress, n_tokens = 1573, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5122 | prompt done, n_tokens = 1573, batch.n_tokens = 64
slot init_sampler: id  3 | task 5122 | init sampler, took 0.25 ms, tokens: text = 1573, total = 1573
slot update_slots: id  3 | task 5122 | created context checkpoint 1 of 8 (pos_min = 485, pos_max = 1508, size = 24.012 MiB)
slot print_timing: id  3 | task 5122 | 
prompt eval time =    1546.52 ms /  1573 tokens (    0.98 ms per token,  1017.13 tokens per second)
       eval time =    3732.10 ms /   160 tokens (   23.33 ms per token,    42.87 tokens per second)
      total time =    5278.62 ms /  1733 tokens
slot      release: id  3 | task 5122 | stop processing: n_tokens = 1732, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.919 (> 0.100 thold), f_keep = 0.902
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5284 | processing task, is_child = 0
slot update_slots: id  3 | task 5284 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1701
slot update_slots: id  3 | task 5284 | n_tokens = 1563, memory_seq_rm [1563, end)
slot update_slots: id  3 | task 5284 | prompt processing progress, n_tokens = 1637, batch.n_tokens = 74, progress = 0.962375
slot update_slots: id  3 | task 5284 | n_tokens = 1637, memory_seq_rm [1637, end)
slot update_slots: id  3 | task 5284 | prompt processing progress, n_tokens = 1701, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5284 | prompt done, n_tokens = 1701, batch.n_tokens = 64
slot init_sampler: id  3 | task 5284 | init sampler, took 0.32 ms, tokens: text = 1701, total = 1701
slot update_slots: id  3 | task 5284 | created context checkpoint 2 of 8 (pos_min = 708, pos_max = 1636, size = 21.784 MiB)
slot print_timing: id  3 | task 5284 | 
prompt eval time =     451.32 ms /   138 tokens (    3.27 ms per token,   305.77 tokens per second)
       eval time =    2603.65 ms /   112 tokens (   23.25 ms per token,    43.02 tokens per second)
      total time =    3054.97 ms /   250 tokens
slot      release: id  3 | task 5284 | stop processing: n_tokens = 1812, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.684 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5398 | processing task, is_child = 0
slot update_slots: id  3 | task 5398 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2488
slot update_slots: id  3 | task 5398 | n_tokens = 1701, memory_seq_rm [1701, end)
slot update_slots: id  3 | task 5398 | prompt processing progress, n_tokens = 2424, batch.n_tokens = 723, progress = 0.974277
slot update_slots: id  3 | task 5398 | n_tokens = 2424, memory_seq_rm [2424, end)
slot update_slots: id  3 | task 5398 | prompt processing progress, n_tokens = 2488, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5398 | prompt done, n_tokens = 2488, batch.n_tokens = 64
slot init_sampler: id  3 | task 5398 | init sampler, took 0.41 ms, tokens: text = 2488, total = 2488
slot update_slots: id  3 | task 5398 | created context checkpoint 3 of 8 (pos_min = 1400, pos_max = 2423, size = 24.012 MiB)
slot print_timing: id  3 | task 5398 | 
prompt eval time =     877.19 ms /   787 tokens (    1.11 ms per token,   897.18 tokens per second)
       eval time =    4107.97 ms /   176 tokens (   23.34 ms per token,    42.84 tokens per second)
      total time =    4985.16 ms /   963 tokens
slot      release: id  3 | task 5398 | stop processing: n_tokens = 2663, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.662 (> 0.100 thold), f_keep = 0.934
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5576 | processing task, is_child = 0
slot update_slots: id  3 | task 5576 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3757
slot update_slots: id  3 | task 5576 | n_tokens = 2488, memory_seq_rm [2488, end)
slot update_slots: id  3 | task 5576 | prompt processing progress, n_tokens = 3693, batch.n_tokens = 1205, progress = 0.982965
slot update_slots: id  3 | task 5576 | n_tokens = 3693, memory_seq_rm [3693, end)
slot update_slots: id  3 | task 5576 | prompt processing progress, n_tokens = 3757, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5576 | prompt done, n_tokens = 3757, batch.n_tokens = 64
slot init_sampler: id  3 | task 5576 | init sampler, took 0.70 ms, tokens: text = 3757, total = 3757
slot update_slots: id  3 | task 5576 | created context checkpoint 4 of 8 (pos_min = 2669, pos_max = 3692, size = 24.012 MiB)
slot print_timing: id  3 | task 5576 | 
prompt eval time =    1348.92 ms /  1269 tokens (    1.06 ms per token,   940.75 tokens per second)
       eval time =   10171.63 ms /   424 tokens (   23.99 ms per token,    41.68 tokens per second)
      total time =   11520.56 ms /  1693 tokens
slot      release: id  3 | task 5576 | stop processing: n_tokens = 4180, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.899
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6002 | processing task, is_child = 0
slot update_slots: id  3 | task 6002 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3861
slot update_slots: id  3 | task 6002 | n_tokens = 3757, memory_seq_rm [3757, end)
slot update_slots: id  3 | task 6002 | prompt processing progress, n_tokens = 3797, batch.n_tokens = 40, progress = 0.983424
slot update_slots: id  3 | task 6002 | n_tokens = 3797, memory_seq_rm [3797, end)
slot update_slots: id  3 | task 6002 | prompt processing progress, n_tokens = 3861, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6002 | prompt done, n_tokens = 3861, batch.n_tokens = 64
slot init_sampler: id  3 | task 6002 | init sampler, took 0.90 ms, tokens: text = 3861, total = 3861
slot update_slots: id  3 | task 6002 | created context checkpoint 5 of 8 (pos_min = 3156, pos_max = 3796, size = 15.031 MiB)
slot print_timing: id  3 | task 6002 | 
prompt eval time =     295.87 ms /   104 tokens (    2.84 ms per token,   351.51 tokens per second)
       eval time =    4392.87 ms /   183 tokens (   24.00 ms per token,    41.66 tokens per second)
      total time =    4688.73 ms /   287 tokens
slot      release: id  3 | task 6002 | stop processing: n_tokens = 4043, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6187 | processing task, is_child = 0
slot update_slots: id  3 | task 6187 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4001
slot update_slots: id  3 | task 6187 | n_tokens = 3861, memory_seq_rm [3861, end)
slot update_slots: id  3 | task 6187 | prompt processing progress, n_tokens = 3937, batch.n_tokens = 76, progress = 0.984004
slot update_slots: id  3 | task 6187 | n_tokens = 3937, memory_seq_rm [3937, end)
slot update_slots: id  3 | task 6187 | prompt processing progress, n_tokens = 4001, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6187 | prompt done, n_tokens = 4001, batch.n_tokens = 64
slot init_sampler: id  3 | task 6187 | init sampler, took 0.79 ms, tokens: text = 4001, total = 4001
slot update_slots: id  3 | task 6187 | created context checkpoint 6 of 8 (pos_min = 3358, pos_max = 3936, size = 13.577 MiB)
slot print_timing: id  3 | task 6187 | 
prompt eval time =     364.42 ms /   140 tokens (    2.60 ms per token,   384.18 tokens per second)
       eval time =    6220.32 ms /   259 tokens (   24.02 ms per token,    41.64 tokens per second)
      total time =    6584.74 ms /   399 tokens
slot      release: id  3 | task 6187 | stop processing: n_tokens = 4259, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6448 | processing task, is_child = 0
slot update_slots: id  3 | task 6448 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4105
slot update_slots: id  3 | task 6448 | n_tokens = 4001, memory_seq_rm [4001, end)
slot update_slots: id  3 | task 6448 | prompt processing progress, n_tokens = 4041, batch.n_tokens = 40, progress = 0.984409
slot update_slots: id  3 | task 6448 | n_tokens = 4041, memory_seq_rm [4041, end)
slot update_slots: id  3 | task 6448 | prompt processing progress, n_tokens = 4105, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6448 | prompt done, n_tokens = 4105, batch.n_tokens = 64
slot init_sampler: id  3 | task 6448 | init sampler, took 0.62 ms, tokens: text = 4105, total = 4105
slot update_slots: id  3 | task 6448 | created context checkpoint 7 of 8 (pos_min = 3574, pos_max = 4040, size = 10.951 MiB)
slot print_timing: id  3 | task 6448 | 
prompt eval time =     298.05 ms /   104 tokens (    2.87 ms per token,   348.94 tokens per second)
       eval time =    2496.56 ms /   103 tokens (   24.24 ms per token,    41.26 tokens per second)
      total time =    2794.61 ms /   207 tokens
slot      release: id  3 | task 6448 | stop processing: n_tokens = 4207, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6553 | processing task, is_child = 0
slot update_slots: id  3 | task 6553 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4209
slot update_slots: id  3 | task 6553 | n_tokens = 4105, memory_seq_rm [4105, end)
slot update_slots: id  3 | task 6553 | prompt processing progress, n_tokens = 4145, batch.n_tokens = 40, progress = 0.984794
slot update_slots: id  3 | task 6553 | n_tokens = 4145, memory_seq_rm [4145, end)
slot update_slots: id  3 | task 6553 | prompt processing progress, n_tokens = 4209, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6553 | prompt done, n_tokens = 4209, batch.n_tokens = 64
slot init_sampler: id  3 | task 6553 | init sampler, took 0.64 ms, tokens: text = 4209, total = 4209
slot update_slots: id  3 | task 6553 | created context checkpoint 8 of 8 (pos_min = 3574, pos_max = 4144, size = 13.390 MiB)
slot print_timing: id  3 | task 6553 | 
prompt eval time =     307.10 ms /   104 tokens (    2.95 ms per token,   338.65 tokens per second)
       eval time =    2486.99 ms /   101 tokens (   24.62 ms per token,    40.61 tokens per second)
      total time =    2794.09 ms /   205 tokens
slot      release: id  3 | task 6553 | stop processing: n_tokens = 4309, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6656 | processing task, is_child = 0
slot update_slots: id  3 | task 6656 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4281
slot update_slots: id  3 | task 6656 | n_tokens = 4209, memory_seq_rm [4209, end)
slot update_slots: id  3 | task 6656 | prompt processing progress, n_tokens = 4217, batch.n_tokens = 8, progress = 0.985050
slot update_slots: id  3 | task 6656 | n_tokens = 4217, memory_seq_rm [4217, end)
slot update_slots: id  3 | task 6656 | prompt processing progress, n_tokens = 4281, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6656 | prompt done, n_tokens = 4281, batch.n_tokens = 64
slot init_sampler: id  3 | task 6656 | init sampler, took 0.93 ms, tokens: text = 4281, total = 4281
slot update_slots: id  3 | task 6656 | erasing old context checkpoint (pos_min = 485, pos_max = 1508, size = 24.012 MiB)
slot update_slots: id  3 | task 6656 | created context checkpoint 8 of 8 (pos_min = 3624, pos_max = 4216, size = 13.906 MiB)
slot print_timing: id  3 | task 6656 | 
prompt eval time =     232.29 ms /    72 tokens (    3.23 ms per token,   309.96 tokens per second)
       eval time =    4089.77 ms /   166 tokens (   24.64 ms per token,    40.59 tokens per second)
      total time =    4322.06 ms /   238 tokens
slot      release: id  3 | task 6656 | stop processing: n_tokens = 4446, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.963
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6824 | processing task, is_child = 0
slot update_slots: id  3 | task 6824 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4353
slot update_slots: id  3 | task 6824 | n_tokens = 4281, memory_seq_rm [4281, end)
slot update_slots: id  3 | task 6824 | prompt processing progress, n_tokens = 4289, batch.n_tokens = 8, progress = 0.985298
slot update_slots: id  3 | task 6824 | n_tokens = 4289, memory_seq_rm [4289, end)
slot update_slots: id  3 | task 6824 | prompt processing progress, n_tokens = 4353, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6824 | prompt done, n_tokens = 4353, batch.n_tokens = 64
slot init_sampler: id  3 | task 6824 | init sampler, took 0.76 ms, tokens: text = 4353, total = 4353
slot update_slots: id  3 | task 6824 | erasing old context checkpoint (pos_min = 708, pos_max = 1636, size = 21.784 MiB)
slot update_slots: id  3 | task 6824 | created context checkpoint 8 of 8 (pos_min = 3757, pos_max = 4288, size = 12.475 MiB)
slot print_timing: id  3 | task 6824 | 
prompt eval time =     226.72 ms /    72 tokens (    3.15 ms per token,   317.57 tokens per second)
       eval time =    2258.67 ms /    93 tokens (   24.29 ms per token,    41.17 tokens per second)
      total time =    2485.39 ms /   165 tokens
slot      release: id  3 | task 6824 | stop processing: n_tokens = 4445, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6919 | processing task, is_child = 0
slot update_slots: id  3 | task 6919 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4425
slot update_slots: id  3 | task 6919 | n_tokens = 4353, memory_seq_rm [4353, end)
slot update_slots: id  3 | task 6919 | prompt processing progress, n_tokens = 4361, batch.n_tokens = 8, progress = 0.985537
slot update_slots: id  3 | task 6919 | n_tokens = 4361, memory_seq_rm [4361, end)
slot update_slots: id  3 | task 6919 | prompt processing progress, n_tokens = 4425, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6919 | prompt done, n_tokens = 4425, batch.n_tokens = 64
slot init_sampler: id  3 | task 6919 | init sampler, took 0.72 ms, tokens: text = 4425, total = 4425
slot update_slots: id  3 | task 6919 | erasing old context checkpoint (pos_min = 1400, pos_max = 2423, size = 24.012 MiB)
slot update_slots: id  3 | task 6919 | created context checkpoint 8 of 8 (pos_min = 3757, pos_max = 4360, size = 14.163 MiB)
slot print_timing: id  3 | task 6919 | 
prompt eval time =     222.63 ms /    72 tokens (    3.09 ms per token,   323.41 tokens per second)
       eval time =    2547.45 ms /   105 tokens (   24.26 ms per token,    41.22 tokens per second)
      total time =    2770.08 ms /   177 tokens
slot      release: id  3 | task 6919 | stop processing: n_tokens = 4529, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7026 | processing task, is_child = 0
slot update_slots: id  3 | task 7026 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4497
slot update_slots: id  3 | task 7026 | n_tokens = 4425, memory_seq_rm [4425, end)
slot update_slots: id  3 | task 7026 | prompt processing progress, n_tokens = 4433, batch.n_tokens = 8, progress = 0.985768
slot update_slots: id  3 | task 7026 | n_tokens = 4433, memory_seq_rm [4433, end)
slot update_slots: id  3 | task 7026 | prompt processing progress, n_tokens = 4497, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7026 | prompt done, n_tokens = 4497, batch.n_tokens = 64
slot init_sampler: id  3 | task 7026 | init sampler, took 0.69 ms, tokens: text = 4497, total = 4497
slot update_slots: id  3 | task 7026 | erasing old context checkpoint (pos_min = 2669, pos_max = 3692, size = 24.012 MiB)
slot update_slots: id  3 | task 7026 | created context checkpoint 8 of 8 (pos_min = 3757, pos_max = 4432, size = 15.852 MiB)
slot print_timing: id  3 | task 7026 | 
prompt eval time =     226.03 ms /    72 tokens (    3.14 ms per token,   318.54 tokens per second)
       eval time =    3170.81 ms /   129 tokens (   24.58 ms per token,    40.68 tokens per second)
      total time =    3396.84 ms /   201 tokens
slot      release: id  3 | task 7026 | stop processing: n_tokens = 4625, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7157 | processing task, is_child = 0
slot update_slots: id  3 | task 7157 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4601
slot update_slots: id  3 | task 7157 | n_tokens = 4497, memory_seq_rm [4497, end)
slot update_slots: id  3 | task 7157 | prompt processing progress, n_tokens = 4537, batch.n_tokens = 40, progress = 0.986090
slot update_slots: id  3 | task 7157 | n_tokens = 4537, memory_seq_rm [4537, end)
slot update_slots: id  3 | task 7157 | prompt processing progress, n_tokens = 4601, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7157 | prompt done, n_tokens = 4601, batch.n_tokens = 64
slot init_sampler: id  3 | task 7157 | init sampler, took 0.87 ms, tokens: text = 4601, total = 4601
slot update_slots: id  3 | task 7157 | erasing old context checkpoint (pos_min = 3156, pos_max = 3796, size = 15.031 MiB)
slot update_slots: id  3 | task 7157 | created context checkpoint 8 of 8 (pos_min = 3757, pos_max = 4536, size = 18.290 MiB)
slot print_timing: id  3 | task 7157 | 
prompt eval time =     303.39 ms /   104 tokens (    2.92 ms per token,   342.79 tokens per second)
       eval time =    3699.23 ms /   150 tokens (   24.66 ms per token,    40.55 tokens per second)
      total time =    4002.63 ms /   254 tokens
slot      release: id  3 | task 7157 | stop processing: n_tokens = 4750, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7309 | processing task, is_child = 0
slot update_slots: id  3 | task 7309 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4673
slot update_slots: id  3 | task 7309 | n_tokens = 4601, memory_seq_rm [4601, end)
slot update_slots: id  3 | task 7309 | prompt processing progress, n_tokens = 4609, batch.n_tokens = 8, progress = 0.986304
slot update_slots: id  3 | task 7309 | n_tokens = 4609, memory_seq_rm [4609, end)
slot update_slots: id  3 | task 7309 | prompt processing progress, n_tokens = 4673, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7309 | prompt done, n_tokens = 4673, batch.n_tokens = 64
slot init_sampler: id  3 | task 7309 | init sampler, took 0.80 ms, tokens: text = 4673, total = 4673
slot update_slots: id  3 | task 7309 | erasing old context checkpoint (pos_min = 3358, pos_max = 3936, size = 13.577 MiB)
slot update_slots: id  3 | task 7309 | created context checkpoint 8 of 8 (pos_min = 3757, pos_max = 4608, size = 19.979 MiB)
slot print_timing: id  3 | task 7309 | 
prompt eval time =     232.56 ms /    72 tokens (    3.23 ms per token,   309.59 tokens per second)
       eval time =    4228.87 ms /   176 tokens (   24.03 ms per token,    41.62 tokens per second)
      total time =    4461.43 ms /   248 tokens
slot      release: id  3 | task 7309 | stop processing: n_tokens = 4848, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7487 | processing task, is_child = 0
slot update_slots: id  3 | task 7487 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4745
slot update_slots: id  3 | task 7487 | n_tokens = 4673, memory_seq_rm [4673, end)
slot update_slots: id  3 | task 7487 | prompt processing progress, n_tokens = 4681, batch.n_tokens = 8, progress = 0.986512
slot update_slots: id  3 | task 7487 | n_tokens = 4681, memory_seq_rm [4681, end)
slot update_slots: id  3 | task 7487 | prompt processing progress, n_tokens = 4745, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7487 | prompt done, n_tokens = 4745, batch.n_tokens = 64
slot init_sampler: id  3 | task 7487 | init sampler, took 0.77 ms, tokens: text = 4745, total = 4745
slot update_slots: id  3 | task 7487 | erasing old context checkpoint (pos_min = 3574, pos_max = 4040, size = 10.951 MiB)
slot update_slots: id  3 | task 7487 | created context checkpoint 8 of 8 (pos_min = 3824, pos_max = 4680, size = 20.096 MiB)
slot print_timing: id  3 | task 7487 | 
prompt eval time =     225.84 ms /    72 tokens (    3.14 ms per token,   318.82 tokens per second)
       eval time =    3118.97 ms /   130 tokens (   23.99 ms per token,    41.68 tokens per second)
      total time =    3344.81 ms /   202 tokens
slot      release: id  3 | task 7487 | stop processing: n_tokens = 4874, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7619 | processing task, is_child = 0
slot update_slots: id  3 | task 7619 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4819
slot update_slots: id  3 | task 7619 | n_tokens = 4745, memory_seq_rm [4745, end)
slot update_slots: id  3 | task 7619 | prompt processing progress, n_tokens = 4755, batch.n_tokens = 10, progress = 0.986719
slot update_slots: id  3 | task 7619 | n_tokens = 4755, memory_seq_rm [4755, end)
slot update_slots: id  3 | task 7619 | prompt processing progress, n_tokens = 4819, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7619 | prompt done, n_tokens = 4819, batch.n_tokens = 64
slot init_sampler: id  3 | task 7619 | init sampler, took 0.71 ms, tokens: text = 4819, total = 4819
slot update_slots: id  3 | task 7619 | erasing old context checkpoint (pos_min = 3574, pos_max = 4144, size = 13.390 MiB)
slot update_slots: id  3 | task 7619 | created context checkpoint 8 of 8 (pos_min = 3958, pos_max = 4754, size = 18.689 MiB)
slot print_timing: id  3 | task 7619 | 
prompt eval time =     234.09 ms /    74 tokens (    3.16 ms per token,   316.12 tokens per second)
       eval time =    3246.66 ms /   130 tokens (   24.97 ms per token,    40.04 tokens per second)
      total time =    3480.75 ms /   204 tokens
slot      release: id  3 | task 7619 | stop processing: n_tokens = 4948, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.791 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7751 | processing task, is_child = 0
slot update_slots: id  3 | task 7751 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6093
slot update_slots: id  3 | task 7751 | n_tokens = 4819, memory_seq_rm [4819, end)
slot update_slots: id  3 | task 7751 | prompt processing progress, n_tokens = 6029, batch.n_tokens = 1210, progress = 0.989496
slot update_slots: id  3 | task 7751 | n_tokens = 6029, memory_seq_rm [6029, end)
slot update_slots: id  3 | task 7751 | prompt processing progress, n_tokens = 6093, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7751 | prompt done, n_tokens = 6093, batch.n_tokens = 64
slot init_sampler: id  3 | task 7751 | init sampler, took 1.78 ms, tokens: text = 6093, total = 6093
slot update_slots: id  3 | task 7751 | erasing old context checkpoint (pos_min = 3624, pos_max = 4216, size = 13.906 MiB)
slot update_slots: id  3 | task 7751 | created context checkpoint 8 of 8 (pos_min = 5005, pos_max = 6028, size = 24.012 MiB)
slot print_timing: id  3 | task 7751 | 
prompt eval time =    1397.16 ms /  1274 tokens (    1.10 ms per token,   911.85 tokens per second)
       eval time =    1921.92 ms /    80 tokens (   24.02 ms per token,    41.62 tokens per second)
      total time =    3319.08 ms /  1354 tokens
slot      release: id  3 | task 7751 | stop processing: n_tokens = 6172, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7833 | processing task, is_child = 0
slot update_slots: id  3 | task 7833 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6165
slot update_slots: id  3 | task 7833 | n_tokens = 6093, memory_seq_rm [6093, end)
slot update_slots: id  3 | task 7833 | prompt processing progress, n_tokens = 6101, batch.n_tokens = 8, progress = 0.989619
slot update_slots: id  3 | task 7833 | n_tokens = 6101, memory_seq_rm [6101, end)
slot update_slots: id  3 | task 7833 | prompt processing progress, n_tokens = 6165, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7833 | prompt done, n_tokens = 6165, batch.n_tokens = 64
slot init_sampler: id  3 | task 7833 | init sampler, took 1.85 ms, tokens: text = 6165, total = 6165
slot update_slots: id  3 | task 7833 | erasing old context checkpoint (pos_min = 3757, pos_max = 4288, size = 12.475 MiB)
slot update_slots: id  3 | task 7833 | created context checkpoint 8 of 8 (pos_min = 5148, pos_max = 6100, size = 22.347 MiB)
slot print_timing: id  3 | task 7833 | 
prompt eval time =     229.85 ms /    72 tokens (    3.19 ms per token,   313.25 tokens per second)
       eval time =    7668.56 ms /   315 tokens (   24.34 ms per token,    41.08 tokens per second)
      total time =    7898.41 ms /   387 tokens
slot      release: id  3 | task 7833 | stop processing: n_tokens = 6479, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.923 (> 0.100 thold), f_keep = 0.952
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8150 | processing task, is_child = 0
slot update_slots: id  3 | task 8150 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6679
slot update_slots: id  3 | task 8150 | n_tokens = 6165, memory_seq_rm [6165, end)
slot update_slots: id  3 | task 8150 | prompt processing progress, n_tokens = 6615, batch.n_tokens = 450, progress = 0.990418
slot update_slots: id  3 | task 8150 | n_tokens = 6615, memory_seq_rm [6615, end)
slot update_slots: id  3 | task 8150 | prompt processing progress, n_tokens = 6679, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8150 | prompt done, n_tokens = 6679, batch.n_tokens = 64
slot init_sampler: id  3 | task 8150 | init sampler, took 1.02 ms, tokens: text = 6679, total = 6679
slot update_slots: id  3 | task 8150 | erasing old context checkpoint (pos_min = 3757, pos_max = 4360, size = 14.163 MiB)
slot update_slots: id  3 | task 8150 | created context checkpoint 8 of 8 (pos_min = 5591, pos_max = 6614, size = 24.012 MiB)
slot print_timing: id  3 | task 8150 | 
prompt eval time =     691.53 ms /   514 tokens (    1.35 ms per token,   743.28 tokens per second)
       eval time =    4711.51 ms /   197 tokens (   23.92 ms per token,    41.81 tokens per second)
      total time =    5403.04 ms /   711 tokens
slot      release: id  3 | task 8150 | stop processing: n_tokens = 6875, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.161 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8349 | processing task, is_child = 0
slot update_slots: id  3 | task 8349 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 41613
slot update_slots: id  3 | task 8349 | n_tokens = 6679, memory_seq_rm [6679, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 8727, batch.n_tokens = 2048, progress = 0.209718
slot update_slots: id  3 | task 8349 | n_tokens = 8727, memory_seq_rm [8727, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 10775, batch.n_tokens = 2048, progress = 0.258934
slot update_slots: id  3 | task 8349 | n_tokens = 10775, memory_seq_rm [10775, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 12823, batch.n_tokens = 2048, progress = 0.308149
slot update_slots: id  3 | task 8349 | n_tokens = 12823, memory_seq_rm [12823, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 14871, batch.n_tokens = 2048, progress = 0.357364
slot update_slots: id  3 | task 8349 | n_tokens = 14871, memory_seq_rm [14871, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 16919, batch.n_tokens = 2048, progress = 0.406580
slot update_slots: id  3 | task 8349 | n_tokens = 16919, memory_seq_rm [16919, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 18967, batch.n_tokens = 2048, progress = 0.455795
slot update_slots: id  3 | task 8349 | n_tokens = 18967, memory_seq_rm [18967, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 21015, batch.n_tokens = 2048, progress = 0.505010
slot update_slots: id  3 | task 8349 | n_tokens = 21015, memory_seq_rm [21015, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 23063, batch.n_tokens = 2048, progress = 0.554226
slot update_slots: id  3 | task 8349 | n_tokens = 23063, memory_seq_rm [23063, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 25111, batch.n_tokens = 2048, progress = 0.603441
slot update_slots: id  3 | task 8349 | n_tokens = 25111, memory_seq_rm [25111, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 27159, batch.n_tokens = 2048, progress = 0.652657
slot update_slots: id  3 | task 8349 | n_tokens = 27159, memory_seq_rm [27159, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 29207, batch.n_tokens = 2048, progress = 0.701872
slot update_slots: id  3 | task 8349 | n_tokens = 29207, memory_seq_rm [29207, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 31255, batch.n_tokens = 2048, progress = 0.751087
slot update_slots: id  3 | task 8349 | n_tokens = 31255, memory_seq_rm [31255, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 33303, batch.n_tokens = 2048, progress = 0.800303
slot update_slots: id  3 | task 8349 | n_tokens = 33303, memory_seq_rm [33303, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 35351, batch.n_tokens = 2048, progress = 0.849518
slot update_slots: id  3 | task 8349 | n_tokens = 35351, memory_seq_rm [35351, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 37399, batch.n_tokens = 2048, progress = 0.898734
slot update_slots: id  3 | task 8349 | n_tokens = 37399, memory_seq_rm [37399, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 39447, batch.n_tokens = 2048, progress = 0.947949
slot update_slots: id  3 | task 8349 | n_tokens = 39447, memory_seq_rm [39447, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 41495, batch.n_tokens = 2048, progress = 0.997164
slot update_slots: id  3 | task 8349 | n_tokens = 41495, memory_seq_rm [41495, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 41549, batch.n_tokens = 54, progress = 0.998462
slot update_slots: id  3 | task 8349 | n_tokens = 41549, memory_seq_rm [41549, end)
slot update_slots: id  3 | task 8349 | prompt processing progress, n_tokens = 41613, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8349 | prompt done, n_tokens = 41613, batch.n_tokens = 64
slot init_sampler: id  3 | task 8349 | init sampler, took 5.89 ms, tokens: text = 41613, total = 41613
slot update_slots: id  3 | task 8349 | erasing old context checkpoint (pos_min = 3757, pos_max = 4432, size = 15.852 MiB)
slot update_slots: id  3 | task 8349 | created context checkpoint 8 of 8 (pos_min = 40525, pos_max = 41548, size = 24.012 MiB)
slot print_timing: id  3 | task 8349 | 
prompt eval time =   40798.66 ms / 34934 tokens (    1.17 ms per token,   856.25 tokens per second)
       eval time =    6303.89 ms /   219 tokens (   28.78 ms per token,    34.74 tokens per second)
      total time =   47102.56 ms / 35153 tokens
slot      release: id  3 | task 8349 | stop processing: n_tokens = 41831, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8587 | processing task, is_child = 0
slot update_slots: id  3 | task 8587 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 41950
slot update_slots: id  3 | task 8587 | n_tokens = 41613, memory_seq_rm [41613, end)
slot update_slots: id  3 | task 8587 | prompt processing progress, n_tokens = 41886, batch.n_tokens = 273, progress = 0.998474
slot update_slots: id  3 | task 8587 | n_tokens = 41886, memory_seq_rm [41886, end)
slot update_slots: id  3 | task 8587 | prompt processing progress, n_tokens = 41950, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8587 | prompt done, n_tokens = 41950, batch.n_tokens = 64
slot init_sampler: id  3 | task 8587 | init sampler, took 6.10 ms, tokens: text = 41950, total = 41950
slot update_slots: id  3 | task 8587 | erasing old context checkpoint (pos_min = 3757, pos_max = 4536, size = 18.290 MiB)
slot update_slots: id  3 | task 8587 | created context checkpoint 8 of 8 (pos_min = 40862, pos_max = 41885, size = 24.012 MiB)
slot print_timing: id  3 | task 8587 | 
prompt eval time =     775.30 ms /   337 tokens (    2.30 ms per token,   434.67 tokens per second)
       eval time =    1766.68 ms /    63 tokens (   28.04 ms per token,    35.66 tokens per second)
      total time =    2541.97 ms /   400 tokens
slot      release: id  3 | task 8587 | stop processing: n_tokens = 42012, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8652 | processing task, is_child = 0
slot update_slots: id  3 | task 8652 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 42123
slot update_slots: id  3 | task 8652 | n_tokens = 41950, memory_seq_rm [41950, end)
slot update_slots: id  3 | task 8652 | prompt processing progress, n_tokens = 42059, batch.n_tokens = 109, progress = 0.998481
slot update_slots: id  3 | task 8652 | n_tokens = 42059, memory_seq_rm [42059, end)
slot update_slots: id  3 | task 8652 | prompt processing progress, n_tokens = 42123, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8652 | prompt done, n_tokens = 42123, batch.n_tokens = 64
slot init_sampler: id  3 | task 8652 | init sampler, took 5.84 ms, tokens: text = 42123, total = 42123
slot update_slots: id  3 | task 8652 | erasing old context checkpoint (pos_min = 3757, pos_max = 4608, size = 19.979 MiB)
slot update_slots: id  3 | task 8652 | created context checkpoint 8 of 8 (pos_min = 41035, pos_max = 42058, size = 24.012 MiB)
slot print_timing: id  3 | task 8652 | 
prompt eval time =     578.59 ms /   173 tokens (    3.34 ms per token,   299.00 tokens per second)
       eval time =    6612.50 ms /   235 tokens (   28.14 ms per token,    35.54 tokens per second)
      total time =    7191.09 ms /   408 tokens
slot      release: id  3 | task 8652 | stop processing: n_tokens = 42357, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8889 | processing task, is_child = 0
slot update_slots: id  3 | task 8889 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 42324
slot update_slots: id  3 | task 8889 | n_tokens = 42123, memory_seq_rm [42123, end)
slot update_slots: id  3 | task 8889 | prompt processing progress, n_tokens = 42260, batch.n_tokens = 137, progress = 0.998488
slot update_slots: id  3 | task 8889 | n_tokens = 42260, memory_seq_rm [42260, end)
slot update_slots: id  3 | task 8889 | prompt processing progress, n_tokens = 42324, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8889 | prompt done, n_tokens = 42324, batch.n_tokens = 64
slot init_sampler: id  3 | task 8889 | init sampler, took 6.05 ms, tokens: text = 42324, total = 42324
slot update_slots: id  3 | task 8889 | erasing old context checkpoint (pos_min = 3824, pos_max = 4680, size = 20.096 MiB)
slot update_slots: id  3 | task 8889 | created context checkpoint 8 of 8 (pos_min = 41333, pos_max = 42259, size = 21.737 MiB)
slot print_timing: id  3 | task 8889 | 
prompt eval time =     619.14 ms /   201 tokens (    3.08 ms per token,   324.65 tokens per second)
       eval time =    8135.57 ms /   290 tokens (   28.05 ms per token,    35.65 tokens per second)
      total time =    8754.71 ms /   491 tokens
slot      release: id  3 | task 8889 | stop processing: n_tokens = 42613, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9181 | processing task, is_child = 0
slot update_slots: id  3 | task 9181 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 42532
slot update_slots: id  3 | task 9181 | n_tokens = 42324, memory_seq_rm [42324, end)
slot update_slots: id  3 | task 9181 | prompt processing progress, n_tokens = 42468, batch.n_tokens = 144, progress = 0.998495
slot update_slots: id  3 | task 9181 | n_tokens = 42468, memory_seq_rm [42468, end)
slot update_slots: id  3 | task 9181 | prompt processing progress, n_tokens = 42532, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9181 | prompt done, n_tokens = 42532, batch.n_tokens = 64
slot init_sampler: id  3 | task 9181 | init sampler, took 5.98 ms, tokens: text = 42532, total = 42532
slot update_slots: id  3 | task 9181 | erasing old context checkpoint (pos_min = 3958, pos_max = 4754, size = 18.689 MiB)
slot update_slots: id  3 | task 9181 | created context checkpoint 8 of 8 (pos_min = 41639, pos_max = 42467, size = 19.439 MiB)
slot print_timing: id  3 | task 9181 | 
prompt eval time =     571.73 ms /   208 tokens (    2.75 ms per token,   363.81 tokens per second)
       eval time =   14262.63 ms /   500 tokens (   28.53 ms per token,    35.06 tokens per second)
      total time =   14834.37 ms /   708 tokens
slot      release: id  3 | task 9181 | stop processing: n_tokens = 43031, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9683 | processing task, is_child = 0
slot update_slots: id  3 | task 9683 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 42758
slot update_slots: id  3 | task 9683 | n_tokens = 42532, memory_seq_rm [42532, end)
slot update_slots: id  3 | task 9683 | prompt processing progress, n_tokens = 42694, batch.n_tokens = 162, progress = 0.998503
slot update_slots: id  3 | task 9683 | n_tokens = 42694, memory_seq_rm [42694, end)
slot update_slots: id  3 | task 9683 | prompt processing progress, n_tokens = 42758, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9683 | prompt done, n_tokens = 42758, batch.n_tokens = 64
slot init_sampler: id  3 | task 9683 | init sampler, took 6.91 ms, tokens: text = 42758, total = 42758
slot update_slots: id  3 | task 9683 | erasing old context checkpoint (pos_min = 5005, pos_max = 6028, size = 24.012 MiB)
slot update_slots: id  3 | task 9683 | created context checkpoint 8 of 8 (pos_min = 42202, pos_max = 42693, size = 11.537 MiB)
slot print_timing: id  3 | task 9683 | 
prompt eval time =     707.24 ms /   226 tokens (    3.13 ms per token,   319.55 tokens per second)
       eval time =   10508.37 ms /   365 tokens (   28.79 ms per token,    34.73 tokens per second)
      total time =   11215.61 ms /   591 tokens
slot      release: id  3 | task 9683 | stop processing: n_tokens = 43122, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10050 | processing task, is_child = 0
slot update_slots: id  3 | task 10050 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 42941
slot update_slots: id  3 | task 10050 | n_tokens = 42758, memory_seq_rm [42758, end)
slot update_slots: id  3 | task 10050 | prompt processing progress, n_tokens = 42877, batch.n_tokens = 119, progress = 0.998510
slot update_slots: id  3 | task 10050 | n_tokens = 42877, memory_seq_rm [42877, end)
slot update_slots: id  3 | task 10050 | prompt processing progress, n_tokens = 42941, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10050 | prompt done, n_tokens = 42941, batch.n_tokens = 64
slot init_sampler: id  3 | task 10050 | init sampler, took 6.46 ms, tokens: text = 42941, total = 42941
slot update_slots: id  3 | task 10050 | erasing old context checkpoint (pos_min = 5148, pos_max = 6100, size = 22.347 MiB)
slot update_slots: id  3 | task 10050 | created context checkpoint 8 of 8 (pos_min = 42293, pos_max = 42876, size = 13.694 MiB)
slot print_timing: id  3 | task 10050 | 
prompt eval time =     688.46 ms /   183 tokens (    3.76 ms per token,   265.81 tokens per second)
       eval time =    8487.61 ms /   293 tokens (   28.97 ms per token,    34.52 tokens per second)
      total time =    9176.08 ms /   476 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 10050 | stop processing: n_tokens = 43233, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10345 | processing task, is_child = 0
slot update_slots: id  3 | task 10345 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 43105
slot update_slots: id  3 | task 10345 | n_tokens = 42941, memory_seq_rm [42941, end)
slot update_slots: id  3 | task 10345 | prompt processing progress, n_tokens = 43041, batch.n_tokens = 100, progress = 0.998515
slot update_slots: id  3 | task 10345 | n_tokens = 43041, memory_seq_rm [43041, end)
slot update_slots: id  3 | task 10345 | prompt processing progress, n_tokens = 43105, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10345 | prompt done, n_tokens = 43105, batch.n_tokens = 64
slot init_sampler: id  3 | task 10345 | init sampler, took 6.11 ms, tokens: text = 43105, total = 43105
slot update_slots: id  3 | task 10345 | erasing old context checkpoint (pos_min = 5591, pos_max = 6614, size = 24.012 MiB)
slot update_slots: id  3 | task 10345 | created context checkpoint 8 of 8 (pos_min = 42324, pos_max = 43040, size = 16.813 MiB)
slot print_timing: id  3 | task 10345 | 
prompt eval time =     635.24 ms /   164 tokens (    3.87 ms per token,   258.17 tokens per second)
       eval time =    8151.49 ms /   280 tokens (   29.11 ms per token,    34.35 tokens per second)
      total time =    8786.73 ms /   444 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 10345 | stop processing: n_tokens = 43384, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.393 (> 0.100 thold), f_keep = 0.039
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 43384, total state size = 1041.321 MiB
srv          load:  - looking for better prompt, base f_keep = 0.039, sim = 0.393
srv        update:  - cache state: 2 prompts, 1559.454 MiB (limits: 8192.000 MiB, 64000 tokens, 269485 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv  get_availabl: prompt cache update took 1382.15 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10627 | processing task, is_child = 0
slot update_slots: id  3 | task 10627 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4288
slot update_slots: id  3 | task 10627 | n_past = 1685, slot.prompt.tokens.size() = 43384, seq_id = 3, pos_min = 42360, n_swa = 128
slot update_slots: id  3 | task 10627 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 40525, pos_max = 41548, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 40862, pos_max = 41885, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 41035, pos_max = 42058, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 41333, pos_max = 42259, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 41639, pos_max = 42467, n_swa = 128, size = 19.439 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 42202, pos_max = 42693, n_swa = 128, size = 11.537 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 42293, pos_max = 42876, n_swa = 128, size = 13.694 MiB)
slot update_slots: id  3 | task 10627 | erased invalidated context checkpoint (pos_min = 42324, pos_max = 43040, n_swa = 128, size = 16.813 MiB)
slot update_slots: id  3 | task 10627 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 10627 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.477612
slot update_slots: id  3 | task 10627 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 10627 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.955224
slot update_slots: id  3 | task 10627 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 10627 | prompt processing progress, n_tokens = 4224, batch.n_tokens = 128, progress = 0.985075
slot update_slots: id  3 | task 10627 | n_tokens = 4224, memory_seq_rm [4224, end)
slot update_slots: id  3 | task 10627 | prompt processing progress, n_tokens = 4288, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10627 | prompt done, n_tokens = 4288, batch.n_tokens = 64
slot init_sampler: id  3 | task 10627 | init sampler, took 0.72 ms, tokens: text = 4288, total = 4288
slot update_slots: id  3 | task 10627 | created context checkpoint 1 of 8 (pos_min = 3200, pos_max = 4223, size = 24.012 MiB)
slot print_timing: id  3 | task 10627 | 
prompt eval time =    4285.18 ms /  4288 tokens (    1.00 ms per token,  1000.66 tokens per second)
       eval time =    1478.73 ms /    62 tokens (   23.85 ms per token,    41.93 tokens per second)
      total time =    5763.91 ms /  4350 tokens
slot      release: id  3 | task 10627 | stop processing: n_tokens = 4349, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.872 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10693 | processing task, is_child = 0
slot update_slots: id  3 | task 10693 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4915
slot update_slots: id  3 | task 10693 | n_tokens = 4288, memory_seq_rm [4288, end)
slot update_slots: id  3 | task 10693 | prompt processing progress, n_tokens = 4851, batch.n_tokens = 563, progress = 0.986979
slot update_slots: id  3 | task 10693 | n_tokens = 4851, memory_seq_rm [4851, end)
slot update_slots: id  3 | task 10693 | prompt processing progress, n_tokens = 4915, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10693 | prompt done, n_tokens = 4915, batch.n_tokens = 64
slot init_sampler: id  3 | task 10693 | init sampler, took 0.80 ms, tokens: text = 4915, total = 4915
slot update_slots: id  3 | task 10693 | created context checkpoint 2 of 8 (pos_min = 3827, pos_max = 4850, size = 24.012 MiB)
slot print_timing: id  3 | task 10693 | 
prompt eval time =     834.97 ms /   627 tokens (    1.33 ms per token,   750.93 tokens per second)
       eval time =    1096.53 ms /    45 tokens (   24.37 ms per token,    41.04 tokens per second)
      total time =    1931.50 ms /   672 tokens
slot      release: id  3 | task 10693 | stop processing: n_tokens = 4959, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10740 | processing task, is_child = 0
slot update_slots: id  3 | task 10740 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4967
slot update_slots: id  3 | task 10740 | n_tokens = 4915, memory_seq_rm [4915, end)
slot update_slots: id  3 | task 10740 | prompt processing progress, n_tokens = 4967, batch.n_tokens = 52, progress = 1.000000
slot update_slots: id  3 | task 10740 | prompt done, n_tokens = 4967, batch.n_tokens = 52
slot init_sampler: id  3 | task 10740 | init sampler, took 0.80 ms, tokens: text = 4967, total = 4967
slot print_timing: id  3 | task 10740 | 
prompt eval time =     147.66 ms /    52 tokens (    2.84 ms per token,   352.15 tokens per second)
       eval time =    1293.91 ms /    53 tokens (   24.41 ms per token,    40.96 tokens per second)
      total time =    1441.58 ms /   105 tokens
slot      release: id  3 | task 10740 | stop processing: n_tokens = 5019, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10794 | processing task, is_child = 0
slot update_slots: id  3 | task 10794 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5049
slot update_slots: id  3 | task 10794 | n_tokens = 4967, memory_seq_rm [4967, end)
slot update_slots: id  3 | task 10794 | prompt processing progress, n_tokens = 4985, batch.n_tokens = 18, progress = 0.987324
slot update_slots: id  3 | task 10794 | n_tokens = 4985, memory_seq_rm [4985, end)
slot update_slots: id  3 | task 10794 | prompt processing progress, n_tokens = 5049, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10794 | prompt done, n_tokens = 5049, batch.n_tokens = 64
slot init_sampler: id  3 | task 10794 | init sampler, took 0.93 ms, tokens: text = 5049, total = 5049
slot update_slots: id  3 | task 10794 | created context checkpoint 3 of 8 (pos_min = 3995, pos_max = 4984, size = 23.215 MiB)
slot print_timing: id  3 | task 10794 | 
prompt eval time =     261.22 ms /    82 tokens (    3.19 ms per token,   313.91 tokens per second)
       eval time =   17281.97 ms /   700 tokens (   24.69 ms per token,    40.50 tokens per second)
      total time =   17543.19 ms /   782 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 10794 | stop processing: n_tokens = 5748, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.925 (> 0.100 thold), f_keep = 0.878
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11496 | processing task, is_child = 0
slot update_slots: id  3 | task 11496 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5456
slot update_slots: id  3 | task 11496 | n_tokens = 5049, memory_seq_rm [5049, end)
slot update_slots: id  3 | task 11496 | prompt processing progress, n_tokens = 5392, batch.n_tokens = 343, progress = 0.988270
slot update_slots: id  3 | task 11496 | n_tokens = 5392, memory_seq_rm [5392, end)
slot update_slots: id  3 | task 11496 | prompt processing progress, n_tokens = 5456, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11496 | prompt done, n_tokens = 5456, batch.n_tokens = 64
slot init_sampler: id  3 | task 11496 | init sampler, took 1.12 ms, tokens: text = 5456, total = 5456
slot update_slots: id  3 | task 11496 | created context checkpoint 4 of 8 (pos_min = 4724, pos_max = 5391, size = 15.664 MiB)
slot print_timing: id  3 | task 11496 | 
prompt eval time =     678.13 ms /   407 tokens (    1.67 ms per token,   600.18 tokens per second)
       eval time =    1349.19 ms /    52 tokens (   25.95 ms per token,    38.54 tokens per second)
      total time =    2027.33 ms /   459 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 11496 | stop processing: n_tokens = 5507, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11550 | processing task, is_child = 0
slot update_slots: id  3 | task 11550 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5514
slot update_slots: id  3 | task 11550 | n_tokens = 5456, memory_seq_rm [5456, end)
slot update_slots: id  3 | task 11550 | prompt processing progress, n_tokens = 5514, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 11550 | prompt done, n_tokens = 5514, batch.n_tokens = 58
slot init_sampler: id  3 | task 11550 | init sampler, took 0.84 ms, tokens: text = 5514, total = 5514
slot print_timing: id  3 | task 11550 | 
prompt eval time =     245.25 ms /    58 tokens (    4.23 ms per token,   236.50 tokens per second)
       eval time =    7266.28 ms /   288 tokens (   25.23 ms per token,    39.64 tokens per second)
      total time =    7511.53 ms /   346 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 11550 | stop processing: n_tokens = 5801, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.951
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11839 | processing task, is_child = 0
slot update_slots: id  3 | task 11839 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5763
slot update_slots: id  3 | task 11839 | n_tokens = 5514, memory_seq_rm [5514, end)
slot update_slots: id  3 | task 11839 | prompt processing progress, n_tokens = 5699, batch.n_tokens = 185, progress = 0.988895
slot update_slots: id  3 | task 11839 | n_tokens = 5699, memory_seq_rm [5699, end)
slot update_slots: id  3 | task 11839 | prompt processing progress, n_tokens = 5763, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11839 | prompt done, n_tokens = 5763, batch.n_tokens = 64
slot init_sampler: id  3 | task 11839 | init sampler, took 1.47 ms, tokens: text = 5763, total = 5763
slot update_slots: id  3 | task 11839 | created context checkpoint 5 of 8 (pos_min = 4848, pos_max = 5698, size = 19.955 MiB)
slot print_timing: id  3 | task 11839 | 
prompt eval time =     462.82 ms /   249 tokens (    1.86 ms per token,   538.00 tokens per second)
       eval time =    1338.54 ms /    50 tokens (   26.77 ms per token,    37.35 tokens per second)
      total time =    1801.36 ms /   299 tokens
slot      release: id  3 | task 11839 | stop processing: n_tokens = 5812, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11891 | processing task, is_child = 0
slot update_slots: id  3 | task 11891 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5873
slot update_slots: id  3 | task 11891 | n_tokens = 5763, memory_seq_rm [5763, end)
slot update_slots: id  3 | task 11891 | prompt processing progress, n_tokens = 5809, batch.n_tokens = 46, progress = 0.989103
slot update_slots: id  3 | task 11891 | n_tokens = 5809, memory_seq_rm [5809, end)
slot update_slots: id  3 | task 11891 | prompt processing progress, n_tokens = 5873, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11891 | prompt done, n_tokens = 5873, batch.n_tokens = 64
slot init_sampler: id  3 | task 11891 | init sampler, took 0.90 ms, tokens: text = 5873, total = 5873
slot update_slots: id  3 | task 11891 | created context checkpoint 6 of 8 (pos_min = 4859, pos_max = 5808, size = 22.277 MiB)
slot print_timing: id  3 | task 11891 | 
prompt eval time =     380.94 ms /   110 tokens (    3.46 ms per token,   288.76 tokens per second)
       eval time =    4436.25 ms /   185 tokens (   23.98 ms per token,    41.70 tokens per second)
      total time =    4817.19 ms /   295 tokens
slot      release: id  3 | task 11891 | stop processing: n_tokens = 6057, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12078 | processing task, is_child = 0
slot update_slots: id  3 | task 12078 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6046
slot update_slots: id  3 | task 12078 | n_tokens = 5873, memory_seq_rm [5873, end)
slot update_slots: id  3 | task 12078 | prompt processing progress, n_tokens = 5982, batch.n_tokens = 109, progress = 0.989415
slot update_slots: id  3 | task 12078 | n_tokens = 5982, memory_seq_rm [5982, end)
slot update_slots: id  3 | task 12078 | prompt processing progress, n_tokens = 6046, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12078 | prompt done, n_tokens = 6046, batch.n_tokens = 64
slot init_sampler: id  3 | task 12078 | init sampler, took 0.91 ms, tokens: text = 6046, total = 6046
slot update_slots: id  3 | task 12078 | created context checkpoint 7 of 8 (pos_min = 5049, pos_max = 5981, size = 21.878 MiB)
slot print_timing: id  3 | task 12078 | 
prompt eval time =     422.68 ms /   173 tokens (    2.44 ms per token,   409.30 tokens per second)
       eval time =    6492.61 ms /   262 tokens (   24.78 ms per token,    40.35 tokens per second)
      total time =    6915.29 ms /   435 tokens
slot      release: id  3 | task 12078 | stop processing: n_tokens = 6307, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12342 | processing task, is_child = 0
slot update_slots: id  3 | task 12342 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6294
slot update_slots: id  3 | task 12342 | n_tokens = 6046, memory_seq_rm [6046, end)
slot update_slots: id  3 | task 12342 | prompt processing progress, n_tokens = 6230, batch.n_tokens = 184, progress = 0.989832
slot update_slots: id  3 | task 12342 | n_tokens = 6230, memory_seq_rm [6230, end)
slot update_slots: id  3 | task 12342 | prompt processing progress, n_tokens = 6294, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12342 | prompt done, n_tokens = 6294, batch.n_tokens = 64
slot init_sampler: id  3 | task 12342 | init sampler, took 1.17 ms, tokens: text = 6294, total = 6294
slot update_slots: id  3 | task 12342 | created context checkpoint 8 of 8 (pos_min = 5283, pos_max = 6229, size = 22.206 MiB)
slot print_timing: id  3 | task 12342 | 
prompt eval time =     461.81 ms /   248 tokens (    1.86 ms per token,   537.02 tokens per second)
       eval time =    1035.94 ms /    40 tokens (   25.90 ms per token,    38.61 tokens per second)
      total time =    1497.75 ms /   288 tokens
slot      release: id  3 | task 12342 | stop processing: n_tokens = 6333, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12384 | processing task, is_child = 0
slot update_slots: id  3 | task 12384 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6402
slot update_slots: id  3 | task 12384 | n_tokens = 6294, memory_seq_rm [6294, end)
slot update_slots: id  3 | task 12384 | prompt processing progress, n_tokens = 6338, batch.n_tokens = 44, progress = 0.990003
slot update_slots: id  3 | task 12384 | n_tokens = 6338, memory_seq_rm [6338, end)
slot update_slots: id  3 | task 12384 | prompt processing progress, n_tokens = 6402, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12384 | prompt done, n_tokens = 6402, batch.n_tokens = 64
slot init_sampler: id  3 | task 12384 | init sampler, took 0.96 ms, tokens: text = 6402, total = 6402
slot update_slots: id  3 | task 12384 | erasing old context checkpoint (pos_min = 3200, pos_max = 4223, size = 24.012 MiB)
slot update_slots: id  3 | task 12384 | created context checkpoint 8 of 8 (pos_min = 5341, pos_max = 6337, size = 23.379 MiB)
slot print_timing: id  3 | task 12384 | 
prompt eval time =     417.53 ms /   108 tokens (    3.87 ms per token,   258.66 tokens per second)
       eval time =    3619.20 ms /   149 tokens (   24.29 ms per token,    41.17 tokens per second)
      total time =    4036.73 ms /   257 tokens
slot      release: id  3 | task 12384 | stop processing: n_tokens = 6550, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12535 | processing task, is_child = 0
slot update_slots: id  3 | task 12535 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6500
slot update_slots: id  3 | task 12535 | n_tokens = 6402, memory_seq_rm [6402, end)
slot update_slots: id  3 | task 12535 | prompt processing progress, n_tokens = 6436, batch.n_tokens = 34, progress = 0.990154
slot update_slots: id  3 | task 12535 | n_tokens = 6436, memory_seq_rm [6436, end)
slot update_slots: id  3 | task 12535 | prompt processing progress, n_tokens = 6500, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12535 | prompt done, n_tokens = 6500, batch.n_tokens = 64
slot init_sampler: id  3 | task 12535 | init sampler, took 1.10 ms, tokens: text = 6500, total = 6500
slot update_slots: id  3 | task 12535 | erasing old context checkpoint (pos_min = 3827, pos_max = 4850, size = 24.012 MiB)
slot update_slots: id  3 | task 12535 | created context checkpoint 8 of 8 (pos_min = 5553, pos_max = 6435, size = 20.706 MiB)
slot print_timing: id  3 | task 12535 | 
prompt eval time =     298.60 ms /    98 tokens (    3.05 ms per token,   328.20 tokens per second)
       eval time =    2875.19 ms /   118 tokens (   24.37 ms per token,    41.04 tokens per second)
      total time =    3173.79 ms /   216 tokens
slot      release: id  3 | task 12535 | stop processing: n_tokens = 6617, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12655 | processing task, is_child = 0
slot update_slots: id  3 | task 12655 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6697
slot update_slots: id  3 | task 12655 | n_tokens = 6500, memory_seq_rm [6500, end)
slot update_slots: id  3 | task 12655 | prompt processing progress, n_tokens = 6633, batch.n_tokens = 133, progress = 0.990443
slot update_slots: id  3 | task 12655 | n_tokens = 6633, memory_seq_rm [6633, end)
slot update_slots: id  3 | task 12655 | prompt processing progress, n_tokens = 6697, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12655 | prompt done, n_tokens = 6697, batch.n_tokens = 64
slot init_sampler: id  3 | task 12655 | init sampler, took 3.18 ms, tokens: text = 6697, total = 6697
slot update_slots: id  3 | task 12655 | erasing old context checkpoint (pos_min = 3995, pos_max = 4984, size = 23.215 MiB)
slot update_slots: id  3 | task 12655 | created context checkpoint 8 of 8 (pos_min = 5636, pos_max = 6632, size = 23.379 MiB)
slot print_timing: id  3 | task 12655 | 
prompt eval time =     393.18 ms /   197 tokens (    2.00 ms per token,   501.04 tokens per second)
       eval time =    6217.42 ms /   248 tokens (   25.07 ms per token,    39.89 tokens per second)
      total time =    6610.60 ms /   445 tokens
slot      release: id  3 | task 12655 | stop processing: n_tokens = 6944, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.363 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12905 | processing task, is_child = 0
slot update_slots: id  3 | task 12905 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18439
slot update_slots: id  3 | task 12905 | n_tokens = 6697, memory_seq_rm [6697, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 8745, batch.n_tokens = 2048, progress = 0.474266
slot update_slots: id  3 | task 12905 | n_tokens = 8745, memory_seq_rm [8745, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 10793, batch.n_tokens = 2048, progress = 0.585335
slot update_slots: id  3 | task 12905 | n_tokens = 10793, memory_seq_rm [10793, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 12841, batch.n_tokens = 2048, progress = 0.696404
slot update_slots: id  3 | task 12905 | n_tokens = 12841, memory_seq_rm [12841, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 14889, batch.n_tokens = 2048, progress = 0.807473
slot update_slots: id  3 | task 12905 | n_tokens = 14889, memory_seq_rm [14889, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 16937, batch.n_tokens = 2048, progress = 0.918542
slot update_slots: id  3 | task 12905 | n_tokens = 16937, memory_seq_rm [16937, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 18375, batch.n_tokens = 1438, progress = 0.996529
slot update_slots: id  3 | task 12905 | n_tokens = 18375, memory_seq_rm [18375, end)
slot update_slots: id  3 | task 12905 | prompt processing progress, n_tokens = 18439, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12905 | prompt done, n_tokens = 18439, batch.n_tokens = 64
slot init_sampler: id  3 | task 12905 | init sampler, took 3.41 ms, tokens: text = 18439, total = 18439
slot update_slots: id  3 | task 12905 | erasing old context checkpoint (pos_min = 4724, pos_max = 5391, size = 15.664 MiB)
slot update_slots: id  3 | task 12905 | created context checkpoint 8 of 8 (pos_min = 17351, pos_max = 18374, size = 24.012 MiB)
slot print_timing: id  3 | task 12905 | 
prompt eval time =   12715.05 ms / 11742 tokens (    1.08 ms per token,   923.47 tokens per second)
       eval time =    2118.73 ms /    80 tokens (   26.48 ms per token,    37.76 tokens per second)
      total time =   14833.78 ms / 11822 tokens
slot      release: id  3 | task 12905 | stop processing: n_tokens = 18518, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12992 | processing task, is_child = 0
slot update_slots: id  3 | task 12992 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19151
slot update_slots: id  3 | task 12992 | n_tokens = 18439, memory_seq_rm [18439, end)
slot update_slots: id  3 | task 12992 | prompt processing progress, n_tokens = 19087, batch.n_tokens = 648, progress = 0.996658
slot update_slots: id  3 | task 12992 | n_tokens = 19087, memory_seq_rm [19087, end)
slot update_slots: id  3 | task 12992 | prompt processing progress, n_tokens = 19151, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12992 | prompt done, n_tokens = 19151, batch.n_tokens = 64
slot init_sampler: id  3 | task 12992 | init sampler, took 3.71 ms, tokens: text = 19151, total = 19151
slot update_slots: id  3 | task 12992 | erasing old context checkpoint (pos_min = 4848, pos_max = 5698, size = 19.955 MiB)
slot update_slots: id  3 | task 12992 | created context checkpoint 8 of 8 (pos_min = 18063, pos_max = 19086, size = 24.012 MiB)
slot print_timing: id  3 | task 12992 | 
prompt eval time =    1096.52 ms /   712 tokens (    1.54 ms per token,   649.32 tokens per second)
       eval time =   14636.69 ms /   565 tokens (   25.91 ms per token,    38.60 tokens per second)
      total time =   15733.22 ms /  1277 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 12992 | stop processing: n_tokens = 19715, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13559 | processing task, is_child = 0
slot update_slots: id  3 | task 13559 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19578
slot update_slots: id  3 | task 13559 | n_tokens = 19151, memory_seq_rm [19151, end)
slot update_slots: id  3 | task 13559 | prompt processing progress, n_tokens = 19514, batch.n_tokens = 363, progress = 0.996731
slot update_slots: id  3 | task 13559 | n_tokens = 19514, memory_seq_rm [19514, end)
slot update_slots: id  3 | task 13559 | prompt processing progress, n_tokens = 19578, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13559 | prompt done, n_tokens = 19578, batch.n_tokens = 64
slot init_sampler: id  3 | task 13559 | init sampler, took 3.98 ms, tokens: text = 19578, total = 19578
slot update_slots: id  3 | task 13559 | erasing old context checkpoint (pos_min = 4859, pos_max = 5808, size = 22.277 MiB)
slot update_slots: id  3 | task 13559 | created context checkpoint 8 of 8 (pos_min = 18697, pos_max = 19513, size = 19.158 MiB)
slot print_timing: id  3 | task 13559 | 
prompt eval time =     720.93 ms /   427 tokens (    1.69 ms per token,   592.29 tokens per second)
       eval time =   10549.17 ms /   410 tokens (   25.73 ms per token,    38.87 tokens per second)
      total time =   11270.10 ms /   837 tokens
slot      release: id  3 | task 13559 | stop processing: n_tokens = 19987, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13971 | processing task, is_child = 0
slot update_slots: id  3 | task 13971 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19993
slot update_slots: id  3 | task 13971 | n_tokens = 19578, memory_seq_rm [19578, end)
slot update_slots: id  3 | task 13971 | prompt processing progress, n_tokens = 19929, batch.n_tokens = 351, progress = 0.996799
slot update_slots: id  3 | task 13971 | n_tokens = 19929, memory_seq_rm [19929, end)
slot update_slots: id  3 | task 13971 | prompt processing progress, n_tokens = 19993, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13971 | prompt done, n_tokens = 19993, batch.n_tokens = 64
slot init_sampler: id  3 | task 13971 | init sampler, took 3.77 ms, tokens: text = 19993, total = 19993
slot update_slots: id  3 | task 13971 | erasing old context checkpoint (pos_min = 5049, pos_max = 5981, size = 21.878 MiB)
slot update_slots: id  3 | task 13971 | created context checkpoint 8 of 8 (pos_min = 19151, pos_max = 19928, size = 18.244 MiB)
slot print_timing: id  3 | task 13971 | 
prompt eval time =     687.87 ms /   415 tokens (    1.66 ms per token,   603.31 tokens per second)
       eval time =   10788.81 ms /   413 tokens (   26.12 ms per token,    38.28 tokens per second)
      total time =   11476.68 ms /   828 tokens
slot      release: id  3 | task 13971 | stop processing: n_tokens = 20405, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14386 | processing task, is_child = 0
slot update_slots: id  3 | task 14386 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20381
slot update_slots: id  3 | task 14386 | n_tokens = 19993, memory_seq_rm [19993, end)
slot update_slots: id  3 | task 14386 | prompt processing progress, n_tokens = 20317, batch.n_tokens = 324, progress = 0.996860
slot update_slots: id  3 | task 14386 | n_tokens = 20317, memory_seq_rm [20317, end)
slot update_slots: id  3 | task 14386 | prompt processing progress, n_tokens = 20381, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 14386 | prompt done, n_tokens = 20381, batch.n_tokens = 64
slot init_sampler: id  3 | task 14386 | init sampler, took 2.88 ms, tokens: text = 20381, total = 20381
slot update_slots: id  3 | task 14386 | erasing old context checkpoint (pos_min = 5283, pos_max = 6229, size = 22.206 MiB)
slot update_slots: id  3 | task 14386 | created context checkpoint 8 of 8 (pos_min = 19475, pos_max = 20316, size = 19.744 MiB)
slot print_timing: id  3 | task 14386 | 
prompt eval time =     648.69 ms /   388 tokens (    1.67 ms per token,   598.12 tokens per second)
       eval time =    1123.78 ms /    43 tokens (   26.13 ms per token,    38.26 tokens per second)
      total time =    1772.47 ms /   431 tokens
slot      release: id  3 | task 14386 | stop processing: n_tokens = 20423, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14431 | processing task, is_child = 0
slot update_slots: id  3 | task 14431 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20489
slot update_slots: id  3 | task 14431 | n_tokens = 20381, memory_seq_rm [20381, end)
slot update_slots: id  3 | task 14431 | prompt processing progress, n_tokens = 20425, batch.n_tokens = 44, progress = 0.996876
slot update_slots: id  3 | task 14431 | n_tokens = 20425, memory_seq_rm [20425, end)
slot update_slots: id  3 | task 14431 | prompt processing progress, n_tokens = 20489, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 14431 | prompt done, n_tokens = 20489, batch.n_tokens = 64
slot init_sampler: id  3 | task 14431 | init sampler, took 3.90 ms, tokens: text = 20489, total = 20489
slot update_slots: id  3 | task 14431 | erasing old context checkpoint (pos_min = 5341, pos_max = 6337, size = 23.379 MiB)
slot update_slots: id  3 | task 14431 | created context checkpoint 8 of 8 (pos_min = 19583, pos_max = 20424, size = 19.744 MiB)
slot print_timing: id  3 | task 14431 | 
prompt eval time =     450.85 ms /   108 tokens (    4.17 ms per token,   239.55 tokens per second)
       eval time =    4390.95 ms /   166 tokens (   26.45 ms per token,    37.81 tokens per second)
      total time =    4841.80 ms /   274 tokens
slot      release: id  3 | task 14431 | stop processing: n_tokens = 20654, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 14599 | processing task, is_child = 0
slot update_slots: id  2 | task 14599 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 301477
srv    send_error: task id = 14599, error: request (301477 tokens) exceeds the available context size (64000 tokens), try increasing it
slot      release: id  2 | task 14599 | stop processing: n_tokens = 0, truncated = 0
srv  update_slots: no tokens to decode
srv  update_slots: all slots are idle
srv          stop: cancel task, id_task = 14599
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 400
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.605 (> 0.100 thold), f_keep = 0.207
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 20654, total state size = 504.059 MiB
srv          load:  - looking for better prompt, base f_keep = 0.207, sim = 0.605
srv        update:  - cache state: 3 prompts, 2232.512 MiB (limits: 8192.000 MiB, 64000 tokens, 264028 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv  get_availabl: prompt cache update took 523.99 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14602 | processing task, is_child = 0
slot update_slots: id  3 | task 14602 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7053
slot update_slots: id  3 | task 14602 | n_past = 4270, slot.prompt.tokens.size() = 20654, seq_id = 3, pos_min = 19812, n_swa = 128
slot update_slots: id  3 | task 14602 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 5553, pos_max = 6435, n_swa = 128, size = 20.706 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 5636, pos_max = 6632, n_swa = 128, size = 23.379 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 17351, pos_max = 18374, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 18063, pos_max = 19086, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 18697, pos_max = 19513, n_swa = 128, size = 19.158 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 19151, pos_max = 19928, n_swa = 128, size = 18.244 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 19475, pos_max = 20316, n_swa = 128, size = 19.744 MiB)
slot update_slots: id  3 | task 14602 | erased invalidated context checkpoint (pos_min = 19583, pos_max = 20424, n_swa = 128, size = 19.744 MiB)
slot update_slots: id  3 | task 14602 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 14602 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.290373
slot update_slots: id  3 | task 14602 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 14602 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.580746
slot update_slots: id  3 | task 14602 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 14602 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.871119
slot update_slots: id  3 | task 14602 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  3 | task 14602 | prompt processing progress, n_tokens = 6989, batch.n_tokens = 845, progress = 0.990926
slot update_slots: id  3 | task 14602 | n_tokens = 6989, memory_seq_rm [6989, end)
slot update_slots: id  3 | task 14602 | prompt processing progress, n_tokens = 7053, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 14602 | prompt done, n_tokens = 7053, batch.n_tokens = 64
slot init_sampler: id  3 | task 14602 | init sampler, took 1.27 ms, tokens: text = 7053, total = 7053
slot update_slots: id  3 | task 14602 | created context checkpoint 1 of 8 (pos_min = 5965, pos_max = 6988, size = 24.012 MiB)
slot print_timing: id  3 | task 14602 | 
prompt eval time =    6849.42 ms /  7053 tokens (    0.97 ms per token,  1029.72 tokens per second)
       eval time =    1632.94 ms /    66 tokens (   24.74 ms per token,    40.42 tokens per second)
      total time =    8482.36 ms /  7119 tokens
slot      release: id  3 | task 14602 | stop processing: n_tokens = 7118, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14673 | processing task, is_child = 0
slot update_slots: id  3 | task 14673 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7107
slot update_slots: id  3 | task 14673 | n_tokens = 7053, memory_seq_rm [7053, end)
slot update_slots: id  3 | task 14673 | prompt processing progress, n_tokens = 7107, batch.n_tokens = 54, progress = 1.000000
slot update_slots: id  3 | task 14673 | prompt done, n_tokens = 7107, batch.n_tokens = 54
slot init_sampler: id  3 | task 14673 | init sampler, took 2.27 ms, tokens: text = 7107, total = 7107
slot print_timing: id  3 | task 14673 | 
prompt eval time =     153.37 ms /    54 tokens (    2.84 ms per token,   352.09 tokens per second)
       eval time =    2550.48 ms /   106 tokens (   24.06 ms per token,    41.56 tokens per second)
      total time =    2703.85 ms /   160 tokens
slot      release: id  3 | task 14673 | stop processing: n_tokens = 7212, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14780 | processing task, is_child = 0
slot update_slots: id  3 | task 14780 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7202
slot update_slots: id  3 | task 14780 | n_tokens = 7040, memory_seq_rm [7040, end)
slot update_slots: id  3 | task 14780 | prompt processing progress, n_tokens = 7138, batch.n_tokens = 98, progress = 0.991114
slot update_slots: id  3 | task 14780 | n_tokens = 7138, memory_seq_rm [7138, end)
slot update_slots: id  3 | task 14780 | prompt processing progress, n_tokens = 7202, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 14780 | prompt done, n_tokens = 7202, batch.n_tokens = 64
slot init_sampler: id  3 | task 14780 | init sampler, took 1.35 ms, tokens: text = 7202, total = 7202
slot update_slots: id  3 | task 14780 | created context checkpoint 2 of 8 (pos_min = 6242, pos_max = 7137, size = 21.011 MiB)
slot print_timing: id  3 | task 14780 | 
prompt eval time =     528.07 ms /   162 tokens (    3.26 ms per token,   306.78 tokens per second)
       eval time =   11865.44 ms /   511 tokens (   23.22 ms per token,    43.07 tokens per second)
      total time =   12393.51 ms /   673 tokens
slot      release: id  3 | task 14780 | stop processing: n_tokens = 7712, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.955 (> 0.100 thold), f_keep = 0.934
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15293 | processing task, is_child = 0
slot update_slots: id  3 | task 15293 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7544
slot update_slots: id  3 | task 15293 | n_tokens = 7202, memory_seq_rm [7202, end)
slot update_slots: id  3 | task 15293 | prompt processing progress, n_tokens = 7480, batch.n_tokens = 278, progress = 0.991516
slot update_slots: id  3 | task 15293 | n_tokens = 7480, memory_seq_rm [7480, end)
slot update_slots: id  3 | task 15293 | prompt processing progress, n_tokens = 7544, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15293 | prompt done, n_tokens = 7544, batch.n_tokens = 64
slot init_sampler: id  3 | task 15293 | init sampler, took 1.61 ms, tokens: text = 7544, total = 7544
slot update_slots: id  3 | task 15293 | created context checkpoint 3 of 8 (pos_min = 6816, pos_max = 7479, size = 15.570 MiB)
slot print_timing: id  3 | task 15293 | 
prompt eval time =     504.55 ms /   342 tokens (    1.48 ms per token,   677.83 tokens per second)
       eval time =    1200.82 ms /    49 tokens (   24.51 ms per token,    40.81 tokens per second)
      total time =    1705.37 ms /   391 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 15293 | stop processing: n_tokens = 7592, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.914 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15344 | processing task, is_child = 0
slot update_slots: id  3 | task 15344 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8256
slot update_slots: id  3 | task 15344 | n_tokens = 7544, memory_seq_rm [7544, end)
slot update_slots: id  3 | task 15344 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 648, progress = 0.992248
slot update_slots: id  3 | task 15344 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  3 | task 15344 | prompt processing progress, n_tokens = 8256, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15344 | prompt done, n_tokens = 8256, batch.n_tokens = 64
slot init_sampler: id  3 | task 15344 | init sampler, took 1.72 ms, tokens: text = 8256, total = 8256
slot update_slots: id  3 | task 15344 | created context checkpoint 4 of 8 (pos_min = 7168, pos_max = 8191, size = 24.012 MiB)
slot print_timing: id  3 | task 15344 | 
prompt eval time =     933.05 ms /   712 tokens (    1.31 ms per token,   763.09 tokens per second)
       eval time =    8465.07 ms /   360 tokens (   23.51 ms per token,    42.53 tokens per second)
      total time =    9398.12 ms /  1072 tokens
slot      release: id  3 | task 15344 | stop processing: n_tokens = 8615, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15706 | processing task, is_child = 0
slot update_slots: id  3 | task 15706 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8402
slot update_slots: id  3 | task 15706 | n_tokens = 8256, memory_seq_rm [8256, end)
slot update_slots: id  3 | task 15706 | prompt processing progress, n_tokens = 8338, batch.n_tokens = 82, progress = 0.992383
slot update_slots: id  3 | task 15706 | n_tokens = 8338, memory_seq_rm [8338, end)
slot update_slots: id  3 | task 15706 | prompt processing progress, n_tokens = 8402, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15706 | prompt done, n_tokens = 8402, batch.n_tokens = 64
slot init_sampler: id  3 | task 15706 | init sampler, took 1.24 ms, tokens: text = 8402, total = 8402
slot update_slots: id  3 | task 15706 | created context checkpoint 5 of 8 (pos_min = 7591, pos_max = 8337, size = 17.517 MiB)
slot print_timing: id  3 | task 15706 | 
prompt eval time =     380.28 ms /   146 tokens (    2.60 ms per token,   383.92 tokens per second)
       eval time =     844.91 ms /    36 tokens (   23.47 ms per token,    42.61 tokens per second)
      total time =    1225.20 ms /   182 tokens
slot      release: id  3 | task 15706 | stop processing: n_tokens = 8437, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15744 | processing task, is_child = 0
slot update_slots: id  3 | task 15744 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8525
slot update_slots: id  3 | task 15744 | n_tokens = 8402, memory_seq_rm [8402, end)
slot update_slots: id  3 | task 15744 | prompt processing progress, n_tokens = 8461, batch.n_tokens = 59, progress = 0.992493
slot update_slots: id  3 | task 15744 | n_tokens = 8461, memory_seq_rm [8461, end)
slot update_slots: id  3 | task 15744 | prompt processing progress, n_tokens = 8525, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15744 | prompt done, n_tokens = 8525, batch.n_tokens = 64
slot init_sampler: id  3 | task 15744 | init sampler, took 1.60 ms, tokens: text = 8525, total = 8525
slot update_slots: id  3 | task 15744 | created context checkpoint 6 of 8 (pos_min = 7591, pos_max = 8460, size = 20.401 MiB)
slot print_timing: id  3 | task 15744 | 
prompt eval time =     406.20 ms /   123 tokens (    3.30 ms per token,   302.81 tokens per second)
       eval time =    1283.50 ms /    52 tokens (   24.68 ms per token,    40.51 tokens per second)
      total time =    1689.70 ms /   175 tokens
slot      release: id  3 | task 15744 | stop processing: n_tokens = 8576, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15798 | processing task, is_child = 0
slot update_slots: id  3 | task 15798 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8584
slot update_slots: id  3 | task 15798 | n_tokens = 8525, memory_seq_rm [8525, end)
slot update_slots: id  3 | task 15798 | prompt processing progress, n_tokens = 8584, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 15798 | prompt done, n_tokens = 8584, batch.n_tokens = 59
slot init_sampler: id  3 | task 15798 | init sampler, took 2.04 ms, tokens: text = 8584, total = 8584
slot print_timing: id  3 | task 15798 | 
prompt eval time =     154.33 ms /    59 tokens (    2.62 ms per token,   382.29 tokens per second)
       eval time =    2014.37 ms /    85 tokens (   23.70 ms per token,    42.20 tokens per second)
      total time =    2168.70 ms /   144 tokens
slot      release: id  3 | task 15798 | stop processing: n_tokens = 8668, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15884 | processing task, is_child = 0
slot update_slots: id  3 | task 15884 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8677
slot update_slots: id  3 | task 15884 | n_tokens = 8584, memory_seq_rm [8584, end)
slot update_slots: id  3 | task 15884 | prompt processing progress, n_tokens = 8613, batch.n_tokens = 29, progress = 0.992624
slot update_slots: id  3 | task 15884 | n_tokens = 8613, memory_seq_rm [8613, end)
slot update_slots: id  3 | task 15884 | prompt processing progress, n_tokens = 8677, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15884 | prompt done, n_tokens = 8677, batch.n_tokens = 64
slot init_sampler: id  3 | task 15884 | init sampler, took 1.46 ms, tokens: text = 8677, total = 8677
slot update_slots: id  3 | task 15884 | created context checkpoint 7 of 8 (pos_min = 7644, pos_max = 8612, size = 22.722 MiB)
slot print_timing: id  3 | task 15884 | 
prompt eval time =     279.41 ms /    93 tokens (    3.00 ms per token,   332.84 tokens per second)
       eval time =    4968.99 ms /   210 tokens (   23.66 ms per token,    42.26 tokens per second)
      total time =    5248.40 ms /   303 tokens
slot      release: id  3 | task 15884 | stop processing: n_tokens = 8886, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 16096 | processing task, is_child = 0
slot update_slots: id  3 | task 16096 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8772
slot update_slots: id  3 | task 16096 | n_tokens = 8677, memory_seq_rm [8677, end)
slot update_slots: id  3 | task 16096 | prompt processing progress, n_tokens = 8708, batch.n_tokens = 31, progress = 0.992704
slot update_slots: id  3 | task 16096 | n_tokens = 8708, memory_seq_rm [8708, end)
slot update_slots: id  3 | task 16096 | prompt processing progress, n_tokens = 8772, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 16096 | prompt done, n_tokens = 8772, batch.n_tokens = 64
slot init_sampler: id  3 | task 16096 | init sampler, took 1.29 ms, tokens: text = 8772, total = 8772
slot update_slots: id  3 | task 16096 | created context checkpoint 8 of 8 (pos_min = 7862, pos_max = 8707, size = 19.838 MiB)
slot print_timing: id  3 | task 16096 | 
prompt eval time =     289.70 ms /    95 tokens (    3.05 ms per token,   327.92 tokens per second)
       eval time =    2429.25 ms /    99 tokens (   24.54 ms per token,    40.75 tokens per second)
      total time =    2718.96 ms /   194 tokens
slot      release: id  3 | task 16096 | stop processing: n_tokens = 8870, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 16197 | processing task, is_child = 0
slot update_slots: id  3 | task 16197 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8866
slot update_slots: id  3 | task 16197 | n_tokens = 8772, memory_seq_rm [8772, end)
slot update_slots: id  3 | task 16197 | prompt processing progress, n_tokens = 8802, batch.n_tokens = 30, progress = 0.992781
slot update_slots: id  3 | task 16197 | n_tokens = 8802, memory_seq_rm [8802, end)
slot update_slots: id  3 | task 16197 | prompt processing progress, n_tokens = 8866, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 16197 | prompt done, n_tokens = 8866, batch.n_tokens = 64
slot init_sampler: id  3 | task 16197 | init sampler, took 1.75 ms, tokens: text = 8866, total = 8866
slot update_slots: id  3 | task 16197 | erasing old context checkpoint (pos_min = 5965, pos_max = 6988, size = 24.012 MiB)
slot update_slots: id  3 | task 16197 | created context checkpoint 8 of 8 (pos_min = 7862, pos_max = 8801, size = 22.042 MiB)
slot print_timing: id  3 | task 16197 | 
prompt eval time =     398.01 ms /    94 tokens (    4.23 ms per token,   236.18 tokens per second)
       eval time =    5654.35 ms /   237 tokens (   23.86 ms per token,    41.91 tokens per second)
      total time =    6052.36 ms /   331 tokens
slot      release: id  3 | task 16197 | stop processing: n_tokens = 9102, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 16436 | processing task, is_child = 0
slot update_slots: id  3 | task 16436 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9066
slot update_slots: id  3 | task 16436 | n_tokens = 8866, memory_seq_rm [8866, end)
slot update_slots: id  3 | task 16436 | prompt processing progress, n_tokens = 9002, batch.n_tokens = 136, progress = 0.992941
slot update_slots: id  3 | task 16436 | n_tokens = 9002, memory_seq_rm [9002, end)
slot update_slots: id  3 | task 16436 | prompt processing progress, n_tokens = 9066, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 16436 | prompt done, n_tokens = 9066, batch.n_tokens = 64
slot init_sampler: id  3 | task 16436 | init sampler, took 1.30 ms, tokens: text = 9066, total = 9066
slot update_slots: id  3 | task 16436 | erasing old context checkpoint (pos_min = 6242, pos_max = 7137, size = 21.011 MiB)
slot update_slots: id  3 | task 16436 | created context checkpoint 8 of 8 (pos_min = 8200, pos_max = 9001, size = 18.806 MiB)
slot print_timing: id  3 | task 16436 | 
prompt eval time =     386.36 ms /   200 tokens (    1.93 ms per token,   517.66 tokens per second)
       eval time =    5476.19 ms /   230 tokens (   23.81 ms per token,    42.00 tokens per second)
      total time =    5862.55 ms /   430 tokens
slot      release: id  3 | task 16436 | stop processing: n_tokens = 9295, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 16668 | processing task, is_child = 0
slot update_slots: id  3 | task 16668 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9229
slot update_slots: id  3 | task 16668 | n_tokens = 9066, memory_seq_rm [9066, end)
slot update_slots: id  3 | task 16668 | prompt processing progress, n_tokens = 9165, batch.n_tokens = 99, progress = 0.993065
slot update_slots: id  3 | task 16668 | n_tokens = 9165, memory_seq_rm [9165, end)
slot update_slots: id  3 | task 16668 | prompt processing progress, n_tokens = 9229, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 16668 | prompt done, n_tokens = 9229, batch.n_tokens = 64
slot init_sampler: id  3 | task 16668 | init sampler, took 1.66 ms, tokens: text = 9229, total = 9229
slot update_slots: id  3 | task 16668 | erasing old context checkpoint (pos_min = 6816, pos_max = 7479, size = 15.570 MiB)
slot update_slots: id  3 | task 16668 | created context checkpoint 8 of 8 (pos_min = 8493, pos_max = 9164, size = 15.758 MiB)
slot print_timing: id  3 | task 16668 | 
prompt eval time =     518.04 ms /   163 tokens (    3.18 ms per token,   314.64 tokens per second)
       eval time =   14779.58 ms /   614 tokens (   24.07 ms per token,    41.54 tokens per second)
      total time =   15297.63 ms /   777 tokens
slot      release: id  3 | task 16668 | stop processing: n_tokens = 9842, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.938
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17284 | processing task, is_child = 0
slot update_slots: id  3 | task 17284 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9564
slot update_slots: id  3 | task 17284 | n_tokens = 9229, memory_seq_rm [9229, end)
slot update_slots: id  3 | task 17284 | prompt processing progress, n_tokens = 9500, batch.n_tokens = 271, progress = 0.993308
slot update_slots: id  3 | task 17284 | n_tokens = 9500, memory_seq_rm [9500, end)
slot update_slots: id  3 | task 17284 | prompt processing progress, n_tokens = 9564, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17284 | prompt done, n_tokens = 9564, batch.n_tokens = 64
slot init_sampler: id  3 | task 17284 | init sampler, took 1.93 ms, tokens: text = 9564, total = 9564
slot update_slots: id  3 | task 17284 | erasing old context checkpoint (pos_min = 7168, pos_max = 8191, size = 24.012 MiB)
slot update_slots: id  3 | task 17284 | created context checkpoint 8 of 8 (pos_min = 8866, pos_max = 9499, size = 14.867 MiB)
slot print_timing: id  3 | task 17284 | 
prompt eval time =     524.57 ms /   335 tokens (    1.57 ms per token,   638.61 tokens per second)
       eval time =     563.19 ms /    22 tokens (   25.60 ms per token,    39.06 tokens per second)
      total time =    1087.76 ms /   357 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 17284 | stop processing: n_tokens = 9585, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17308 | processing task, is_child = 0
slot update_slots: id  3 | task 17308 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9622
slot update_slots: id  3 | task 17308 | n_tokens = 9564, memory_seq_rm [9564, end)
slot update_slots: id  3 | task 17308 | prompt processing progress, n_tokens = 9622, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 17308 | prompt done, n_tokens = 9622, batch.n_tokens = 58
slot init_sampler: id  3 | task 17308 | init sampler, took 1.37 ms, tokens: text = 9622, total = 9622
slot print_timing: id  3 | task 17308 | 
prompt eval time =     274.52 ms /    58 tokens (    4.73 ms per token,   211.28 tokens per second)
       eval time =   12959.54 ms /   525 tokens (   24.68 ms per token,    40.51 tokens per second)
      total time =   13234.06 ms /   583 tokens
slot      release: id  3 | task 17308 | stop processing: n_tokens = 10146, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.948
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17834 | processing task, is_child = 0
slot update_slots: id  3 | task 17834 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10131
slot update_slots: id  3 | task 17834 | n_tokens = 9622, memory_seq_rm [9622, end)
slot update_slots: id  3 | task 17834 | prompt processing progress, n_tokens = 10067, batch.n_tokens = 445, progress = 0.993683
slot update_slots: id  3 | task 17834 | n_tokens = 10067, memory_seq_rm [10067, end)
slot update_slots: id  3 | task 17834 | prompt processing progress, n_tokens = 10131, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17834 | prompt done, n_tokens = 10131, batch.n_tokens = 64
slot init_sampler: id  3 | task 17834 | init sampler, took 1.96 ms, tokens: text = 10131, total = 10131
slot update_slots: id  3 | task 17834 | erasing old context checkpoint (pos_min = 7591, pos_max = 8337, size = 17.517 MiB)
slot update_slots: id  3 | task 17834 | created context checkpoint 8 of 8 (pos_min = 9311, pos_max = 10066, size = 17.728 MiB)
slot print_timing: id  3 | task 17834 | 
prompt eval time =     656.34 ms /   509 tokens (    1.29 ms per token,   775.51 tokens per second)
       eval time =    1780.00 ms /    73 tokens (   24.38 ms per token,    41.01 tokens per second)
      total time =    2436.34 ms /   582 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 17834 | stop processing: n_tokens = 10203, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.061
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 10203, total state size = 256.978 MiB
srv          load:  - looking for better prompt, base f_keep = 0.061, sim = 0.958
srv        update:  - cache state: 4 prompts, 2641.652 MiB (limits: 8192.000 MiB, 64000 tokens, 254776 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv  get_availabl: prompt cache update took 408.90 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17909 | processing task, is_child = 0
slot update_slots: id  3 | task 17909 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 648
slot update_slots: id  3 | task 17909 | n_past = 621, slot.prompt.tokens.size() = 10203, seq_id = 3, pos_min = 9447, n_swa = 128
slot update_slots: id  3 | task 17909 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 7591, pos_max = 8460, n_swa = 128, size = 20.401 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 7644, pos_max = 8612, n_swa = 128, size = 22.722 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 7862, pos_max = 8707, n_swa = 128, size = 19.838 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 7862, pos_max = 8801, n_swa = 128, size = 22.042 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 8200, pos_max = 9001, n_swa = 128, size = 18.806 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 8493, pos_max = 9164, n_swa = 128, size = 15.758 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 8866, pos_max = 9499, n_swa = 128, size = 14.867 MiB)
slot update_slots: id  3 | task 17909 | erased invalidated context checkpoint (pos_min = 9311, pos_max = 10066, n_swa = 128, size = 17.728 MiB)
slot update_slots: id  3 | task 17909 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 17909 | prompt processing progress, n_tokens = 584, batch.n_tokens = 584, progress = 0.901235
slot update_slots: id  3 | task 17909 | n_tokens = 584, memory_seq_rm [584, end)
slot update_slots: id  3 | task 17909 | prompt processing progress, n_tokens = 648, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17909 | prompt done, n_tokens = 648, batch.n_tokens = 64
slot init_sampler: id  3 | task 17909 | init sampler, took 0.11 ms, tokens: text = 648, total = 648
slot update_slots: id  3 | task 17909 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 583, size = 13.694 MiB)
slot print_timing: id  3 | task 17909 | 
prompt eval time =     793.32 ms /   648 tokens (    1.22 ms per token,   816.82 tokens per second)
       eval time =    2554.02 ms /   108 tokens (   23.65 ms per token,    42.29 tokens per second)
      total time =    3347.33 ms /   756 tokens
slot      release: id  3 | task 17909 | stop processing: n_tokens = 755, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.893 (> 0.100 thold), f_keep = 0.858
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18019 | processing task, is_child = 0
slot update_slots: id  3 | task 18019 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 726
slot update_slots: id  3 | task 18019 | n_tokens = 648, memory_seq_rm [648, end)
slot update_slots: id  3 | task 18019 | prompt processing progress, n_tokens = 662, batch.n_tokens = 14, progress = 0.911846
slot update_slots: id  3 | task 18019 | n_tokens = 662, memory_seq_rm [662, end)
slot update_slots: id  3 | task 18019 | prompt processing progress, n_tokens = 726, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18019 | prompt done, n_tokens = 726, batch.n_tokens = 64
slot init_sampler: id  3 | task 18019 | init sampler, took 0.14 ms, tokens: text = 726, total = 726
slot update_slots: id  3 | task 18019 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 661, size = 15.523 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18022 | processing task, is_child = 0
slot update_slots: id  1 | task 18022 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10249
slot update_slots: id  1 | task 18022 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 2047, batch.n_tokens = 2048, progress = 0.199727
slot update_slots: id  1 | task 18022 | n_tokens = 2047, memory_seq_rm [2047, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 4094, batch.n_tokens = 2048, progress = 0.399454
slot update_slots: id  1 | task 18022 | n_tokens = 4094, memory_seq_rm [4094, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 6141, batch.n_tokens = 2048, progress = 0.599180
slot update_slots: id  1 | task 18022 | n_tokens = 6141, memory_seq_rm [6141, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 8188, batch.n_tokens = 2048, progress = 0.798907
slot update_slots: id  1 | task 18022 | n_tokens = 8188, memory_seq_rm [8188, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 10185, batch.n_tokens = 1998, progress = 0.993755
slot update_slots: id  1 | task 18022 | n_tokens = 10185, memory_seq_rm [10185, end)
slot update_slots: id  1 | task 18022 | prompt processing progress, n_tokens = 10249, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 18022 | prompt done, n_tokens = 10249, batch.n_tokens = 65
slot init_sampler: id  1 | task 18022 | init sampler, took 1.82 ms, tokens: text = 10249, total = 10249
slot update_slots: id  1 | task 18022 | created context checkpoint 1 of 8 (pos_min = 9288, pos_max = 10184, size = 21.034 MiB)
slot print_timing: id  1 | task 18022 | 
prompt eval time =   10320.58 ms / 10249 tokens (    1.01 ms per token,   993.06 tokens per second)
       eval time =    5097.21 ms /   116 tokens (   43.94 ms per token,    22.76 tokens per second)
      total time =   15417.79 ms / 10365 tokens
slot      release: id  1 | task 18022 | stop processing: n_tokens = 10364, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 18019 | 
prompt eval time =     226.30 ms /    78 tokens (    2.90 ms per token,   344.68 tokens per second)
       eval time =   15768.90 ms /   136 tokens (  115.95 ms per token,     8.62 tokens per second)
      total time =   15995.20 ms /   214 tokens
slot      release: id  3 | task 18019 | stop processing: n_tokens = 861, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.902 (> 0.100 thold), f_keep = 0.843
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18158 | processing task, is_child = 0
slot update_slots: id  3 | task 18158 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 805
slot update_slots: id  3 | task 18158 | n_past = 726, slot.prompt.tokens.size() = 861, seq_id = 3, pos_min = 604, n_swa = 128
slot update_slots: id  3 | task 18158 | restored context checkpoint (pos_min = 0, pos_max = 661, size = 15.523 MiB)
slot update_slots: id  3 | task 18158 | n_tokens = 661, memory_seq_rm [661, end)
slot update_slots: id  3 | task 18158 | prompt processing progress, n_tokens = 741, batch.n_tokens = 80, progress = 0.920497
slot update_slots: id  3 | task 18158 | n_tokens = 741, memory_seq_rm [741, end)
slot update_slots: id  3 | task 18158 | prompt processing progress, n_tokens = 805, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18158 | prompt done, n_tokens = 805, batch.n_tokens = 64
slot init_sampler: id  3 | task 18158 | init sampler, took 0.15 ms, tokens: text = 805, total = 805
slot update_slots: id  3 | task 18158 | created context checkpoint 3 of 8 (pos_min = 0, pos_max = 740, size = 17.376 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18192 | processing task, is_child = 0
slot update_slots: id  1 | task 18192 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10747
slot update_slots: id  1 | task 18192 | n_past = 10249, slot.prompt.tokens.size() = 10364, seq_id = 1, pos_min = 10237, n_swa = 128
slot update_slots: id  1 | task 18192 | restored context checkpoint (pos_min = 9288, pos_max = 10184, size = 21.034 MiB)
slot update_slots: id  1 | task 18192 | n_tokens = 10184, memory_seq_rm [10184, end)
slot update_slots: id  1 | task 18192 | prompt processing progress, n_tokens = 10683, batch.n_tokens = 500, progress = 0.994045
slot update_slots: id  1 | task 18192 | n_tokens = 10683, memory_seq_rm [10683, end)
slot update_slots: id  1 | task 18192 | prompt processing progress, n_tokens = 10747, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 18192 | prompt done, n_tokens = 10747, batch.n_tokens = 65
slot init_sampler: id  1 | task 18192 | init sampler, took 5.34 ms, tokens: text = 10747, total = 10747
slot update_slots: id  1 | task 18192 | created context checkpoint 2 of 8 (pos_min = 9787, pos_max = 10682, size = 21.011 MiB)
slot print_timing: id  3 | task 18158 | 
prompt eval time =     608.77 ms /   144 tokens (    4.23 ms per token,   236.54 tokens per second)
       eval time =    5063.66 ms /   107 tokens (   47.32 ms per token,    21.13 tokens per second)
      total time =    5672.43 ms /   251 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 18158 | stop processing: n_tokens = 911, truncated = 0
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.927 (> 0.100 thold), f_keep = 0.884
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18279 | processing task, is_child = 0
slot update_slots: id  3 | task 18279 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 868
slot update_slots: id  3 | task 18279 | n_past = 805, slot.prompt.tokens.size() = 911, seq_id = 3, pos_min = 709, n_swa = 128
slot update_slots: id  3 | task 18279 | restored context checkpoint (pos_min = 0, pos_max = 740, size = 17.376 MiB)
slot update_slots: id  3 | task 18279 | n_tokens = 740, memory_seq_rm [740, end)
slot update_slots: id  3 | task 18279 | prompt processing progress, n_tokens = 804, batch.n_tokens = 65, progress = 0.926267
slot update_slots: id  3 | task 18279 | n_tokens = 804, memory_seq_rm [804, end)
slot update_slots: id  3 | task 18279 | prompt processing progress, n_tokens = 868, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 18279 | prompt done, n_tokens = 868, batch.n_tokens = 65
slot init_sampler: id  3 | task 18279 | init sampler, took 0.17 ms, tokens: text = 868, total = 868
slot print_timing: id  1 | task 18192 | 
prompt eval time =     981.99 ms /   563 tokens (    1.74 ms per token,   573.33 tokens per second)
       eval time =    6146.05 ms /   129 tokens (   47.64 ms per token,    20.99 tokens per second)
      total time =    7128.04 ms /   692 tokens
slot      release: id  1 | task 18192 | stop processing: n_tokens = 10875, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 18279 | 
prompt eval time =     624.85 ms /   128 tokens (    4.88 ms per token,   204.85 tokens per second)
       eval time =    3812.42 ms /   113 tokens (   33.74 ms per token,    29.64 tokens per second)
      total time =    4437.26 ms /   241 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 18279 | stop processing: n_tokens = 980, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.886
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18394 | processing task, is_child = 0
slot update_slots: id  3 | task 18394 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 931
slot update_slots: id  3 | task 18394 | n_tokens = 868, memory_seq_rm [868, end)
slot update_slots: id  3 | task 18394 | prompt processing progress, n_tokens = 931, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 18394 | prompt done, n_tokens = 931, batch.n_tokens = 63
slot init_sampler: id  3 | task 18394 | init sampler, took 0.20 ms, tokens: text = 931, total = 931
slot update_slots: id  3 | task 18394 | created context checkpoint 4 of 8 (pos_min = 127, pos_max = 867, size = 17.376 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18400 | processing task, is_child = 0
slot update_slots: id  1 | task 18400 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10888
slot update_slots: id  1 | task 18400 | n_past = 10747, slot.prompt.tokens.size() = 10875, seq_id = 1, pos_min = 10704, n_swa = 128
slot update_slots: id  1 | task 18400 | restored context checkpoint (pos_min = 9787, pos_max = 10682, size = 21.011 MiB)
slot update_slots: id  1 | task 18400 | n_tokens = 10682, memory_seq_rm [10682, end)
slot update_slots: id  1 | task 18400 | prompt processing progress, n_tokens = 10824, batch.n_tokens = 143, progress = 0.994122
slot update_slots: id  1 | task 18400 | n_tokens = 10824, memory_seq_rm [10824, end)
slot update_slots: id  1 | task 18400 | prompt processing progress, n_tokens = 10888, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 18400 | prompt done, n_tokens = 10888, batch.n_tokens = 65
slot init_sampler: id  1 | task 18400 | init sampler, took 2.00 ms, tokens: text = 10888, total = 10888
slot update_slots: id  1 | task 18400 | created context checkpoint 3 of 8 (pos_min = 9929, pos_max = 10823, size = 20.987 MiB)
slot print_timing: id  1 | task 18400 | 
prompt eval time =     656.09 ms /   206 tokens (    3.18 ms per token,   313.98 tokens per second)
       eval time =    2027.09 ms /    45 tokens (   45.05 ms per token,    22.20 tokens per second)
      total time =    2683.18 ms /   251 tokens
slot      release: id  1 | task 18400 | stop processing: n_tokens = 10932, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 18394 | 
prompt eval time =     206.84 ms /    63 tokens (    3.28 ms per token,   304.59 tokens per second)
       eval time =    7321.87 ms /   229 tokens (   31.97 ms per token,    31.28 tokens per second)
      total time =    7528.70 ms /   292 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 18394 | stop processing: n_tokens = 1159, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.937 (> 0.100 thold), f_keep = 0.803
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18625 | processing task, is_child = 0
slot update_slots: id  3 | task 18625 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 994
slot update_slots: id  3 | task 18625 | n_past = 931, slot.prompt.tokens.size() = 1159, seq_id = 3, pos_min = 888, n_swa = 128
slot update_slots: id  3 | task 18625 | restored context checkpoint (pos_min = 127, pos_max = 867, size = 17.376 MiB)
slot update_slots: id  3 | task 18625 | n_tokens = 867, memory_seq_rm [867, end)
slot update_slots: id  3 | task 18625 | prompt processing progress, n_tokens = 930, batch.n_tokens = 63, progress = 0.935614
slot update_slots: id  3 | task 18625 | n_tokens = 930, memory_seq_rm [930, end)
slot update_slots: id  3 | task 18625 | prompt processing progress, n_tokens = 994, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18625 | prompt done, n_tokens = 994, batch.n_tokens = 64
slot init_sampler: id  3 | task 18625 | init sampler, took 0.18 ms, tokens: text = 994, total = 994
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18708 | processing task, is_child = 0
slot update_slots: id  1 | task 18708 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11021
slot update_slots: id  1 | task 18708 | n_past = 10888, slot.prompt.tokens.size() = 10932, seq_id = 1, pos_min = 10805, n_swa = 128
slot update_slots: id  1 | task 18708 | restored context checkpoint (pos_min = 9929, pos_max = 10823, size = 20.987 MiB)
slot update_slots: id  1 | task 18708 | n_tokens = 10823, memory_seq_rm [10823, end)
slot update_slots: id  1 | task 18708 | prompt processing progress, n_tokens = 10957, batch.n_tokens = 135, progress = 0.994193
slot update_slots: id  1 | task 18708 | n_tokens = 10957, memory_seq_rm [10957, end)
slot update_slots: id  1 | task 18708 | prompt processing progress, n_tokens = 11021, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 18708 | prompt done, n_tokens = 11021, batch.n_tokens = 65
slot init_sampler: id  1 | task 18708 | init sampler, took 2.31 ms, tokens: text = 11021, total = 11021
slot update_slots: id  1 | task 18708 | created context checkpoint 4 of 8 (pos_min = 10063, pos_max = 10956, size = 20.964 MiB)
slot print_timing: id  1 | task 18708 | 
prompt eval time =     485.29 ms /   198 tokens (    2.45 ms per token,   408.00 tokens per second)
       eval time =    1900.94 ms /    40 tokens (   47.52 ms per token,    21.04 tokens per second)
      total time =    2386.23 ms /   238 tokens
slot      release: id  1 | task 18708 | stop processing: n_tokens = 11060, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 18625 | 
prompt eval time =     699.37 ms /   127 tokens (    5.51 ms per token,   181.59 tokens per second)
       eval time =    5460.35 ms /   154 tokens (   35.46 ms per token,    28.20 tokens per second)
      total time =    6159.72 ms /   281 tokens
slot      release: id  3 | task 18625 | stop processing: n_tokens = 1147, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.940 (> 0.100 thold), f_keep = 0.867
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18782 | processing task, is_child = 0
slot update_slots: id  3 | task 18782 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1057
slot update_slots: id  3 | task 18782 | n_past = 994, slot.prompt.tokens.size() = 1147, seq_id = 3, pos_min = 984, n_swa = 128
slot update_slots: id  3 | task 18782 | restored context checkpoint (pos_min = 127, pos_max = 867, size = 17.376 MiB)
slot update_slots: id  3 | task 18782 | n_tokens = 867, memory_seq_rm [867, end)
slot update_slots: id  3 | task 18782 | prompt processing progress, n_tokens = 993, batch.n_tokens = 126, progress = 0.939451
slot update_slots: id  3 | task 18782 | n_tokens = 993, memory_seq_rm [993, end)
slot update_slots: id  3 | task 18782 | prompt processing progress, n_tokens = 1057, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18782 | prompt done, n_tokens = 1057, batch.n_tokens = 64
slot init_sampler: id  3 | task 18782 | init sampler, took 0.19 ms, tokens: text = 1057, total = 1057
slot update_slots: id  3 | task 18782 | created context checkpoint 5 of 8 (pos_min = 127, pos_max = 992, size = 20.307 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.755 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18824 | processing task, is_child = 0
slot update_slots: id  1 | task 18824 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14604
slot update_slots: id  1 | task 18824 | n_past = 11021, slot.prompt.tokens.size() = 11060, seq_id = 1, pos_min = 10933, n_swa = 128
slot update_slots: id  1 | task 18824 | restored context checkpoint (pos_min = 10063, pos_max = 10956, size = 20.964 MiB)
slot update_slots: id  1 | task 18824 | n_tokens = 10956, memory_seq_rm [10956, end)
slot update_slots: id  1 | task 18824 | prompt processing progress, n_tokens = 13003, batch.n_tokens = 2048, progress = 0.890373
slot update_slots: id  1 | task 18824 | n_tokens = 13003, memory_seq_rm [13003, end)
slot update_slots: id  1 | task 18824 | prompt processing progress, n_tokens = 14540, batch.n_tokens = 1538, progress = 0.995618
slot update_slots: id  1 | task 18824 | n_tokens = 14540, memory_seq_rm [14540, end)
slot update_slots: id  1 | task 18824 | prompt processing progress, n_tokens = 14604, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 18824 | prompt done, n_tokens = 14604, batch.n_tokens = 65
slot init_sampler: id  1 | task 18824 | init sampler, took 2.76 ms, tokens: text = 14604, total = 14604
slot update_slots: id  1 | task 18824 | created context checkpoint 5 of 8 (pos_min = 13643, pos_max = 14539, size = 21.034 MiB)
slot print_timing: id  3 | task 18782 | 
prompt eval time =     748.34 ms /   190 tokens (    3.94 ms per token,   253.90 tokens per second)
       eval time =   13876.85 ms /   223 tokens (   62.23 ms per token,    16.07 tokens per second)
      total time =   14625.19 ms /   413 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 18782 | stop processing: n_tokens = 1279, truncated = 0
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.826
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19026 | processing task, is_child = 0
slot update_slots: id  3 | task 19026 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1132
slot update_slots: id  3 | task 19026 | n_past = 1057, slot.prompt.tokens.size() = 1279, seq_id = 3, pos_min = 1011, n_swa = 128
slot update_slots: id  3 | task 19026 | restored context checkpoint (pos_min = 127, pos_max = 992, size = 20.307 MiB)
slot update_slots: id  3 | task 19026 | n_tokens = 992, memory_seq_rm [992, end)
slot update_slots: id  3 | task 19026 | prompt processing progress, n_tokens = 1068, batch.n_tokens = 77, progress = 0.943463
slot update_slots: id  3 | task 19026 | n_tokens = 1068, memory_seq_rm [1068, end)
slot update_slots: id  3 | task 19026 | prompt processing progress, n_tokens = 1132, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 19026 | prompt done, n_tokens = 1132, batch.n_tokens = 65
slot init_sampler: id  3 | task 19026 | init sampler, took 0.21 ms, tokens: text = 1132, total = 1132
slot update_slots: id  3 | task 19026 | created context checkpoint 6 of 8 (pos_min = 172, pos_max = 1067, size = 21.011 MiB)
slot print_timing: id  3 | task 19026 | 
prompt eval time =     736.34 ms /   140 tokens (    5.26 ms per token,   190.13 tokens per second)
       eval time =   12634.90 ms /   278 tokens (   45.45 ms per token,    22.00 tokens per second)
      total time =   13371.24 ms /   418 tokens
slot      release: id  3 | task 19026 | stop processing: n_tokens = 1409, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.803
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19316 | processing task, is_child = 0
slot update_slots: id  3 | task 19316 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1205
slot update_slots: id  3 | task 19316 | n_tokens = 1132, memory_seq_rm [1132, end)
slot update_slots: id  3 | task 19316 | prompt processing progress, n_tokens = 1141, batch.n_tokens = 10, progress = 0.946888
slot update_slots: id  3 | task 19316 | n_tokens = 1141, memory_seq_rm [1141, end)
slot update_slots: id  3 | task 19316 | prompt processing progress, n_tokens = 1205, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 19316 | prompt done, n_tokens = 1205, batch.n_tokens = 65
slot init_sampler: id  3 | task 19316 | init sampler, took 0.23 ms, tokens: text = 1205, total = 1205
slot update_slots: id  3 | task 19316 | created context checkpoint 7 of 8 (pos_min = 674, pos_max = 1140, size = 10.951 MiB)
slot print_timing: id  1 | task 18824 | 
prompt eval time =    4425.31 ms /  3648 tokens (    1.21 ms per token,   824.35 tokens per second)
       eval time =   28424.30 ms /   609 tokens (   46.67 ms per token,    21.43 tokens per second)
      total time =   32849.61 ms /  4257 tokens
slot      release: id  1 | task 18824 | stop processing: n_tokens = 15212, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.960
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 19531 | processing task, is_child = 0
slot update_slots: id  1 | task 19531 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14925
slot update_slots: id  1 | task 19531 | n_past = 14604, slot.prompt.tokens.size() = 15212, seq_id = 1, pos_min = 15008, n_swa = 128
slot update_slots: id  1 | task 19531 | restored context checkpoint (pos_min = 13643, pos_max = 14539, size = 21.034 MiB)
slot update_slots: id  1 | task 19531 | n_tokens = 14539, memory_seq_rm [14539, end)
slot update_slots: id  1 | task 19531 | prompt processing progress, n_tokens = 14861, batch.n_tokens = 323, progress = 0.995712
slot update_slots: id  1 | task 19531 | n_tokens = 14861, memory_seq_rm [14861, end)
slot update_slots: id  1 | task 19531 | prompt processing progress, n_tokens = 14925, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 19531 | prompt done, n_tokens = 14925, batch.n_tokens = 65
slot init_sampler: id  1 | task 19531 | init sampler, took 4.43 ms, tokens: text = 14925, total = 14925
slot update_slots: id  1 | task 19531 | created context checkpoint 6 of 8 (pos_min = 13965, pos_max = 14860, size = 21.011 MiB)
slot print_timing: id  3 | task 19316 | 
prompt eval time =     309.32 ms /    73 tokens (    4.24 ms per token,   236.00 tokens per second)
       eval time =    9472.05 ms /   230 tokens (   41.18 ms per token,    24.28 tokens per second)
      total time =    9781.37 ms /   303 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 19316 | stop processing: n_tokens = 1434, truncated = 0
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.840
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19560 | processing task, is_child = 0
slot update_slots: id  3 | task 19560 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1268
slot update_slots: id  3 | task 19560 | n_past = 1205, slot.prompt.tokens.size() = 1434, seq_id = 3, pos_min = 1291, n_swa = 128
slot update_slots: id  3 | task 19560 | restored context checkpoint (pos_min = 674, pos_max = 1140, size = 10.951 MiB)
slot update_slots: id  3 | task 19560 | n_tokens = 1140, memory_seq_rm [1140, end)
slot update_slots: id  3 | task 19560 | prompt processing progress, n_tokens = 1204, batch.n_tokens = 65, progress = 0.949527
slot update_slots: id  3 | task 19560 | n_tokens = 1204, memory_seq_rm [1204, end)
slot update_slots: id  3 | task 19560 | prompt processing progress, n_tokens = 1268, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 19560 | prompt done, n_tokens = 1268, batch.n_tokens = 65
slot init_sampler: id  3 | task 19560 | init sampler, took 0.22 ms, tokens: text = 1268, total = 1268
slot print_timing: id  1 | task 19531 | 
prompt eval time =     844.10 ms /   386 tokens (    2.19 ms per token,   457.29 tokens per second)
       eval time =    6991.28 ms /   149 tokens (   46.92 ms per token,    21.31 tokens per second)
      total time =    7835.38 ms /   535 tokens
slot      release: id  1 | task 19531 | stop processing: n_tokens = 15073, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 19560 | 
prompt eval time =     528.65 ms /   128 tokens (    4.13 ms per token,   242.13 tokens per second)
       eval time =    6683.89 ms /   161 tokens (   41.51 ms per token,    24.09 tokens per second)
      total time =    7212.54 ms /   289 tokens
slot      release: id  3 | task 19560 | stop processing: n_tokens = 1428, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.946 (> 0.100 thold), f_keep = 0.888
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19723 | processing task, is_child = 0
slot update_slots: id  3 | task 19723 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1341
slot update_slots: id  3 | task 19723 | n_tokens = 1268, memory_seq_rm [1268, end)
slot update_slots: id  3 | task 19723 | prompt processing progress, n_tokens = 1277, batch.n_tokens = 9, progress = 0.952274
slot update_slots: id  3 | task 19723 | n_tokens = 1277, memory_seq_rm [1277, end)
slot update_slots: id  3 | task 19723 | prompt processing progress, n_tokens = 1341, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 19723 | prompt done, n_tokens = 1341, batch.n_tokens = 64
slot init_sampler: id  3 | task 19723 | init sampler, took 0.26 ms, tokens: text = 1341, total = 1341
slot update_slots: id  3 | task 19723 | created context checkpoint 8 of 8 (pos_min = 1061, pos_max = 1276, size = 5.065 MiB)
slot print_timing: id  3 | task 19723 | 
prompt eval time =     284.93 ms /    73 tokens (    3.90 ms per token,   256.20 tokens per second)
       eval time =    4240.36 ms /   145 tokens (   29.24 ms per token,    34.20 tokens per second)
      total time =    4525.29 ms /   218 tokens
slot      release: id  3 | task 19723 | stop processing: n_tokens = 1485, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.948 (> 0.100 thold), f_keep = 0.903
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19870 | processing task, is_child = 0
slot update_slots: id  3 | task 19870 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1414
slot update_slots: id  3 | task 19870 | n_past = 1341, slot.prompt.tokens.size() = 1485, seq_id = 3, pos_min = 1236, n_swa = 128
slot update_slots: id  3 | task 19870 | restored context checkpoint (pos_min = 1061, pos_max = 1276, size = 5.065 MiB)
slot update_slots: id  3 | task 19870 | n_tokens = 1276, memory_seq_rm [1276, end)
slot update_slots: id  3 | task 19870 | prompt processing progress, n_tokens = 1350, batch.n_tokens = 74, progress = 0.954738
slot update_slots: id  3 | task 19870 | n_tokens = 1350, memory_seq_rm [1350, end)
slot update_slots: id  3 | task 19870 | prompt processing progress, n_tokens = 1414, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 19870 | prompt done, n_tokens = 1414, batch.n_tokens = 64
slot init_sampler: id  3 | task 19870 | init sampler, took 1.30 ms, tokens: text = 1414, total = 1414
slot update_slots: id  3 | task 19870 | erasing old context checkpoint (pos_min = 0, pos_max = 583, size = 13.694 MiB)
slot update_slots: id  3 | task 19870 | created context checkpoint 8 of 8 (pos_min = 1134, pos_max = 1349, size = 5.065 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 20075 | processing task, is_child = 0
slot update_slots: id  1 | task 20075 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14995
slot update_slots: id  1 | task 20075 | n_past = 14925, slot.prompt.tokens.size() = 15073, seq_id = 1, pos_min = 14937, n_swa = 128
slot update_slots: id  1 | task 20075 | restored context checkpoint (pos_min = 13965, pos_max = 14860, size = 21.011 MiB)
slot update_slots: id  1 | task 20075 | n_tokens = 14860, memory_seq_rm [14860, end)
slot update_slots: id  1 | task 20075 | prompt processing progress, n_tokens = 14931, batch.n_tokens = 72, progress = 0.995732
slot update_slots: id  1 | task 20075 | n_tokens = 14931, memory_seq_rm [14931, end)
slot update_slots: id  1 | task 20075 | prompt processing progress, n_tokens = 14995, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 20075 | prompt done, n_tokens = 14995, batch.n_tokens = 65
slot init_sampler: id  1 | task 20075 | init sampler, took 2.68 ms, tokens: text = 14995, total = 14995
slot update_slots: id  1 | task 20075 | created context checkpoint 7 of 8 (pos_min = 14036, pos_max = 14930, size = 20.987 MiB)
slot print_timing: id  3 | task 19870 | 
prompt eval time =     473.80 ms /   138 tokens (    3.43 ms per token,   291.26 tokens per second)
       eval time =    6371.15 ms /   214 tokens (   29.77 ms per token,    33.59 tokens per second)
      total time =    6844.95 ms /   352 tokens
slot      release: id  3 | task 19870 | stop processing: n_tokens = 1627, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.946 (> 0.100 thold), f_keep = 0.869
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 20100 | processing task, is_child = 0
slot update_slots: id  3 | task 20100 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1494
slot update_slots: id  3 | task 20100 | n_past = 1414, slot.prompt.tokens.size() = 1627, seq_id = 3, pos_min = 1489, n_swa = 128
slot update_slots: id  3 | task 20100 | restored context checkpoint (pos_min = 1134, pos_max = 1349, size = 5.065 MiB)
slot update_slots: id  3 | task 20100 | n_tokens = 1349, memory_seq_rm [1349, end)
slot update_slots: id  3 | task 20100 | prompt processing progress, n_tokens = 1430, batch.n_tokens = 82, progress = 0.957162
slot update_slots: id  3 | task 20100 | n_tokens = 1430, memory_seq_rm [1430, end)
slot update_slots: id  3 | task 20100 | prompt processing progress, n_tokens = 1494, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 20100 | prompt done, n_tokens = 1494, batch.n_tokens = 65
slot init_sampler: id  3 | task 20100 | init sampler, took 0.29 ms, tokens: text = 1494, total = 1494
slot update_slots: id  3 | task 20100 | erasing old context checkpoint (pos_min = 0, pos_max = 661, size = 15.523 MiB)
slot update_slots: id  3 | task 20100 | created context checkpoint 8 of 8 (pos_min = 1215, pos_max = 1429, size = 5.042 MiB)
slot print_timing: id  1 | task 20075 | 
prompt eval time =     610.47 ms /   135 tokens (    4.52 ms per token,   221.14 tokens per second)
       eval time =    3300.26 ms /    70 tokens (   47.15 ms per token,    21.21 tokens per second)
      total time =    3910.72 ms /   205 tokens
slot      release: id  1 | task 20075 | stop processing: n_tokens = 15064, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 20100 | 
prompt eval time =     473.70 ms /   145 tokens (    3.27 ms per token,   306.10 tokens per second)
       eval time =    7489.21 ms /   247 tokens (   30.32 ms per token,    32.98 tokens per second)
      total time =    7962.91 ms /   392 tokens
slot      release: id  3 | task 20100 | stop processing: n_tokens = 1740, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.859
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 20349 | processing task, is_child = 0
slot update_slots: id  3 | task 20349 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1567
slot update_slots: id  3 | task 20349 | n_tokens = 1494, memory_seq_rm [1494, end)
slot update_slots: id  3 | task 20349 | prompt processing progress, n_tokens = 1503, batch.n_tokens = 9, progress = 0.959158
slot update_slots: id  3 | task 20349 | n_tokens = 1503, memory_seq_rm [1503, end)
slot update_slots: id  3 | task 20349 | prompt processing progress, n_tokens = 1567, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 20349 | prompt done, n_tokens = 1567, batch.n_tokens = 64
slot init_sampler: id  3 | task 20349 | init sampler, took 0.26 ms, tokens: text = 1567, total = 1567
slot update_slots: id  3 | task 20349 | erasing old context checkpoint (pos_min = 0, pos_max = 740, size = 17.376 MiB)
slot update_slots: id  3 | task 20349 | created context checkpoint 8 of 8 (pos_min = 1334, pos_max = 1502, size = 3.963 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 20481 | processing task, is_child = 0
slot update_slots: id  1 | task 20481 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15128
slot update_slots: id  1 | task 20481 | n_tokens = 14995, memory_seq_rm [14995, end)
slot update_slots: id  1 | task 20481 | prompt processing progress, n_tokens = 15064, batch.n_tokens = 70, progress = 0.995769
slot update_slots: id  1 | task 20481 | n_tokens = 15064, memory_seq_rm [15064, end)
slot update_slots: id  1 | task 20481 | prompt processing progress, n_tokens = 15128, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  1 | task 20481 | prompt done, n_tokens = 15128, batch.n_tokens = 65
slot init_sampler: id  1 | task 20481 | init sampler, took 2.57 ms, tokens: text = 15128, total = 15128
slot update_slots: id  1 | task 20481 | created context checkpoint 8 of 8 (pos_min = 14859, pos_max = 15063, size = 4.807 MiB)
slot print_timing: id  3 | task 20349 | 
prompt eval time =     265.05 ms /    73 tokens (    3.63 ms per token,   275.42 tokens per second)
       eval time =    6333.96 ms /   185 tokens (   34.24 ms per token,    29.21 tokens per second)
      total time =    6599.00 ms /   258 tokens
slot      release: id  3 | task 20349 | stop processing: n_tokens = 1751, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.895
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 20553 | processing task, is_child = 0
slot update_slots: id  3 | task 20553 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1647
slot update_slots: id  3 | task 20553 | n_past = 1567, slot.prompt.tokens.size() = 1751, seq_id = 3, pos_min = 1570, n_swa = 128
slot update_slots: id  3 | task 20553 | restored context checkpoint (pos_min = 1334, pos_max = 1502, size = 3.963 MiB)
slot update_slots: id  3 | task 20553 | n_tokens = 1502, memory_seq_rm [1502, end)
slot update_slots: id  3 | task 20553 | prompt processing progress, n_tokens = 1583, batch.n_tokens = 82, progress = 0.961141
slot update_slots: id  3 | task 20553 | n_tokens = 1583, memory_seq_rm [1583, end)
slot update_slots: id  3 | task 20553 | prompt processing progress, n_tokens = 1647, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 20553 | prompt done, n_tokens = 1647, batch.n_tokens = 65
slot init_sampler: id  3 | task 20553 | init sampler, took 0.31 ms, tokens: text = 1647, total = 1647
slot update_slots: id  3 | task 20553 | erasing old context checkpoint (pos_min = 127, pos_max = 867, size = 17.376 MiB)
slot update_slots: id  3 | task 20553 | created context checkpoint 8 of 8 (pos_min = 1375, pos_max = 1582, size = 4.878 MiB)
slot print_timing: id  1 | task 20481 | 
prompt eval time =     461.02 ms /   133 tokens (    3.47 ms per token,   288.49 tokens per second)
       eval time =   12035.69 ms /   259 tokens (   46.47 ms per token,    21.52 tokens per second)
      total time =   12496.71 ms /   392 tokens
slot      release: id  1 | task 20481 | stop processing: n_tokens = 15386, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 20553 | 
prompt eval time =     503.11 ms /   145 tokens (    3.47 ms per token,   288.21 tokens per second)
       eval time =   26520.87 ms /   876 tokens (   30.27 ms per token,    33.03 tokens per second)
      total time =   27023.98 ms /  1021 tokens
slot      release: id  3 | task 20553 | stop processing: n_tokens = 2522, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.653
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21431 | processing task, is_child = 0
slot update_slots: id  3 | task 21431 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1716
slot update_slots: id  3 | task 21431 | n_past = 1647, slot.prompt.tokens.size() = 2522, seq_id = 3, pos_min = 1666, n_swa = 128
slot update_slots: id  3 | task 21431 | restored context checkpoint (pos_min = 1375, pos_max = 1582, size = 4.878 MiB)
slot update_slots: id  3 | task 21431 | n_tokens = 1582, memory_seq_rm [1582, end)
slot update_slots: id  3 | task 21431 | prompt processing progress, n_tokens = 1652, batch.n_tokens = 70, progress = 0.962704
slot update_slots: id  3 | task 21431 | n_tokens = 1652, memory_seq_rm [1652, end)
slot update_slots: id  3 | task 21431 | prompt processing progress, n_tokens = 1716, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21431 | prompt done, n_tokens = 1716, batch.n_tokens = 64
slot init_sampler: id  3 | task 21431 | init sampler, took 0.30 ms, tokens: text = 1716, total = 1716
slot update_slots: id  3 | task 21431 | erasing old context checkpoint (pos_min = 127, pos_max = 992, size = 20.307 MiB)
slot update_slots: id  3 | task 21431 | created context checkpoint 8 of 8 (pos_min = 1375, pos_max = 1651, size = 6.496 MiB)
slot print_timing: id  3 | task 21431 | 
prompt eval time =     417.03 ms /   134 tokens (    3.11 ms per token,   321.32 tokens per second)
       eval time =    6090.25 ms /   238 tokens (   25.59 ms per token,    39.08 tokens per second)
      total time =    6507.28 ms /   372 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 21431 | stop processing: n_tokens = 1953, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.955 (> 0.100 thold), f_keep = 0.879
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21671 | processing task, is_child = 0
slot update_slots: id  3 | task 21671 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1796
slot update_slots: id  3 | task 21671 | n_tokens = 1716, memory_seq_rm [1716, end)
slot update_slots: id  3 | task 21671 | prompt processing progress, n_tokens = 1732, batch.n_tokens = 16, progress = 0.964365
slot update_slots: id  3 | task 21671 | n_tokens = 1732, memory_seq_rm [1732, end)
slot update_slots: id  3 | task 21671 | prompt processing progress, n_tokens = 1796, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21671 | prompt done, n_tokens = 1796, batch.n_tokens = 64
slot init_sampler: id  3 | task 21671 | init sampler, took 0.33 ms, tokens: text = 1796, total = 1796
slot update_slots: id  3 | task 21671 | erasing old context checkpoint (pos_min = 172, pos_max = 1067, size = 21.011 MiB)
slot update_slots: id  3 | task 21671 | created context checkpoint 8 of 8 (pos_min = 1375, pos_max = 1731, size = 8.372 MiB)
slot print_timing: id  3 | task 21671 | 
prompt eval time =     274.29 ms /    80 tokens (    3.43 ms per token,   291.67 tokens per second)
       eval time =    5057.12 ms /   196 tokens (   25.80 ms per token,    38.76 tokens per second)
      total time =    5331.40 ms /   276 tokens
slot      release: id  3 | task 21671 | stop processing: n_tokens = 1991, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.720 (> 0.100 thold), f_keep = 0.465
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 15386, total state size = 363.764 MiB
srv          load:  - looking for better prompt, base f_keep = 0.465, sim = 0.720
srv        update:  - cache state: 5 prompts, 3157.251 MiB (limits: 8192.000 MiB, 64000 tokens, 253091 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv  get_availabl: prompt cache update took 616.16 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 21869 | processing task, is_child = 0
slot update_slots: id  1 | task 21869 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9942
slot update_slots: id  1 | task 21869 | n_past = 7159, slot.prompt.tokens.size() = 15386, seq_id = 1, pos_min = 15259, n_swa = 128
slot update_slots: id  1 | task 21869 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 9288, pos_max = 10184, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 9787, pos_max = 10682, n_swa = 128, size = 21.011 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 9929, pos_max = 10823, n_swa = 128, size = 20.987 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 10063, pos_max = 10956, n_swa = 128, size = 20.964 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 13643, pos_max = 14539, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 13965, pos_max = 14860, n_swa = 128, size = 21.011 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 14036, pos_max = 14930, n_swa = 128, size = 20.987 MiB)
slot update_slots: id  1 | task 21869 | erased invalidated context checkpoint (pos_min = 14859, pos_max = 15063, n_swa = 128, size = 4.807 MiB)
slot update_slots: id  1 | task 21869 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.205995
slot update_slots: id  1 | task 21869 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.411990
slot update_slots: id  1 | task 21869 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.617984
slot update_slots: id  1 | task 21869 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.823979
slot update_slots: id  1 | task 21869 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 9878, batch.n_tokens = 1686, progress = 0.993563
slot update_slots: id  1 | task 21869 | n_tokens = 9878, memory_seq_rm [9878, end)
slot update_slots: id  1 | task 21869 | prompt processing progress, n_tokens = 9942, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 21869 | prompt done, n_tokens = 9942, batch.n_tokens = 64
slot init_sampler: id  1 | task 21869 | init sampler, took 1.48 ms, tokens: text = 9942, total = 9942
slot update_slots: id  1 | task 21869 | created context checkpoint 1 of 8 (pos_min = 8981, pos_max = 9877, size = 21.034 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.359 (> 0.100 thold), f_keep = 0.312
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1991, total state size = 49.665 MiB
srv          load:  - looking for better prompt, base f_keep = 0.312, sim = 0.359
srv        update:  - cache state: 6 prompts, 3256.747 MiB (limits: 8192.000 MiB, 64000 tokens, 250367 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x57387f4603d0:    1991 tokens, checkpoints:  8,    99.497 MiB
srv  get_availabl: prompt cache update took 213.85 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22581 | processing task, is_child = 0
slot update_slots: id  3 | task 22581 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1731
slot update_slots: id  3 | task 22581 | n_past = 621, slot.prompt.tokens.size() = 1991, seq_id = 3, pos_min = 1864, n_swa = 128
slot update_slots: id  3 | task 22581 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 674, pos_max = 1140, n_swa = 128, size = 10.951 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1061, pos_max = 1276, n_swa = 128, size = 5.065 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1134, pos_max = 1349, n_swa = 128, size = 5.065 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1215, pos_max = 1429, n_swa = 128, size = 5.042 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1334, pos_max = 1502, n_swa = 128, size = 3.963 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1582, n_swa = 128, size = 4.878 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1651, n_swa = 128, size = 6.496 MiB)
slot update_slots: id  3 | task 22581 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1731, n_swa = 128, size = 8.372 MiB)
slot update_slots: id  3 | task 22581 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 22581 | prompt processing progress, n_tokens = 1667, batch.n_tokens = 1668, progress = 0.963027
slot update_slots: id  3 | task 22581 | n_tokens = 1667, memory_seq_rm [1667, end)
slot update_slots: id  3 | task 22581 | prompt processing progress, n_tokens = 1731, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 22581 | prompt done, n_tokens = 1731, batch.n_tokens = 65
slot init_sampler: id  3 | task 22581 | init sampler, took 0.29 ms, tokens: text = 1731, total = 1731
slot update_slots: id  3 | task 22581 | created context checkpoint 1 of 8 (pos_min = 770, pos_max = 1666, size = 21.034 MiB)
slot print_timing: id  3 | task 22581 | 
prompt eval time =    2161.79 ms /  1731 tokens (    1.25 ms per token,   800.72 tokens per second)
       eval time =   33780.89 ms /   737 tokens (   45.84 ms per token,    21.82 tokens per second)
      total time =   35942.68 ms /  2468 tokens
slot      release: id  3 | task 22581 | stop processing: n_tokens = 2467, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  1 | task 21869 | 
prompt eval time =   11814.54 ms /  9942 tokens (    1.19 ms per token,   841.51 tokens per second)
       eval time =   55791.46 ms /  1495 tokens (   37.32 ms per token,    26.80 tokens per second)
      total time =   67606.00 ms / 11437 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 21869 | stop processing: n_tokens = 11436, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.894 (> 0.100 thold), f_keep = 0.864
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 23371 | processing task, is_child = 0
slot update_slots: id  1 | task 23371 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11050
slot update_slots: id  1 | task 23371 | n_past = 9884, slot.prompt.tokens.size() = 11436, seq_id = 1, pos_min = 10899, n_swa = 128
slot update_slots: id  1 | task 23371 | restored context checkpoint (pos_min = 8981, pos_max = 9877, size = 21.034 MiB)
slot update_slots: id  1 | task 23371 | n_tokens = 9877, memory_seq_rm [9877, end)
slot update_slots: id  1 | task 23371 | prompt processing progress, n_tokens = 10986, batch.n_tokens = 1109, progress = 0.994208
slot update_slots: id  1 | task 23371 | n_tokens = 10986, memory_seq_rm [10986, end)
slot update_slots: id  1 | task 23371 | prompt processing progress, n_tokens = 11050, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 23371 | prompt done, n_tokens = 11050, batch.n_tokens = 64
slot init_sampler: id  1 | task 23371 | init sampler, took 1.57 ms, tokens: text = 11050, total = 11050
slot update_slots: id  1 | task 23371 | created context checkpoint 2 of 8 (pos_min = 10089, pos_max = 10985, size = 21.034 MiB)
slot print_timing: id  1 | task 23371 | 
prompt eval time =    1799.35 ms /  1173 tokens (    1.53 ms per token,   651.90 tokens per second)
       eval time =   26223.42 ms /  1035 tokens (   25.34 ms per token,    39.47 tokens per second)
      total time =   28022.77 ms /  2208 tokens
slot      release: id  1 | task 23371 | stop processing: n_tokens = 12084, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.914 (> 0.100 thold), f_keep = 0.914
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24408 | processing task, is_child = 0
slot update_slots: id  1 | task 24408 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12091
slot update_slots: id  1 | task 24408 | n_past = 11050, slot.prompt.tokens.size() = 12084, seq_id = 1, pos_min = 11187, n_swa = 128
slot update_slots: id  1 | task 24408 | restored context checkpoint (pos_min = 10089, pos_max = 10985, size = 21.034 MiB)
slot update_slots: id  1 | task 24408 | n_tokens = 10985, memory_seq_rm [10985, end)
slot update_slots: id  1 | task 24408 | prompt processing progress, n_tokens = 12027, batch.n_tokens = 1042, progress = 0.994707
slot update_slots: id  1 | task 24408 | n_tokens = 12027, memory_seq_rm [12027, end)
slot update_slots: id  1 | task 24408 | prompt processing progress, n_tokens = 12091, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24408 | prompt done, n_tokens = 12091, batch.n_tokens = 64
slot init_sampler: id  1 | task 24408 | init sampler, took 1.75 ms, tokens: text = 12091, total = 12091
slot update_slots: id  1 | task 24408 | created context checkpoint 3 of 8 (pos_min = 11130, pos_max = 12026, size = 21.034 MiB)
slot print_timing: id  1 | task 24408 | 
prompt eval time =    1623.69 ms /  1106 tokens (    1.47 ms per token,   681.16 tokens per second)
       eval time =   27560.17 ms /  1087 tokens (   25.35 ms per token,    39.44 tokens per second)
      total time =   29183.86 ms /  2193 tokens
slot      release: id  1 | task 24408 | stop processing: n_tokens = 13177, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.917 (> 0.100 thold), f_keep = 0.918
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25497 | processing task, is_child = 0
slot update_slots: id  1 | task 25497 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13186
slot update_slots: id  1 | task 25497 | n_past = 12091, slot.prompt.tokens.size() = 13177, seq_id = 1, pos_min = 12280, n_swa = 128
slot update_slots: id  1 | task 25497 | restored context checkpoint (pos_min = 11130, pos_max = 12026, size = 21.034 MiB)
slot update_slots: id  1 | task 25497 | n_tokens = 12026, memory_seq_rm [12026, end)
slot update_slots: id  1 | task 25497 | prompt processing progress, n_tokens = 13122, batch.n_tokens = 1096, progress = 0.995146
slot update_slots: id  1 | task 25497 | n_tokens = 13122, memory_seq_rm [13122, end)
slot update_slots: id  1 | task 25497 | prompt processing progress, n_tokens = 13186, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25497 | prompt done, n_tokens = 13186, batch.n_tokens = 64
slot init_sampler: id  1 | task 25497 | init sampler, took 1.89 ms, tokens: text = 13186, total = 13186
slot update_slots: id  1 | task 25497 | created context checkpoint 4 of 8 (pos_min = 12225, pos_max = 13121, size = 21.034 MiB)
slot print_timing: id  1 | task 25497 | 
prompt eval time =    1742.84 ms /  1160 tokens (    1.50 ms per token,   665.58 tokens per second)
       eval time =     596.91 ms /    25 tokens (   23.88 ms per token,    41.88 tokens per second)
      total time =    2339.75 ms /  1185 tokens
slot      release: id  1 | task 25497 | stop processing: n_tokens = 13210, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.722 (> 0.100 thold), f_keep = 0.678
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25524 | processing task, is_child = 0
slot update_slots: id  3 | task 25524 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2318
slot update_slots: id  3 | task 25524 | n_past = 1673, slot.prompt.tokens.size() = 2467, seq_id = 3, pos_min = 2340, n_swa = 128
slot update_slots: id  3 | task 25524 | restored context checkpoint (pos_min = 770, pos_max = 1666, size = 21.034 MiB)
slot update_slots: id  3 | task 25524 | n_tokens = 1666, memory_seq_rm [1666, end)
slot update_slots: id  3 | task 25524 | prompt processing progress, n_tokens = 2254, batch.n_tokens = 588, progress = 0.972390
slot update_slots: id  3 | task 25524 | n_tokens = 2254, memory_seq_rm [2254, end)
slot update_slots: id  3 | task 25524 | prompt processing progress, n_tokens = 2318, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25524 | prompt done, n_tokens = 2318, batch.n_tokens = 64
slot init_sampler: id  3 | task 25524 | init sampler, took 0.40 ms, tokens: text = 2318, total = 2318
slot update_slots: id  3 | task 25524 | created context checkpoint 2 of 8 (pos_min = 1357, pos_max = 2253, size = 21.034 MiB)
slot print_timing: id  3 | task 25524 | 
prompt eval time =    1209.85 ms /   652 tokens (    1.86 ms per token,   538.91 tokens per second)
       eval time =    1757.46 ms /    72 tokens (   24.41 ms per token,    40.97 tokens per second)
      total time =    2967.31 ms /   724 tokens
slot      release: id  3 | task 25524 | stop processing: n_tokens = 2389, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25598 | processing task, is_child = 0
slot update_slots: id  3 | task 25598 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2386
slot update_slots: id  3 | task 25598 | n_tokens = 2318, memory_seq_rm [2318, end)
slot update_slots: id  3 | task 25598 | prompt processing progress, n_tokens = 2322, batch.n_tokens = 4, progress = 0.973177
slot update_slots: id  3 | task 25598 | n_tokens = 2322, memory_seq_rm [2322, end)
slot update_slots: id  3 | task 25598 | prompt processing progress, n_tokens = 2386, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25598 | prompt done, n_tokens = 2386, batch.n_tokens = 64
slot init_sampler: id  3 | task 25598 | init sampler, took 0.36 ms, tokens: text = 2386, total = 2386
slot update_slots: id  3 | task 25598 | created context checkpoint 3 of 8 (pos_min = 1492, pos_max = 2321, size = 19.463 MiB)
slot print_timing: id  3 | task 25598 | 
prompt eval time =     224.83 ms /    68 tokens (    3.31 ms per token,   302.46 tokens per second)
       eval time =   14847.60 ms /   586 tokens (   25.34 ms per token,    39.47 tokens per second)
      total time =   15072.43 ms /   654 tokens
slot      release: id  3 | task 25598 | stop processing: n_tokens = 2971, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.856 (> 0.100 thold), f_keep = 0.803
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26186 | processing task, is_child = 0
slot update_slots: id  3 | task 26186 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2789
slot update_slots: id  3 | task 26186 | n_tokens = 2386, memory_seq_rm [2386, end)
slot update_slots: id  3 | task 26186 | prompt processing progress, n_tokens = 2725, batch.n_tokens = 339, progress = 0.977053
slot update_slots: id  3 | task 26186 | n_tokens = 2725, memory_seq_rm [2725, end)
slot update_slots: id  3 | task 26186 | prompt processing progress, n_tokens = 2789, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 26186 | prompt done, n_tokens = 2789, batch.n_tokens = 64
slot init_sampler: id  3 | task 26186 | init sampler, took 0.45 ms, tokens: text = 2789, total = 2789
slot update_slots: id  3 | task 26186 | created context checkpoint 4 of 8 (pos_min = 2084, pos_max = 2724, size = 15.031 MiB)
slot print_timing: id  3 | task 26186 | 
prompt eval time =     671.99 ms /   403 tokens (    1.67 ms per token,   599.71 tokens per second)
       eval time =     847.57 ms /    34 tokens (   24.93 ms per token,    40.11 tokens per second)
      total time =    1519.56 ms /   437 tokens
slot      release: id  3 | task 26186 | stop processing: n_tokens = 2822, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.835 (> 0.100 thold), f_keep = 0.832
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26222 | processing task, is_child = 0
slot update_slots: id  1 | task 26222 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13169
slot update_slots: id  1 | task 26222 | n_past = 10992, slot.prompt.tokens.size() = 13210, seq_id = 1, pos_min = 13083, n_swa = 128
slot update_slots: id  1 | task 26222 | restored context checkpoint (pos_min = 10089, pos_max = 10985, size = 21.034 MiB)
slot update_slots: id  1 | task 26222 | erased invalidated context checkpoint (pos_min = 11130, pos_max = 12026, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 26222 | erased invalidated context checkpoint (pos_min = 12225, pos_max = 13121, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  1 | task 26222 | n_tokens = 10985, memory_seq_rm [10985, end)
slot update_slots: id  1 | task 26222 | prompt processing progress, n_tokens = 13033, batch.n_tokens = 2048, progress = 0.989673
slot update_slots: id  1 | task 26222 | n_tokens = 13033, memory_seq_rm [13033, end)
slot update_slots: id  1 | task 26222 | prompt processing progress, n_tokens = 13105, batch.n_tokens = 72, progress = 0.995140
slot update_slots: id  1 | task 26222 | n_tokens = 13105, memory_seq_rm [13105, end)
slot update_slots: id  1 | task 26222 | prompt processing progress, n_tokens = 13169, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26222 | prompt done, n_tokens = 13169, batch.n_tokens = 64
slot init_sampler: id  1 | task 26222 | init sampler, took 2.40 ms, tokens: text = 13169, total = 13169
slot update_slots: id  1 | task 26222 | created context checkpoint 3 of 8 (pos_min = 12208, pos_max = 13104, size = 21.034 MiB)
slot print_timing: id  1 | task 26222 | 
prompt eval time =    3015.41 ms /  2184 tokens (    1.38 ms per token,   724.28 tokens per second)
       eval time =   27556.51 ms /  1073 tokens (   25.68 ms per token,    38.94 tokens per second)
      total time =   30571.92 ms /  3257 tokens
slot      release: id  1 | task 26222 | stop processing: n_tokens = 14241, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.936 (> 0.100 thold), f_keep = 0.925
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27298 | processing task, is_child = 0
slot update_slots: id  1 | task 27298 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14072
slot update_slots: id  1 | task 27298 | n_past = 13169, slot.prompt.tokens.size() = 14241, seq_id = 1, pos_min = 13344, n_swa = 128
slot update_slots: id  1 | task 27298 | restored context checkpoint (pos_min = 12208, pos_max = 13104, size = 21.034 MiB)
slot update_slots: id  1 | task 27298 | n_tokens = 13104, memory_seq_rm [13104, end)
slot update_slots: id  1 | task 27298 | prompt processing progress, n_tokens = 14008, batch.n_tokens = 904, progress = 0.995452
slot update_slots: id  1 | task 27298 | n_tokens = 14008, memory_seq_rm [14008, end)
slot update_slots: id  1 | task 27298 | prompt processing progress, n_tokens = 14072, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27298 | prompt done, n_tokens = 14072, batch.n_tokens = 64
slot init_sampler: id  1 | task 27298 | init sampler, took 3.00 ms, tokens: text = 14072, total = 14072
slot update_slots: id  1 | task 27298 | created context checkpoint 4 of 8 (pos_min = 13111, pos_max = 14007, size = 21.034 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.817 (> 0.100 thold), f_keep = 0.794
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27841 | processing task, is_child = 0
slot update_slots: id  3 | task 27841 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2744
slot update_slots: id  3 | task 27841 | n_past = 2241, slot.prompt.tokens.size() = 2822, seq_id = 3, pos_min = 2695, n_swa = 128
slot update_slots: id  3 | task 27841 | restored context checkpoint (pos_min = 2084, pos_max = 2724, size = 15.031 MiB)
slot update_slots: id  3 | task 27841 | n_tokens = 2241, memory_seq_rm [2241, end)
slot update_slots: id  3 | task 27841 | prompt processing progress, n_tokens = 2680, batch.n_tokens = 440, progress = 0.976676
slot update_slots: id  3 | task 27841 | n_tokens = 2680, memory_seq_rm [2680, end)
slot update_slots: id  3 | task 27841 | prompt processing progress, n_tokens = 2744, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 27841 | prompt done, n_tokens = 2744, batch.n_tokens = 65
slot init_sampler: id  3 | task 27841 | init sampler, took 0.48 ms, tokens: text = 2744, total = 2744
slot print_timing: id  3 | task 27841 | 
prompt eval time =     754.32 ms /   503 tokens (    1.50 ms per token,   666.83 tokens per second)
       eval time =   16327.60 ms /   352 tokens (   46.39 ms per token,    21.56 tokens per second)
      total time =   17081.92 ms /   855 tokens
slot      release: id  3 | task 27841 | stop processing: n_tokens = 3095, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.887
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28210 | processing task, is_child = 0
slot update_slots: id  3 | task 28210 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2943
slot update_slots: id  3 | task 28210 | n_tokens = 2744, memory_seq_rm [2744, end)
slot update_slots: id  3 | task 28210 | prompt processing progress, n_tokens = 2879, batch.n_tokens = 136, progress = 0.978253
slot update_slots: id  3 | task 28210 | n_tokens = 2879, memory_seq_rm [2879, end)
slot update_slots: id  3 | task 28210 | prompt processing progress, n_tokens = 2943, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 28210 | prompt done, n_tokens = 2943, batch.n_tokens = 65
slot init_sampler: id  3 | task 28210 | init sampler, took 0.56 ms, tokens: text = 2943, total = 2943
slot update_slots: id  3 | task 28210 | created context checkpoint 5 of 8 (pos_min = 2565, pos_max = 2878, size = 7.363 MiB)
slot print_timing: id  1 | task 27298 | 
prompt eval time =    1557.30 ms /   968 tokens (    1.61 ms per token,   621.59 tokens per second)
       eval time =   31916.81 ms /   913 tokens (   34.96 ms per token,    28.61 tokens per second)
      total time =   33474.11 ms /  1881 tokens
slot      release: id  1 | task 27298 | stop processing: n_tokens = 14984, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  3 | task 28210 | 
prompt eval time =     505.71 ms /   199 tokens (    2.54 ms per token,   393.51 tokens per second)
       eval time =     772.98 ms /    30 tokens (   25.77 ms per token,    38.81 tokens per second)
      total time =    1278.69 ms /   229 tokens
slot      release: id  3 | task 28210 | stop processing: n_tokens = 2972, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.940 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28242 | processing task, is_child = 0
slot update_slots: id  1 | task 28242 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14975
slot update_slots: id  1 | task 28242 | n_past = 14072, slot.prompt.tokens.size() = 14984, seq_id = 1, pos_min = 14849, n_swa = 128
slot update_slots: id  1 | task 28242 | restored context checkpoint (pos_min = 13111, pos_max = 14007, size = 21.034 MiB)
slot update_slots: id  1 | task 28242 | n_tokens = 14007, memory_seq_rm [14007, end)
slot update_slots: id  1 | task 28242 | prompt processing progress, n_tokens = 14911, batch.n_tokens = 904, progress = 0.995726
slot update_slots: id  1 | task 28242 | n_tokens = 14911, memory_seq_rm [14911, end)
slot update_slots: id  1 | task 28242 | prompt processing progress, n_tokens = 14975, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28242 | prompt done, n_tokens = 14975, batch.n_tokens = 64
slot init_sampler: id  1 | task 28242 | init sampler, took 2.20 ms, tokens: text = 14975, total = 14975
slot update_slots: id  1 | task 28242 | created context checkpoint 5 of 8 (pos_min = 14141, pos_max = 14910, size = 18.056 MiB)
slot print_timing: id  1 | task 28242 | 
prompt eval time =    1438.60 ms /   968 tokens (    1.49 ms per token,   672.88 tokens per second)
       eval time =   23910.69 ms /   936 tokens (   25.55 ms per token,    39.15 tokens per second)
      total time =   25349.29 ms /  1904 tokens
slot      release: id  1 | task 28242 | stop processing: n_tokens = 15910, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.943 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29180 | processing task, is_child = 0
slot update_slots: id  1 | task 29180 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15887
slot update_slots: id  1 | task 29180 | n_past = 14975, slot.prompt.tokens.size() = 15910, seq_id = 1, pos_min = 15013, n_swa = 128
slot update_slots: id  1 | task 29180 | restored context checkpoint (pos_min = 14141, pos_max = 14910, size = 18.056 MiB)
slot update_slots: id  1 | task 29180 | n_tokens = 14910, memory_seq_rm [14910, end)
slot update_slots: id  1 | task 29180 | prompt processing progress, n_tokens = 15823, batch.n_tokens = 913, progress = 0.995972
slot update_slots: id  1 | task 29180 | n_tokens = 15823, memory_seq_rm [15823, end)
slot update_slots: id  1 | task 29180 | prompt processing progress, n_tokens = 15887, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29180 | prompt done, n_tokens = 15887, batch.n_tokens = 64
slot init_sampler: id  1 | task 29180 | init sampler, took 2.28 ms, tokens: text = 15887, total = 15887
slot update_slots: id  1 | task 29180 | created context checkpoint 6 of 8 (pos_min = 14926, pos_max = 15822, size = 21.034 MiB)
slot print_timing: id  1 | task 29180 | 
prompt eval time =    1448.98 ms /   977 tokens (    1.48 ms per token,   674.27 tokens per second)
       eval time =   25436.53 ms /   985 tokens (   25.82 ms per token,    38.72 tokens per second)
      total time =   26885.51 ms /  1962 tokens
slot      release: id  1 | task 29180 | stop processing: n_tokens = 16871, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.945 (> 0.100 thold), f_keep = 0.942
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30167 | processing task, is_child = 0
slot update_slots: id  1 | task 30167 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16804
slot update_slots: id  1 | task 30167 | n_past = 15887, slot.prompt.tokens.size() = 16871, seq_id = 1, pos_min = 15974, n_swa = 128
slot update_slots: id  1 | task 30167 | restored context checkpoint (pos_min = 14926, pos_max = 15822, size = 21.034 MiB)
slot update_slots: id  1 | task 30167 | n_tokens = 15822, memory_seq_rm [15822, end)
slot update_slots: id  1 | task 30167 | prompt processing progress, n_tokens = 16740, batch.n_tokens = 918, progress = 0.996191
slot update_slots: id  1 | task 30167 | n_tokens = 16740, memory_seq_rm [16740, end)
slot update_slots: id  1 | task 30167 | prompt processing progress, n_tokens = 16804, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30167 | prompt done, n_tokens = 16804, batch.n_tokens = 64
slot init_sampler: id  1 | task 30167 | init sampler, took 2.53 ms, tokens: text = 16804, total = 16804
slot update_slots: id  1 | task 30167 | created context checkpoint 7 of 8 (pos_min = 15843, pos_max = 16739, size = 21.034 MiB)
slot print_timing: id  1 | task 30167 | 
prompt eval time =    1578.34 ms /   982 tokens (    1.61 ms per token,   622.17 tokens per second)
       eval time =    1075.81 ms /    43 tokens (   25.02 ms per token,    39.97 tokens per second)
      total time =    2654.14 ms /  1025 tokens
slot      release: id  1 | task 30167 | stop processing: n_tokens = 16846, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.942 (> 0.100 thold), f_keep = 0.037
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 16846, total state size = 416.055 MiB
srv          load:  - looking for better prompt, base f_keep = 0.037, sim = 0.942
srv          load:  - found better prompt with f_keep = 0.313, sim = 0.945
srv        update:  - cache state: 6 prompts, 3717.566 MiB (limits: 8192.000 MiB, 64000 tokens, 252066 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv  get_availabl: prompt cache update took 1167.86 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30212 | processing task, is_child = 0
slot update_slots: id  1 | task 30212 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 659
slot update_slots: id  1 | task 30212 | n_past = 623, slot.prompt.tokens.size() = 1991, seq_id = 1, pos_min = 1864, n_swa = 128
slot update_slots: id  1 | task 30212 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 674, pos_max = 1140, n_swa = 128, size = 10.951 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1061, pos_max = 1276, n_swa = 128, size = 5.065 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1134, pos_max = 1349, n_swa = 128, size = 5.065 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1215, pos_max = 1429, n_swa = 128, size = 5.042 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1334, pos_max = 1502, n_swa = 128, size = 3.963 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1582, n_swa = 128, size = 4.878 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1651, n_swa = 128, size = 6.496 MiB)
slot update_slots: id  1 | task 30212 | erased invalidated context checkpoint (pos_min = 1375, pos_max = 1731, n_swa = 128, size = 8.372 MiB)
slot update_slots: id  1 | task 30212 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 30212 | prompt processing progress, n_tokens = 595, batch.n_tokens = 595, progress = 0.902883
slot update_slots: id  1 | task 30212 | n_tokens = 595, memory_seq_rm [595, end)
slot update_slots: id  1 | task 30212 | prompt processing progress, n_tokens = 659, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30212 | prompt done, n_tokens = 659, batch.n_tokens = 64
slot init_sampler: id  1 | task 30212 | init sampler, took 0.13 ms, tokens: text = 659, total = 659
slot update_slots: id  1 | task 30212 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 594, size = 13.952 MiB)
slot print_timing: id  1 | task 30212 | 
prompt eval time =    1075.84 ms /   659 tokens (    1.63 ms per token,   612.55 tokens per second)
       eval time =    1097.47 ms /    44 tokens (   24.94 ms per token,    40.09 tokens per second)
      total time =    2173.31 ms /   703 tokens
slot      release: id  1 | task 30212 | stop processing: n_tokens = 702, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.459 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30258 | processing task, is_child = 0
slot update_slots: id  1 | task 30258 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1437
slot update_slots: id  1 | task 30258 | n_tokens = 659, memory_seq_rm [659, end)
slot update_slots: id  1 | task 30258 | prompt processing progress, n_tokens = 1373, batch.n_tokens = 714, progress = 0.955463
slot update_slots: id  1 | task 30258 | n_tokens = 1373, memory_seq_rm [1373, end)
slot update_slots: id  1 | task 30258 | prompt processing progress, n_tokens = 1437, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30258 | prompt done, n_tokens = 1437, batch.n_tokens = 64
slot init_sampler: id  1 | task 30258 | init sampler, took 0.35 ms, tokens: text = 1437, total = 1437
slot update_slots: id  1 | task 30258 | created context checkpoint 2 of 8 (pos_min = 476, pos_max = 1372, size = 21.034 MiB)
slot print_timing: id  1 | task 30258 | 
prompt eval time =    1108.22 ms /   778 tokens (    1.42 ms per token,   702.03 tokens per second)
       eval time =    1363.49 ms /    53 tokens (   25.73 ms per token,    38.87 tokens per second)
      total time =    2471.71 ms /   831 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 30258 | stop processing: n_tokens = 1489, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 30313 | processing task, is_child = 0
slot update_slots: id  0 | task 30313 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 40397
slot update_slots: id  0 | task 30313 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.050697
slot update_slots: id  0 | task 30313 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.101394
slot update_slots: id  0 | task 30313 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.152091
slot update_slots: id  0 | task 30313 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.202787
slot update_slots: id  0 | task 30313 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.253484
slot update_slots: id  0 | task 30313 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.304181
slot update_slots: id  0 | task 30313 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.354878
slot update_slots: id  0 | task 30313 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.405575
slot update_slots: id  0 | task 30313 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.456271
slot update_slots: id  0 | task 30313 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 2048, progress = 0.506968
slot update_slots: id  0 | task 30313 | n_tokens = 20480, memory_seq_rm [20480, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 22528, batch.n_tokens = 2048, progress = 0.557665
slot update_slots: id  0 | task 30313 | n_tokens = 22528, memory_seq_rm [22528, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 24576, batch.n_tokens = 2048, progress = 0.608362
slot update_slots: id  0 | task 30313 | n_tokens = 24576, memory_seq_rm [24576, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 26624, batch.n_tokens = 2048, progress = 0.659059
slot update_slots: id  0 | task 30313 | n_tokens = 26624, memory_seq_rm [26624, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 28672, batch.n_tokens = 2048, progress = 0.709756
slot update_slots: id  0 | task 30313 | n_tokens = 28672, memory_seq_rm [28672, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 30720, batch.n_tokens = 2048, progress = 0.760453
slot update_slots: id  0 | task 30313 | n_tokens = 30720, memory_seq_rm [30720, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 32768, batch.n_tokens = 2048, progress = 0.811149
slot update_slots: id  0 | task 30313 | n_tokens = 32768, memory_seq_rm [32768, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 34816, batch.n_tokens = 2048, progress = 0.861846
slot update_slots: id  0 | task 30313 | n_tokens = 34816, memory_seq_rm [34816, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 36864, batch.n_tokens = 2048, progress = 0.912543
slot update_slots: id  0 | task 30313 | n_tokens = 36864, memory_seq_rm [36864, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 38912, batch.n_tokens = 2048, progress = 0.963240
slot update_slots: id  0 | task 30313 | n_tokens = 38912, memory_seq_rm [38912, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 40333, batch.n_tokens = 1421, progress = 0.998416
slot update_slots: id  0 | task 30313 | n_tokens = 40333, memory_seq_rm [40333, end)
slot update_slots: id  0 | task 30313 | prompt processing progress, n_tokens = 40397, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 30313 | prompt done, n_tokens = 40397, batch.n_tokens = 64
slot init_sampler: id  0 | task 30313 | init sampler, took 5.61 ms, tokens: text = 40397, total = 40397
slot update_slots: id  0 | task 30313 | created context checkpoint 1 of 8 (pos_min = 39563, pos_max = 40332, size = 18.056 MiB)
slot print_timing: id  0 | task 30313 | 
prompt eval time =   52297.10 ms / 40397 tokens (    1.29 ms per token,   772.45 tokens per second)
       eval time =    2311.90 ms /    83 tokens (   27.85 ms per token,    35.90 tokens per second)
      total time =   54609.01 ms / 40480 tokens
slot      release: id  0 | task 30313 | stop processing: n_tokens = 40479, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.536 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 30417 | processing task, is_child = 0
slot update_slots: id  0 | task 30417 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 75329
srv    send_error: task id = 30417, error: request (75329 tokens) exceeds the available context size (64000 tokens), try increasing it
slot      release: id  0 | task 30417 | stop processing: n_tokens = 40479, truncated = 0
srv  update_slots: no tokens to decode
srv  update_slots: all slots are idle
srv          stop: cancel task, id_task = 30417
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 400
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.530 (> 0.100 thold), f_keep = 0.210
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2972, total state size = 72.669 MiB
srv          load:  - looking for better prompt, base f_keep = 0.210, sim = 0.530
srv        update:  - cache state: 7 prompts, 3874.160 MiB (limits: 8192.000 MiB, 64000 tokens, 248162 est)
srv        update:    - prompt 0x573880c52a80:    7916 tokens, checkpoints:  8,   362.875 MiB
srv        update:    - prompt 0x5738805cdb40:   43384 tokens, checkpoints:  8,  1196.579 MiB
srv        update:    - prompt 0x57387f7ea870:   20654 tokens, checkpoints:  8,   673.058 MiB
srv        update:    - prompt 0x573880ef62c0:   10203 tokens, checkpoints:  8,   409.140 MiB
srv        update:    - prompt 0x57389118bff0:   15386 tokens, checkpoints:  8,   515.598 MiB
srv        update:    - prompt 0x573881f6b690:   16846 tokens, checkpoints:  7,   560.315 MiB
srv        update:    - prompt 0x573890ede3e0:    2972 tokens, checkpoints:  5,   156.594 MiB
srv  get_availabl: prompt cache update took 427.05 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 30420 | processing task, is_child = 0
slot update_slots: id  3 | task 30420 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1179
slot update_slots: id  3 | task 30420 | n_past = 625, slot.prompt.tokens.size() = 2972, seq_id = 3, pos_min = 2845, n_swa = 128
slot update_slots: id  3 | task 30420 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 30420 | erased invalidated context checkpoint (pos_min = 770, pos_max = 1666, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 30420 | erased invalidated context checkpoint (pos_min = 1357, pos_max = 2253, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 30420 | erased invalidated context checkpoint (pos_min = 1492, pos_max = 2321, n_swa = 128, size = 19.463 MiB)
slot update_slots: id  3 | task 30420 | erased invalidated context checkpoint (pos_min = 2084, pos_max = 2724, n_swa = 128, size = 15.031 MiB)
slot update_slots: id  3 | task 30420 | erased invalidated context checkpoint (pos_min = 2565, pos_max = 2878, n_swa = 128, size = 7.363 MiB)
slot update_slots: id  3 | task 30420 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 30420 | prompt processing progress, n_tokens = 1115, batch.n_tokens = 1115, progress = 0.945717
slot update_slots: id  3 | task 30420 | n_tokens = 1115, memory_seq_rm [1115, end)
slot update_slots: id  3 | task 30420 | prompt processing progress, n_tokens = 1179, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 30420 | prompt done, n_tokens = 1179, batch.n_tokens = 64
slot init_sampler: id  3 | task 30420 | init sampler, took 0.25 ms, tokens: text = 1179, total = 1179
slot update_slots: id  3 | task 30420 | created context checkpoint 1 of 8 (pos_min = 345, pos_max = 1114, size = 18.056 MiB)
slot print_timing: id  3 | task 30420 | 
prompt eval time =    2195.21 ms /  1179 tokens (    1.86 ms per token,   537.08 tokens per second)
       eval time =    2417.54 ms /    85 tokens (   28.44 ms per token,    35.16 tokens per second)
      total time =    4612.75 ms /  1264 tokens
slot      release: id  3 | task 30420 | stop processing: n_tokens = 1263, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.417 (> 0.100 thold), f_keep = 0.933
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 30507 | processing task, is_child = 0
slot update_slots: id  3 | task 30507 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2825
slot update_slots: id  3 | task 30507 | n_tokens = 1179, memory_seq_rm [1179, end)
slot update_slots: id  3 | task 30507 | prompt processing progress, n_tokens = 2761, batch.n_tokens = 1582, progress = 0.977345
slot update_slots: id  3 | task 30507 | n_tokens = 2761, memory_seq_rm [2761, end)
slot update_slots: id  3 | task 30507 | prompt processing progress, n_tokens = 2825, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 30507 | prompt done, n_tokens = 2825, batch.n_tokens = 64
slot init_sampler: id  3 | task 30507 | init sampler, took 0.53 ms, tokens: text = 2825, total = 2825
slot update_slots: id  3 | task 30507 | created context checkpoint 2 of 8 (pos_min = 1991, pos_max = 2760, size = 18.056 MiB)
slot print_timing: id  3 | task 30507 | 
prompt eval time =    2883.78 ms /  1646 tokens (    1.75 ms per token,   570.78 tokens per second)
       eval time =   27310.11 ms /   957 tokens (   28.54 ms per token,    35.04 tokens per second)
      total time =   30193.88 ms /  2603 tokens
slot      release: id  3 | task 30507 | stop processing: n_tokens = 3781, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.786 (> 0.100 thold), f_keep = 0.747
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31466 | processing task, is_child = 0
slot update_slots: id  3 | task 31466 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3593
slot update_slots: id  3 | task 31466 | n_past = 2825, slot.prompt.tokens.size() = 3781, seq_id = 3, pos_min = 3011, n_swa = 128
slot update_slots: id  3 | task 31466 | restored context checkpoint (pos_min = 1991, pos_max = 2760, size = 18.056 MiB)
slot update_slots: id  3 | task 31466 | n_tokens = 2760, memory_seq_rm [2760, end)
slot update_slots: id  3 | task 31466 | prompt processing progress, n_tokens = 3529, batch.n_tokens = 769, progress = 0.982188
slot update_slots: id  3 | task 31466 | n_tokens = 3529, memory_seq_rm [3529, end)
slot update_slots: id  3 | task 31466 | prompt processing progress, n_tokens = 3593, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31466 | prompt done, n_tokens = 3593, batch.n_tokens = 64
slot init_sampler: id  3 | task 31466 | init sampler, took 0.61 ms, tokens: text = 3593, total = 3593
slot update_slots: id  3 | task 31466 | created context checkpoint 3 of 8 (pos_min = 2759, pos_max = 3528, size = 18.056 MiB)
slot print_timing: id  3 | task 31466 | 
prompt eval time =    1739.61 ms /   833 tokens (    2.09 ms per token,   478.84 tokens per second)
       eval time =    2433.92 ms /    84 tokens (   28.98 ms per token,    34.51 tokens per second)
      total time =    4173.53 ms /   917 tokens
slot      release: id  3 | task 31466 | stop processing: n_tokens = 3676, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31552 | processing task, is_child = 0
slot update_slots: id  3 | task 31552 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3664
slot update_slots: id  3 | task 31552 | n_tokens = 3593, memory_seq_rm [3593, end)
slot update_slots: id  3 | task 31552 | prompt processing progress, n_tokens = 3600, batch.n_tokens = 7, progress = 0.982533
slot update_slots: id  3 | task 31552 | n_tokens = 3600, memory_seq_rm [3600, end)
slot update_slots: id  3 | task 31552 | prompt processing progress, n_tokens = 3664, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31552 | prompt done, n_tokens = 3664, batch.n_tokens = 64
slot init_sampler: id  3 | task 31552 | init sampler, took 0.57 ms, tokens: text = 3664, total = 3664
slot update_slots: id  3 | task 31552 | created context checkpoint 4 of 8 (pos_min = 2906, pos_max = 3599, size = 16.274 MiB)
slot print_timing: id  3 | task 31552 | 
prompt eval time =     319.81 ms /    71 tokens (    4.50 ms per token,   222.01 tokens per second)
       eval time =     969.81 ms /    33 tokens (   29.39 ms per token,    34.03 tokens per second)
      total time =    1289.61 ms /   104 tokens
slot      release: id  3 | task 31552 | stop processing: n_tokens = 3696, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.825 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31587 | processing task, is_child = 0
slot update_slots: id  3 | task 31587 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4442
slot update_slots: id  3 | task 31587 | n_tokens = 3664, memory_seq_rm [3664, end)
slot update_slots: id  3 | task 31587 | prompt processing progress, n_tokens = 4378, batch.n_tokens = 714, progress = 0.985592
slot update_slots: id  3 | task 31587 | n_tokens = 4378, memory_seq_rm [4378, end)
slot update_slots: id  3 | task 31587 | prompt processing progress, n_tokens = 4442, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31587 | prompt done, n_tokens = 4442, batch.n_tokens = 64
slot init_sampler: id  3 | task 31587 | init sampler, took 0.69 ms, tokens: text = 4442, total = 4442
slot update_slots: id  3 | task 31587 | created context checkpoint 5 of 8 (pos_min = 3608, pos_max = 4377, size = 18.056 MiB)
slot print_timing: id  3 | task 31587 | 
prompt eval time =    1548.36 ms /   778 tokens (    1.99 ms per token,   502.47 tokens per second)
       eval time =   10662.35 ms /   365 tokens (   29.21 ms per token,    34.23 tokens per second)
      total time =   12210.71 ms /  1143 tokens
slot      release: id  3 | task 31587 | stop processing: n_tokens = 4806, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.924
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31954 | processing task, is_child = 0
slot update_slots: id  3 | task 31954 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4626
slot update_slots: id  3 | task 31954 | n_tokens = 4442, memory_seq_rm [4442, end)
slot update_slots: id  3 | task 31954 | prompt processing progress, n_tokens = 4562, batch.n_tokens = 120, progress = 0.986165
slot update_slots: id  3 | task 31954 | n_tokens = 4562, memory_seq_rm [4562, end)
slot update_slots: id  3 | task 31954 | prompt processing progress, n_tokens = 4626, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31954 | prompt done, n_tokens = 4626, batch.n_tokens = 64
slot init_sampler: id  3 | task 31954 | init sampler, took 0.70 ms, tokens: text = 4626, total = 4626
slot update_slots: id  3 | task 31954 | created context checkpoint 6 of 8 (pos_min = 4036, pos_max = 4561, size = 12.334 MiB)
slot print_timing: id  3 | task 31954 | 
prompt eval time =     661.45 ms /   184 tokens (    3.59 ms per token,   278.18 tokens per second)
       eval time =    4232.25 ms /   147 tokens (   28.79 ms per token,    34.73 tokens per second)
      total time =    4893.69 ms /   331 tokens
slot      release: id  3 | task 31954 | stop processing: n_tokens = 4772, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32103 | processing task, is_child = 0
slot update_slots: id  3 | task 32103 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4688
slot update_slots: id  3 | task 32103 | n_tokens = 4626, memory_seq_rm [4626, end)
slot update_slots: id  3 | task 32103 | prompt processing progress, n_tokens = 4688, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 32103 | prompt done, n_tokens = 4688, batch.n_tokens = 62
slot init_sampler: id  3 | task 32103 | init sampler, took 0.85 ms, tokens: text = 4688, total = 4688
slot print_timing: id  3 | task 32103 | 
prompt eval time =     343.63 ms /    62 tokens (    5.54 ms per token,   180.43 tokens per second)
       eval time =   15818.68 ms /   543 tokens (   29.13 ms per token,    34.33 tokens per second)
      total time =   16162.32 ms /   605 tokens
slot      release: id  3 | task 32103 | stop processing: n_tokens = 5230, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.927 (> 0.100 thold), f_keep = 0.896
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32647 | processing task, is_child = 0
slot update_slots: id  3 | task 32647 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5058
slot update_slots: id  3 | task 32647 | n_tokens = 4688, memory_seq_rm [4688, end)
slot update_slots: id  3 | task 32647 | prompt processing progress, n_tokens = 4994, batch.n_tokens = 306, progress = 0.987347
slot update_slots: id  3 | task 32647 | n_tokens = 4994, memory_seq_rm [4994, end)
slot update_slots: id  3 | task 32647 | prompt processing progress, n_tokens = 5058, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 32647 | prompt done, n_tokens = 5058, batch.n_tokens = 64
slot init_sampler: id  3 | task 32647 | init sampler, took 0.89 ms, tokens: text = 5058, total = 5058
slot update_slots: id  3 | task 32647 | created context checkpoint 7 of 8 (pos_min = 4561, pos_max = 4993, size = 10.154 MiB)
slot print_timing: id  3 | task 32647 | 
prompt eval time =     824.91 ms /   370 tokens (    2.23 ms per token,   448.53 tokens per second)
       eval time =    1967.87 ms /    68 tokens (   28.94 ms per token,    34.56 tokens per second)
      total time =    2792.78 ms /   438 tokens
slot      release: id  3 | task 32647 | stop processing: n_tokens = 5125, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32717 | processing task, is_child = 0
slot update_slots: id  3 | task 32717 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5125
slot update_slots: id  3 | task 32717 | n_tokens = 5058, memory_seq_rm [5058, end)
slot update_slots: id  3 | task 32717 | prompt processing progress, n_tokens = 5061, batch.n_tokens = 3, progress = 0.987512
slot update_slots: id  3 | task 32717 | n_tokens = 5061, memory_seq_rm [5061, end)
slot update_slots: id  3 | task 32717 | prompt processing progress, n_tokens = 5125, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 32717 | prompt done, n_tokens = 5125, batch.n_tokens = 64
slot init_sampler: id  3 | task 32717 | init sampler, took 0.94 ms, tokens: text = 5125, total = 5125
slot update_slots: id  3 | task 32717 | created context checkpoint 8 of 8 (pos_min = 4561, pos_max = 5060, size = 11.725 MiB)
slot print_timing: id  3 | task 32717 | 
prompt eval time =     297.85 ms /    67 tokens (    4.45 ms per token,   224.94 tokens per second)
       eval time =    4575.96 ms /   152 tokens (   30.11 ms per token,    33.22 tokens per second)
      total time =    4873.82 ms /   219 tokens
slot      release: id  3 | task 32717 | stop processing: n_tokens = 5276, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.749 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32871 | processing task, is_child = 0
slot update_slots: id  3 | task 32871 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6842
slot update_slots: id  3 | task 32871 | n_tokens = 5125, memory_seq_rm [5125, end)
slot update_slots: id  3 | task 32871 | prompt processing progress, n_tokens = 6778, batch.n_tokens = 1653, progress = 0.990646
slot update_slots: id  3 | task 32871 | n_tokens = 6778, memory_seq_rm [6778, end)
slot update_slots: id  3 | task 32871 | prompt processing progress, n_tokens = 6842, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 32871 | prompt done, n_tokens = 6842, batch.n_tokens = 64
slot init_sampler: id  3 | task 32871 | init sampler, took 3.31 ms, tokens: text = 6842, total = 6842
slot update_slots: id  3 | task 32871 | erasing old context checkpoint (pos_min = 345, pos_max = 1114, size = 18.056 MiB)
slot update_slots: id  3 | task 32871 | created context checkpoint 8 of 8 (pos_min = 6008, pos_max = 6777, size = 18.056 MiB)
slot print_timing: id  3 | task 32871 | 
prompt eval time =    3208.19 ms /  1717 tokens (    1.87 ms per token,   535.19 tokens per second)
       eval time =   23784.14 ms /   797 tokens (   29.84 ms per token,    33.51 tokens per second)
      total time =   26992.33 ms /  2514 tokens
slot      release: id  3 | task 32871 | stop processing: n_tokens = 7638, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.922 (> 0.100 thold), f_keep = 0.896
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33670 | processing task, is_child = 0
slot update_slots: id  3 | task 33670 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7422
slot update_slots: id  3 | task 33670 | n_past = 6842, slot.prompt.tokens.size() = 7638, seq_id = 3, pos_min = 6868, n_swa = 128
slot update_slots: id  3 | task 33670 | restored context checkpoint (pos_min = 6008, pos_max = 6777, size = 18.056 MiB)
slot update_slots: id  3 | task 33670 | n_tokens = 6777, memory_seq_rm [6777, end)
slot update_slots: id  3 | task 33670 | prompt processing progress, n_tokens = 7358, batch.n_tokens = 581, progress = 0.991377
slot update_slots: id  3 | task 33670 | n_tokens = 7358, memory_seq_rm [7358, end)
slot update_slots: id  3 | task 33670 | prompt processing progress, n_tokens = 7422, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33670 | prompt done, n_tokens = 7422, batch.n_tokens = 64
slot init_sampler: id  3 | task 33670 | init sampler, took 1.31 ms, tokens: text = 7422, total = 7422
slot update_slots: id  3 | task 33670 | erasing old context checkpoint (pos_min = 1991, pos_max = 2760, size = 18.056 MiB)
slot update_slots: id  3 | task 33670 | created context checkpoint 8 of 8 (pos_min = 6588, pos_max = 7357, size = 18.056 MiB)
slot print_timing: id  3 | task 33670 | 
prompt eval time =    1592.48 ms /   645 tokens (    2.47 ms per token,   405.03 tokens per second)
       eval time =    5706.09 ms /   177 tokens (   32.24 ms per token,    31.02 tokens per second)
      total time =    7298.56 ms /   822 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 33670 | stop processing: n_tokens = 7598, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33849 | processing task, is_child = 0
slot update_slots: id  3 | task 33849 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7479
slot update_slots: id  3 | task 33849 | n_tokens = 7422, memory_seq_rm [7422, end)
slot update_slots: id  3 | task 33849 | prompt processing progress, n_tokens = 7479, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 33849 | prompt done, n_tokens = 7479, batch.n_tokens = 57
slot init_sampler: id  3 | task 33849 | init sampler, took 1.11 ms, tokens: text = 7479, total = 7479
slot print_timing: id  3 | task 33849 | 
prompt eval time =     304.48 ms /    57 tokens (    5.34 ms per token,   187.21 tokens per second)
       eval time =    2088.21 ms /    60 tokens (   34.80 ms per token,    28.73 tokens per second)
      total time =    2392.69 ms /   117 tokens
slot      release: id  3 | task 33849 | stop processing: n_tokens = 7538, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33910 | processing task, is_child = 0
slot update_slots: id  3 | task 33910 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7553
slot update_slots: id  3 | task 33910 | n_tokens = 7479, memory_seq_rm [7479, end)
slot update_slots: id  3 | task 33910 | prompt processing progress, n_tokens = 7489, batch.n_tokens = 10, progress = 0.991527
slot update_slots: id  3 | task 33910 | n_tokens = 7489, memory_seq_rm [7489, end)
slot update_slots: id  3 | task 33910 | prompt processing progress, n_tokens = 7553, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33910 | prompt done, n_tokens = 7553, batch.n_tokens = 64
slot init_sampler: id  3 | task 33910 | init sampler, took 2.21 ms, tokens: text = 7553, total = 7553
slot update_slots: id  3 | task 33910 | erasing old context checkpoint (pos_min = 2759, pos_max = 3528, size = 18.056 MiB)
slot update_slots: id  3 | task 33910 | created context checkpoint 8 of 8 (pos_min = 6828, pos_max = 7488, size = 15.500 MiB)
slot print_timing: id  3 | task 33910 | 
prompt eval time =     391.19 ms /    74 tokens (    5.29 ms per token,   189.17 tokens per second)
       eval time =    2368.83 ms /    82 tokens (   28.89 ms per token,    34.62 tokens per second)
      total time =    2760.02 ms /   156 tokens
slot      release: id  3 | task 33910 | stop processing: n_tokens = 7634, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
