ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: using cached file: /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14992 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1578 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 64000 -> need 1580 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 2.82 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
srv  log_server_r: request: GET /health 127.0.0.1 503
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 64000
llama_context: n_ctx_seq     = 64000
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (64000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 64000 cells
llama_kv_cache:      CUDA0 KV buffer size =  1500.00 MiB
llama_kv_cache: size = 1500.00 MiB ( 64000 cells,  12 layers,  4/1 seqs), K (f16):  750.00 MiB, V (f16):  750.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   132.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 91.05 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv  log_server_r: request: GET /health 127.0.0.1 503
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  1 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  2 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  3 | task -1 | new slot, n_ctx = 64000
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-02

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2067
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 2003, batch.n_tokens = 2003, progress = 0.969037
slot update_slots: id  3 | task 0 | n_tokens = 2003, memory_seq_rm [2003, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 2067, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 2067, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.36 ms, tokens: text = 2067, total = 2067
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 979, pos_max = 2002, size = 24.012 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =    2317.66 ms /  2067 tokens (    1.12 ms per token,   891.85 tokens per second)
       eval time =     893.34 ms /    38 tokens (   23.51 ms per token,    42.54 tokens per second)
      total time =    3211.01 ms /  2105 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 2104, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40 | processing task, is_child = 0
slot update_slots: id  3 | task 40 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2067
slot update_slots: id  3 | task 40 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 40 | prompt processing progress, n_tokens = 2067, batch.n_tokens = 6, progress = 1.000000
slot update_slots: id  3 | task 40 | prompt done, n_tokens = 2067, batch.n_tokens = 6
slot init_sampler: id  3 | task 40 | init sampler, took 0.42 ms, tokens: text = 2067, total = 2067
slot print_timing: id  3 | task 40 | 
prompt eval time =     120.61 ms /     6 tokens (   20.10 ms per token,    49.75 tokens per second)
       eval time =    1191.14 ms /    48 tokens (   24.82 ms per token,    40.30 tokens per second)
      total time =    1311.75 ms /    54 tokens
slot      release: id  3 | task 40 | stop processing: n_tokens = 2114, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 89 | processing task, is_child = 0
slot update_slots: id  3 | task 89 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2067
slot update_slots: id  3 | task 89 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 89 | prompt processing progress, n_tokens = 2067, batch.n_tokens = 6, progress = 1.000000
slot update_slots: id  3 | task 89 | prompt done, n_tokens = 2067, batch.n_tokens = 6
slot init_sampler: id  3 | task 89 | init sampler, took 0.30 ms, tokens: text = 2067, total = 2067
slot print_timing: id  3 | task 89 | 
prompt eval time =      93.05 ms /     6 tokens (   15.51 ms per token,    64.48 tokens per second)
       eval time =     868.14 ms /    36 tokens (   24.12 ms per token,    41.47 tokens per second)
      total time =     961.19 ms /    42 tokens
slot      release: id  3 | task 89 | stop processing: n_tokens = 2102, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 126 | processing task, is_child = 0
slot update_slots: id  3 | task 126 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2067
slot update_slots: id  3 | task 126 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 126 | prompt processing progress, n_tokens = 2067, batch.n_tokens = 6, progress = 1.000000
slot update_slots: id  3 | task 126 | prompt done, n_tokens = 2067, batch.n_tokens = 6
slot init_sampler: id  3 | task 126 | init sampler, took 0.35 ms, tokens: text = 2067, total = 2067
slot print_timing: id  3 | task 126 | 
prompt eval time =      92.64 ms /     6 tokens (   15.44 ms per token,    64.77 tokens per second)
       eval time =    1401.06 ms /    56 tokens (   25.02 ms per token,    39.97 tokens per second)
      total time =    1493.69 ms /    62 tokens
slot      release: id  3 | task 126 | stop processing: n_tokens = 2122, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 183 | processing task, is_child = 0
slot update_slots: id  3 | task 183 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2075
slot update_slots: id  3 | task 183 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 183 | prompt processing progress, n_tokens = 2075, batch.n_tokens = 14, progress = 1.000000
slot update_slots: id  3 | task 183 | prompt done, n_tokens = 2075, batch.n_tokens = 14
slot init_sampler: id  3 | task 183 | init sampler, took 0.41 ms, tokens: text = 2075, total = 2075
slot print_timing: id  3 | task 183 | 
prompt eval time =     139.38 ms /    14 tokens (    9.96 ms per token,   100.44 tokens per second)
       eval time =    1845.77 ms /    76 tokens (   24.29 ms per token,    41.18 tokens per second)
      total time =    1985.16 ms /    90 tokens
slot      release: id  3 | task 183 | stop processing: n_tokens = 2150, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 260 | processing task, is_child = 0
slot update_slots: id  3 | task 260 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2076
slot update_slots: id  3 | task 260 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 260 | prompt processing progress, n_tokens = 2076, batch.n_tokens = 15, progress = 1.000000
slot update_slots: id  3 | task 260 | prompt done, n_tokens = 2076, batch.n_tokens = 15
slot init_sampler: id  3 | task 260 | init sampler, took 0.34 ms, tokens: text = 2076, total = 2076
slot print_timing: id  3 | task 260 | 
prompt eval time =     137.79 ms /    15 tokens (    9.19 ms per token,   108.86 tokens per second)
       eval time =    3856.26 ms /   157 tokens (   24.56 ms per token,    40.71 tokens per second)
      total time =    3994.05 ms /   172 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 260 | stop processing: n_tokens = 2232, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.930
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 418 | processing task, is_child = 0
slot update_slots: id  3 | task 418 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2118
slot update_slots: id  3 | task 418 | n_tokens = 2076, memory_seq_rm [2076, end)
slot update_slots: id  3 | task 418 | prompt processing progress, n_tokens = 2118, batch.n_tokens = 42, progress = 1.000000
slot update_slots: id  3 | task 418 | prompt done, n_tokens = 2118, batch.n_tokens = 42
slot init_sampler: id  3 | task 418 | init sampler, took 0.38 ms, tokens: text = 2118, total = 2118
slot update_slots: id  3 | task 418 | created context checkpoint 2 of 8 (pos_min = 1208, pos_max = 2075, size = 20.354 MiB)
slot print_timing: id  3 | task 418 | 
prompt eval time =     239.32 ms /    42 tokens (    5.70 ms per token,   175.50 tokens per second)
       eval time =    2058.42 ms /    84 tokens (   24.51 ms per token,    40.81 tokens per second)
      total time =    2297.74 ms /   126 tokens
slot      release: id  3 | task 418 | stop processing: n_tokens = 2201, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 503 | processing task, is_child = 0
slot update_slots: id  3 | task 503 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2177
slot update_slots: id  3 | task 503 | n_tokens = 2118, memory_seq_rm [2118, end)
slot update_slots: id  3 | task 503 | prompt processing progress, n_tokens = 2177, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 503 | prompt done, n_tokens = 2177, batch.n_tokens = 59
slot init_sampler: id  3 | task 503 | init sampler, took 0.36 ms, tokens: text = 2177, total = 2177
slot print_timing: id  3 | task 503 | 
prompt eval time =     231.25 ms /    59 tokens (    3.92 ms per token,   255.13 tokens per second)
       eval time =    5219.47 ms /   211 tokens (   24.74 ms per token,    40.43 tokens per second)
      total time =    5450.72 ms /   270 tokens
slot      release: id  3 | task 503 | stop processing: n_tokens = 2387, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.863
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 715 | processing task, is_child = 0
slot update_slots: id  3 | task 715 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2073
slot update_slots: id  3 | task 715 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 715 | prompt processing progress, n_tokens = 2073, batch.n_tokens = 12, progress = 1.000000
slot update_slots: id  3 | task 715 | prompt done, n_tokens = 2073, batch.n_tokens = 12
slot init_sampler: id  3 | task 715 | init sampler, took 0.33 ms, tokens: text = 2073, total = 2073
slot print_timing: id  3 | task 715 | 
prompt eval time =     133.67 ms /    12 tokens (   11.14 ms per token,    89.78 tokens per second)
       eval time =    7319.46 ms /   301 tokens (   24.32 ms per token,    41.12 tokens per second)
      total time =    7453.13 ms /   313 tokens
slot      release: id  3 | task 715 | stop processing: n_tokens = 2373, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.869
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1017 | processing task, is_child = 0
slot update_slots: id  3 | task 1017 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2076
slot update_slots: id  3 | task 1017 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 1017 | prompt processing progress, n_tokens = 2076, batch.n_tokens = 15, progress = 1.000000
slot update_slots: id  3 | task 1017 | prompt done, n_tokens = 2076, batch.n_tokens = 15
slot init_sampler: id  3 | task 1017 | init sampler, took 0.34 ms, tokens: text = 2076, total = 2076
slot print_timing: id  3 | task 1017 | 
prompt eval time =     137.56 ms /    15 tokens (    9.17 ms per token,   109.04 tokens per second)
       eval time =    1242.62 ms /    49 tokens (   25.36 ms per token,    39.43 tokens per second)
      total time =    1380.18 ms /    64 tokens
slot      release: id  3 | task 1017 | stop processing: n_tokens = 2124, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1067 | processing task, is_child = 0
slot update_slots: id  3 | task 1067 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2118
slot update_slots: id  3 | task 1067 | n_tokens = 2076, memory_seq_rm [2076, end)
slot update_slots: id  3 | task 1067 | prompt processing progress, n_tokens = 2118, batch.n_tokens = 42, progress = 1.000000
slot update_slots: id  3 | task 1067 | prompt done, n_tokens = 2118, batch.n_tokens = 42
slot init_sampler: id  3 | task 1067 | init sampler, took 0.33 ms, tokens: text = 2118, total = 2118
slot print_timing: id  3 | task 1067 | 
prompt eval time =     195.07 ms /    42 tokens (    4.64 ms per token,   215.31 tokens per second)
       eval time =    1576.09 ms /    63 tokens (   25.02 ms per token,    39.97 tokens per second)
      total time =    1771.15 ms /   105 tokens
slot      release: id  3 | task 1067 | stop processing: n_tokens = 2180, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1131 | processing task, is_child = 0
slot update_slots: id  3 | task 1131 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2159
slot update_slots: id  3 | task 1131 | n_tokens = 2118, memory_seq_rm [2118, end)
slot update_slots: id  3 | task 1131 | prompt processing progress, n_tokens = 2159, batch.n_tokens = 41, progress = 1.000000
slot update_slots: id  3 | task 1131 | prompt done, n_tokens = 2159, batch.n_tokens = 41
slot init_sampler: id  3 | task 1131 | init sampler, took 0.34 ms, tokens: text = 2159, total = 2159
slot print_timing: id  3 | task 1131 | 
prompt eval time =     216.73 ms /    41 tokens (    5.29 ms per token,   189.17 tokens per second)
       eval time =    2627.97 ms /   105 tokens (   25.03 ms per token,    39.95 tokens per second)
      total time =    2844.70 ms /   146 tokens
slot      release: id  3 | task 1131 | stop processing: n_tokens = 2263, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1237 | processing task, is_child = 0
slot update_slots: id  3 | task 1237 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2216
slot update_slots: id  3 | task 1237 | n_tokens = 2159, memory_seq_rm [2159, end)
slot update_slots: id  3 | task 1237 | prompt processing progress, n_tokens = 2216, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 1237 | prompt done, n_tokens = 2216, batch.n_tokens = 57
slot init_sampler: id  3 | task 1237 | init sampler, took 0.36 ms, tokens: text = 2216, total = 2216
slot update_slots: id  3 | task 1237 | created context checkpoint 3 of 8 (pos_min = 1363, pos_max = 2158, size = 18.666 MiB)
slot print_timing: id  3 | task 1237 | 
prompt eval time =     172.24 ms /    57 tokens (    3.02 ms per token,   330.93 tokens per second)
       eval time =    2509.78 ms /   100 tokens (   25.10 ms per token,    39.84 tokens per second)
      total time =    2682.02 ms /   157 tokens
slot      release: id  3 | task 1237 | stop processing: n_tokens = 2315, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.957
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1338 | processing task, is_child = 0
slot update_slots: id  3 | task 1338 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2273
slot update_slots: id  3 | task 1338 | n_tokens = 2216, memory_seq_rm [2216, end)
slot update_slots: id  3 | task 1338 | prompt processing progress, n_tokens = 2273, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 1338 | prompt done, n_tokens = 2273, batch.n_tokens = 57
slot init_sampler: id  3 | task 1338 | init sampler, took 0.43 ms, tokens: text = 2273, total = 2273
slot print_timing: id  3 | task 1338 | 
prompt eval time =     158.51 ms /    57 tokens (    2.78 ms per token,   359.59 tokens per second)
       eval time =    1934.20 ms /    75 tokens (   25.79 ms per token,    38.78 tokens per second)
      total time =    2092.71 ms /   132 tokens
slot      release: id  3 | task 1338 | stop processing: n_tokens = 2347, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.968
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1414 | processing task, is_child = 0
slot update_slots: id  3 | task 1414 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2330
slot update_slots: id  3 | task 1414 | n_tokens = 2273, memory_seq_rm [2273, end)
slot update_slots: id  3 | task 1414 | prompt processing progress, n_tokens = 2330, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 1414 | prompt done, n_tokens = 2330, batch.n_tokens = 57
slot init_sampler: id  3 | task 1414 | init sampler, took 0.82 ms, tokens: text = 2330, total = 2330
slot update_slots: id  3 | task 1414 | created context checkpoint 4 of 8 (pos_min = 1363, pos_max = 2272, size = 21.339 MiB)
slot print_timing: id  3 | task 1414 | 
prompt eval time =     176.79 ms /    57 tokens (    3.10 ms per token,   322.41 tokens per second)
       eval time =    2279.76 ms /    89 tokens (   25.62 ms per token,    39.04 tokens per second)
      total time =    2456.55 ms /   146 tokens
slot      release: id  3 | task 1414 | stop processing: n_tokens = 2418, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1504 | processing task, is_child = 0
slot update_slots: id  3 | task 1504 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2387
slot update_slots: id  3 | task 1504 | n_tokens = 2330, memory_seq_rm [2330, end)
slot update_slots: id  3 | task 1504 | prompt processing progress, n_tokens = 2387, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 1504 | prompt done, n_tokens = 2387, batch.n_tokens = 57
slot init_sampler: id  3 | task 1504 | init sampler, took 0.46 ms, tokens: text = 2387, total = 2387
slot print_timing: id  3 | task 1504 | 
prompt eval time =     159.29 ms /    57 tokens (    2.79 ms per token,   357.84 tokens per second)
       eval time =    8435.16 ms /   322 tokens (   26.20 ms per token,    38.17 tokens per second)
      total time =    8594.45 ms /   379 tokens
slot      release: id  3 | task 1504 | stop processing: n_tokens = 2708, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.881
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1827 | processing task, is_child = 0
slot update_slots: id  3 | task 1827 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2444
slot update_slots: id  3 | task 1827 | n_tokens = 2387, memory_seq_rm [2387, end)
slot update_slots: id  3 | task 1827 | prompt processing progress, n_tokens = 2444, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 1827 | prompt done, n_tokens = 2444, batch.n_tokens = 57
slot init_sampler: id  3 | task 1827 | init sampler, took 0.45 ms, tokens: text = 2444, total = 2444
slot update_slots: id  3 | task 1827 | created context checkpoint 5 of 8 (pos_min = 1684, pos_max = 2386, size = 16.485 MiB)
slot print_timing: id  3 | task 1827 | 
prompt eval time =     176.79 ms /    57 tokens (    3.10 ms per token,   322.42 tokens per second)
       eval time =    7359.11 ms /   290 tokens (   25.38 ms per token,    39.41 tokens per second)
      total time =    7535.90 ms /   347 tokens
slot      release: id  3 | task 1827 | stop processing: n_tokens = 2733, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.894
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2118 | processing task, is_child = 0
slot update_slots: id  3 | task 2118 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2501
slot update_slots: id  3 | task 2118 | n_tokens = 2444, memory_seq_rm [2444, end)
slot update_slots: id  3 | task 2118 | prompt processing progress, n_tokens = 2501, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 2118 | prompt done, n_tokens = 2501, batch.n_tokens = 57
slot init_sampler: id  3 | task 2118 | init sampler, took 0.38 ms, tokens: text = 2501, total = 2501
slot print_timing: id  3 | task 2118 | 
prompt eval time =     153.06 ms /    57 tokens (    2.69 ms per token,   372.40 tokens per second)
       eval time =    5732.06 ms /   234 tokens (   24.50 ms per token,    40.82 tokens per second)
      total time =    5885.12 ms /   291 tokens
slot      release: id  3 | task 2118 | stop processing: n_tokens = 2734, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.915
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2353 | processing task, is_child = 0
slot update_slots: id  3 | task 2353 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2550
slot update_slots: id  3 | task 2353 | n_tokens = 2501, memory_seq_rm [2501, end)
slot update_slots: id  3 | task 2353 | prompt processing progress, n_tokens = 2550, batch.n_tokens = 49, progress = 1.000000
slot update_slots: id  3 | task 2353 | prompt done, n_tokens = 2550, batch.n_tokens = 49
slot init_sampler: id  3 | task 2353 | init sampler, took 0.47 ms, tokens: text = 2550, total = 2550
slot update_slots: id  3 | task 2353 | created context checkpoint 6 of 8 (pos_min = 1710, pos_max = 2500, size = 18.548 MiB)
slot print_timing: id  3 | task 2353 | 
prompt eval time =     167.18 ms /    49 tokens (    3.41 ms per token,   293.11 tokens per second)
       eval time =   43928.58 ms /  1826 tokens (   24.06 ms per token,    41.57 tokens per second)
      total time =   44095.75 ms /  1875 tokens
slot      release: id  3 | task 2353 | stop processing: n_tokens = 4375, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.471
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4375, total state size = 126.601 MiB
srv          load:  - looking for better prompt, base f_keep = 0.471, sim = 0.980
srv        update:  - cache state: 1 prompts, 246.005 MiB (limits: 8192.000 MiB, 64000 tokens, 145687 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv  get_availabl: prompt cache update took 185.85 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4180 | processing task, is_child = 0
slot update_slots: id  3 | task 4180 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2103
slot update_slots: id  3 | task 4180 | n_past = 2061, slot.prompt.tokens.size() = 4375, seq_id = 3, pos_min = 3351, n_swa = 128
slot update_slots: id  3 | task 4180 | restored context checkpoint (pos_min = 1710, pos_max = 2500, size = 18.548 MiB)
slot update_slots: id  3 | task 4180 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 4180 | prompt processing progress, n_tokens = 2103, batch.n_tokens = 42, progress = 1.000000
slot update_slots: id  3 | task 4180 | prompt done, n_tokens = 2103, batch.n_tokens = 42
slot init_sampler: id  3 | task 4180 | init sampler, took 0.37 ms, tokens: text = 2103, total = 2103
slot print_timing: id  3 | task 4180 | 
prompt eval time =     195.38 ms /    42 tokens (    4.65 ms per token,   214.96 tokens per second)
       eval time =    1390.28 ms /    60 tokens (   23.17 ms per token,    43.16 tokens per second)
      total time =    1585.66 ms /   102 tokens
slot      release: id  3 | task 4180 | stop processing: n_tokens = 2162, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4241 | processing task, is_child = 0
slot update_slots: id  3 | task 4241 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2150
slot update_slots: id  3 | task 4241 | n_tokens = 2103, memory_seq_rm [2103, end)
slot update_slots: id  3 | task 4241 | prompt processing progress, n_tokens = 2150, batch.n_tokens = 47, progress = 1.000000
slot update_slots: id  3 | task 4241 | prompt done, n_tokens = 2150, batch.n_tokens = 47
slot init_sampler: id  3 | task 4241 | init sampler, took 0.30 ms, tokens: text = 2150, total = 2150
slot print_timing: id  3 | task 4241 | 
prompt eval time =     121.09 ms /    47 tokens (    2.58 ms per token,   388.13 tokens per second)
       eval time =    3637.99 ms /   153 tokens (   23.78 ms per token,    42.06 tokens per second)
      total time =    3759.08 ms /   200 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 4241 | stop processing: n_tokens = 2302, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.791 (> 0.100 thold), f_keep = 0.934
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4395 | processing task, is_child = 0
slot update_slots: id  3 | task 4395 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2719
slot update_slots: id  3 | task 4395 | n_tokens = 2150, memory_seq_rm [2150, end)
slot update_slots: id  3 | task 4395 | prompt processing progress, n_tokens = 2655, batch.n_tokens = 505, progress = 0.976462
slot update_slots: id  3 | task 4395 | n_tokens = 2655, memory_seq_rm [2655, end)
slot update_slots: id  3 | task 4395 | prompt processing progress, n_tokens = 2719, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4395 | prompt done, n_tokens = 2719, batch.n_tokens = 64
slot init_sampler: id  3 | task 4395 | init sampler, took 0.56 ms, tokens: text = 2719, total = 2719
slot update_slots: id  3 | task 4395 | created context checkpoint 7 of 8 (pos_min = 1864, pos_max = 2654, size = 18.548 MiB)
slot print_timing: id  3 | task 4395 | 
prompt eval time =     626.12 ms /   569 tokens (    1.10 ms per token,   908.78 tokens per second)
       eval time =   13406.65 ms /   550 tokens (   24.38 ms per token,    41.02 tokens per second)
      total time =   14032.76 ms /  1119 tokens
slot      release: id  3 | task 4395 | stop processing: n_tokens = 3268, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.631
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4947 | processing task, is_child = 0
slot update_slots: id  3 | task 4947 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2069
slot update_slots: id  3 | task 4947 | n_past = 2061, slot.prompt.tokens.size() = 3268, seq_id = 3, pos_min = 2244, n_swa = 128
slot update_slots: id  3 | task 4947 | restored context checkpoint (pos_min = 1864, pos_max = 2654, size = 18.548 MiB)
slot update_slots: id  3 | task 4947 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 4947 | prompt processing progress, n_tokens = 2069, batch.n_tokens = 8, progress = 1.000000
slot update_slots: id  3 | task 4947 | prompt done, n_tokens = 2069, batch.n_tokens = 8
slot init_sampler: id  3 | task 4947 | init sampler, took 0.32 ms, tokens: text = 2069, total = 2069
slot print_timing: id  3 | task 4947 | 
prompt eval time =     111.83 ms /     8 tokens (   13.98 ms per token,    71.53 tokens per second)
       eval time =    2117.65 ms /    87 tokens (   24.34 ms per token,    41.08 tokens per second)
      total time =    2229.48 ms /    95 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 4947 | stop processing: n_tokens = 2155, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.960
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5035 | processing task, is_child = 0
slot update_slots: id  3 | task 5035 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2117
slot update_slots: id  3 | task 5035 | n_tokens = 2069, memory_seq_rm [2069, end)
slot update_slots: id  3 | task 5035 | prompt processing progress, n_tokens = 2117, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 5035 | prompt done, n_tokens = 2117, batch.n_tokens = 48
slot init_sampler: id  3 | task 5035 | init sampler, took 0.39 ms, tokens: text = 2117, total = 2117
slot print_timing: id  3 | task 5035 | 
prompt eval time =     120.93 ms /    48 tokens (    2.52 ms per token,   396.93 tokens per second)
       eval time =    1593.00 ms /    67 tokens (   23.78 ms per token,    42.06 tokens per second)
      total time =    1713.93 ms /   115 tokens
slot      release: id  3 | task 5035 | stop processing: n_tokens = 2183, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.944
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5103 | processing task, is_child = 0
slot update_slots: id  3 | task 5103 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2093
slot update_slots: id  3 | task 5103 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 5103 | prompt processing progress, n_tokens = 2093, batch.n_tokens = 32, progress = 1.000000
slot update_slots: id  3 | task 5103 | prompt done, n_tokens = 2093, batch.n_tokens = 32
slot init_sampler: id  3 | task 5103 | init sampler, took 0.29 ms, tokens: text = 2093, total = 2093
slot print_timing: id  3 | task 5103 | 
prompt eval time =     183.81 ms /    32 tokens (    5.74 ms per token,   174.09 tokens per second)
       eval time =    2552.15 ms /   104 tokens (   24.54 ms per token,    40.75 tokens per second)
      total time =    2735.96 ms /   136 tokens
slot      release: id  3 | task 5103 | stop processing: n_tokens = 2196, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.530 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5208 | processing task, is_child = 0
slot update_slots: id  3 | task 5208 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3946
slot update_slots: id  3 | task 5208 | n_tokens = 2093, memory_seq_rm [2093, end)
slot update_slots: id  3 | task 5208 | prompt processing progress, n_tokens = 3882, batch.n_tokens = 1789, progress = 0.983781
slot update_slots: id  3 | task 5208 | n_tokens = 3882, memory_seq_rm [3882, end)
slot update_slots: id  3 | task 5208 | prompt processing progress, n_tokens = 3946, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5208 | prompt done, n_tokens = 3946, batch.n_tokens = 64
slot init_sampler: id  3 | task 5208 | init sampler, took 0.65 ms, tokens: text = 3946, total = 3946
slot update_slots: id  3 | task 5208 | created context checkpoint 8 of 8 (pos_min = 2858, pos_max = 3881, size = 24.012 MiB)
slot print_timing: id  3 | task 5208 | 
prompt eval time =    2012.02 ms /  1853 tokens (    1.09 ms per token,   920.97 tokens per second)
       eval time =     713.28 ms /    29 tokens (   24.60 ms per token,    40.66 tokens per second)
      total time =    2725.29 ms /  1882 tokens
slot      release: id  3 | task 5208 | stop processing: n_tokens = 3974, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.920 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5239 | processing task, is_child = 0
slot update_slots: id  3 | task 5239 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4288
slot update_slots: id  3 | task 5239 | n_tokens = 3946, memory_seq_rm [3946, end)
slot update_slots: id  3 | task 5239 | prompt processing progress, n_tokens = 4224, batch.n_tokens = 278, progress = 0.985075
slot update_slots: id  3 | task 5239 | n_tokens = 4224, memory_seq_rm [4224, end)
slot update_slots: id  3 | task 5239 | prompt processing progress, n_tokens = 4288, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5239 | prompt done, n_tokens = 4288, batch.n_tokens = 64
slot init_sampler: id  3 | task 5239 | init sampler, took 0.84 ms, tokens: text = 4288, total = 4288
slot update_slots: id  3 | task 5239 | erasing old context checkpoint (pos_min = 979, pos_max = 2002, size = 24.012 MiB)
slot update_slots: id  3 | task 5239 | created context checkpoint 8 of 8 (pos_min = 3200, pos_max = 4223, size = 24.012 MiB)
slot print_timing: id  3 | task 5239 | 
prompt eval time =     521.00 ms /   342 tokens (    1.52 ms per token,   656.42 tokens per second)
       eval time =    9480.97 ms /   376 tokens (   25.22 ms per token,    39.66 tokens per second)
      total time =   10001.98 ms /   718 tokens
slot      release: id  3 | task 5239 | stop processing: n_tokens = 4663, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.734 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5617 | processing task, is_child = 0
slot update_slots: id  3 | task 5617 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5839
slot update_slots: id  3 | task 5617 | n_tokens = 4288, memory_seq_rm [4288, end)
slot update_slots: id  3 | task 5617 | prompt processing progress, n_tokens = 5775, batch.n_tokens = 1487, progress = 0.989039
slot update_slots: id  3 | task 5617 | n_tokens = 5775, memory_seq_rm [5775, end)
slot update_slots: id  3 | task 5617 | prompt processing progress, n_tokens = 5839, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5617 | prompt done, n_tokens = 5839, batch.n_tokens = 64
slot init_sampler: id  3 | task 5617 | init sampler, took 0.87 ms, tokens: text = 5839, total = 5839
slot update_slots: id  3 | task 5617 | erasing old context checkpoint (pos_min = 1208, pos_max = 2075, size = 20.354 MiB)
slot update_slots: id  3 | task 5617 | created context checkpoint 8 of 8 (pos_min = 4751, pos_max = 5774, size = 24.012 MiB)
slot print_timing: id  3 | task 5617 | 
prompt eval time =    1731.63 ms /  1551 tokens (    1.12 ms per token,   895.69 tokens per second)
       eval time =    5700.29 ms /   217 tokens (   26.27 ms per token,    38.07 tokens per second)
      total time =    7431.92 ms /  1768 tokens
slot      release: id  3 | task 5617 | stop processing: n_tokens = 6055, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.869 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5836 | processing task, is_child = 0
slot update_slots: id  3 | task 5836 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6717
slot update_slots: id  3 | task 5836 | n_tokens = 5839, memory_seq_rm [5839, end)
slot update_slots: id  3 | task 5836 | prompt processing progress, n_tokens = 6653, batch.n_tokens = 814, progress = 0.990472
slot update_slots: id  3 | task 5836 | n_tokens = 6653, memory_seq_rm [6653, end)
slot update_slots: id  3 | task 5836 | prompt processing progress, n_tokens = 6717, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5836 | prompt done, n_tokens = 6717, batch.n_tokens = 64
slot init_sampler: id  3 | task 5836 | init sampler, took 1.25 ms, tokens: text = 6717, total = 6717
slot update_slots: id  3 | task 5836 | erasing old context checkpoint (pos_min = 1363, pos_max = 2158, size = 18.666 MiB)
slot update_slots: id  3 | task 5836 | created context checkpoint 8 of 8 (pos_min = 5629, pos_max = 6652, size = 24.012 MiB)
slot print_timing: id  3 | task 5836 | 
prompt eval time =    1155.75 ms /   878 tokens (    1.32 ms per token,   759.68 tokens per second)
       eval time =  151099.04 ms /  5923 tokens (   25.51 ms per token,    39.20 tokens per second)
      total time =  152254.78 ms /  6801 tokens
slot      release: id  3 | task 5836 | stop processing: n_tokens = 12639, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.165 (> 0.100 thold), f_keep = 0.166
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 12639, total state size = 320.384 MiB
srv          load:  - looking for better prompt, base f_keep = 0.166, sim = 0.165
srv        update:  - cache state: 2 prompts, 737.357 MiB (limits: 8192.000 MiB, 64000 tokens, 189024 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv  get_availabl: prompt cache update took 371.65 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11761 | processing task, is_child = 0
slot update_slots: id  3 | task 11761 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12713
slot update_slots: id  3 | task 11761 | n_past = 2093, slot.prompt.tokens.size() = 12639, seq_id = 3, pos_min = 11615, n_swa = 128
slot update_slots: id  3 | task 11761 | restored context checkpoint (pos_min = 1864, pos_max = 2654, size = 18.548 MiB)
slot update_slots: id  3 | task 11761 | erased invalidated context checkpoint (pos_min = 2858, pos_max = 3881, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11761 | erased invalidated context checkpoint (pos_min = 3200, pos_max = 4223, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11761 | erased invalidated context checkpoint (pos_min = 4751, pos_max = 5774, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11761 | erased invalidated context checkpoint (pos_min = 5629, pos_max = 6652, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11761 | n_tokens = 2093, memory_seq_rm [2093, end)
slot update_slots: id  3 | task 11761 | prompt processing progress, n_tokens = 4141, batch.n_tokens = 2048, progress = 0.325730
slot update_slots: id  3 | task 11761 | n_tokens = 4141, memory_seq_rm [4141, end)
slot update_slots: id  3 | task 11761 | prompt processing progress, n_tokens = 6189, batch.n_tokens = 2048, progress = 0.486825
slot update_slots: id  3 | task 11761 | n_tokens = 6189, memory_seq_rm [6189, end)
slot update_slots: id  3 | task 11761 | prompt processing progress, n_tokens = 8237, batch.n_tokens = 2048, progress = 0.647919
slot update_slots: id  3 | task 11761 | n_tokens = 8237, memory_seq_rm [8237, end)
slot update_slots: id  3 | task 11761 | prompt processing progress, n_tokens = 10285, batch.n_tokens = 2048, progress = 0.809014
slot update_slots: id  3 | task 11761 | n_tokens = 10285, memory_seq_rm [10285, end)
slot update_slots: id  3 | task 11761 | prompt processing progress, n_tokens = 12333, batch.n_tokens = 2048, progress = 0.970109
slot update_slots: id  3 | task 11761 | n_tokens = 12333, memory_seq_rm [12333, end)
slot update_slots: id  3 | task 11761 | prompt processing progress, n_tokens = 12649, batch.n_tokens = 316, progress = 0.994966
slot update_slots: id  3 | task 11761 | n_tokens = 12649, memory_seq_rm [12649, end)
slot update_slots: id  3 | task 11761 | prompt processing progress, n_tokens = 12713, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11761 | prompt done, n_tokens = 12713, batch.n_tokens = 64
slot init_sampler: id  3 | task 11761 | init sampler, took 2.50 ms, tokens: text = 12713, total = 12713
slot update_slots: id  3 | task 11761 | created context checkpoint 5 of 8 (pos_min = 11625, pos_max = 12648, size = 24.012 MiB)
slot print_timing: id  3 | task 11761 | 
prompt eval time =   11505.44 ms / 10620 tokens (    1.08 ms per token,   923.04 tokens per second)
       eval time =   24886.94 ms /   932 tokens (   26.70 ms per token,    37.45 tokens per second)
      total time =   36392.38 ms / 11552 tokens
slot      release: id  3 | task 11761 | stop processing: n_tokens = 13644, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.885 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 12700 | processing task, is_child = 0
slot update_slots: id  3 | task 12700 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14366
slot update_slots: id  3 | task 12700 | n_past = 12714, slot.prompt.tokens.size() = 13644, seq_id = 3, pos_min = 12620, n_swa = 128
slot update_slots: id  3 | task 12700 | restored context checkpoint (pos_min = 11625, pos_max = 12648, size = 24.012 MiB)
slot update_slots: id  3 | task 12700 | n_tokens = 12648, memory_seq_rm [12648, end)
slot update_slots: id  3 | task 12700 | prompt processing progress, n_tokens = 14302, batch.n_tokens = 1654, progress = 0.995545
slot update_slots: id  3 | task 12700 | n_tokens = 14302, memory_seq_rm [14302, end)
slot update_slots: id  3 | task 12700 | prompt processing progress, n_tokens = 14366, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 12700 | prompt done, n_tokens = 14366, batch.n_tokens = 64
slot init_sampler: id  3 | task 12700 | init sampler, took 2.02 ms, tokens: text = 14366, total = 14366
slot update_slots: id  3 | task 12700 | created context checkpoint 6 of 8 (pos_min = 13278, pos_max = 14301, size = 24.012 MiB)
slot print_timing: id  3 | task 12700 | 
prompt eval time =    2251.66 ms /  1718 tokens (    1.31 ms per token,   762.99 tokens per second)
       eval time =   22383.29 ms /   880 tokens (   25.44 ms per token,    39.32 tokens per second)
      total time =   24634.95 ms /  2598 tokens
slot      release: id  3 | task 12700 | stop processing: n_tokens = 15245, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.942
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13582 | processing task, is_child = 0
slot update_slots: id  3 | task 13582 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15300
slot update_slots: id  3 | task 13582 | n_tokens = 14366, memory_seq_rm [14366, end)
slot update_slots: id  3 | task 13582 | prompt processing progress, n_tokens = 15236, batch.n_tokens = 870, progress = 0.995817
slot update_slots: id  3 | task 13582 | n_tokens = 15236, memory_seq_rm [15236, end)
slot update_slots: id  3 | task 13582 | prompt processing progress, n_tokens = 15300, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13582 | prompt done, n_tokens = 15300, batch.n_tokens = 64
slot init_sampler: id  3 | task 13582 | init sampler, took 2.21 ms, tokens: text = 15300, total = 15300
slot update_slots: id  3 | task 13582 | created context checkpoint 7 of 8 (pos_min = 14221, pos_max = 15235, size = 23.801 MiB)
slot print_timing: id  3 | task 13582 | 
prompt eval time =    1334.65 ms /   934 tokens (    1.43 ms per token,   699.81 tokens per second)
       eval time =     770.56 ms /    29 tokens (   26.57 ms per token,    37.63 tokens per second)
      total time =    2105.22 ms /   963 tokens
slot      release: id  3 | task 13582 | stop processing: n_tokens = 15328, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.134
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 15328, total state size = 383.438 MiB
srv          load:  - looking for better prompt, base f_keep = 0.134, sim = 0.987
srv        update:  - cache state: 3 prompts, 1267.541 MiB (limits: 8192.000 MiB, 64000 tokens, 209023 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv  get_availabl: prompt cache update took 426.53 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13613 | processing task, is_child = 0
slot update_slots: id  3 | task 13613 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2089
slot update_slots: id  3 | task 13613 | n_past = 2061, slot.prompt.tokens.size() = 15328, seq_id = 3, pos_min = 14304, n_swa = 128
slot update_slots: id  3 | task 13613 | restored context checkpoint (pos_min = 1864, pos_max = 2654, size = 18.548 MiB)
slot update_slots: id  3 | task 13613 | erased invalidated context checkpoint (pos_min = 11625, pos_max = 12648, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 13613 | erased invalidated context checkpoint (pos_min = 13278, pos_max = 14301, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 13613 | erased invalidated context checkpoint (pos_min = 14221, pos_max = 15235, n_swa = 128, size = 23.801 MiB)
slot update_slots: id  3 | task 13613 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 13613 | prompt processing progress, n_tokens = 2089, batch.n_tokens = 28, progress = 1.000000
slot update_slots: id  3 | task 13613 | prompt done, n_tokens = 2089, batch.n_tokens = 28
slot init_sampler: id  3 | task 13613 | init sampler, took 0.61 ms, tokens: text = 2089, total = 2089
slot print_timing: id  3 | task 13613 | 
prompt eval time =     183.08 ms /    28 tokens (    6.54 ms per token,   152.94 tokens per second)
       eval time =  102244.18 ms /  4100 tokens (   24.94 ms per token,    40.10 tokens per second)
      total time =  102427.26 ms /  4128 tokens
slot      release: id  3 | task 13613 | stop processing: n_tokens = 6188, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.338
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6188, total state size = 169.114 MiB
srv          load:  - looking for better prompt, base f_keep = 0.338, sim = 0.973
srv        update:  - cache state: 4 prompts, 1511.575 MiB (limits: 8192.000 MiB, 64000 tokens, 208813 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv        update:    - prompt 0x5a0b7e703bc0:    6188 tokens, checkpoints:  4,   244.035 MiB
srv  get_availabl: prompt cache update took 162.96 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17714 | processing task, is_child = 0
slot update_slots: id  3 | task 17714 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2146
slot update_slots: id  3 | task 17714 | n_past = 2089, slot.prompt.tokens.size() = 6188, seq_id = 3, pos_min = 5164, n_swa = 128
slot update_slots: id  3 | task 17714 | restored context checkpoint (pos_min = 1864, pos_max = 2654, size = 18.548 MiB)
slot update_slots: id  3 | task 17714 | n_tokens = 2089, memory_seq_rm [2089, end)
slot update_slots: id  3 | task 17714 | prompt processing progress, n_tokens = 2146, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  3 | task 17714 | prompt done, n_tokens = 2146, batch.n_tokens = 57
slot init_sampler: id  3 | task 17714 | init sampler, took 0.38 ms, tokens: text = 2146, total = 2146
slot print_timing: id  3 | task 17714 | 
prompt eval time =     155.67 ms /    57 tokens (    2.73 ms per token,   366.16 tokens per second)
       eval time =    1027.57 ms /    43 tokens (   23.90 ms per token,    41.85 tokens per second)
      total time =    1183.24 ms /   100 tokens
slot      release: id  3 | task 17714 | stop processing: n_tokens = 2188, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.696 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17758 | processing task, is_child = 0
slot update_slots: id  3 | task 17758 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3084
slot update_slots: id  3 | task 17758 | n_tokens = 2146, memory_seq_rm [2146, end)
slot update_slots: id  3 | task 17758 | prompt processing progress, n_tokens = 3020, batch.n_tokens = 874, progress = 0.979248
slot update_slots: id  3 | task 17758 | n_tokens = 3020, memory_seq_rm [3020, end)
slot update_slots: id  3 | task 17758 | prompt processing progress, n_tokens = 3084, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17758 | prompt done, n_tokens = 3084, batch.n_tokens = 64
slot init_sampler: id  3 | task 17758 | init sampler, took 0.52 ms, tokens: text = 3084, total = 3084
slot update_slots: id  3 | task 17758 | created context checkpoint 5 of 8 (pos_min = 2048, pos_max = 3019, size = 22.793 MiB)
slot print_timing: id  3 | task 17758 | 
prompt eval time =    1072.83 ms /   938 tokens (    1.14 ms per token,   874.32 tokens per second)
       eval time =    1772.20 ms /    73 tokens (   24.28 ms per token,    41.19 tokens per second)
      total time =    2845.03 ms /  1011 tokens
slot      release: id  3 | task 17758 | stop processing: n_tokens = 3156, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.766 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17833 | processing task, is_child = 0
slot update_slots: id  3 | task 17833 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4024
slot update_slots: id  3 | task 17833 | n_tokens = 3084, memory_seq_rm [3084, end)
slot update_slots: id  3 | task 17833 | prompt processing progress, n_tokens = 3960, batch.n_tokens = 876, progress = 0.984095
slot update_slots: id  3 | task 17833 | n_tokens = 3960, memory_seq_rm [3960, end)
slot update_slots: id  3 | task 17833 | prompt processing progress, n_tokens = 4024, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17833 | prompt done, n_tokens = 4024, batch.n_tokens = 64
slot init_sampler: id  3 | task 17833 | init sampler, took 0.74 ms, tokens: text = 4024, total = 4024
slot update_slots: id  3 | task 17833 | created context checkpoint 6 of 8 (pos_min = 2936, pos_max = 3959, size = 24.012 MiB)
slot print_timing: id  3 | task 17833 | 
prompt eval time =    1128.00 ms /   940 tokens (    1.20 ms per token,   833.34 tokens per second)
       eval time =    1427.44 ms /    57 tokens (   25.04 ms per token,    39.93 tokens per second)
      total time =    2555.44 ms /   997 tokens
slot      release: id  3 | task 17833 | stop processing: n_tokens = 4080, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.685 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17892 | processing task, is_child = 0
slot update_slots: id  3 | task 17892 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5877
slot update_slots: id  3 | task 17892 | n_tokens = 4024, memory_seq_rm [4024, end)
slot update_slots: id  3 | task 17892 | prompt processing progress, n_tokens = 5813, batch.n_tokens = 1789, progress = 0.989110
slot update_slots: id  3 | task 17892 | n_tokens = 5813, memory_seq_rm [5813, end)
slot update_slots: id  3 | task 17892 | prompt processing progress, n_tokens = 5877, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 17892 | prompt done, n_tokens = 5877, batch.n_tokens = 64
slot init_sampler: id  3 | task 17892 | init sampler, took 0.89 ms, tokens: text = 5877, total = 5877
slot update_slots: id  3 | task 17892 | created context checkpoint 7 of 8 (pos_min = 4789, pos_max = 5812, size = 24.012 MiB)
slot print_timing: id  3 | task 17892 | 
prompt eval time =    2104.18 ms /  1853 tokens (    1.14 ms per token,   880.63 tokens per second)
       eval time =    2715.41 ms /   110 tokens (   24.69 ms per token,    40.51 tokens per second)
      total time =    4819.58 ms /  1963 tokens
slot      release: id  3 | task 17892 | stop processing: n_tokens = 5986, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18004 | processing task, is_child = 0
slot update_slots: id  3 | task 18004 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5959
slot update_slots: id  3 | task 18004 | n_tokens = 5877, memory_seq_rm [5877, end)
slot update_slots: id  3 | task 18004 | prompt processing progress, n_tokens = 5895, batch.n_tokens = 18, progress = 0.989260
slot update_slots: id  3 | task 18004 | n_tokens = 5895, memory_seq_rm [5895, end)
slot update_slots: id  3 | task 18004 | prompt processing progress, n_tokens = 5959, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18004 | prompt done, n_tokens = 5959, batch.n_tokens = 64
slot init_sampler: id  3 | task 18004 | init sampler, took 0.86 ms, tokens: text = 5959, total = 5959
slot update_slots: id  3 | task 18004 | created context checkpoint 8 of 8 (pos_min = 4962, pos_max = 5894, size = 21.878 MiB)
slot print_timing: id  3 | task 18004 | 
prompt eval time =     258.71 ms /    82 tokens (    3.16 ms per token,   316.95 tokens per second)
       eval time =    2975.24 ms /   120 tokens (   24.79 ms per token,    40.33 tokens per second)
      total time =    3233.95 ms /   202 tokens
slot      release: id  3 | task 18004 | stop processing: n_tokens = 6078, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18126 | processing task, is_child = 0
slot update_slots: id  3 | task 18126 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6148
slot update_slots: id  3 | task 18126 | n_tokens = 5959, memory_seq_rm [5959, end)
slot update_slots: id  3 | task 18126 | prompt processing progress, n_tokens = 6084, batch.n_tokens = 125, progress = 0.989590
slot update_slots: id  3 | task 18126 | n_tokens = 6084, memory_seq_rm [6084, end)
slot update_slots: id  3 | task 18126 | prompt processing progress, n_tokens = 6148, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18126 | prompt done, n_tokens = 6148, batch.n_tokens = 64
slot init_sampler: id  3 | task 18126 | init sampler, took 0.90 ms, tokens: text = 6148, total = 6148
slot update_slots: id  3 | task 18126 | erasing old context checkpoint (pos_min = 1363, pos_max = 2272, size = 21.339 MiB)
slot update_slots: id  3 | task 18126 | created context checkpoint 8 of 8 (pos_min = 5060, pos_max = 6083, size = 24.012 MiB)
slot print_timing: id  3 | task 18126 | 
prompt eval time =     467.58 ms /   189 tokens (    2.47 ms per token,   404.21 tokens per second)
       eval time =    2174.79 ms /    86 tokens (   25.29 ms per token,    39.54 tokens per second)
      total time =    2642.37 ms /   275 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 18126 | stop processing: n_tokens = 6233, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.799 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18214 | processing task, is_child = 0
slot update_slots: id  3 | task 18214 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7699
slot update_slots: id  3 | task 18214 | n_tokens = 6148, memory_seq_rm [6148, end)
slot update_slots: id  3 | task 18214 | prompt processing progress, n_tokens = 7635, batch.n_tokens = 1487, progress = 0.991687
slot update_slots: id  3 | task 18214 | n_tokens = 7635, memory_seq_rm [7635, end)
slot update_slots: id  3 | task 18214 | prompt processing progress, n_tokens = 7699, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18214 | prompt done, n_tokens = 7699, batch.n_tokens = 64
slot init_sampler: id  3 | task 18214 | init sampler, took 1.42 ms, tokens: text = 7699, total = 7699
slot update_slots: id  3 | task 18214 | erasing old context checkpoint (pos_min = 1684, pos_max = 2386, size = 16.485 MiB)
slot update_slots: id  3 | task 18214 | created context checkpoint 8 of 8 (pos_min = 6611, pos_max = 7634, size = 24.012 MiB)
slot print_timing: id  3 | task 18214 | 
prompt eval time =    1750.55 ms /  1551 tokens (    1.13 ms per token,   886.01 tokens per second)
       eval time =    3235.83 ms /   129 tokens (   25.08 ms per token,    39.87 tokens per second)
      total time =    4986.38 ms /  1680 tokens
slot      release: id  3 | task 18214 | stop processing: n_tokens = 7827, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18345 | processing task, is_child = 0
slot update_slots: id  3 | task 18345 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8041
slot update_slots: id  3 | task 18345 | n_tokens = 7699, memory_seq_rm [7699, end)
slot update_slots: id  3 | task 18345 | prompt processing progress, n_tokens = 7977, batch.n_tokens = 278, progress = 0.992041
slot update_slots: id  3 | task 18345 | n_tokens = 7977, memory_seq_rm [7977, end)
slot update_slots: id  3 | task 18345 | prompt processing progress, n_tokens = 8041, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 18345 | prompt done, n_tokens = 8041, batch.n_tokens = 64
slot init_sampler: id  3 | task 18345 | init sampler, took 1.14 ms, tokens: text = 8041, total = 8041
slot update_slots: id  3 | task 18345 | erasing old context checkpoint (pos_min = 1710, pos_max = 2500, size = 18.548 MiB)
slot update_slots: id  3 | task 18345 | created context checkpoint 8 of 8 (pos_min = 6953, pos_max = 7976, size = 24.012 MiB)
slot print_timing: id  3 | task 18345 | 
prompt eval time =     537.75 ms /   342 tokens (    1.57 ms per token,   635.98 tokens per second)
       eval time =   32450.07 ms /  1295 tokens (   25.06 ms per token,    39.91 tokens per second)
      total time =   32987.83 ms /  1637 tokens
slot      release: id  3 | task 18345 | stop processing: n_tokens = 9335, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.928 (> 0.100 thold), f_keep = 0.861
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19642 | processing task, is_child = 0
slot update_slots: id  3 | task 19642 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8661
slot update_slots: id  3 | task 19642 | n_past = 8041, slot.prompt.tokens.size() = 9335, seq_id = 3, pos_min = 8311, n_swa = 128
slot update_slots: id  3 | task 19642 | restored context checkpoint (pos_min = 6953, pos_max = 7976, size = 24.012 MiB)
slot update_slots: id  3 | task 19642 | n_tokens = 7976, memory_seq_rm [7976, end)
slot update_slots: id  3 | task 19642 | prompt processing progress, n_tokens = 8597, batch.n_tokens = 621, progress = 0.992611
slot update_slots: id  3 | task 19642 | n_tokens = 8597, memory_seq_rm [8597, end)
slot update_slots: id  3 | task 19642 | prompt processing progress, n_tokens = 8661, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 19642 | prompt done, n_tokens = 8661, batch.n_tokens = 64
slot init_sampler: id  3 | task 19642 | init sampler, took 1.28 ms, tokens: text = 8661, total = 8661
slot update_slots: id  3 | task 19642 | erasing old context checkpoint (pos_min = 1864, pos_max = 2654, size = 18.548 MiB)
slot update_slots: id  3 | task 19642 | created context checkpoint 8 of 8 (pos_min = 7573, pos_max = 8596, size = 24.012 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20043 | processing task, is_child = 0
slot update_slots: id  2 | task 20043 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2112
slot update_slots: id  2 | task 20043 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 20043 | prompt processing progress, n_tokens = 2047, batch.n_tokens = 2048, progress = 0.969223
slot update_slots: id  2 | task 20043 | n_tokens = 2047, memory_seq_rm [2047, end)
slot update_slots: id  2 | task 20043 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2, progress = 0.969697
slot update_slots: id  2 | task 20043 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 20043 | prompt processing progress, n_tokens = 2112, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  2 | task 20043 | prompt done, n_tokens = 2112, batch.n_tokens = 65
slot init_sampler: id  2 | task 20043 | init sampler, took 0.36 ms, tokens: text = 2112, total = 2112
slot update_slots: id  2 | task 20043 | created context checkpoint 1 of 8 (pos_min = 1152, pos_max = 2047, size = 21.011 MiB)
slot print_timing: id  2 | task 20043 | 
prompt eval time =    2448.87 ms /  2112 tokens (    1.16 ms per token,   862.44 tokens per second)
       eval time =    2638.40 ms /    58 tokens (   45.49 ms per token,    21.98 tokens per second)
      total time =    5087.27 ms /  2170 tokens
slot      release: id  2 | task 20043 | stop processing: n_tokens = 2169, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.890 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20110 | processing task, is_child = 0
slot update_slots: id  2 | task 20110 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2373
slot update_slots: id  2 | task 20110 | n_tokens = 2112, memory_seq_rm [2112, end)
slot update_slots: id  2 | task 20110 | prompt processing progress, n_tokens = 2309, batch.n_tokens = 198, progress = 0.973030
slot update_slots: id  2 | task 20110 | n_tokens = 2309, memory_seq_rm [2309, end)
slot update_slots: id  2 | task 20110 | prompt processing progress, n_tokens = 2373, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  2 | task 20110 | prompt done, n_tokens = 2373, batch.n_tokens = 65
slot init_sampler: id  2 | task 20110 | init sampler, took 0.48 ms, tokens: text = 2373, total = 2373
slot update_slots: id  2 | task 20110 | created context checkpoint 2 of 8 (pos_min = 1478, pos_max = 2308, size = 19.486 MiB)
slot print_timing: id  2 | task 20110 | 
prompt eval time =     542.46 ms /   261 tokens (    2.08 ms per token,   481.14 tokens per second)
       eval time =    2162.39 ms /    46 tokens (   47.01 ms per token,    21.27 tokens per second)
      total time =    2704.85 ms /   307 tokens
slot      release: id  2 | task 20110 | stop processing: n_tokens = 2418, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.714 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20162 | processing task, is_child = 0
slot update_slots: id  2 | task 20162 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3324
slot update_slots: id  2 | task 20162 | n_tokens = 2373, memory_seq_rm [2373, end)
slot update_slots: id  2 | task 20162 | prompt processing progress, n_tokens = 3260, batch.n_tokens = 888, progress = 0.980746
slot update_slots: id  2 | task 20162 | n_tokens = 3260, memory_seq_rm [3260, end)
slot update_slots: id  2 | task 20162 | prompt processing progress, n_tokens = 3324, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  2 | task 20162 | prompt done, n_tokens = 3324, batch.n_tokens = 65
slot init_sampler: id  2 | task 20162 | init sampler, took 0.58 ms, tokens: text = 3324, total = 3324
slot update_slots: id  2 | task 20162 | created context checkpoint 3 of 8 (pos_min = 2364, pos_max = 3259, size = 21.011 MiB)
slot print_timing: id  2 | task 20162 | 
prompt eval time =    1292.60 ms /   951 tokens (    1.36 ms per token,   735.73 tokens per second)
       eval time =    8956.91 ms /   192 tokens (   46.65 ms per token,    21.44 tokens per second)
      total time =   10249.51 ms /  1143 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 20162 | stop processing: n_tokens = 3515, truncated = 0
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.641 (> 0.100 thold), f_keep = 0.946
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20365 | processing task, is_child = 0
slot update_slots: id  2 | task 20365 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5182
slot update_slots: id  2 | task 20365 | n_tokens = 3324, memory_seq_rm [3324, end)
slot update_slots: id  2 | task 20365 | prompt processing progress, n_tokens = 5118, batch.n_tokens = 1795, progress = 0.987650
slot update_slots: id  2 | task 20365 | n_tokens = 5118, memory_seq_rm [5118, end)
slot update_slots: id  2 | task 20365 | prompt processing progress, n_tokens = 5182, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  2 | task 20365 | prompt done, n_tokens = 5182, batch.n_tokens = 65
slot init_sampler: id  2 | task 20365 | init sampler, took 0.97 ms, tokens: text = 5182, total = 5182
slot update_slots: id  2 | task 20365 | created context checkpoint 4 of 8 (pos_min = 4221, pos_max = 5117, size = 21.034 MiB)
slot print_timing: id  3 | task 19642 | 
prompt eval time =    1037.02 ms /   685 tokens (    1.51 ms per token,   660.55 tokens per second)
       eval time =   37067.39 ms /   853 tokens (   43.46 ms per token,    23.01 tokens per second)
      total time =   38104.40 ms /  1538 tokens
slot      release: id  3 | task 19642 | stop processing: n_tokens = 9513, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  2 | task 20365 | 
prompt eval time =    2425.52 ms /  1858 tokens (    1.31 ms per token,   766.02 tokens per second)
       eval time =   13502.85 ms /   416 tokens (   32.46 ms per token,    30.81 tokens per second)
      total time =   15928.37 ms /  2274 tokens
slot      release: id  2 | task 20365 | stop processing: n_tokens = 5597, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.769 (> 0.100 thold), f_keep = 0.926
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20783 | processing task, is_child = 0
slot update_slots: id  2 | task 20783 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6738
slot update_slots: id  2 | task 20783 | n_tokens = 5182, memory_seq_rm [5182, end)
slot update_slots: id  2 | task 20783 | prompt processing progress, n_tokens = 6674, batch.n_tokens = 1492, progress = 0.990502
slot update_slots: id  2 | task 20783 | n_tokens = 6674, memory_seq_rm [6674, end)
slot update_slots: id  2 | task 20783 | prompt processing progress, n_tokens = 6738, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 20783 | prompt done, n_tokens = 6738, batch.n_tokens = 64
slot init_sampler: id  2 | task 20783 | init sampler, took 1.02 ms, tokens: text = 6738, total = 6738
slot update_slots: id  2 | task 20783 | created context checkpoint 5 of 8 (pos_min = 5777, pos_max = 6673, size = 21.034 MiB)
slot print_timing: id  2 | task 20783 | 
prompt eval time =    2013.00 ms /  1556 tokens (    1.29 ms per token,   772.97 tokens per second)
       eval time =   47673.10 ms /  1825 tokens (   26.12 ms per token,    38.28 tokens per second)
      total time =   49686.10 ms /  3381 tokens
slot      release: id  2 | task 20783 | stop processing: n_tokens = 8562, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.844 (> 0.100 thold), f_keep = 0.787
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 22610 | processing task, is_child = 0
slot update_slots: id  2 | task 22610 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7982
slot update_slots: id  2 | task 22610 | n_past = 6738, slot.prompt.tokens.size() = 8562, seq_id = 2, pos_min = 7665, n_swa = 128
slot update_slots: id  2 | task 22610 | restored context checkpoint (pos_min = 5777, pos_max = 6673, size = 21.034 MiB)
slot update_slots: id  2 | task 22610 | n_tokens = 6673, memory_seq_rm [6673, end)
slot update_slots: id  2 | task 22610 | prompt processing progress, n_tokens = 7918, batch.n_tokens = 1245, progress = 0.991982
slot update_slots: id  2 | task 22610 | n_tokens = 7918, memory_seq_rm [7918, end)
slot update_slots: id  2 | task 22610 | prompt processing progress, n_tokens = 7982, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 22610 | prompt done, n_tokens = 7982, batch.n_tokens = 64
slot init_sampler: id  2 | task 22610 | init sampler, took 1.51 ms, tokens: text = 7982, total = 7982
slot update_slots: id  2 | task 22610 | created context checkpoint 6 of 8 (pos_min = 7021, pos_max = 7917, size = 21.034 MiB)
slot print_timing: id  2 | task 22610 | 
prompt eval time =    2021.01 ms /  1309 tokens (    1.54 ms per token,   647.70 tokens per second)
       eval time =   59393.26 ms /  2274 tokens (   26.12 ms per token,    38.29 tokens per second)
      total time =   61414.27 ms /  3583 tokens
slot      release: id  2 | task 22610 | stop processing: n_tokens = 10255, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.778
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 24886 | processing task, is_child = 0
slot update_slots: id  2 | task 24886 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8069
slot update_slots: id  2 | task 24886 | n_past = 7982, slot.prompt.tokens.size() = 10255, seq_id = 2, pos_min = 9358, n_swa = 128
slot update_slots: id  2 | task 24886 | restored context checkpoint (pos_min = 7021, pos_max = 7917, size = 21.034 MiB)
slot update_slots: id  2 | task 24886 | n_tokens = 7917, memory_seq_rm [7917, end)
slot update_slots: id  2 | task 24886 | prompt processing progress, n_tokens = 8005, batch.n_tokens = 88, progress = 0.992068
slot update_slots: id  2 | task 24886 | n_tokens = 8005, memory_seq_rm [8005, end)
slot update_slots: id  2 | task 24886 | prompt processing progress, n_tokens = 8069, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 24886 | prompt done, n_tokens = 8069, batch.n_tokens = 64
slot init_sampler: id  2 | task 24886 | init sampler, took 1.19 ms, tokens: text = 8069, total = 8069
slot update_slots: id  2 | task 24886 | created context checkpoint 7 of 8 (pos_min = 7108, pos_max = 8004, size = 21.034 MiB)
slot print_timing: id  2 | task 24886 | 
prompt eval time =     604.62 ms /   152 tokens (    3.98 ms per token,   251.40 tokens per second)
       eval time =   36594.05 ms /  1405 tokens (   26.05 ms per token,    38.39 tokens per second)
      total time =   37198.67 ms /  1557 tokens
slot      release: id  2 | task 24886 | stop processing: n_tokens = 9473, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.217
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9513, total state size = 226.048 MiB
srv          load:  - looking for better prompt, base f_keep = 0.217, sim = 0.997
srv        update:  - cache state: 5 prompts, 1926.367 MiB (limits: 8192.000 MiB, 64000 tokens, 204306 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv        update:    - prompt 0x5a0b7e703bc0:    6188 tokens, checkpoints:  4,   244.035 MiB
srv        update:    - prompt 0x5a0b83e530c0:    9513 tokens, checkpoints:  8,   414.791 MiB
srv  get_availabl: prompt cache update took 434.45 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26293 | processing task, is_child = 0
slot update_slots: id  3 | task 26293 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2067
slot update_slots: id  3 | task 26293 | n_past = 2061, slot.prompt.tokens.size() = 9513, seq_id = 3, pos_min = 9386, n_swa = 128
slot update_slots: id  3 | task 26293 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 26293 | erased invalidated context checkpoint (pos_min = 2048, pos_max = 3019, n_swa = 128, size = 22.793 MiB)
slot update_slots: id  3 | task 26293 | erased invalidated context checkpoint (pos_min = 2936, pos_max = 3959, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 26293 | erased invalidated context checkpoint (pos_min = 4789, pos_max = 5812, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 26293 | erased invalidated context checkpoint (pos_min = 4962, pos_max = 5894, n_swa = 128, size = 21.878 MiB)
slot update_slots: id  3 | task 26293 | erased invalidated context checkpoint (pos_min = 5060, pos_max = 6083, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 26293 | erased invalidated context checkpoint (pos_min = 6611, pos_max = 7634, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 26293 | erased invalidated context checkpoint (pos_min = 6953, pos_max = 7976, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 26293 | erased invalidated context checkpoint (pos_min = 7573, pos_max = 8596, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 26293 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 26293 | prompt processing progress, n_tokens = 2003, batch.n_tokens = 2003, progress = 0.969037
slot update_slots: id  3 | task 26293 | n_tokens = 2003, memory_seq_rm [2003, end)
slot update_slots: id  3 | task 26293 | prompt processing progress, n_tokens = 2067, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 26293 | prompt done, n_tokens = 2067, batch.n_tokens = 64
slot init_sampler: id  3 | task 26293 | init sampler, took 0.38 ms, tokens: text = 2067, total = 2067
slot update_slots: id  3 | task 26293 | created context checkpoint 1 of 8 (pos_min = 1106, pos_max = 2002, size = 21.034 MiB)
slot print_timing: id  3 | task 26293 | 
prompt eval time =    2649.65 ms /  2067 tokens (    1.28 ms per token,   780.10 tokens per second)
       eval time =    1375.87 ms /    55 tokens (   25.02 ms per token,    39.97 tokens per second)
      total time =    4025.52 ms /  2122 tokens
slot      release: id  3 | task 26293 | stop processing: n_tokens = 2121, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26350 | processing task, is_child = 0
slot update_slots: id  3 | task 26350 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2081
slot update_slots: id  3 | task 26350 | n_tokens = 2068, memory_seq_rm [2068, end)
slot update_slots: id  3 | task 26350 | prompt processing progress, n_tokens = 2081, batch.n_tokens = 13, progress = 1.000000
slot update_slots: id  3 | task 26350 | prompt done, n_tokens = 2081, batch.n_tokens = 13
slot init_sampler: id  3 | task 26350 | init sampler, took 0.34 ms, tokens: text = 2081, total = 2081
slot update_slots: id  3 | task 26350 | created context checkpoint 2 of 8 (pos_min = 1224, pos_max = 2067, size = 19.791 MiB)
slot print_timing: id  3 | task 26350 | 
prompt eval time =     141.22 ms /    13 tokens (   10.86 ms per token,    92.06 tokens per second)
       eval time =     612.12 ms /    25 tokens (   24.48 ms per token,    40.84 tokens per second)
      total time =     753.34 ms /    38 tokens
slot      release: id  3 | task 26350 | stop processing: n_tokens = 2105, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26376 | processing task, is_child = 0
slot update_slots: id  3 | task 26376 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2069
slot update_slots: id  3 | task 26376 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 26376 | prompt processing progress, n_tokens = 2069, batch.n_tokens = 8, progress = 1.000000
slot update_slots: id  3 | task 26376 | prompt done, n_tokens = 2069, batch.n_tokens = 8
slot init_sampler: id  3 | task 26376 | init sampler, took 0.38 ms, tokens: text = 2069, total = 2069
slot print_timing: id  3 | task 26376 | 
prompt eval time =     102.81 ms /     8 tokens (   12.85 ms per token,    77.81 tokens per second)
       eval time =    3325.10 ms /   129 tokens (   25.78 ms per token,    38.80 tokens per second)
      total time =    3427.91 ms /   137 tokens
slot      release: id  3 | task 26376 | stop processing: n_tokens = 2197, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.942
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26506 | processing task, is_child = 0
slot update_slots: id  3 | task 26506 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2101
slot update_slots: id  3 | task 26506 | n_tokens = 2070, memory_seq_rm [2070, end)
slot update_slots: id  3 | task 26506 | prompt processing progress, n_tokens = 2101, batch.n_tokens = 31, progress = 1.000000
slot update_slots: id  3 | task 26506 | prompt done, n_tokens = 2101, batch.n_tokens = 31
slot init_sampler: id  3 | task 26506 | init sampler, took 0.36 ms, tokens: text = 2101, total = 2101
slot print_timing: id  3 | task 26506 | 
prompt eval time =     197.74 ms /    31 tokens (    6.38 ms per token,   156.77 tokens per second)
       eval time =    1650.34 ms /    68 tokens (   24.27 ms per token,    41.20 tokens per second)
      total time =    1848.08 ms /    99 tokens
slot      release: id  3 | task 26506 | stop processing: n_tokens = 2168, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26575 | processing task, is_child = 0
slot update_slots: id  3 | task 26575 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2149
slot update_slots: id  3 | task 26575 | n_tokens = 2101, memory_seq_rm [2101, end)
slot update_slots: id  3 | task 26575 | prompt processing progress, n_tokens = 2149, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 26575 | prompt done, n_tokens = 2149, batch.n_tokens = 48
slot init_sampler: id  3 | task 26575 | init sampler, took 0.36 ms, tokens: text = 2149, total = 2149
slot print_timing: id  3 | task 26575 | 
prompt eval time =     136.55 ms /    48 tokens (    2.84 ms per token,   351.52 tokens per second)
       eval time =     396.27 ms /    17 tokens (   23.31 ms per token,    42.90 tokens per second)
      total time =     532.82 ms /    65 tokens
slot      release: id  3 | task 26575 | stop processing: n_tokens = 2165, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.959 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26593 | processing task, is_child = 0
slot update_slots: id  3 | task 26593 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2191
slot update_slots: id  3 | task 26593 | n_tokens = 2101, memory_seq_rm [2101, end)
slot update_slots: id  3 | task 26593 | prompt processing progress, n_tokens = 2127, batch.n_tokens = 26, progress = 0.970790
slot update_slots: id  3 | task 26593 | n_tokens = 2127, memory_seq_rm [2127, end)
slot update_slots: id  3 | task 26593 | prompt processing progress, n_tokens = 2191, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 26593 | prompt done, n_tokens = 2191, batch.n_tokens = 64
slot init_sampler: id  3 | task 26593 | init sampler, took 0.42 ms, tokens: text = 2191, total = 2191
slot print_timing: id  3 | task 26593 | 
prompt eval time =     424.06 ms /    90 tokens (    4.71 ms per token,   212.23 tokens per second)
       eval time =     877.44 ms /    36 tokens (   24.37 ms per token,    41.03 tokens per second)
      total time =    1301.50 ms /   126 tokens
slot      release: id  3 | task 26593 | stop processing: n_tokens = 2226, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26631 | processing task, is_child = 0
slot update_slots: id  3 | task 26631 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2250
slot update_slots: id  3 | task 26631 | n_tokens = 2191, memory_seq_rm [2191, end)
slot update_slots: id  3 | task 26631 | prompt processing progress, n_tokens = 2250, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 26631 | prompt done, n_tokens = 2250, batch.n_tokens = 59
slot init_sampler: id  3 | task 26631 | init sampler, took 0.41 ms, tokens: text = 2250, total = 2250
slot update_slots: id  3 | task 26631 | created context checkpoint 3 of 8 (pos_min = 1383, pos_max = 2190, size = 18.947 MiB)
slot print_timing: id  3 | task 26631 | 
prompt eval time =     256.08 ms /    59 tokens (    4.34 ms per token,   230.40 tokens per second)
       eval time =    1547.80 ms /    63 tokens (   24.57 ms per token,    40.70 tokens per second)
      total time =    1803.88 ms /   122 tokens
slot      release: id  3 | task 26631 | stop processing: n_tokens = 2312, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.892
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 26695 | processing task, is_child = 0
slot update_slots: id  3 | task 26695 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2068
slot update_slots: id  3 | task 26695 | n_tokens = 2062, memory_seq_rm [2062, end)
slot update_slots: id  3 | task 26695 | prompt processing progress, n_tokens = 2068, batch.n_tokens = 6, progress = 1.000000
slot update_slots: id  3 | task 26695 | prompt done, n_tokens = 2068, batch.n_tokens = 6
slot init_sampler: id  3 | task 26695 | init sampler, took 0.33 ms, tokens: text = 2068, total = 2068
slot print_timing: id  3 | task 26695 | 
prompt eval time =      98.17 ms /     6 tokens (   16.36 ms per token,    61.12 tokens per second)
       eval time =   12940.03 ms /   508 tokens (   25.47 ms per token,    39.26 tokens per second)
      total time =   13038.20 ms /   514 tokens
slot      release: id  3 | task 26695 | stop processing: n_tokens = 2575, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.803
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27204 | processing task, is_child = 0
slot update_slots: id  3 | task 27204 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2094
slot update_slots: id  3 | task 27204 | n_tokens = 2069, memory_seq_rm [2069, end)
slot update_slots: id  3 | task 27204 | prompt processing progress, n_tokens = 2094, batch.n_tokens = 25, progress = 1.000000
slot update_slots: id  3 | task 27204 | prompt done, n_tokens = 2094, batch.n_tokens = 25
slot init_sampler: id  3 | task 27204 | init sampler, took 0.35 ms, tokens: text = 2094, total = 2094
slot print_timing: id  3 | task 27204 | 
prompt eval time =     186.91 ms /    25 tokens (    7.48 ms per token,   133.75 tokens per second)
       eval time =    1408.29 ms /    55 tokens (   25.61 ms per token,    39.05 tokens per second)
      total time =    1595.21 ms /    80 tokens
slot      release: id  3 | task 27204 | stop processing: n_tokens = 2148, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27260 | processing task, is_child = 0
slot update_slots: id  3 | task 27260 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2121
slot update_slots: id  3 | task 27260 | n_tokens = 2095, memory_seq_rm [2095, end)
slot update_slots: id  3 | task 27260 | prompt processing progress, n_tokens = 2121, batch.n_tokens = 26, progress = 1.000000
slot update_slots: id  3 | task 27260 | prompt done, n_tokens = 2121, batch.n_tokens = 26
slot init_sampler: id  3 | task 27260 | init sampler, took 0.34 ms, tokens: text = 2121, total = 2121
slot print_timing: id  3 | task 27260 | 
prompt eval time =     191.81 ms /    26 tokens (    7.38 ms per token,   135.55 tokens per second)
       eval time =    1423.65 ms /    56 tokens (   25.42 ms per token,    39.34 tokens per second)
      total time =    1615.46 ms /    82 tokens
slot      release: id  3 | task 27260 | stop processing: n_tokens = 2176, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.947
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27317 | processing task, is_child = 0
slot update_slots: id  3 | task 27317 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2069
slot update_slots: id  3 | task 27317 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 27317 | prompt processing progress, n_tokens = 2069, batch.n_tokens = 8, progress = 1.000000
slot update_slots: id  3 | task 27317 | prompt done, n_tokens = 2069, batch.n_tokens = 8
slot init_sampler: id  3 | task 27317 | init sampler, took 0.35 ms, tokens: text = 2069, total = 2069
slot print_timing: id  3 | task 27317 | 
prompt eval time =     106.37 ms /     8 tokens (   13.30 ms per token,    75.21 tokens per second)
       eval time =    4116.88 ms /   159 tokens (   25.89 ms per token,    38.62 tokens per second)
      total time =    4223.25 ms /   167 tokens
slot      release: id  3 | task 27317 | stop processing: n_tokens = 2227, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.930
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27477 | processing task, is_child = 0
slot update_slots: id  3 | task 27477 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2101
slot update_slots: id  3 | task 27477 | n_tokens = 2070, memory_seq_rm [2070, end)
slot update_slots: id  3 | task 27477 | prompt processing progress, n_tokens = 2101, batch.n_tokens = 31, progress = 1.000000
slot update_slots: id  3 | task 27477 | prompt done, n_tokens = 2101, batch.n_tokens = 31
slot init_sampler: id  3 | task 27477 | init sampler, took 0.42 ms, tokens: text = 2101, total = 2101
slot print_timing: id  3 | task 27477 | 
prompt eval time =     203.75 ms /    31 tokens (    6.57 ms per token,   152.15 tokens per second)
       eval time =    1386.95 ms /    54 tokens (   25.68 ms per token,    38.93 tokens per second)
      total time =    1590.70 ms /    85 tokens
slot      release: id  3 | task 27477 | stop processing: n_tokens = 2154, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27532 | processing task, is_child = 0
slot update_slots: id  3 | task 27532 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2149
slot update_slots: id  3 | task 27532 | n_tokens = 2101, memory_seq_rm [2101, end)
slot update_slots: id  3 | task 27532 | prompt processing progress, n_tokens = 2149, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 27532 | prompt done, n_tokens = 2149, batch.n_tokens = 48
slot init_sampler: id  3 | task 27532 | init sampler, took 0.48 ms, tokens: text = 2149, total = 2149
slot print_timing: id  3 | task 27532 | 
prompt eval time =     143.31 ms /    48 tokens (    2.99 ms per token,   334.93 tokens per second)
       eval time =     412.64 ms /    17 tokens (   24.27 ms per token,    41.20 tokens per second)
      total time =     555.95 ms /    65 tokens
slot      release: id  3 | task 27532 | stop processing: n_tokens = 2165, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.956
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27550 | processing task, is_child = 0
slot update_slots: id  3 | task 27550 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2069
slot update_slots: id  3 | task 27550 | need to evaluate at least 1 token for each active slot (n_past = 2069, task.n_tokens() = 2069)
slot update_slots: id  3 | task 27550 | n_past was set to 2068
slot update_slots: id  3 | task 27550 | n_tokens = 2068, memory_seq_rm [2068, end)
slot update_slots: id  3 | task 27550 | prompt processing progress, n_tokens = 2069, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  3 | task 27550 | prompt done, n_tokens = 2069, batch.n_tokens = 1
slot init_sampler: id  3 | task 27550 | init sampler, took 0.32 ms, tokens: text = 2069, total = 2069
slot print_timing: id  3 | task 27550 | 
prompt eval time =      36.32 ms /     1 tokens (   36.32 ms per token,    27.53 tokens per second)
       eval time =   14476.15 ms /   537 tokens (   26.96 ms per token,    37.10 tokens per second)
      total time =   14512.47 ms /   538 tokens
slot      release: id  3 | task 27550 | stop processing: n_tokens = 2605, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.791
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28088 | processing task, is_child = 0
slot update_slots: id  3 | task 28088 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2068
slot update_slots: id  3 | task 28088 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 28088 | prompt processing progress, n_tokens = 2068, batch.n_tokens = 7, progress = 1.000000
slot update_slots: id  3 | task 28088 | prompt done, n_tokens = 2068, batch.n_tokens = 7
slot init_sampler: id  3 | task 28088 | init sampler, took 0.32 ms, tokens: text = 2068, total = 2068
slot print_timing: id  3 | task 28088 | 
prompt eval time =     102.03 ms /     7 tokens (   14.58 ms per token,    68.61 tokens per second)
       eval time =    3954.23 ms /   155 tokens (   25.51 ms per token,    39.20 tokens per second)
      total time =    4056.26 ms /   162 tokens
slot      release: id  3 | task 28088 | stop processing: n_tokens = 2222, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.931
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28244 | processing task, is_child = 0
slot update_slots: id  3 | task 28244 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2116
slot update_slots: id  3 | task 28244 | n_tokens = 2068, memory_seq_rm [2068, end)
slot update_slots: id  3 | task 28244 | prompt processing progress, n_tokens = 2116, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 28244 | prompt done, n_tokens = 2116, batch.n_tokens = 48
slot init_sampler: id  3 | task 28244 | init sampler, took 0.37 ms, tokens: text = 2116, total = 2116
slot print_timing: id  3 | task 28244 | 
prompt eval time =     138.98 ms /    48 tokens (    2.90 ms per token,   345.38 tokens per second)
       eval time =    1303.69 ms /    51 tokens (   25.56 ms per token,    39.12 tokens per second)
      total time =    1442.66 ms /    99 tokens
slot      release: id  3 | task 28244 | stop processing: n_tokens = 2166, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28296 | processing task, is_child = 0
slot update_slots: id  3 | task 28296 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2159
slot update_slots: id  3 | task 28296 | n_tokens = 2068, memory_seq_rm [2068, end)
slot update_slots: id  3 | task 28296 | prompt processing progress, n_tokens = 2095, batch.n_tokens = 27, progress = 0.970357
slot update_slots: id  3 | task 28296 | n_tokens = 2095, memory_seq_rm [2095, end)
slot update_slots: id  3 | task 28296 | prompt processing progress, n_tokens = 2159, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 28296 | prompt done, n_tokens = 2159, batch.n_tokens = 64
slot init_sampler: id  3 | task 28296 | init sampler, took 0.34 ms, tokens: text = 2159, total = 2159
slot print_timing: id  3 | task 28296 | 
prompt eval time =     401.70 ms /    91 tokens (    4.41 ms per token,   226.54 tokens per second)
       eval time =     831.51 ms /    33 tokens (   25.20 ms per token,    39.69 tokens per second)
      total time =    1233.21 ms /   124 tokens
slot      release: id  3 | task 28296 | stop processing: n_tokens = 2191, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28331 | processing task, is_child = 0
slot update_slots: id  3 | task 28331 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2218
slot update_slots: id  3 | task 28331 | n_tokens = 2159, memory_seq_rm [2159, end)
slot update_slots: id  3 | task 28331 | prompt processing progress, n_tokens = 2218, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 28331 | prompt done, n_tokens = 2218, batch.n_tokens = 59
slot init_sampler: id  3 | task 28331 | init sampler, took 0.32 ms, tokens: text = 2218, total = 2218
slot print_timing: id  3 | task 28331 | 
prompt eval time =     242.81 ms /    59 tokens (    4.12 ms per token,   242.98 tokens per second)
       eval time =     906.18 ms /    36 tokens (   25.17 ms per token,    39.73 tokens per second)
      total time =    1149.00 ms /    95 tokens
slot      release: id  3 | task 28331 | stop processing: n_tokens = 2253, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.915
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28368 | processing task, is_child = 0
slot update_slots: id  3 | task 28368 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2068
slot update_slots: id  3 | task 28368 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 28368 | prompt processing progress, n_tokens = 2068, batch.n_tokens = 7, progress = 1.000000
slot update_slots: id  3 | task 28368 | prompt done, n_tokens = 2068, batch.n_tokens = 7
slot init_sampler: id  3 | task 28368 | init sampler, took 0.36 ms, tokens: text = 2068, total = 2068
slot print_timing: id  3 | task 28368 | 
prompt eval time =     103.40 ms /     7 tokens (   14.77 ms per token,    67.70 tokens per second)
       eval time =    7596.64 ms /   285 tokens (   26.65 ms per token,    37.52 tokens per second)
      total time =    7700.04 ms /   292 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 28368 | stop processing: n_tokens = 2352, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.879
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28654 | processing task, is_child = 0
slot update_slots: id  3 | task 28654 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2126
slot update_slots: id  3 | task 28654 | n_tokens = 2068, memory_seq_rm [2068, end)
slot update_slots: id  3 | task 28654 | prompt processing progress, n_tokens = 2126, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  3 | task 28654 | prompt done, n_tokens = 2126, batch.n_tokens = 58
slot init_sampler: id  3 | task 28654 | init sampler, took 0.42 ms, tokens: text = 2126, total = 2126
slot print_timing: id  3 | task 28654 | 
prompt eval time =     229.84 ms /    58 tokens (    3.96 ms per token,   252.35 tokens per second)
       eval time =    1173.15 ms /    44 tokens (   26.66 ms per token,    37.51 tokens per second)
      total time =    1402.99 ms /   102 tokens
slot      release: id  3 | task 28654 | stop processing: n_tokens = 2169, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28699 | processing task, is_child = 0
slot update_slots: id  3 | task 28699 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2180
slot update_slots: id  3 | task 28699 | n_tokens = 2068, memory_seq_rm [2068, end)
slot update_slots: id  3 | task 28699 | prompt processing progress, n_tokens = 2116, batch.n_tokens = 48, progress = 0.970642
slot update_slots: id  3 | task 28699 | n_tokens = 2116, memory_seq_rm [2116, end)
slot update_slots: id  3 | task 28699 | prompt processing progress, n_tokens = 2180, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 28699 | prompt done, n_tokens = 2180, batch.n_tokens = 64
slot init_sampler: id  3 | task 28699 | init sampler, took 0.34 ms, tokens: text = 2180, total = 2180
slot print_timing: id  3 | task 28699 | 
prompt eval time =     440.76 ms /   112 tokens (    3.94 ms per token,   254.11 tokens per second)
       eval time =    2300.62 ms /    90 tokens (   25.56 ms per token,    39.12 tokens per second)
      total time =    2741.38 ms /   202 tokens
slot      release: id  3 | task 28699 | stop processing: n_tokens = 2269, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.539 (> 0.100 thold), f_keep = 0.907
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28791 | processing task, is_child = 0
slot update_slots: id  3 | task 28791 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3820
slot update_slots: id  3 | task 28791 | n_tokens = 2059, memory_seq_rm [2059, end)
slot update_slots: id  3 | task 28791 | prompt processing progress, n_tokens = 3756, batch.n_tokens = 1697, progress = 0.983246
slot update_slots: id  3 | task 28791 | n_tokens = 3756, memory_seq_rm [3756, end)
slot update_slots: id  3 | task 28791 | prompt processing progress, n_tokens = 3820, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 28791 | prompt done, n_tokens = 3820, batch.n_tokens = 64
slot init_sampler: id  3 | task 28791 | init sampler, took 0.60 ms, tokens: text = 3820, total = 3820
slot update_slots: id  3 | task 28791 | created context checkpoint 4 of 8 (pos_min = 2859, pos_max = 3755, size = 21.034 MiB)
slot print_timing: id  3 | task 28791 | 
prompt eval time =    2464.49 ms /  1761 tokens (    1.40 ms per token,   714.55 tokens per second)
       eval time =    1137.01 ms /    45 tokens (   25.27 ms per token,    39.58 tokens per second)
      total time =    3601.51 ms /  1806 tokens
slot      release: id  3 | task 28791 | stop processing: n_tokens = 3864, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.802 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28838 | processing task, is_child = 0
slot update_slots: id  3 | task 28838 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4766
slot update_slots: id  3 | task 28838 | n_tokens = 3820, memory_seq_rm [3820, end)
slot update_slots: id  3 | task 28838 | prompt processing progress, n_tokens = 4702, batch.n_tokens = 882, progress = 0.986572
slot update_slots: id  3 | task 28838 | n_tokens = 4702, memory_seq_rm [4702, end)
slot update_slots: id  3 | task 28838 | prompt processing progress, n_tokens = 4766, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 28838 | prompt done, n_tokens = 4766, batch.n_tokens = 64
slot init_sampler: id  3 | task 28838 | init sampler, took 0.70 ms, tokens: text = 4766, total = 4766
slot update_slots: id  3 | task 28838 | created context checkpoint 5 of 8 (pos_min = 3820, pos_max = 4701, size = 20.682 MiB)
slot print_timing: id  3 | task 28838 | 
prompt eval time =    1346.59 ms /   946 tokens (    1.42 ms per token,   702.52 tokens per second)
       eval time =    2908.01 ms /   111 tokens (   26.20 ms per token,    38.17 tokens per second)
      total time =    4254.60 ms /  1057 tokens
slot      release: id  3 | task 28838 | stop processing: n_tokens = 4876, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.714 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28951 | processing task, is_child = 0
slot update_slots: id  3 | task 28951 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6671
slot update_slots: id  3 | task 28951 | n_tokens = 4766, memory_seq_rm [4766, end)
slot update_slots: id  3 | task 28951 | prompt processing progress, n_tokens = 6607, batch.n_tokens = 1841, progress = 0.990406
slot update_slots: id  3 | task 28951 | n_tokens = 6607, memory_seq_rm [6607, end)
slot update_slots: id  3 | task 28951 | prompt processing progress, n_tokens = 6671, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 28951 | prompt done, n_tokens = 6671, batch.n_tokens = 64
slot init_sampler: id  3 | task 28951 | init sampler, took 1.25 ms, tokens: text = 6671, total = 6671
slot update_slots: id  3 | task 28951 | created context checkpoint 6 of 8 (pos_min = 5710, pos_max = 6606, size = 21.034 MiB)
slot print_timing: id  3 | task 28951 | 
prompt eval time =    2626.80 ms /  1905 tokens (    1.38 ms per token,   725.22 tokens per second)
       eval time =    3842.43 ms /   144 tokens (   26.68 ms per token,    37.48 tokens per second)
      total time =    6469.23 ms /  2049 tokens
slot      release: id  3 | task 28951 | stop processing: n_tokens = 6814, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29097 | processing task, is_child = 0
slot update_slots: id  3 | task 29097 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7141
slot update_slots: id  3 | task 29097 | n_tokens = 6671, memory_seq_rm [6671, end)
slot update_slots: id  3 | task 29097 | prompt processing progress, n_tokens = 7077, batch.n_tokens = 406, progress = 0.991038
slot update_slots: id  3 | task 29097 | n_tokens = 7077, memory_seq_rm [7077, end)
slot update_slots: id  3 | task 29097 | prompt processing progress, n_tokens = 7141, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29097 | prompt done, n_tokens = 7141, batch.n_tokens = 64
slot init_sampler: id  3 | task 29097 | init sampler, took 1.03 ms, tokens: text = 7141, total = 7141
slot update_slots: id  3 | task 29097 | created context checkpoint 7 of 8 (pos_min = 6180, pos_max = 7076, size = 21.034 MiB)
slot print_timing: id  3 | task 29097 | 
prompt eval time =     760.51 ms /   470 tokens (    1.62 ms per token,   618.01 tokens per second)
       eval time =    3993.38 ms /   145 tokens (   27.54 ms per token,    36.31 tokens per second)
      total time =    4753.89 ms /   615 tokens
slot      release: id  3 | task 29097 | stop processing: n_tokens = 7285, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.832 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29244 | processing task, is_child = 0
slot update_slots: id  3 | task 29244 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8578
slot update_slots: id  3 | task 29244 | n_tokens = 7141, memory_seq_rm [7141, end)
slot update_slots: id  3 | task 29244 | prompt processing progress, n_tokens = 8514, batch.n_tokens = 1373, progress = 0.992539
slot update_slots: id  3 | task 29244 | n_tokens = 8514, memory_seq_rm [8514, end)
slot update_slots: id  3 | task 29244 | prompt processing progress, n_tokens = 8578, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29244 | prompt done, n_tokens = 8578, batch.n_tokens = 64
slot init_sampler: id  3 | task 29244 | init sampler, took 1.24 ms, tokens: text = 8578, total = 8578
slot update_slots: id  3 | task 29244 | created context checkpoint 8 of 8 (pos_min = 7617, pos_max = 8513, size = 21.034 MiB)
slot print_timing: id  3 | task 29244 | 
prompt eval time =    2091.91 ms /  1437 tokens (    1.46 ms per token,   686.93 tokens per second)
       eval time =   31130.65 ms /  1160 tokens (   26.84 ms per token,    37.26 tokens per second)
      total time =   33222.56 ms /  2597 tokens
slot      release: id  3 | task 29244 | stop processing: n_tokens = 9737, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.431 (> 0.100 thold), f_keep = 0.392
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9737, total state size = 249.357 MiB
srv          load:  - looking for better prompt, base f_keep = 0.392, sim = 0.431
srv        update:  - cache state: 6 prompts, 2340.314 MiB (limits: 8192.000 MiB, 64000 tokens, 202252 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv        update:    - prompt 0x5a0b7e703bc0:    6188 tokens, checkpoints:  4,   244.035 MiB
srv        update:    - prompt 0x5a0b83e530c0:    9513 tokens, checkpoints:  8,   414.791 MiB
srv        update:    - prompt 0x5a0b7f017ff0:    9737 tokens, checkpoints:  8,   413.947 MiB
srv  get_availabl: prompt cache update took 421.24 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 30406 | processing task, is_child = 0
slot update_slots: id  3 | task 30406 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8873
slot update_slots: id  3 | task 30406 | n_past = 3820, slot.prompt.tokens.size() = 9737, seq_id = 3, pos_min = 8840, n_swa = 128
slot update_slots: id  3 | task 30406 | restored context checkpoint (pos_min = 2859, pos_max = 3755, size = 21.034 MiB)
slot update_slots: id  3 | task 30406 | erased invalidated context checkpoint (pos_min = 3820, pos_max = 4701, n_swa = 128, size = 20.682 MiB)
slot update_slots: id  3 | task 30406 | erased invalidated context checkpoint (pos_min = 5710, pos_max = 6606, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 30406 | erased invalidated context checkpoint (pos_min = 6180, pos_max = 7076, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 30406 | erased invalidated context checkpoint (pos_min = 7617, pos_max = 8513, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 30406 | n_tokens = 3755, memory_seq_rm [3755, end)
slot update_slots: id  3 | task 30406 | prompt processing progress, n_tokens = 5803, batch.n_tokens = 2048, progress = 0.654007
slot update_slots: id  3 | task 30406 | n_tokens = 5803, memory_seq_rm [5803, end)
slot update_slots: id  3 | task 30406 | prompt processing progress, n_tokens = 7851, batch.n_tokens = 2048, progress = 0.884819
slot update_slots: id  3 | task 30406 | n_tokens = 7851, memory_seq_rm [7851, end)
slot update_slots: id  3 | task 30406 | prompt processing progress, n_tokens = 8809, batch.n_tokens = 958, progress = 0.992787
slot update_slots: id  3 | task 30406 | n_tokens = 8809, memory_seq_rm [8809, end)
slot update_slots: id  3 | task 30406 | prompt processing progress, n_tokens = 8873, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 30406 | prompt done, n_tokens = 8873, batch.n_tokens = 64
slot init_sampler: id  3 | task 30406 | init sampler, took 1.29 ms, tokens: text = 8873, total = 8873
slot update_slots: id  3 | task 30406 | created context checkpoint 5 of 8 (pos_min = 7912, pos_max = 8808, size = 21.034 MiB)
slot print_timing: id  3 | task 30406 | 
prompt eval time =    6533.55 ms /  5118 tokens (    1.28 ms per token,   783.34 tokens per second)
       eval time =    1213.37 ms /    49 tokens (   24.76 ms per token,    40.38 tokens per second)
      total time =    7746.92 ms /  5167 tokens
slot      release: id  3 | task 30406 | stop processing: n_tokens = 8921, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 30459 | processing task, is_child = 0
slot update_slots: id  3 | task 30459 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8928
slot update_slots: id  3 | task 30459 | n_tokens = 8873, memory_seq_rm [8873, end)
slot update_slots: id  3 | task 30459 | prompt processing progress, n_tokens = 8928, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 30459 | prompt done, n_tokens = 8928, batch.n_tokens = 55
slot init_sampler: id  3 | task 30459 | init sampler, took 1.24 ms, tokens: text = 8928, total = 8928
slot print_timing: id  3 | task 30459 | 
prompt eval time =     165.25 ms /    55 tokens (    3.00 ms per token,   332.83 tokens per second)
       eval time =    1256.06 ms /    49 tokens (   25.63 ms per token,    39.01 tokens per second)
      total time =    1421.31 ms /   104 tokens
slot      release: id  3 | task 30459 | stop processing: n_tokens = 8976, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.904 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 30509 | processing task, is_child = 0
slot update_slots: id  3 | task 30509 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9874
slot update_slots: id  3 | task 30509 | n_tokens = 8928, memory_seq_rm [8928, end)
slot update_slots: id  3 | task 30509 | prompt processing progress, n_tokens = 9810, batch.n_tokens = 882, progress = 0.993518
slot update_slots: id  3 | task 30509 | n_tokens = 9810, memory_seq_rm [9810, end)
slot update_slots: id  3 | task 30509 | prompt processing progress, n_tokens = 9874, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 30509 | prompt done, n_tokens = 9874, batch.n_tokens = 64
slot init_sampler: id  3 | task 30509 | init sampler, took 1.47 ms, tokens: text = 9874, total = 9874
slot update_slots: id  3 | task 30509 | created context checkpoint 6 of 8 (pos_min = 8913, pos_max = 9809, size = 21.034 MiB)
slot print_timing: id  3 | task 30509 | 
prompt eval time =    1317.55 ms /   946 tokens (    1.39 ms per token,   718.00 tokens per second)
       eval time =   41263.64 ms /  1532 tokens (   26.93 ms per token,    37.13 tokens per second)
      total time =   42581.19 ms /  2478 tokens
slot      release: id  3 | task 30509 | stop processing: n_tokens = 11405, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.943 (> 0.100 thold), f_keep = 0.866
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32043 | processing task, is_child = 0
slot update_slots: id  3 | task 32043 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10466
slot update_slots: id  3 | task 32043 | n_past = 9874, slot.prompt.tokens.size() = 11405, seq_id = 3, pos_min = 10508, n_swa = 128
slot update_slots: id  3 | task 32043 | restored context checkpoint (pos_min = 8913, pos_max = 9809, size = 21.034 MiB)
slot update_slots: id  3 | task 32043 | n_tokens = 9809, memory_seq_rm [9809, end)
slot update_slots: id  3 | task 32043 | prompt processing progress, n_tokens = 10402, batch.n_tokens = 593, progress = 0.993885
slot update_slots: id  3 | task 32043 | n_tokens = 10402, memory_seq_rm [10402, end)
slot update_slots: id  3 | task 32043 | prompt processing progress, n_tokens = 10466, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 32043 | prompt done, n_tokens = 10466, batch.n_tokens = 64
slot init_sampler: id  3 | task 32043 | init sampler, took 1.52 ms, tokens: text = 10466, total = 10466
slot update_slots: id  3 | task 32043 | created context checkpoint 7 of 8 (pos_min = 9505, pos_max = 10401, size = 21.034 MiB)
slot print_timing: id  3 | task 32043 | 
prompt eval time =    1263.71 ms /   657 tokens (    1.92 ms per token,   519.90 tokens per second)
       eval time =   13424.15 ms /   512 tokens (   26.22 ms per token,    38.14 tokens per second)
      total time =   14687.86 ms /  1169 tokens
slot      release: id  3 | task 32043 | stop processing: n_tokens = 10977, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.188
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 10977, total state size = 278.433 MiB
srv          load:  - looking for better prompt, base f_keep = 0.188, sim = 0.996
srv          load:  - found better prompt with f_keep = 0.471, sim = 0.997
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
srv          load: failed to restore state with size 132751220
slot  prompt_load: id  3 | task -1 | failed to load prompt from cache
slot prompt_clear: id  3 | task -1 | clearing prompt with 10977 tokens
srv        update:  - cache state: 7 prompts, 2762.655 MiB (limits: 8192.000 MiB, 64000 tokens, 203882 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv        update:    - prompt 0x5a0b7e703bc0:    6188 tokens, checkpoints:  4,   244.035 MiB
srv        update:    - prompt 0x5a0b83e530c0:    9513 tokens, checkpoints:  8,   414.791 MiB
srv        update:    - prompt 0x5a0b7f017ff0:    9737 tokens, checkpoints:  8,   413.947 MiB
srv        update:    - prompt 0x5a0ba6c10af0:   10977 tokens, checkpoints:  7,   422.342 MiB
srv  get_availabl: prompt cache update took 459.49 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32557 | processing task, is_child = 0
slot update_slots: id  3 | task 32557 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2068
slot update_slots: id  3 | task 32557 | erased invalidated context checkpoint (pos_min = 1106, pos_max = 2002, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 32557 | erased invalidated context checkpoint (pos_min = 1224, pos_max = 2067, n_swa = 128, size = 19.791 MiB)
slot update_slots: id  3 | task 32557 | erased invalidated context checkpoint (pos_min = 1383, pos_max = 2190, n_swa = 128, size = 18.947 MiB)
slot update_slots: id  3 | task 32557 | erased invalidated context checkpoint (pos_min = 2859, pos_max = 3755, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 32557 | erased invalidated context checkpoint (pos_min = 7912, pos_max = 8808, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 32557 | erased invalidated context checkpoint (pos_min = 8913, pos_max = 9809, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 32557 | erased invalidated context checkpoint (pos_min = 9505, pos_max = 10401, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 32557 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 32557 | prompt processing progress, n_tokens = 2004, batch.n_tokens = 2004, progress = 0.969052
slot update_slots: id  3 | task 32557 | n_tokens = 2004, memory_seq_rm [2004, end)
slot update_slots: id  3 | task 32557 | prompt processing progress, n_tokens = 2068, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 32557 | prompt done, n_tokens = 2068, batch.n_tokens = 64
slot init_sampler: id  3 | task 32557 | init sampler, took 0.35 ms, tokens: text = 2068, total = 2068
slot update_slots: id  3 | task 32557 | created context checkpoint 1 of 8 (pos_min = 1107, pos_max = 2003, size = 21.034 MiB)
slot print_timing: id  3 | task 32557 | 
prompt eval time =    2758.32 ms /  2068 tokens (    1.33 ms per token,   749.73 tokens per second)
       eval time =    4782.97 ms /   176 tokens (   27.18 ms per token,    36.80 tokens per second)
      total time =    7541.29 ms /  2244 tokens
slot      release: id  3 | task 32557 | stop processing: n_tokens = 2243, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.919
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32735 | processing task, is_child = 0
slot update_slots: id  3 | task 32735 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2070
slot update_slots: id  3 | task 32735 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 32735 | prompt processing progress, n_tokens = 2070, batch.n_tokens = 9, progress = 1.000000
slot update_slots: id  3 | task 32735 | prompt done, n_tokens = 2070, batch.n_tokens = 9
slot init_sampler: id  3 | task 32735 | init sampler, took 0.36 ms, tokens: text = 2070, total = 2070
slot print_timing: id  3 | task 32735 | 
prompt eval time =     122.66 ms /     9 tokens (   13.63 ms per token,    73.37 tokens per second)
       eval time =    3990.54 ms /   154 tokens (   25.91 ms per token,    38.59 tokens per second)
      total time =    4113.21 ms /   163 tokens
slot      release: id  3 | task 32735 | stop processing: n_tokens = 2223, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32890 | processing task, is_child = 0
slot update_slots: id  3 | task 32890 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2133
slot update_slots: id  3 | task 32890 | n_tokens = 2071, memory_seq_rm [2071, end)
slot update_slots: id  3 | task 32890 | prompt processing progress, n_tokens = 2133, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 32890 | prompt done, n_tokens = 2133, batch.n_tokens = 62
slot init_sampler: id  3 | task 32890 | init sampler, took 0.35 ms, tokens: text = 2133, total = 2133
slot update_slots: id  3 | task 32890 | created context checkpoint 2 of 8 (pos_min = 1346, pos_max = 2070, size = 17.001 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 32890
slot      release: id  3 | task 32890 | stop processing: n_tokens = 2273, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.908
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33033 | processing task, is_child = 0
slot update_slots: id  3 | task 33033 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2071
slot update_slots: id  3 | task 33033 | n_tokens = 2065, memory_seq_rm [2065, end)
slot update_slots: id  3 | task 33033 | prompt processing progress, n_tokens = 2071, batch.n_tokens = 6, progress = 1.000000
slot update_slots: id  3 | task 33033 | prompt done, n_tokens = 2071, batch.n_tokens = 6
slot init_sampler: id  3 | task 33033 | init sampler, took 0.37 ms, tokens: text = 2071, total = 2071
slot print_timing: id  3 | task 33033 | 
prompt eval time =      98.10 ms /     6 tokens (   16.35 ms per token,    61.16 tokens per second)
       eval time =   15951.50 ms /   574 tokens (   27.79 ms per token,    35.98 tokens per second)
      total time =   16049.60 ms /   580 tokens
slot      release: id  3 | task 33033 | stop processing: n_tokens = 2644, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.780
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33608 | processing task, is_child = 0
slot update_slots: id  3 | task 33608 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2070
slot update_slots: id  3 | task 33608 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 33608 | prompt processing progress, n_tokens = 2070, batch.n_tokens = 9, progress = 1.000000
slot update_slots: id  3 | task 33608 | prompt done, n_tokens = 2070, batch.n_tokens = 9
slot init_sampler: id  3 | task 33608 | init sampler, took 0.46 ms, tokens: text = 2070, total = 2070
slot print_timing: id  3 | task 33608 | 
prompt eval time =     122.71 ms /     9 tokens (   13.63 ms per token,    73.34 tokens per second)
       eval time =    1277.40 ms /    49 tokens (   26.07 ms per token,    38.36 tokens per second)
      total time =    1400.11 ms /    58 tokens
slot      release: id  3 | task 33608 | stop processing: n_tokens = 2118, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33658 | processing task, is_child = 0
slot update_slots: id  3 | task 33658 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2118
slot update_slots: id  3 | task 33658 | n_tokens = 2070, memory_seq_rm [2070, end)
slot update_slots: id  3 | task 33658 | prompt processing progress, n_tokens = 2118, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 33658 | prompt done, n_tokens = 2118, batch.n_tokens = 48
slot init_sampler: id  3 | task 33658 | init sampler, took 0.39 ms, tokens: text = 2118, total = 2118
slot print_timing: id  3 | task 33658 | 
prompt eval time =     148.08 ms /    48 tokens (    3.09 ms per token,   324.14 tokens per second)
       eval time =    3140.15 ms /   119 tokens (   26.39 ms per token,    37.90 tokens per second)
      total time =    3288.23 ms /   167 tokens
slot      release: id  3 | task 33658 | stop processing: n_tokens = 2236, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.922
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33778 | processing task, is_child = 0
slot update_slots: id  3 | task 33778 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2069
slot update_slots: id  3 | task 33778 | n_tokens = 2062, memory_seq_rm [2062, end)
slot update_slots: id  3 | task 33778 | prompt processing progress, n_tokens = 2069, batch.n_tokens = 7, progress = 1.000000
slot update_slots: id  3 | task 33778 | prompt done, n_tokens = 2069, batch.n_tokens = 7
slot init_sampler: id  3 | task 33778 | init sampler, took 0.34 ms, tokens: text = 2069, total = 2069
slot print_timing: id  3 | task 33778 | 
prompt eval time =     100.99 ms /     7 tokens (   14.43 ms per token,    69.32 tokens per second)
       eval time =    1394.13 ms /    53 tokens (   26.30 ms per token,    38.02 tokens per second)
      total time =    1495.12 ms /    60 tokens
slot      release: id  3 | task 33778 | stop processing: n_tokens = 2121, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33832 | processing task, is_child = 0
slot update_slots: id  3 | task 33832 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2117
slot update_slots: id  3 | task 33832 | n_tokens = 2069, memory_seq_rm [2069, end)
slot update_slots: id  3 | task 33832 | prompt processing progress, n_tokens = 2117, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 33832 | prompt done, n_tokens = 2117, batch.n_tokens = 48
slot init_sampler: id  3 | task 33832 | init sampler, took 0.28 ms, tokens: text = 2117, total = 2117
slot print_timing: id  3 | task 33832 | 
prompt eval time =     142.25 ms /    48 tokens (    2.96 ms per token,   337.43 tokens per second)
       eval time =    1169.51 ms /    45 tokens (   25.99 ms per token,    38.48 tokens per second)
      total time =    1311.76 ms /    93 tokens
slot      release: id  3 | task 33832 | stop processing: n_tokens = 2161, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.957
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33878 | processing task, is_child = 0
slot update_slots: id  3 | task 33878 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2163
slot update_slots: id  3 | task 33878 | n_tokens = 2069, memory_seq_rm [2069, end)
slot update_slots: id  3 | task 33878 | prompt processing progress, n_tokens = 2099, batch.n_tokens = 30, progress = 0.970411
slot update_slots: id  3 | task 33878 | n_tokens = 2099, memory_seq_rm [2099, end)
slot update_slots: id  3 | task 33878 | prompt processing progress, n_tokens = 2163, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33878 | prompt done, n_tokens = 2163, batch.n_tokens = 64
slot init_sampler: id  3 | task 33878 | init sampler, took 0.34 ms, tokens: text = 2163, total = 2163
slot print_timing: id  3 | task 33878 | 
prompt eval time =     432.32 ms /    94 tokens (    4.60 ms per token,   217.43 tokens per second)
       eval time =    1192.33 ms /    46 tokens (   25.92 ms per token,    38.58 tokens per second)
      total time =    1624.65 ms /   140 tokens
slot      release: id  3 | task 33878 | stop processing: n_tokens = 2208, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33926 | processing task, is_child = 0
slot update_slots: id  3 | task 33926 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2211
slot update_slots: id  3 | task 33926 | n_tokens = 2163, memory_seq_rm [2163, end)
slot update_slots: id  3 | task 33926 | prompt processing progress, n_tokens = 2211, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 33926 | prompt done, n_tokens = 2211, batch.n_tokens = 48
slot init_sampler: id  3 | task 33926 | init sampler, took 0.36 ms, tokens: text = 2211, total = 2211
slot update_slots: id  3 | task 33926 | created context checkpoint 3 of 8 (pos_min = 1747, pos_max = 2162, size = 9.755 MiB)
slot print_timing: id  3 | task 33926 | 
prompt eval time =     150.15 ms /    48 tokens (    3.13 ms per token,   319.67 tokens per second)
       eval time =     470.51 ms /    19 tokens (   24.76 ms per token,    40.38 tokens per second)
      total time =     620.67 ms /    67 tokens
slot      release: id  3 | task 33926 | stop processing: n_tokens = 2229, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33946 | processing task, is_child = 0
slot update_slots: id  3 | task 33946 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2261
slot update_slots: id  3 | task 33946 | n_tokens = 2163, memory_seq_rm [2163, end)
slot update_slots: id  3 | task 33946 | prompt processing progress, n_tokens = 2197, batch.n_tokens = 34, progress = 0.971694
slot update_slots: id  3 | task 33946 | n_tokens = 2197, memory_seq_rm [2197, end)
slot update_slots: id  3 | task 33946 | prompt processing progress, n_tokens = 2261, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33946 | prompt done, n_tokens = 2261, batch.n_tokens = 64
slot init_sampler: id  3 | task 33946 | init sampler, took 0.35 ms, tokens: text = 2261, total = 2261
slot print_timing: id  3 | task 33946 | 
prompt eval time =     414.39 ms /    98 tokens (    4.23 ms per token,   236.49 tokens per second)
       eval time =    4746.35 ms /   181 tokens (   26.22 ms per token,    38.13 tokens per second)
      total time =    5160.74 ms /   279 tokens
slot      release: id  3 | task 33946 | stop processing: n_tokens = 2441, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.926
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 34129 | processing task, is_child = 0
slot update_slots: id  3 | task 34129 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2309
slot update_slots: id  3 | task 34129 | n_tokens = 2261, memory_seq_rm [2261, end)
slot update_slots: id  3 | task 34129 | prompt processing progress, n_tokens = 2309, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 34129 | prompt done, n_tokens = 2309, batch.n_tokens = 48
slot init_sampler: id  3 | task 34129 | init sampler, took 0.38 ms, tokens: text = 2309, total = 2309
slot update_slots: id  3 | task 34129 | created context checkpoint 4 of 8 (pos_min = 1747, pos_max = 2260, size = 12.053 MiB)
slot print_timing: id  3 | task 34129 | 
prompt eval time =     153.18 ms /    48 tokens (    3.19 ms per token,   313.36 tokens per second)
       eval time =    2220.84 ms /    83 tokens (   26.76 ms per token,    37.37 tokens per second)
      total time =    2374.02 ms /   131 tokens
slot      release: id  3 | task 34129 | stop processing: n_tokens = 2391, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 34213 | processing task, is_child = 0
slot update_slots: id  3 | task 34213 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2357
slot update_slots: id  3 | task 34213 | n_tokens = 2309, memory_seq_rm [2309, end)
slot update_slots: id  3 | task 34213 | prompt processing progress, n_tokens = 2357, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 34213 | prompt done, n_tokens = 2357, batch.n_tokens = 48
slot init_sampler: id  3 | task 34213 | init sampler, took 0.38 ms, tokens: text = 2357, total = 2357
slot print_timing: id  3 | task 34213 | 
prompt eval time =     149.88 ms /    48 tokens (    3.12 ms per token,   320.26 tokens per second)
       eval time =    1492.86 ms /    56 tokens (   26.66 ms per token,    37.51 tokens per second)
      total time =    1642.74 ms /   104 tokens
slot      release: id  3 | task 34213 | stop processing: n_tokens = 2412, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 34270 | processing task, is_child = 0
slot update_slots: id  3 | task 34270 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2405
slot update_slots: id  3 | task 34270 | n_tokens = 2357, memory_seq_rm [2357, end)
slot update_slots: id  3 | task 34270 | prompt processing progress, n_tokens = 2405, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 34270 | prompt done, n_tokens = 2405, batch.n_tokens = 48
slot init_sampler: id  3 | task 34270 | init sampler, took 0.39 ms, tokens: text = 2405, total = 2405
slot update_slots: id  3 | task 34270 | created context checkpoint 5 of 8 (pos_min = 1747, pos_max = 2356, size = 14.304 MiB)
slot print_timing: id  3 | task 34270 | 
prompt eval time =     156.65 ms /    48 tokens (    3.26 ms per token,   306.41 tokens per second)
       eval time =    1957.13 ms /    72 tokens (   27.18 ms per token,    36.79 tokens per second)
      total time =    2113.78 ms /   120 tokens
slot      release: id  3 | task 34270 | stop processing: n_tokens = 2476, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.832
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 34343 | processing task, is_child = 0
slot update_slots: id  3 | task 34343 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2075
slot update_slots: id  3 | task 34343 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 34343 | prompt processing progress, n_tokens = 2075, batch.n_tokens = 14, progress = 1.000000
slot update_slots: id  3 | task 34343 | prompt done, n_tokens = 2075, batch.n_tokens = 14
slot init_sampler: id  3 | task 34343 | init sampler, took 0.33 ms, tokens: text = 2075, total = 2075
slot print_timing: id  3 | task 34343 | 
prompt eval time =     141.34 ms /    14 tokens (   10.10 ms per token,    99.05 tokens per second)
       eval time =    1294.23 ms /    49 tokens (   26.41 ms per token,    37.86 tokens per second)
      total time =    1435.57 ms /    63 tokens
slot      release: id  3 | task 34343 | stop processing: n_tokens = 2123, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 34393 | processing task, is_child = 0
slot update_slots: id  3 | task 34393 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2125
slot update_slots: id  3 | task 34393 | n_tokens = 2075, memory_seq_rm [2075, end)
slot update_slots: id  3 | task 34393 | prompt processing progress, n_tokens = 2125, batch.n_tokens = 50, progress = 1.000000
slot update_slots: id  3 | task 34393 | prompt done, n_tokens = 2125, batch.n_tokens = 50
slot init_sampler: id  3 | task 34393 | init sampler, took 0.37 ms, tokens: text = 2125, total = 2125
slot print_timing: id  3 | task 34393 | 
prompt eval time =     164.21 ms /    50 tokens (    3.28 ms per token,   304.48 tokens per second)
       eval time =    1710.34 ms /    63 tokens (   27.15 ms per token,    36.83 tokens per second)
      total time =    1874.55 ms /   113 tokens
slot      release: id  3 | task 34393 | stop processing: n_tokens = 2187, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.694 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 34457 | processing task, is_child = 0
slot update_slots: id  3 | task 34457 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3063
slot update_slots: id  3 | task 34457 | n_tokens = 2125, memory_seq_rm [2125, end)
slot update_slots: id  3 | task 34457 | prompt processing progress, n_tokens = 2999, batch.n_tokens = 874, progress = 0.979105
slot update_slots: id  3 | task 34457 | n_tokens = 2999, memory_seq_rm [2999, end)
slot update_slots: id  3 | task 34457 | prompt processing progress, n_tokens = 3063, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 34457 | prompt done, n_tokens = 3063, batch.n_tokens = 64
slot init_sampler: id  3 | task 34457 | init sampler, took 0.56 ms, tokens: text = 3063, total = 3063
slot update_slots: id  3 | task 34457 | created context checkpoint 6 of 8 (pos_min = 2102, pos_max = 2998, size = 21.034 MiB)
slot print_timing: id  3 | task 34457 | 
prompt eval time =    1407.20 ms /   938 tokens (    1.50 ms per token,   666.57 tokens per second)
       eval time =    4569.54 ms /   163 tokens (   28.03 ms per token,    35.67 tokens per second)
      total time =    5976.74 ms /  1101 tokens
slot      release: id  3 | task 34457 | stop processing: n_tokens = 3225, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.950
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 34622 | processing task, is_child = 0
slot update_slots: id  3 | task 34622 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3122
slot update_slots: id  3 | task 34622 | n_tokens = 3063, memory_seq_rm [3063, end)
slot update_slots: id  3 | task 34622 | prompt processing progress, n_tokens = 3122, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 34622 | prompt done, n_tokens = 3122, batch.n_tokens = 59
slot init_sampler: id  3 | task 34622 | init sampler, took 0.42 ms, tokens: text = 3122, total = 3122
slot print_timing: id  3 | task 34622 | 
prompt eval time =     195.07 ms /    59 tokens (    3.31 ms per token,   302.45 tokens per second)
       eval time =    6130.85 ms /   214 tokens (   28.65 ms per token,    34.91 tokens per second)
      total time =    6325.92 ms /   273 tokens
slot      release: id  3 | task 34622 | stop processing: n_tokens = 3335, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.936
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 34837 | processing task, is_child = 0
slot update_slots: id  3 | task 34837 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3173
slot update_slots: id  3 | task 34837 | n_tokens = 3122, memory_seq_rm [3122, end)
slot update_slots: id  3 | task 34837 | prompt processing progress, n_tokens = 3173, batch.n_tokens = 51, progress = 1.000000
slot update_slots: id  3 | task 34837 | prompt done, n_tokens = 3173, batch.n_tokens = 51
slot init_sampler: id  3 | task 34837 | init sampler, took 0.48 ms, tokens: text = 3173, total = 3173
slot update_slots: id  3 | task 34837 | created context checkpoint 7 of 8 (pos_min = 2438, pos_max = 3121, size = 16.039 MiB)
slot print_timing: id  3 | task 34837 | 
prompt eval time =     185.18 ms /    51 tokens (    3.63 ms per token,   275.41 tokens per second)
       eval time =    2585.55 ms /    91 tokens (   28.41 ms per token,    35.20 tokens per second)
      total time =    2770.73 ms /   142 tokens
slot      release: id  3 | task 34837 | stop processing: n_tokens = 3263, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 34929 | processing task, is_child = 0
slot update_slots: id  3 | task 34929 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3219
slot update_slots: id  3 | task 34929 | n_tokens = 3173, memory_seq_rm [3173, end)
slot update_slots: id  3 | task 34929 | prompt processing progress, n_tokens = 3219, batch.n_tokens = 46, progress = 1.000000
slot update_slots: id  3 | task 34929 | prompt done, n_tokens = 3219, batch.n_tokens = 46
slot init_sampler: id  3 | task 34929 | init sampler, took 0.60 ms, tokens: text = 3219, total = 3219
slot print_timing: id  3 | task 34929 | 
prompt eval time =     156.23 ms /    46 tokens (    3.40 ms per token,   294.43 tokens per second)
       eval time =    1858.96 ms /    66 tokens (   28.17 ms per token,    35.50 tokens per second)
      total time =    2015.19 ms /   112 tokens
slot      release: id  3 | task 34929 | stop processing: n_tokens = 3284, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 34996 | processing task, is_child = 0
slot update_slots: id  3 | task 34996 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3267
slot update_slots: id  3 | task 34996 | n_tokens = 3219, memory_seq_rm [3219, end)
slot update_slots: id  3 | task 34996 | prompt processing progress, n_tokens = 3267, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 34996 | prompt done, n_tokens = 3267, batch.n_tokens = 48
slot init_sampler: id  3 | task 34996 | init sampler, took 0.56 ms, tokens: text = 3267, total = 3267
slot update_slots: id  3 | task 34996 | created context checkpoint 8 of 8 (pos_min = 2438, pos_max = 3218, size = 18.314 MiB)
slot print_timing: id  3 | task 34996 | 
prompt eval time =     157.23 ms /    48 tokens (    3.28 ms per token,   305.28 tokens per second)
       eval time =    4561.14 ms /   166 tokens (   27.48 ms per token,    36.39 tokens per second)
      total time =    4718.37 ms /   214 tokens
slot      release: id  3 | task 34996 | stop processing: n_tokens = 3432, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.654 (> 0.100 thold), f_keep = 0.952
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 35163 | processing task, is_child = 0
slot update_slots: id  3 | task 35163 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4999
slot update_slots: id  3 | task 35163 | n_tokens = 3267, memory_seq_rm [3267, end)
slot update_slots: id  3 | task 35163 | prompt processing progress, n_tokens = 4935, batch.n_tokens = 1668, progress = 0.987197
slot update_slots: id  3 | task 35163 | n_tokens = 4935, memory_seq_rm [4935, end)
slot update_slots: id  3 | task 35163 | prompt processing progress, n_tokens = 4999, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 35163 | prompt done, n_tokens = 4999, batch.n_tokens = 64
slot init_sampler: id  3 | task 35163 | init sampler, took 0.77 ms, tokens: text = 4999, total = 4999
slot update_slots: id  3 | task 35163 | erasing old context checkpoint (pos_min = 1107, pos_max = 2003, size = 21.034 MiB)
slot update_slots: id  3 | task 35163 | created context checkpoint 8 of 8 (pos_min = 4038, pos_max = 4934, size = 21.034 MiB)
slot print_timing: id  3 | task 35163 | 
prompt eval time =    2431.84 ms /  1732 tokens (    1.40 ms per token,   712.22 tokens per second)
       eval time =    3740.16 ms /   140 tokens (   26.72 ms per token,    37.43 tokens per second)
      total time =    6172.00 ms /  1872 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 35163 | stop processing: n_tokens = 5138, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 35305 | processing task, is_child = 0
slot update_slots: id  3 | task 35305 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5074
slot update_slots: id  3 | task 35305 | n_tokens = 4999, memory_seq_rm [4999, end)
slot update_slots: id  3 | task 35305 | prompt processing progress, n_tokens = 5010, batch.n_tokens = 11, progress = 0.987387
slot update_slots: id  3 | task 35305 | n_tokens = 5010, memory_seq_rm [5010, end)
slot update_slots: id  3 | task 35305 | prompt processing progress, n_tokens = 5074, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 35305 | prompt done, n_tokens = 5074, batch.n_tokens = 64
slot init_sampler: id  3 | task 35305 | init sampler, took 0.94 ms, tokens: text = 5074, total = 5074
slot update_slots: id  3 | task 35305 | erasing old context checkpoint (pos_min = 1346, pos_max = 2070, size = 17.001 MiB)
slot update_slots: id  3 | task 35305 | created context checkpoint 8 of 8 (pos_min = 4241, pos_max = 5009, size = 18.033 MiB)
slot print_timing: id  3 | task 35305 | 
prompt eval time =     265.89 ms /    75 tokens (    3.55 ms per token,   282.07 tokens per second)
       eval time =    3905.10 ms /   148 tokens (   26.39 ms per token,    37.90 tokens per second)
      total time =    4170.99 ms /   223 tokens
slot      release: id  3 | task 35305 | stop processing: n_tokens = 5221, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.956 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 35455 | processing task, is_child = 0
slot update_slots: id  3 | task 35455 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5310
slot update_slots: id  3 | task 35455 | n_tokens = 5074, memory_seq_rm [5074, end)
slot update_slots: id  3 | task 35455 | prompt processing progress, n_tokens = 5246, batch.n_tokens = 172, progress = 0.987947
slot update_slots: id  3 | task 35455 | n_tokens = 5246, memory_seq_rm [5246, end)
slot update_slots: id  3 | task 35455 | prompt processing progress, n_tokens = 5310, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 35455 | prompt done, n_tokens = 5310, batch.n_tokens = 64
slot init_sampler: id  3 | task 35455 | init sampler, took 0.77 ms, tokens: text = 5310, total = 5310
slot update_slots: id  3 | task 35455 | erasing old context checkpoint (pos_min = 1747, pos_max = 2162, size = 9.755 MiB)
slot update_slots: id  3 | task 35455 | created context checkpoint 8 of 8 (pos_min = 4349, pos_max = 5245, size = 21.034 MiB)
slot print_timing: id  3 | task 35455 | 
prompt eval time =     489.55 ms /   236 tokens (    2.07 ms per token,   482.08 tokens per second)
       eval time =    7572.20 ms /   292 tokens (   25.93 ms per token,    38.56 tokens per second)
      total time =    8061.75 ms /   528 tokens
slot      release: id  3 | task 35455 | stop processing: n_tokens = 5601, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.948
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 35749 | processing task, is_child = 0
slot update_slots: id  3 | task 35749 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5428
slot update_slots: id  3 | task 35749 | n_tokens = 5310, memory_seq_rm [5310, end)
slot update_slots: id  3 | task 35749 | prompt processing progress, n_tokens = 5364, batch.n_tokens = 54, progress = 0.988209
slot update_slots: id  3 | task 35749 | n_tokens = 5364, memory_seq_rm [5364, end)
slot update_slots: id  3 | task 35749 | prompt processing progress, n_tokens = 5428, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 35749 | prompt done, n_tokens = 5428, batch.n_tokens = 64
slot init_sampler: id  3 | task 35749 | init sampler, took 1.02 ms, tokens: text = 5428, total = 5428
slot update_slots: id  3 | task 35749 | erasing old context checkpoint (pos_min = 1747, pos_max = 2260, size = 12.053 MiB)
slot update_slots: id  3 | task 35749 | created context checkpoint 8 of 8 (pos_min = 4704, pos_max = 5363, size = 15.477 MiB)
slot print_timing: id  3 | task 35749 | 
prompt eval time =     389.35 ms /   118 tokens (    3.30 ms per token,   303.07 tokens per second)
       eval time =    6981.81 ms /   273 tokens (   25.57 ms per token,    39.10 tokens per second)
      total time =    7371.15 ms /   391 tokens
slot      release: id  3 | task 35749 | stop processing: n_tokens = 5700, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.840 (> 0.100 thold), f_keep = 0.952
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 36024 | processing task, is_child = 0
slot update_slots: id  3 | task 36024 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6460
slot update_slots: id  3 | task 36024 | n_tokens = 5428, memory_seq_rm [5428, end)
slot update_slots: id  3 | task 36024 | prompt processing progress, n_tokens = 6396, batch.n_tokens = 968, progress = 0.990093
slot update_slots: id  3 | task 36024 | n_tokens = 6396, memory_seq_rm [6396, end)
slot update_slots: id  3 | task 36024 | prompt processing progress, n_tokens = 6460, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 36024 | prompt done, n_tokens = 6460, batch.n_tokens = 64
slot init_sampler: id  3 | task 36024 | init sampler, took 1.18 ms, tokens: text = 6460, total = 6460
slot update_slots: id  3 | task 36024 | erasing old context checkpoint (pos_min = 1747, pos_max = 2356, size = 14.304 MiB)
slot update_slots: id  3 | task 36024 | created context checkpoint 8 of 8 (pos_min = 5499, pos_max = 6395, size = 21.034 MiB)
slot print_timing: id  3 | task 36024 | 
prompt eval time =    1376.54 ms /  1032 tokens (    1.33 ms per token,   749.71 tokens per second)
       eval time =    4157.73 ms /   163 tokens (   25.51 ms per token,    39.20 tokens per second)
      total time =    5534.26 ms /  1195 tokens
slot      release: id  3 | task 36024 | stop processing: n_tokens = 6622, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.815 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 36189 | processing task, is_child = 0
slot update_slots: id  3 | task 36189 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7927
slot update_slots: id  3 | task 36189 | n_tokens = 6460, memory_seq_rm [6460, end)
slot update_slots: id  3 | task 36189 | prompt processing progress, n_tokens = 7863, batch.n_tokens = 1403, progress = 0.991926
slot update_slots: id  3 | task 36189 | n_tokens = 7863, memory_seq_rm [7863, end)
slot update_slots: id  3 | task 36189 | prompt processing progress, n_tokens = 7927, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 36189 | prompt done, n_tokens = 7927, batch.n_tokens = 64
slot init_sampler: id  3 | task 36189 | init sampler, took 1.43 ms, tokens: text = 7927, total = 7927
slot update_slots: id  3 | task 36189 | erasing old context checkpoint (pos_min = 2102, pos_max = 2998, size = 21.034 MiB)
slot update_slots: id  3 | task 36189 | created context checkpoint 8 of 8 (pos_min = 6966, pos_max = 7862, size = 21.034 MiB)
slot print_timing: id  3 | task 36189 | 
prompt eval time =    1940.49 ms /  1467 tokens (    1.32 ms per token,   756.00 tokens per second)
       eval time =   19742.08 ms /   770 tokens (   25.64 ms per token,    39.00 tokens per second)
      total time =   21682.57 ms /  2237 tokens
slot      release: id  3 | task 36189 | stop processing: n_tokens = 8696, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.245 (> 0.100 thold), f_keep = 0.239
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8696, total state size = 224.946 MiB
srv          load:  - looking for better prompt, base f_keep = 0.239, sim = 0.245
srv        update:  - cache state: 8 prompts, 3139.600 MiB (limits: 8192.000 MiB, 64000 tokens, 202094 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv        update:    - prompt 0x5a0b7e703bc0:    6188 tokens, checkpoints:  4,   244.035 MiB
srv        update:    - prompt 0x5a0b83e530c0:    9513 tokens, checkpoints:  8,   414.791 MiB
srv        update:    - prompt 0x5a0b7f017ff0:    9737 tokens, checkpoints:  8,   413.947 MiB
srv        update:    - prompt 0x5a0ba6c10af0:   10977 tokens, checkpoints:  7,   422.342 MiB
srv        update:    - prompt 0x5a0ba6c62d80:    8696 tokens, checkpoints:  8,   376.945 MiB
srv  get_availabl: prompt cache update took 278.42 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 36961 | processing task, is_child = 0
slot update_slots: id  3 | task 36961 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8471
slot update_slots: id  3 | task 36961 | n_past = 2075, slot.prompt.tokens.size() = 8696, seq_id = 3, pos_min = 7799, n_swa = 128
slot update_slots: id  3 | task 36961 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 36961 | erased invalidated context checkpoint (pos_min = 2438, pos_max = 3121, n_swa = 128, size = 16.039 MiB)
slot update_slots: id  3 | task 36961 | erased invalidated context checkpoint (pos_min = 2438, pos_max = 3218, n_swa = 128, size = 18.314 MiB)
slot update_slots: id  3 | task 36961 | erased invalidated context checkpoint (pos_min = 4038, pos_max = 4934, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 36961 | erased invalidated context checkpoint (pos_min = 4241, pos_max = 5009, n_swa = 128, size = 18.033 MiB)
slot update_slots: id  3 | task 36961 | erased invalidated context checkpoint (pos_min = 4349, pos_max = 5245, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 36961 | erased invalidated context checkpoint (pos_min = 4704, pos_max = 5363, n_swa = 128, size = 15.477 MiB)
slot update_slots: id  3 | task 36961 | erased invalidated context checkpoint (pos_min = 5499, pos_max = 6395, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 36961 | erased invalidated context checkpoint (pos_min = 6966, pos_max = 7862, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 36961 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 36961 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.241766
slot update_slots: id  3 | task 36961 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 36961 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.483532
slot update_slots: id  3 | task 36961 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 36961 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.725298
slot update_slots: id  3 | task 36961 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  3 | task 36961 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.967064
slot update_slots: id  3 | task 36961 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  3 | task 36961 | prompt processing progress, n_tokens = 8407, batch.n_tokens = 215, progress = 0.992445
slot update_slots: id  3 | task 36961 | n_tokens = 8407, memory_seq_rm [8407, end)
slot update_slots: id  3 | task 36961 | prompt processing progress, n_tokens = 8471, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 36961 | prompt done, n_tokens = 8471, batch.n_tokens = 64
slot init_sampler: id  3 | task 36961 | init sampler, took 1.23 ms, tokens: text = 8471, total = 8471
slot update_slots: id  3 | task 36961 | created context checkpoint 1 of 8 (pos_min = 7510, pos_max = 8406, size = 21.034 MiB)
slot print_timing: id  3 | task 36961 | 
prompt eval time =   10805.05 ms /  8471 tokens (    1.28 ms per token,   783.99 tokens per second)
       eval time =   33916.25 ms /  1252 tokens (   27.09 ms per token,    36.91 tokens per second)
      total time =   44721.29 ms /  9723 tokens
slot      release: id  3 | task 36961 | stop processing: n_tokens = 9722, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.899 (> 0.100 thold), f_keep = 0.871
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 38219 | processing task, is_child = 0
slot update_slots: id  3 | task 38219 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9425
slot update_slots: id  3 | task 38219 | n_past = 8472, slot.prompt.tokens.size() = 9722, seq_id = 3, pos_min = 8825, n_swa = 128
slot update_slots: id  3 | task 38219 | restored context checkpoint (pos_min = 7510, pos_max = 8406, size = 21.034 MiB)
slot update_slots: id  3 | task 38219 | n_tokens = 8406, memory_seq_rm [8406, end)
slot update_slots: id  3 | task 38219 | prompt processing progress, n_tokens = 9361, batch.n_tokens = 955, progress = 0.993210
slot update_slots: id  3 | task 38219 | n_tokens = 9361, memory_seq_rm [9361, end)
slot update_slots: id  3 | task 38219 | prompt processing progress, n_tokens = 9425, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 38219 | prompt done, n_tokens = 9425, batch.n_tokens = 64
slot init_sampler: id  3 | task 38219 | init sampler, took 1.39 ms, tokens: text = 9425, total = 9425
slot update_slots: id  3 | task 38219 | created context checkpoint 2 of 8 (pos_min = 8464, pos_max = 9360, size = 21.034 MiB)
slot print_timing: id  3 | task 38219 | 
prompt eval time =    1673.49 ms /  1019 tokens (    1.64 ms per token,   608.91 tokens per second)
       eval time =   35402.97 ms /  1297 tokens (   27.30 ms per token,    36.64 tokens per second)
      total time =   37076.45 ms /  2316 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 38219 | stop processing: n_tokens = 10721, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.192
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 10721, total state size = 272.430 MiB
srv          load:  - looking for better prompt, base f_keep = 0.192, sim = 0.993
srv        update:  - cache state: 9 prompts, 3454.098 MiB (limits: 8192.000 MiB, 64000 tokens, 209120 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv        update:    - prompt 0x5a0b7e703bc0:    6188 tokens, checkpoints:  4,   244.035 MiB
srv        update:    - prompt 0x5a0b83e530c0:    9513 tokens, checkpoints:  8,   414.791 MiB
srv        update:    - prompt 0x5a0b7f017ff0:    9737 tokens, checkpoints:  8,   413.947 MiB
srv        update:    - prompt 0x5a0ba6c10af0:   10977 tokens, checkpoints:  7,   422.342 MiB
srv        update:    - prompt 0x5a0ba6c62d80:    8696 tokens, checkpoints:  8,   376.945 MiB
srv        update:    - prompt 0x5a0b7e115ed0:   10721 tokens, checkpoints:  2,   314.498 MiB
srv  get_availabl: prompt cache update took 337.10 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 39518 | processing task, is_child = 0
slot update_slots: id  3 | task 39518 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2075
slot update_slots: id  3 | task 39518 | n_past = 2061, slot.prompt.tokens.size() = 10721, seq_id = 3, pos_min = 9824, n_swa = 128
slot update_slots: id  3 | task 39518 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 39518 | erased invalidated context checkpoint (pos_min = 7510, pos_max = 8406, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 39518 | erased invalidated context checkpoint (pos_min = 8464, pos_max = 9360, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 39518 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 39518 | prompt processing progress, n_tokens = 2011, batch.n_tokens = 2011, progress = 0.969157
slot update_slots: id  3 | task 39518 | n_tokens = 2011, memory_seq_rm [2011, end)
slot update_slots: id  3 | task 39518 | prompt processing progress, n_tokens = 2075, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 39518 | prompt done, n_tokens = 2075, batch.n_tokens = 64
slot init_sampler: id  3 | task 39518 | init sampler, took 0.36 ms, tokens: text = 2075, total = 2075
slot update_slots: id  3 | task 39518 | created context checkpoint 1 of 8 (pos_min = 1114, pos_max = 2010, size = 21.034 MiB)
slot print_timing: id  3 | task 39518 | 
prompt eval time =    2756.71 ms /  2075 tokens (    1.33 ms per token,   752.71 tokens per second)
       eval time =   10335.39 ms /   380 tokens (   27.20 ms per token,    36.77 tokens per second)
      total time =   13092.10 ms /  2455 tokens
slot      release: id  3 | task 39518 | stop processing: n_tokens = 2454, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.846
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 39900 | processing task, is_child = 0
slot update_slots: id  3 | task 39900 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2137
slot update_slots: id  3 | task 39900 | n_tokens = 2076, memory_seq_rm [2076, end)
slot update_slots: id  3 | task 39900 | prompt processing progress, n_tokens = 2137, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  3 | task 39900 | prompt done, n_tokens = 2137, batch.n_tokens = 61
slot init_sampler: id  3 | task 39900 | init sampler, took 0.34 ms, tokens: text = 2137, total = 2137
slot update_slots: id  3 | task 39900 | created context checkpoint 2 of 8 (pos_min = 1557, pos_max = 2075, size = 12.170 MiB)
slot print_timing: id  3 | task 39900 | 
prompt eval time =     277.97 ms /    61 tokens (    4.56 ms per token,   219.45 tokens per second)
       eval time =    1048.36 ms /    41 tokens (   25.57 ms per token,    39.11 tokens per second)
      total time =    1326.33 ms /   102 tokens
slot      release: id  3 | task 39900 | stop processing: n_tokens = 2177, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 39942 | processing task, is_child = 0
slot update_slots: id  3 | task 39942 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2075
slot update_slots: id  3 | task 39942 | need to evaluate at least 1 token for each active slot (n_past = 2075, task.n_tokens() = 2075)
slot update_slots: id  3 | task 39942 | n_past was set to 2074
slot update_slots: id  3 | task 39942 | n_tokens = 2074, memory_seq_rm [2074, end)
slot update_slots: id  3 | task 39942 | prompt processing progress, n_tokens = 2075, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  3 | task 39942 | prompt done, n_tokens = 2075, batch.n_tokens = 1
slot init_sampler: id  3 | task 39942 | init sampler, took 0.37 ms, tokens: text = 2075, total = 2075
slot print_timing: id  3 | task 39942 | 
prompt eval time =      36.49 ms /     1 tokens (   36.49 ms per token,    27.40 tokens per second)
       eval time =    4600.64 ms /   176 tokens (   26.14 ms per token,    38.26 tokens per second)
      total time =    4637.13 ms /   177 tokens
slot      release: id  3 | task 39942 | stop processing: n_tokens = 2250, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.922
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40119 | processing task, is_child = 0
slot update_slots: id  3 | task 40119 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2123
slot update_slots: id  3 | task 40119 | n_tokens = 2075, memory_seq_rm [2075, end)
slot update_slots: id  3 | task 40119 | prompt processing progress, n_tokens = 2123, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 40119 | prompt done, n_tokens = 2123, batch.n_tokens = 48
slot init_sampler: id  3 | task 40119 | init sampler, took 0.36 ms, tokens: text = 2123, total = 2123
slot print_timing: id  3 | task 40119 | 
prompt eval time =     144.75 ms /    48 tokens (    3.02 ms per token,   331.60 tokens per second)
       eval time =    2835.56 ms /   108 tokens (   26.26 ms per token,    38.09 tokens per second)
      total time =    2980.31 ms /   156 tokens
slot      release: id  3 | task 40119 | stop processing: n_tokens = 2230, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.952
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40228 | processing task, is_child = 0
slot update_slots: id  3 | task 40228 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2171
slot update_slots: id  3 | task 40228 | n_tokens = 2123, memory_seq_rm [2123, end)
slot update_slots: id  3 | task 40228 | prompt processing progress, n_tokens = 2171, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 40228 | prompt done, n_tokens = 2171, batch.n_tokens = 48
slot init_sampler: id  3 | task 40228 | init sampler, took 0.41 ms, tokens: text = 2171, total = 2171
slot print_timing: id  3 | task 40228 | 
prompt eval time =     149.82 ms /    48 tokens (    3.12 ms per token,   320.39 tokens per second)
       eval time =    2147.90 ms /    81 tokens (   26.52 ms per token,    37.71 tokens per second)
      total time =    2297.72 ms /   129 tokens
slot      release: id  3 | task 40228 | stop processing: n_tokens = 2251, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.922
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40310 | processing task, is_child = 0
slot update_slots: id  3 | task 40310 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2184
slot update_slots: id  3 | task 40310 | n_tokens = 2075, memory_seq_rm [2075, end)
slot update_slots: id  3 | task 40310 | prompt processing progress, n_tokens = 2120, batch.n_tokens = 45, progress = 0.970696
slot update_slots: id  3 | task 40310 | n_tokens = 2120, memory_seq_rm [2120, end)
slot update_slots: id  3 | task 40310 | prompt processing progress, n_tokens = 2184, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 40310 | prompt done, n_tokens = 2184, batch.n_tokens = 64
slot init_sampler: id  3 | task 40310 | init sampler, took 0.34 ms, tokens: text = 2184, total = 2184
slot print_timing: id  3 | task 40310 | 
prompt eval time =     467.89 ms /   109 tokens (    4.29 ms per token,   232.96 tokens per second)
       eval time =    1037.10 ms /    41 tokens (   25.30 ms per token,    39.53 tokens per second)
      total time =    1504.99 ms /   150 tokens
slot      release: id  3 | task 40310 | stop processing: n_tokens = 2224, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40353 | processing task, is_child = 0
slot update_slots: id  3 | task 40353 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2232
slot update_slots: id  3 | task 40353 | n_tokens = 2184, memory_seq_rm [2184, end)
slot update_slots: id  3 | task 40353 | prompt processing progress, n_tokens = 2232, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 40353 | prompt done, n_tokens = 2232, batch.n_tokens = 48
slot init_sampler: id  3 | task 40353 | init sampler, took 0.36 ms, tokens: text = 2232, total = 2232
slot update_slots: id  3 | task 40353 | created context checkpoint 3 of 8 (pos_min = 1557, pos_max = 2183, size = 14.703 MiB)
slot print_timing: id  3 | task 40353 | 
prompt eval time =     153.03 ms /    48 tokens (    3.19 ms per token,   313.65 tokens per second)
       eval time =     982.40 ms /    38 tokens (   25.85 ms per token,    38.68 tokens per second)
      total time =    1135.43 ms /    86 tokens
slot      release: id  3 | task 40353 | stop processing: n_tokens = 2269, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40392 | processing task, is_child = 0
slot update_slots: id  3 | task 40392 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2280
slot update_slots: id  3 | task 40392 | n_tokens = 2232, memory_seq_rm [2232, end)
slot update_slots: id  3 | task 40392 | prompt processing progress, n_tokens = 2280, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 40392 | prompt done, n_tokens = 2280, batch.n_tokens = 48
slot init_sampler: id  3 | task 40392 | init sampler, took 0.42 ms, tokens: text = 2280, total = 2280
slot print_timing: id  3 | task 40392 | 
prompt eval time =     144.96 ms /    48 tokens (    3.02 ms per token,   331.12 tokens per second)
       eval time =    3942.29 ms /   149 tokens (   26.46 ms per token,    37.80 tokens per second)
      total time =    4087.25 ms /   197 tokens
slot      release: id  3 | task 40392 | stop processing: n_tokens = 2428, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.855
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40542 | processing task, is_child = 0
slot update_slots: id  3 | task 40542 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2075
slot update_slots: id  3 | task 40542 | need to evaluate at least 1 token for each active slot (n_past = 2075, task.n_tokens() = 2075)
slot update_slots: id  3 | task 40542 | n_past was set to 2074
slot update_slots: id  3 | task 40542 | n_tokens = 2074, memory_seq_rm [2074, end)
slot update_slots: id  3 | task 40542 | prompt processing progress, n_tokens = 2075, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  3 | task 40542 | prompt done, n_tokens = 2075, batch.n_tokens = 1
slot init_sampler: id  3 | task 40542 | init sampler, took 0.40 ms, tokens: text = 2075, total = 2075
slot print_timing: id  3 | task 40542 | 
prompt eval time =      36.75 ms /     1 tokens (   36.75 ms per token,    27.21 tokens per second)
       eval time =   10418.89 ms /   391 tokens (   26.65 ms per token,    37.53 tokens per second)
      total time =   10455.64 ms /   392 tokens
slot      release: id  3 | task 40542 | stop processing: n_tokens = 2465, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.842
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 40934 | processing task, is_child = 0
slot update_slots: id  3 | task 40934 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2158
slot update_slots: id  3 | task 40934 | n_tokens = 2076, memory_seq_rm [2076, end)
slot update_slots: id  3 | task 40934 | prompt processing progress, n_tokens = 2094, batch.n_tokens = 18, progress = 0.970343
slot update_slots: id  3 | task 40934 | n_tokens = 2094, memory_seq_rm [2094, end)
slot update_slots: id  3 | task 40934 | prompt processing progress, n_tokens = 2158, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 40934 | prompt done, n_tokens = 2158, batch.n_tokens = 64
slot init_sampler: id  3 | task 40934 | init sampler, took 0.34 ms, tokens: text = 2158, total = 2158
slot print_timing: id  3 | task 40934 | 
prompt eval time =     363.19 ms /    82 tokens (    4.43 ms per token,   225.78 tokens per second)
       eval time =    2052.52 ms /    75 tokens (   27.37 ms per token,    36.54 tokens per second)
      total time =    2415.71 ms /   157 tokens
slot      release: id  3 | task 40934 | stop processing: n_tokens = 2232, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41011 | processing task, is_child = 0
slot update_slots: id  3 | task 41011 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2206
slot update_slots: id  3 | task 41011 | n_tokens = 2158, memory_seq_rm [2158, end)
slot update_slots: id  3 | task 41011 | prompt processing progress, n_tokens = 2206, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 41011 | prompt done, n_tokens = 2206, batch.n_tokens = 48
slot init_sampler: id  3 | task 41011 | init sampler, took 0.39 ms, tokens: text = 2206, total = 2206
slot print_timing: id  3 | task 41011 | 
prompt eval time =     148.67 ms /    48 tokens (    3.10 ms per token,   322.86 tokens per second)
       eval time =     535.43 ms /    20 tokens (   26.77 ms per token,    37.35 tokens per second)
      total time =     684.10 ms /    68 tokens
slot      release: id  3 | task 41011 | stop processing: n_tokens = 2225, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41032 | processing task, is_child = 0
slot update_slots: id  3 | task 41032 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2254
slot update_slots: id  3 | task 41032 | n_tokens = 2206, memory_seq_rm [2206, end)
slot update_slots: id  3 | task 41032 | prompt processing progress, n_tokens = 2254, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 41032 | prompt done, n_tokens = 2254, batch.n_tokens = 48
slot init_sampler: id  3 | task 41032 | init sampler, took 0.30 ms, tokens: text = 2254, total = 2254
slot print_timing: id  3 | task 41032 | 
prompt eval time =     156.72 ms /    48 tokens (    3.27 ms per token,   306.27 tokens per second)
       eval time =     796.87 ms /    29 tokens (   27.48 ms per token,    36.39 tokens per second)
      total time =     953.59 ms /    77 tokens
slot      release: id  3 | task 41032 | stop processing: n_tokens = 2282, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.909
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41062 | processing task, is_child = 0
slot update_slots: id  3 | task 41062 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2075
slot update_slots: id  3 | task 41062 | need to evaluate at least 1 token for each active slot (n_past = 2075, task.n_tokens() = 2075)
slot update_slots: id  3 | task 41062 | n_past was set to 2074
slot update_slots: id  3 | task 41062 | n_tokens = 2074, memory_seq_rm [2074, end)
slot update_slots: id  3 | task 41062 | prompt processing progress, n_tokens = 2075, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  3 | task 41062 | prompt done, n_tokens = 2075, batch.n_tokens = 1
slot init_sampler: id  3 | task 41062 | init sampler, took 0.32 ms, tokens: text = 2075, total = 2075
slot print_timing: id  3 | task 41062 | 
prompt eval time =      36.36 ms /     1 tokens (   36.36 ms per token,    27.50 tokens per second)
       eval time =    6077.09 ms /   228 tokens (   26.65 ms per token,    37.52 tokens per second)
      total time =    6113.45 ms /   229 tokens
slot      release: id  3 | task 41062 | stop processing: n_tokens = 2302, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.901
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41291 | processing task, is_child = 0
slot update_slots: id  3 | task 41291 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2123
slot update_slots: id  3 | task 41291 | n_tokens = 2075, memory_seq_rm [2075, end)
slot update_slots: id  3 | task 41291 | prompt processing progress, n_tokens = 2123, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 41291 | prompt done, n_tokens = 2123, batch.n_tokens = 48
slot init_sampler: id  3 | task 41291 | init sampler, took 0.29 ms, tokens: text = 2123, total = 2123
slot print_timing: id  3 | task 41291 | 
prompt eval time =     144.23 ms /    48 tokens (    3.00 ms per token,   332.79 tokens per second)
       eval time =    2650.25 ms /   101 tokens (   26.24 ms per token,    38.11 tokens per second)
      total time =    2794.48 ms /   149 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 41291 | stop processing: n_tokens = 2223, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41393 | processing task, is_child = 0
slot update_slots: id  3 | task 41393 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2171
slot update_slots: id  3 | task 41393 | n_tokens = 2123, memory_seq_rm [2123, end)
slot update_slots: id  3 | task 41393 | prompt processing progress, n_tokens = 2171, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 41393 | prompt done, n_tokens = 2171, batch.n_tokens = 48
slot init_sampler: id  3 | task 41393 | init sampler, took 0.49 ms, tokens: text = 2171, total = 2171
slot print_timing: id  3 | task 41393 | 
prompt eval time =     148.26 ms /    48 tokens (    3.09 ms per token,   323.76 tokens per second)
       eval time =    1049.28 ms /    40 tokens (   26.23 ms per token,    38.12 tokens per second)
      total time =    1197.54 ms /    88 tokens
slot      release: id  3 | task 41393 | stop processing: n_tokens = 2210, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41434 | processing task, is_child = 0
slot update_slots: id  3 | task 41434 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2075
slot update_slots: id  3 | task 41434 | need to evaluate at least 1 token for each active slot (n_past = 2075, task.n_tokens() = 2075)
slot update_slots: id  3 | task 41434 | n_past was set to 2074
slot update_slots: id  3 | task 41434 | n_tokens = 2074, memory_seq_rm [2074, end)
slot update_slots: id  3 | task 41434 | prompt processing progress, n_tokens = 2075, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  3 | task 41434 | prompt done, n_tokens = 2075, batch.n_tokens = 1
slot init_sampler: id  3 | task 41434 | init sampler, took 0.34 ms, tokens: text = 2075, total = 2075
slot print_timing: id  3 | task 41434 | 
prompt eval time =      36.45 ms /     1 tokens (   36.45 ms per token,    27.44 tokens per second)
       eval time =    3574.29 ms /   140 tokens (   25.53 ms per token,    39.17 tokens per second)
      total time =    3610.74 ms /   141 tokens
slot      release: id  3 | task 41434 | stop processing: n_tokens = 2214, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.937
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41575 | processing task, is_child = 0
slot update_slots: id  3 | task 41575 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2123
slot update_slots: id  3 | task 41575 | n_tokens = 2075, memory_seq_rm [2075, end)
slot update_slots: id  3 | task 41575 | prompt processing progress, n_tokens = 2123, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 41575 | prompt done, n_tokens = 2123, batch.n_tokens = 48
slot init_sampler: id  3 | task 41575 | init sampler, took 0.36 ms, tokens: text = 2123, total = 2123
slot print_timing: id  3 | task 41575 | 
prompt eval time =     142.92 ms /    48 tokens (    2.98 ms per token,   335.85 tokens per second)
       eval time =    1077.49 ms /    42 tokens (   25.65 ms per token,    38.98 tokens per second)
      total time =    1220.41 ms /    90 tokens
slot      release: id  3 | task 41575 | stop processing: n_tokens = 2164, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41618 | processing task, is_child = 0
slot update_slots: id  3 | task 41618 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2171
slot update_slots: id  3 | task 41618 | n_tokens = 2123, memory_seq_rm [2123, end)
slot update_slots: id  3 | task 41618 | prompt processing progress, n_tokens = 2171, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 41618 | prompt done, n_tokens = 2171, batch.n_tokens = 48
slot init_sampler: id  3 | task 41618 | init sampler, took 0.35 ms, tokens: text = 2171, total = 2171
slot print_timing: id  3 | task 41618 | 
prompt eval time =     140.61 ms /    48 tokens (    2.93 ms per token,   341.38 tokens per second)
       eval time =    1420.21 ms /    56 tokens (   25.36 ms per token,    39.43 tokens per second)
      total time =    1560.81 ms /   104 tokens
slot      release: id  3 | task 41618 | stop processing: n_tokens = 2226, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 41675 | processing task, is_child = 0
slot update_slots: id  3 | task 41675 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2075
slot update_slots: id  3 | task 41675 | need to evaluate at least 1 token for each active slot (n_past = 2075, task.n_tokens() = 2075)
slot update_slots: id  3 | task 41675 | n_past was set to 2074
slot update_slots: id  3 | task 41675 | n_tokens = 2074, memory_seq_rm [2074, end)
slot update_slots: id  3 | task 41675 | prompt processing progress, n_tokens = 2075, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  3 | task 41675 | prompt done, n_tokens = 2075, batch.n_tokens = 1
slot init_sampler: id  3 | task 41675 | init sampler, took 0.41 ms, tokens: text = 2075, total = 2075
slot print_timing: id  3 | task 41675 | 
prompt eval time =      37.10 ms /     1 tokens (   37.10 ms per token,    26.95 tokens per second)
       eval time =    9708.51 ms /   362 tokens (   26.82 ms per token,    37.29 tokens per second)
      total time =    9745.62 ms /   363 tokens
slot      release: id  3 | task 41675 | stop processing: n_tokens = 2436, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.852
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 42038 | processing task, is_child = 0
slot update_slots: id  3 | task 42038 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2123
slot update_slots: id  3 | task 42038 | n_tokens = 2075, memory_seq_rm [2075, end)
slot update_slots: id  3 | task 42038 | prompt processing progress, n_tokens = 2123, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 42038 | prompt done, n_tokens = 2123, batch.n_tokens = 48
slot init_sampler: id  3 | task 42038 | init sampler, took 0.32 ms, tokens: text = 2123, total = 2123
slot print_timing: id  3 | task 42038 | 
prompt eval time =     150.56 ms /    48 tokens (    3.14 ms per token,   318.80 tokens per second)
       eval time =    1893.36 ms /    69 tokens (   27.44 ms per token,    36.44 tokens per second)
      total time =    2043.93 ms /   117 tokens
slot      release: id  3 | task 42038 | stop processing: n_tokens = 2191, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 42108 | processing task, is_child = 0
slot update_slots: id  3 | task 42108 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2171
slot update_slots: id  3 | task 42108 | n_tokens = 2123, memory_seq_rm [2123, end)
slot update_slots: id  3 | task 42108 | prompt processing progress, n_tokens = 2171, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 42108 | prompt done, n_tokens = 2171, batch.n_tokens = 48
slot init_sampler: id  3 | task 42108 | init sampler, took 0.32 ms, tokens: text = 2171, total = 2171
slot print_timing: id  3 | task 42108 | 
prompt eval time =     151.49 ms /    48 tokens (    3.16 ms per token,   316.86 tokens per second)
       eval time =    2760.64 ms /    99 tokens (   27.89 ms per token,    35.86 tokens per second)
      total time =    2912.12 ms /   147 tokens
slot      release: id  3 | task 42108 | stop processing: n_tokens = 2269, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.908
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 42208 | processing task, is_child = 0
slot update_slots: id  3 | task 42208 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2089
slot update_slots: id  3 | task 42208 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 42208 | prompt processing progress, n_tokens = 2089, batch.n_tokens = 28, progress = 1.000000
slot update_slots: id  3 | task 42208 | prompt done, n_tokens = 2089, batch.n_tokens = 28
slot init_sampler: id  3 | task 42208 | init sampler, took 0.34 ms, tokens: text = 2089, total = 2089
slot print_timing: id  3 | task 42208 | 
prompt eval time =     177.81 ms /    28 tokens (    6.35 ms per token,   157.48 tokens per second)
       eval time =    1977.80 ms /    77 tokens (   25.69 ms per token,    38.93 tokens per second)
      total time =    2155.60 ms /   105 tokens
slot      release: id  3 | task 42208 | stop processing: n_tokens = 2165, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 42286 | processing task, is_child = 0
slot update_slots: id  3 | task 42286 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2150
slot update_slots: id  3 | task 42286 | n_tokens = 2089, memory_seq_rm [2089, end)
slot update_slots: id  3 | task 42286 | prompt processing progress, n_tokens = 2150, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  3 | task 42286 | prompt done, n_tokens = 2150, batch.n_tokens = 61
slot init_sampler: id  3 | task 42286 | init sampler, took 0.35 ms, tokens: text = 2150, total = 2150
slot print_timing: id  3 | task 42286 | 
prompt eval time =     185.36 ms /    61 tokens (    3.04 ms per token,   329.10 tokens per second)
       eval time =    1434.36 ms /    55 tokens (   26.08 ms per token,    38.34 tokens per second)
      total time =    1619.71 ms /   116 tokens
slot      release: id  3 | task 42286 | stop processing: n_tokens = 2204, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.720 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 42342 | processing task, is_child = 0
slot update_slots: id  3 | task 42342 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2985
slot update_slots: id  3 | task 42342 | n_tokens = 2150, memory_seq_rm [2150, end)
slot update_slots: id  3 | task 42342 | prompt processing progress, n_tokens = 2921, batch.n_tokens = 771, progress = 0.978559
slot update_slots: id  3 | task 42342 | n_tokens = 2921, memory_seq_rm [2921, end)
slot update_slots: id  3 | task 42342 | prompt processing progress, n_tokens = 2985, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 42342 | prompt done, n_tokens = 2985, batch.n_tokens = 64
slot init_sampler: id  3 | task 42342 | init sampler, took 0.50 ms, tokens: text = 2985, total = 2985
slot update_slots: id  3 | task 42342 | created context checkpoint 4 of 8 (pos_min = 2024, pos_max = 2920, size = 21.034 MiB)
slot print_timing: id  3 | task 42342 | 
prompt eval time =    1257.32 ms /   835 tokens (    1.51 ms per token,   664.11 tokens per second)
       eval time =    2442.36 ms /    91 tokens (   26.84 ms per token,    37.26 tokens per second)
      total time =    3699.68 ms /   926 tokens
slot      release: id  3 | task 42342 | stop processing: n_tokens = 3075, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.633 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 42435 | processing task, is_child = 0
slot update_slots: id  3 | task 42435 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4717
slot update_slots: id  3 | task 42435 | n_tokens = 2985, memory_seq_rm [2985, end)
slot update_slots: id  3 | task 42435 | prompt processing progress, n_tokens = 4653, batch.n_tokens = 1668, progress = 0.986432
slot update_slots: id  3 | task 42435 | n_tokens = 4653, memory_seq_rm [4653, end)
slot update_slots: id  3 | task 42435 | prompt processing progress, n_tokens = 4717, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 42435 | prompt done, n_tokens = 4717, batch.n_tokens = 64
slot init_sampler: id  3 | task 42435 | init sampler, took 0.87 ms, tokens: text = 4717, total = 4717
slot update_slots: id  3 | task 42435 | created context checkpoint 5 of 8 (pos_min = 3756, pos_max = 4652, size = 21.034 MiB)
slot print_timing: id  3 | task 42435 | 
prompt eval time =    2473.53 ms /  1732 tokens (    1.43 ms per token,   700.21 tokens per second)
       eval time =    7831.06 ms /   278 tokens (   28.17 ms per token,    35.50 tokens per second)
      total time =   10304.59 ms /  2010 tokens
slot      release: id  3 | task 42435 | stop processing: n_tokens = 4994, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.945
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 42715 | processing task, is_child = 0
slot update_slots: id  3 | task 42715 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4966
slot update_slots: id  3 | task 42715 | n_tokens = 4717, memory_seq_rm [4717, end)
slot update_slots: id  3 | task 42715 | prompt processing progress, n_tokens = 4902, batch.n_tokens = 185, progress = 0.987112
slot update_slots: id  3 | task 42715 | n_tokens = 4902, memory_seq_rm [4902, end)
slot update_slots: id  3 | task 42715 | prompt processing progress, n_tokens = 4966, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 42715 | prompt done, n_tokens = 4966, batch.n_tokens = 64
slot init_sampler: id  3 | task 42715 | init sampler, took 0.74 ms, tokens: text = 4966, total = 4966
slot update_slots: id  3 | task 42715 | created context checkpoint 6 of 8 (pos_min = 4097, pos_max = 4901, size = 18.877 MiB)
slot print_timing: id  3 | task 42715 | 
prompt eval time =     533.23 ms /   249 tokens (    2.14 ms per token,   466.97 tokens per second)
       eval time =    5990.54 ms /   213 tokens (   28.12 ms per token,    35.56 tokens per second)
      total time =    6523.77 ms /   462 tokens
slot      release: id  3 | task 42715 | stop processing: n_tokens = 5178, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.772 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 42930 | processing task, is_child = 0
slot update_slots: id  3 | task 42930 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6433
slot update_slots: id  3 | task 42930 | n_tokens = 4966, memory_seq_rm [4966, end)
slot update_slots: id  3 | task 42930 | prompt processing progress, n_tokens = 6369, batch.n_tokens = 1403, progress = 0.990051
slot update_slots: id  3 | task 42930 | n_tokens = 6369, memory_seq_rm [6369, end)
slot update_slots: id  3 | task 42930 | prompt processing progress, n_tokens = 6433, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 42930 | prompt done, n_tokens = 6433, batch.n_tokens = 64
slot init_sampler: id  3 | task 42930 | init sampler, took 0.98 ms, tokens: text = 6433, total = 6433
slot update_slots: id  3 | task 42930 | created context checkpoint 7 of 8 (pos_min = 5472, pos_max = 6368, size = 21.034 MiB)
slot print_timing: id  3 | task 42930 | 
prompt eval time =    2114.34 ms /  1467 tokens (    1.44 ms per token,   693.83 tokens per second)
       eval time =   74708.49 ms /  2858 tokens (   26.14 ms per token,    38.26 tokens per second)
      total time =   76822.83 ms /  4325 tokens
slot      release: id  3 | task 42930 | stop processing: n_tokens = 9290, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.291 (> 0.100 thold), f_keep = 0.225
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9290, total state size = 238.875 MiB
srv          load:  - looking for better prompt, base f_keep = 0.225, sim = 0.291
srv        update:  - cache state: 10 prompts, 3822.859 MiB (limits: 8192.000 MiB, 64000 tokens, 208855 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv        update:    - prompt 0x5a0b7e703bc0:    6188 tokens, checkpoints:  4,   244.035 MiB
srv        update:    - prompt 0x5a0b83e530c0:    9513 tokens, checkpoints:  8,   414.791 MiB
srv        update:    - prompt 0x5a0b7f017ff0:    9737 tokens, checkpoints:  8,   413.947 MiB
srv        update:    - prompt 0x5a0ba6c10af0:   10977 tokens, checkpoints:  7,   422.342 MiB
srv        update:    - prompt 0x5a0ba6c62d80:    8696 tokens, checkpoints:  8,   376.945 MiB
srv        update:    - prompt 0x5a0b7e115ed0:   10721 tokens, checkpoints:  2,   314.498 MiB
srv        update:    - prompt 0x5a0b83831940:    9290 tokens, checkpoints:  7,   368.761 MiB
srv  get_availabl: prompt cache update took 326.94 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 45790 | processing task, is_child = 0
slot update_slots: id  3 | task 45790 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7175
slot update_slots: id  3 | task 45790 | n_past = 2089, slot.prompt.tokens.size() = 9290, seq_id = 3, pos_min = 8393, n_swa = 128
slot update_slots: id  3 | task 45790 | restored context checkpoint (pos_min = 1557, pos_max = 2183, size = 14.703 MiB)
slot update_slots: id  3 | task 45790 | erased invalidated context checkpoint (pos_min = 2024, pos_max = 2920, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 45790 | erased invalidated context checkpoint (pos_min = 3756, pos_max = 4652, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 45790 | erased invalidated context checkpoint (pos_min = 4097, pos_max = 4901, n_swa = 128, size = 18.877 MiB)
slot update_slots: id  3 | task 45790 | erased invalidated context checkpoint (pos_min = 5472, pos_max = 6368, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 45790 | n_tokens = 2089, memory_seq_rm [2089, end)
slot update_slots: id  3 | task 45790 | prompt processing progress, n_tokens = 4137, batch.n_tokens = 2048, progress = 0.576585
slot update_slots: id  3 | task 45790 | n_tokens = 4137, memory_seq_rm [4137, end)
slot update_slots: id  3 | task 45790 | prompt processing progress, n_tokens = 6185, batch.n_tokens = 2048, progress = 0.862021
slot update_slots: id  3 | task 45790 | n_tokens = 6185, memory_seq_rm [6185, end)
slot update_slots: id  3 | task 45790 | prompt processing progress, n_tokens = 7111, batch.n_tokens = 926, progress = 0.991080
slot update_slots: id  3 | task 45790 | n_tokens = 7111, memory_seq_rm [7111, end)
slot update_slots: id  3 | task 45790 | prompt processing progress, n_tokens = 7175, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 45790 | prompt done, n_tokens = 7175, batch.n_tokens = 64
slot init_sampler: id  3 | task 45790 | init sampler, took 1.35 ms, tokens: text = 7175, total = 7175
slot update_slots: id  3 | task 45790 | created context checkpoint 4 of 8 (pos_min = 6214, pos_max = 7110, size = 21.034 MiB)
slot print_timing: id  3 | task 45790 | 
prompt eval time =    6766.48 ms /  5086 tokens (    1.33 ms per token,   751.65 tokens per second)
       eval time =    7389.40 ms /   263 tokens (   28.10 ms per token,    35.59 tokens per second)
      total time =   14155.88 ms /  5349 tokens
slot      release: id  3 | task 45790 | stop processing: n_tokens = 7437, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.277
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7437, total state size = 195.424 MiB
srv          load:  - looking for better prompt, base f_keep = 0.277, sim = 0.993
srv        update:  - cache state: 11 prompts, 4087.224 MiB (limits: 8192.000 MiB, 64000 tokens, 210252 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv        update:    - prompt 0x5a0b7e703bc0:    6188 tokens, checkpoints:  4,   244.035 MiB
srv        update:    - prompt 0x5a0b83e530c0:    9513 tokens, checkpoints:  8,   414.791 MiB
srv        update:    - prompt 0x5a0b7f017ff0:    9737 tokens, checkpoints:  8,   413.947 MiB
srv        update:    - prompt 0x5a0ba6c10af0:   10977 tokens, checkpoints:  7,   422.342 MiB
srv        update:    - prompt 0x5a0ba6c62d80:    8696 tokens, checkpoints:  8,   376.945 MiB
srv        update:    - prompt 0x5a0b7e115ed0:   10721 tokens, checkpoints:  2,   314.498 MiB
srv        update:    - prompt 0x5a0b83831940:    9290 tokens, checkpoints:  7,   368.761 MiB
srv        update:    - prompt 0x5a0ba6a9e940:    7437 tokens, checkpoints:  4,   264.365 MiB
srv  get_availabl: prompt cache update took 184.66 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46057 | processing task, is_child = 0
slot update_slots: id  3 | task 46057 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2076
slot update_slots: id  3 | task 46057 | n_past = 2061, slot.prompt.tokens.size() = 7437, seq_id = 3, pos_min = 6540, n_swa = 128
slot update_slots: id  3 | task 46057 | restored context checkpoint (pos_min = 1557, pos_max = 2183, size = 14.703 MiB)
slot update_slots: id  3 | task 46057 | erased invalidated context checkpoint (pos_min = 6214, pos_max = 7110, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 46057 | n_tokens = 2061, memory_seq_rm [2061, end)
slot update_slots: id  3 | task 46057 | prompt processing progress, n_tokens = 2076, batch.n_tokens = 15, progress = 1.000000
slot update_slots: id  3 | task 46057 | prompt done, n_tokens = 2076, batch.n_tokens = 15
slot init_sampler: id  3 | task 46057 | init sampler, took 0.39 ms, tokens: text = 2076, total = 2076
slot print_timing: id  3 | task 46057 | 
prompt eval time =     251.85 ms /    15 tokens (   16.79 ms per token,    59.56 tokens per second)
       eval time =    5864.76 ms /   224 tokens (   26.18 ms per token,    38.19 tokens per second)
      total time =    6116.61 ms /   239 tokens
slot      release: id  3 | task 46057 | stop processing: n_tokens = 2299, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.903
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46282 | processing task, is_child = 0
slot update_slots: id  3 | task 46282 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2162
slot update_slots: id  3 | task 46282 | n_tokens = 2077, memory_seq_rm [2077, end)
slot update_slots: id  3 | task 46282 | prompt processing progress, n_tokens = 2098, batch.n_tokens = 21, progress = 0.970398
slot update_slots: id  3 | task 46282 | n_tokens = 2098, memory_seq_rm [2098, end)
slot update_slots: id  3 | task 46282 | prompt processing progress, n_tokens = 2162, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 46282 | prompt done, n_tokens = 2162, batch.n_tokens = 64
slot init_sampler: id  3 | task 46282 | init sampler, took 0.41 ms, tokens: text = 2162, total = 2162
slot print_timing: id  3 | task 46282 | 
prompt eval time =     383.28 ms /    85 tokens (    4.51 ms per token,   221.77 tokens per second)
       eval time =    1093.25 ms /    42 tokens (   26.03 ms per token,    38.42 tokens per second)
      total time =    1476.54 ms /   127 tokens
slot      release: id  3 | task 46282 | stop processing: n_tokens = 2203, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46326 | processing task, is_child = 0
slot update_slots: id  3 | task 46326 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2210
slot update_slots: id  3 | task 46326 | n_tokens = 2162, memory_seq_rm [2162, end)
slot update_slots: id  3 | task 46326 | prompt processing progress, n_tokens = 2210, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 46326 | prompt done, n_tokens = 2210, batch.n_tokens = 48
slot init_sampler: id  3 | task 46326 | init sampler, took 0.36 ms, tokens: text = 2210, total = 2210
slot print_timing: id  3 | task 46326 | 
prompt eval time =     147.68 ms /    48 tokens (    3.08 ms per token,   325.03 tokens per second)
       eval time =     518.36 ms /    20 tokens (   25.92 ms per token,    38.58 tokens per second)
      total time =     666.03 ms /    68 tokens
slot      release: id  3 | task 46326 | stop processing: n_tokens = 2229, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46347 | processing task, is_child = 0
slot update_slots: id  3 | task 46347 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2258
slot update_slots: id  3 | task 46347 | n_tokens = 2210, memory_seq_rm [2210, end)
slot update_slots: id  3 | task 46347 | prompt processing progress, n_tokens = 2258, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 46347 | prompt done, n_tokens = 2258, batch.n_tokens = 48
slot init_sampler: id  3 | task 46347 | init sampler, took 0.35 ms, tokens: text = 2258, total = 2258
slot print_timing: id  3 | task 46347 | 
prompt eval time =     147.39 ms /    48 tokens (    3.07 ms per token,   325.66 tokens per second)
       eval time =     836.40 ms /    32 tokens (   26.14 ms per token,    38.26 tokens per second)
      total time =     983.80 ms /    80 tokens
slot      release: id  3 | task 46347 | stop processing: n_tokens = 2289, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.926 (> 0.100 thold), f_keep = 0.945
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46380 | processing task, is_child = 0
slot update_slots: id  3 | task 46380 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2335
slot update_slots: id  3 | task 46380 | n_tokens = 2162, memory_seq_rm [2162, end)
slot update_slots: id  3 | task 46380 | prompt processing progress, n_tokens = 2271, batch.n_tokens = 109, progress = 0.972591
slot update_slots: id  3 | task 46380 | n_tokens = 2271, memory_seq_rm [2271, end)
slot update_slots: id  3 | task 46380 | prompt processing progress, n_tokens = 2335, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 46380 | prompt done, n_tokens = 2335, batch.n_tokens = 64
slot init_sampler: id  3 | task 46380 | init sampler, took 0.38 ms, tokens: text = 2335, total = 2335
slot update_slots: id  3 | task 46380 | created context checkpoint 4 of 8 (pos_min = 1672, pos_max = 2270, size = 14.046 MiB)
slot print_timing: id  3 | task 46380 | 
prompt eval time =     588.81 ms /   173 tokens (    3.40 ms per token,   293.81 tokens per second)
       eval time =    1480.81 ms /    56 tokens (   26.44 ms per token,    37.82 tokens per second)
      total time =    2069.62 ms /   229 tokens
slot      release: id  3 | task 46380 | stop processing: n_tokens = 2390, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46438 | processing task, is_child = 0
slot update_slots: id  3 | task 46438 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2383
slot update_slots: id  3 | task 46438 | n_tokens = 2335, memory_seq_rm [2335, end)
slot update_slots: id  3 | task 46438 | prompt processing progress, n_tokens = 2383, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 46438 | prompt done, n_tokens = 2383, batch.n_tokens = 48
slot init_sampler: id  3 | task 46438 | init sampler, took 0.36 ms, tokens: text = 2383, total = 2383
slot print_timing: id  3 | task 46438 | 
prompt eval time =     151.50 ms /    48 tokens (    3.16 ms per token,   316.84 tokens per second)
       eval time =    1521.39 ms /    56 tokens (   27.17 ms per token,    36.81 tokens per second)
      total time =    1672.89 ms /   104 tokens
slot      release: id  3 | task 46438 | stop processing: n_tokens = 2438, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46495 | processing task, is_child = 0
slot update_slots: id  3 | task 46495 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2431
slot update_slots: id  3 | task 46495 | n_tokens = 2383, memory_seq_rm [2383, end)
slot update_slots: id  3 | task 46495 | prompt processing progress, n_tokens = 2431, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 46495 | prompt done, n_tokens = 2431, batch.n_tokens = 48
slot init_sampler: id  3 | task 46495 | init sampler, took 0.40 ms, tokens: text = 2431, total = 2431
slot update_slots: id  3 | task 46495 | created context checkpoint 5 of 8 (pos_min = 1794, pos_max = 2382, size = 13.812 MiB)
slot print_timing: id  3 | task 46495 | 
prompt eval time =     160.34 ms /    48 tokens (    3.34 ms per token,   299.35 tokens per second)
       eval time =    2812.39 ms /   102 tokens (   27.57 ms per token,    36.27 tokens per second)
      total time =    2972.73 ms /   150 tokens
slot      release: id  3 | task 46495 | stop processing: n_tokens = 2532, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.928 (> 0.100 thold), f_keep = 0.922
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46598 | processing task, is_child = 0
slot update_slots: id  3 | task 46598 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2517
slot update_slots: id  3 | task 46598 | n_tokens = 2335, memory_seq_rm [2335, end)
slot update_slots: id  3 | task 46598 | prompt processing progress, n_tokens = 2453, batch.n_tokens = 118, progress = 0.974573
slot update_slots: id  3 | task 46598 | n_tokens = 2453, memory_seq_rm [2453, end)
slot update_slots: id  3 | task 46598 | prompt processing progress, n_tokens = 2517, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 46598 | prompt done, n_tokens = 2517, batch.n_tokens = 64
slot init_sampler: id  3 | task 46598 | init sampler, took 0.47 ms, tokens: text = 2517, total = 2517
slot update_slots: id  3 | task 46598 | created context checkpoint 6 of 8 (pos_min = 1794, pos_max = 2452, size = 15.453 MiB)
slot print_timing: id  3 | task 46598 | 
prompt eval time =     618.24 ms /   182 tokens (    3.40 ms per token,   294.39 tokens per second)
       eval time =    2227.31 ms /    88 tokens (   25.31 ms per token,    39.51 tokens per second)
      total time =    2845.55 ms /   270 tokens
slot      release: id  3 | task 46598 | stop processing: n_tokens = 2604, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46688 | processing task, is_child = 0
slot update_slots: id  3 | task 46688 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2559
slot update_slots: id  3 | task 46688 | n_tokens = 2517, memory_seq_rm [2517, end)
slot update_slots: id  3 | task 46688 | prompt processing progress, n_tokens = 2559, batch.n_tokens = 42, progress = 1.000000
slot update_slots: id  3 | task 46688 | prompt done, n_tokens = 2559, batch.n_tokens = 42
slot init_sampler: id  3 | task 46688 | init sampler, took 0.43 ms, tokens: text = 2559, total = 2559
slot print_timing: id  3 | task 46688 | 
prompt eval time =     240.00 ms /    42 tokens (    5.71 ms per token,   175.00 tokens per second)
       eval time =    1421.69 ms /    55 tokens (   25.85 ms per token,    38.69 tokens per second)
      total time =    1661.69 ms /    97 tokens
slot      release: id  3 | task 46688 | stop processing: n_tokens = 2613, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46744 | processing task, is_child = 0
slot update_slots: id  3 | task 46744 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2600
slot update_slots: id  3 | task 46744 | n_tokens = 2559, memory_seq_rm [2559, end)
slot update_slots: id  3 | task 46744 | prompt processing progress, n_tokens = 2600, batch.n_tokens = 41, progress = 1.000000
slot update_slots: id  3 | task 46744 | prompt done, n_tokens = 2600, batch.n_tokens = 41
slot init_sampler: id  3 | task 46744 | init sampler, took 0.50 ms, tokens: text = 2600, total = 2600
slot update_slots: id  3 | task 46744 | created context checkpoint 7 of 8 (pos_min = 1794, pos_max = 2558, size = 17.939 MiB)
slot print_timing: id  3 | task 46744 | 
prompt eval time =     232.81 ms /    41 tokens (    5.68 ms per token,   176.11 tokens per second)
       eval time =    2613.50 ms /   100 tokens (   26.14 ms per token,    38.26 tokens per second)
      total time =    2846.32 ms /   141 tokens
slot      release: id  3 | task 46744 | stop processing: n_tokens = 2699, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.963
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46845 | processing task, is_child = 0
slot update_slots: id  3 | task 46845 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2650
slot update_slots: id  3 | task 46845 | n_tokens = 2600, memory_seq_rm [2600, end)
slot update_slots: id  3 | task 46845 | prompt processing progress, n_tokens = 2650, batch.n_tokens = 50, progress = 1.000000
slot update_slots: id  3 | task 46845 | prompt done, n_tokens = 2650, batch.n_tokens = 50
slot init_sampler: id  3 | task 46845 | init sampler, took 0.40 ms, tokens: text = 2650, total = 2650
slot print_timing: id  3 | task 46845 | 
prompt eval time =     236.75 ms /    50 tokens (    4.74 ms per token,   211.19 tokens per second)
       eval time =    3141.58 ms /   118 tokens (   26.62 ms per token,    37.56 tokens per second)
      total time =    3378.33 ms /   168 tokens
slot      release: id  3 | task 46845 | stop processing: n_tokens = 2767, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 46964 | processing task, is_child = 0
slot update_slots: id  3 | task 46964 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2706
slot update_slots: id  3 | task 46964 | n_tokens = 2650, memory_seq_rm [2650, end)
slot update_slots: id  3 | task 46964 | prompt processing progress, n_tokens = 2706, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  3 | task 46964 | prompt done, n_tokens = 2706, batch.n_tokens = 56
slot init_sampler: id  3 | task 46964 | init sampler, took 0.45 ms, tokens: text = 2706, total = 2706
slot update_slots: id  3 | task 46964 | created context checkpoint 8 of 8 (pos_min = 1961, pos_max = 2649, size = 16.157 MiB)
slot print_timing: id  3 | task 46964 | 
prompt eval time =     271.35 ms /    56 tokens (    4.85 ms per token,   206.38 tokens per second)
       eval time =    4605.35 ms /   168 tokens (   27.41 ms per token,    36.48 tokens per second)
      total time =    4876.69 ms /   224 tokens
slot      release: id  3 | task 46964 | stop processing: n_tokens = 2873, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.942
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 47133 | processing task, is_child = 0
slot update_slots: id  3 | task 47133 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2756
slot update_slots: id  3 | task 47133 | n_tokens = 2706, memory_seq_rm [2706, end)
slot update_slots: id  3 | task 47133 | prompt processing progress, n_tokens = 2756, batch.n_tokens = 50, progress = 1.000000
slot update_slots: id  3 | task 47133 | prompt done, n_tokens = 2756, batch.n_tokens = 50
slot init_sampler: id  3 | task 47133 | init sampler, took 0.41 ms, tokens: text = 2756, total = 2756
slot print_timing: id  3 | task 47133 | 
prompt eval time =     242.32 ms /    50 tokens (    4.85 ms per token,   206.34 tokens per second)
       eval time =    5137.06 ms /   195 tokens (   26.34 ms per token,    37.96 tokens per second)
      total time =    5379.38 ms /   245 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 47133 | stop processing: n_tokens = 2950, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.934
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 47329 | processing task, is_child = 0
slot update_slots: id  3 | task 47329 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2799
slot update_slots: id  3 | task 47329 | n_tokens = 2756, memory_seq_rm [2756, end)
slot update_slots: id  3 | task 47329 | prompt processing progress, n_tokens = 2799, batch.n_tokens = 43, progress = 1.000000
slot update_slots: id  3 | task 47329 | prompt done, n_tokens = 2799, batch.n_tokens = 43
slot init_sampler: id  3 | task 47329 | init sampler, took 0.44 ms, tokens: text = 2799, total = 2799
slot update_slots: id  3 | task 47329 | erasing old context checkpoint (pos_min = 1114, pos_max = 2010, size = 21.034 MiB)
slot update_slots: id  3 | task 47329 | created context checkpoint 8 of 8 (pos_min = 2144, pos_max = 2755, size = 14.351 MiB)
slot print_timing: id  3 | task 47329 | 
prompt eval time =     256.03 ms /    43 tokens (    5.95 ms per token,   167.95 tokens per second)
       eval time =    4193.55 ms /   161 tokens (   26.05 ms per token,    38.39 tokens per second)
      total time =    4449.59 ms /   204 tokens
slot      release: id  3 | task 47329 | stop processing: n_tokens = 2959, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.946
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 47491 | processing task, is_child = 0
slot update_slots: id  3 | task 47491 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2855
slot update_slots: id  3 | task 47491 | n_tokens = 2799, memory_seq_rm [2799, end)
slot update_slots: id  3 | task 47491 | prompt processing progress, n_tokens = 2855, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  3 | task 47491 | prompt done, n_tokens = 2855, batch.n_tokens = 56
slot init_sampler: id  3 | task 47491 | init sampler, took 0.47 ms, tokens: text = 2855, total = 2855
slot print_timing: id  3 | task 47491 | 
prompt eval time =     250.06 ms /    56 tokens (    4.47 ms per token,   223.94 tokens per second)
       eval time =    4968.21 ms /   190 tokens (   26.15 ms per token,    38.24 tokens per second)
      total time =    5218.27 ms /   246 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 47491 | stop processing: n_tokens = 3044, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.811 (> 0.100 thold), f_keep = 0.827
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 47682 | processing task, is_child = 0
slot update_slots: id  3 | task 47682 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3104
slot update_slots: id  3 | task 47682 | n_tokens = 2517, memory_seq_rm [2517, end)
slot update_slots: id  3 | task 47682 | prompt processing progress, n_tokens = 3040, batch.n_tokens = 523, progress = 0.979381
slot update_slots: id  3 | task 47682 | n_tokens = 3040, memory_seq_rm [3040, end)
slot update_slots: id  3 | task 47682 | prompt processing progress, n_tokens = 3104, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 47682 | prompt done, n_tokens = 3104, batch.n_tokens = 64
slot init_sampler: id  3 | task 47682 | init sampler, took 0.53 ms, tokens: text = 3104, total = 3104
slot update_slots: id  3 | task 47682 | erasing old context checkpoint (pos_min = 1557, pos_max = 2075, size = 12.170 MiB)
slot update_slots: id  3 | task 47682 | created context checkpoint 8 of 8 (pos_min = 2317, pos_max = 3039, size = 16.954 MiB)
slot print_timing: id  3 | task 47682 | 
prompt eval time =    1003.46 ms /   587 tokens (    1.71 ms per token,   584.98 tokens per second)
       eval time =   35730.39 ms /  1303 tokens (   27.42 ms per token,    36.47 tokens per second)
      total time =   36733.85 ms /  1890 tokens
slot      release: id  3 | task 47682 | stop processing: n_tokens = 4406, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.817 (> 0.100 thold), f_keep = 0.705
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 48987 | processing task, is_child = 0
slot update_slots: id  3 | task 48987 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3800
slot update_slots: id  3 | task 48987 | n_past = 3105, slot.prompt.tokens.size() = 4406, seq_id = 3, pos_min = 3509, n_swa = 128
slot update_slots: id  3 | task 48987 | restored context checkpoint (pos_min = 2317, pos_max = 3039, size = 16.954 MiB)
slot update_slots: id  3 | task 48987 | n_tokens = 3039, memory_seq_rm [3039, end)
slot update_slots: id  3 | task 48987 | prompt processing progress, n_tokens = 3736, batch.n_tokens = 697, progress = 0.983158
slot update_slots: id  3 | task 48987 | n_tokens = 3736, memory_seq_rm [3736, end)
slot update_slots: id  3 | task 48987 | prompt processing progress, n_tokens = 3800, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 48987 | prompt done, n_tokens = 3800, batch.n_tokens = 64
slot init_sampler: id  3 | task 48987 | init sampler, took 0.78 ms, tokens: text = 3800, total = 3800
slot update_slots: id  3 | task 48987 | erasing old context checkpoint (pos_min = 1557, pos_max = 2183, size = 14.703 MiB)
slot update_slots: id  3 | task 48987 | created context checkpoint 8 of 8 (pos_min = 2839, pos_max = 3735, size = 21.034 MiB)
slot print_timing: id  3 | task 48987 | 
prompt eval time =    1333.81 ms /   761 tokens (    1.75 ms per token,   570.54 tokens per second)
       eval time =     993.96 ms /    36 tokens (   27.61 ms per token,    36.22 tokens per second)
      total time =    2327.78 ms /   797 tokens
slot      release: id  3 | task 48987 | stop processing: n_tokens = 3835, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.869 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 49025 | processing task, is_child = 0
slot update_slots: id  3 | task 49025 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4371
slot update_slots: id  3 | task 49025 | n_tokens = 3800, memory_seq_rm [3800, end)
slot update_slots: id  3 | task 49025 | prompt processing progress, n_tokens = 4307, batch.n_tokens = 507, progress = 0.985358
slot update_slots: id  3 | task 49025 | n_tokens = 4307, memory_seq_rm [4307, end)
slot update_slots: id  3 | task 49025 | prompt processing progress, n_tokens = 4371, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 49025 | prompt done, n_tokens = 4371, batch.n_tokens = 64
slot init_sampler: id  3 | task 49025 | init sampler, took 0.87 ms, tokens: text = 4371, total = 4371
slot update_slots: id  3 | task 49025 | erasing old context checkpoint (pos_min = 1672, pos_max = 2270, size = 14.046 MiB)
slot update_slots: id  3 | task 49025 | created context checkpoint 8 of 8 (pos_min = 3410, pos_max = 4306, size = 21.034 MiB)
slot print_timing: id  3 | task 49025 | 
prompt eval time =     809.81 ms /   571 tokens (    1.42 ms per token,   705.10 tokens per second)
       eval time =    1514.85 ms /    56 tokens (   27.05 ms per token,    36.97 tokens per second)
      total time =    2324.66 ms /   627 tokens
slot      release: id  3 | task 49025 | stop processing: n_tokens = 4426, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.717 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 49083 | processing task, is_child = 0
slot update_slots: id  3 | task 49083 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6098
slot update_slots: id  3 | task 49083 | n_tokens = 4371, memory_seq_rm [4371, end)
slot update_slots: id  3 | task 49083 | prompt processing progress, n_tokens = 6034, batch.n_tokens = 1663, progress = 0.989505
slot update_slots: id  3 | task 49083 | n_tokens = 6034, memory_seq_rm [6034, end)
slot update_slots: id  3 | task 49083 | prompt processing progress, n_tokens = 6098, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 49083 | prompt done, n_tokens = 6098, batch.n_tokens = 64
slot init_sampler: id  3 | task 49083 | init sampler, took 0.91 ms, tokens: text = 6098, total = 6098
slot update_slots: id  3 | task 49083 | erasing old context checkpoint (pos_min = 1794, pos_max = 2382, size = 13.812 MiB)
slot update_slots: id  3 | task 49083 | created context checkpoint 8 of 8 (pos_min = 5137, pos_max = 6033, size = 21.034 MiB)
slot print_timing: id  3 | task 49083 | 
prompt eval time =    2526.74 ms /  1727 tokens (    1.46 ms per token,   683.49 tokens per second)
       eval time =   22510.76 ms /   819 tokens (   27.49 ms per token,    36.38 tokens per second)
      total time =   25037.50 ms /  2546 tokens
slot      release: id  3 | task 49083 | stop processing: n_tokens = 6916, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.882
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 49904 | processing task, is_child = 0
slot update_slots: id  3 | task 49904 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6154
slot update_slots: id  3 | task 49904 | n_past = 6098, slot.prompt.tokens.size() = 6916, seq_id = 3, pos_min = 6019, n_swa = 128
slot update_slots: id  3 | task 49904 | restored context checkpoint (pos_min = 5137, pos_max = 6033, size = 21.034 MiB)
slot update_slots: id  3 | task 49904 | n_tokens = 6033, memory_seq_rm [6033, end)
slot update_slots: id  3 | task 49904 | prompt processing progress, n_tokens = 6090, batch.n_tokens = 57, progress = 0.989600
slot update_slots: id  3 | task 49904 | n_tokens = 6090, memory_seq_rm [6090, end)
slot update_slots: id  3 | task 49904 | prompt processing progress, n_tokens = 6154, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 49904 | prompt done, n_tokens = 6154, batch.n_tokens = 64
slot init_sampler: id  3 | task 49904 | init sampler, took 1.15 ms, tokens: text = 6154, total = 6154
slot print_timing: id  3 | task 49904 | 
prompt eval time =     530.60 ms /   121 tokens (    4.39 ms per token,   228.05 tokens per second)
       eval time =    3436.22 ms /   128 tokens (   26.85 ms per token,    37.25 tokens per second)
      total time =    3966.81 ms /   249 tokens
slot      release: id  3 | task 49904 | stop processing: n_tokens = 6281, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 50034 | processing task, is_child = 0
slot update_slots: id  3 | task 50034 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6326
slot update_slots: id  3 | task 50034 | n_tokens = 6154, memory_seq_rm [6154, end)
slot update_slots: id  3 | task 50034 | prompt processing progress, n_tokens = 6262, batch.n_tokens = 108, progress = 0.989883
slot update_slots: id  3 | task 50034 | n_tokens = 6262, memory_seq_rm [6262, end)
slot update_slots: id  3 | task 50034 | prompt processing progress, n_tokens = 6326, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 50034 | prompt done, n_tokens = 6326, batch.n_tokens = 64
slot init_sampler: id  3 | task 50034 | init sampler, took 0.95 ms, tokens: text = 6326, total = 6326
slot update_slots: id  3 | task 50034 | erasing old context checkpoint (pos_min = 1794, pos_max = 2452, size = 15.453 MiB)
slot update_slots: id  3 | task 50034 | created context checkpoint 8 of 8 (pos_min = 5384, pos_max = 6261, size = 20.588 MiB)
slot print_timing: id  3 | task 50034 | 
prompt eval time =     485.13 ms /   172 tokens (    2.82 ms per token,   354.54 tokens per second)
       eval time =    3630.26 ms /   138 tokens (   26.31 ms per token,    38.01 tokens per second)
      total time =    4115.39 ms /   310 tokens
slot      release: id  3 | task 50034 | stop processing: n_tokens = 6463, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 50174 | processing task, is_child = 0
slot update_slots: id  3 | task 50174 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6442
slot update_slots: id  3 | task 50174 | n_tokens = 6326, memory_seq_rm [6326, end)
slot update_slots: id  3 | task 50174 | prompt processing progress, n_tokens = 6378, batch.n_tokens = 52, progress = 0.990065
slot update_slots: id  3 | task 50174 | n_tokens = 6378, memory_seq_rm [6378, end)
slot update_slots: id  3 | task 50174 | prompt processing progress, n_tokens = 6442, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 50174 | prompt done, n_tokens = 6442, batch.n_tokens = 64
slot init_sampler: id  3 | task 50174 | init sampler, took 0.93 ms, tokens: text = 6442, total = 6442
slot update_slots: id  3 | task 50174 | erasing old context checkpoint (pos_min = 1794, pos_max = 2558, size = 17.939 MiB)
slot update_slots: id  3 | task 50174 | created context checkpoint 8 of 8 (pos_min = 5566, pos_max = 6377, size = 19.041 MiB)
slot print_timing: id  3 | task 50174 | 
prompt eval time =     380.29 ms /   116 tokens (    3.28 ms per token,   305.03 tokens per second)
       eval time =    2290.89 ms /    87 tokens (   26.33 ms per token,    37.98 tokens per second)
      total time =    2671.17 ms /   203 tokens
slot      release: id  3 | task 50174 | stop processing: n_tokens = 6528, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 50263 | processing task, is_child = 0
slot update_slots: id  3 | task 50263 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6705
slot update_slots: id  3 | task 50263 | n_tokens = 6442, memory_seq_rm [6442, end)
slot update_slots: id  3 | task 50263 | prompt processing progress, n_tokens = 6641, batch.n_tokens = 199, progress = 0.990455
slot update_slots: id  3 | task 50263 | n_tokens = 6641, memory_seq_rm [6641, end)
slot update_slots: id  3 | task 50263 | prompt processing progress, n_tokens = 6705, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 50263 | prompt done, n_tokens = 6705, batch.n_tokens = 64
slot init_sampler: id  3 | task 50263 | init sampler, took 1.24 ms, tokens: text = 6705, total = 6705
slot update_slots: id  3 | task 50263 | erasing old context checkpoint (pos_min = 1961, pos_max = 2649, size = 16.157 MiB)
slot update_slots: id  3 | task 50263 | created context checkpoint 8 of 8 (pos_min = 5744, pos_max = 6640, size = 21.034 MiB)
slot print_timing: id  3 | task 50263 | 
prompt eval time =     548.76 ms /   263 tokens (    2.09 ms per token,   479.27 tokens per second)
       eval time =    2879.53 ms /   111 tokens (   25.94 ms per token,    38.55 tokens per second)
      total time =    3428.28 ms /   374 tokens
slot      release: id  3 | task 50263 | stop processing: n_tokens = 6815, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.819 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 50376 | processing task, is_child = 0
slot update_slots: id  3 | task 50376 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8182
slot update_slots: id  3 | task 50376 | n_tokens = 6705, memory_seq_rm [6705, end)
slot update_slots: id  3 | task 50376 | prompt processing progress, n_tokens = 8118, batch.n_tokens = 1413, progress = 0.992178
slot update_slots: id  3 | task 50376 | n_tokens = 8118, memory_seq_rm [8118, end)
slot update_slots: id  3 | task 50376 | prompt processing progress, n_tokens = 8182, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 50376 | prompt done, n_tokens = 8182, batch.n_tokens = 64
slot init_sampler: id  3 | task 50376 | init sampler, took 1.23 ms, tokens: text = 8182, total = 8182
slot update_slots: id  3 | task 50376 | erasing old context checkpoint (pos_min = 2144, pos_max = 2755, size = 14.351 MiB)
slot update_slots: id  3 | task 50376 | created context checkpoint 8 of 8 (pos_min = 7221, pos_max = 8117, size = 21.034 MiB)
slot print_timing: id  3 | task 50376 | 
prompt eval time =    1977.09 ms /  1477 tokens (    1.34 ms per token,   747.06 tokens per second)
       eval time =   12525.91 ms /   484 tokens (   25.88 ms per token,    38.64 tokens per second)
      total time =   14503.00 ms /  1961 tokens
slot      release: id  3 | task 50376 | stop processing: n_tokens = 8665, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.968 (> 0.100 thold), f_keep = 0.944
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 50862 | processing task, is_child = 0
slot update_slots: id  3 | task 50862 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8450
slot update_slots: id  3 | task 50862 | n_tokens = 8182, memory_seq_rm [8182, end)
slot update_slots: id  3 | task 50862 | prompt processing progress, n_tokens = 8386, batch.n_tokens = 204, progress = 0.992426
slot update_slots: id  3 | task 50862 | n_tokens = 8386, memory_seq_rm [8386, end)
slot update_slots: id  3 | task 50862 | prompt processing progress, n_tokens = 8450, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 50862 | prompt done, n_tokens = 8450, batch.n_tokens = 64
slot init_sampler: id  3 | task 50862 | init sampler, took 1.23 ms, tokens: text = 8450, total = 8450
slot update_slots: id  3 | task 50862 | erasing old context checkpoint (pos_min = 2317, pos_max = 3039, size = 16.954 MiB)
slot update_slots: id  3 | task 50862 | created context checkpoint 8 of 8 (pos_min = 7768, pos_max = 8385, size = 14.492 MiB)
slot print_timing: id  3 | task 50862 | 
prompt eval time =     540.34 ms /   268 tokens (    2.02 ms per token,   495.98 tokens per second)
       eval time =   25959.18 ms /  1005 tokens (   25.83 ms per token,    38.71 tokens per second)
      total time =   26499.52 ms /  1273 tokens
slot      release: id  3 | task 50862 | stop processing: n_tokens = 9454, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.218
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9454, total state size = 242.721 MiB
srv          load:  - looking for better prompt, base f_keep = 0.218, sim = 0.986
srv        update:  - cache state: 12 prompts, 4489.236 MiB (limits: 8192.000 MiB, 64000 tokens, 208676 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv        update:    - prompt 0x5a0b7e703bc0:    6188 tokens, checkpoints:  4,   244.035 MiB
srv        update:    - prompt 0x5a0b83e530c0:    9513 tokens, checkpoints:  8,   414.791 MiB
srv        update:    - prompt 0x5a0b7f017ff0:    9737 tokens, checkpoints:  8,   413.947 MiB
srv        update:    - prompt 0x5a0ba6c10af0:   10977 tokens, checkpoints:  7,   422.342 MiB
srv        update:    - prompt 0x5a0ba6c62d80:    8696 tokens, checkpoints:  8,   376.945 MiB
srv        update:    - prompt 0x5a0b7e115ed0:   10721 tokens, checkpoints:  2,   314.498 MiB
srv        update:    - prompt 0x5a0b83831940:    9290 tokens, checkpoints:  7,   368.761 MiB
srv        update:    - prompt 0x5a0ba6a9e940:    7437 tokens, checkpoints:  4,   264.365 MiB
srv        update:    - prompt 0x5a0ba48fa680:    9454 tokens, checkpoints:  8,   402.012 MiB
srv  get_availabl: prompt cache update took 415.98 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 51869 | processing task, is_child = 0
slot update_slots: id  3 | task 51869 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2090
slot update_slots: id  3 | task 51869 | n_past = 2061, slot.prompt.tokens.size() = 9454, seq_id = 3, pos_min = 8557, n_swa = 128
slot update_slots: id  3 | task 51869 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 51869 | erased invalidated context checkpoint (pos_min = 2839, pos_max = 3735, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 51869 | erased invalidated context checkpoint (pos_min = 3410, pos_max = 4306, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 51869 | erased invalidated context checkpoint (pos_min = 5137, pos_max = 6033, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 51869 | erased invalidated context checkpoint (pos_min = 5384, pos_max = 6261, n_swa = 128, size = 20.588 MiB)
slot update_slots: id  3 | task 51869 | erased invalidated context checkpoint (pos_min = 5566, pos_max = 6377, n_swa = 128, size = 19.041 MiB)
slot update_slots: id  3 | task 51869 | erased invalidated context checkpoint (pos_min = 5744, pos_max = 6640, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 51869 | erased invalidated context checkpoint (pos_min = 7221, pos_max = 8117, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 51869 | erased invalidated context checkpoint (pos_min = 7768, pos_max = 8385, n_swa = 128, size = 14.492 MiB)
slot update_slots: id  3 | task 51869 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 51869 | prompt processing progress, n_tokens = 2026, batch.n_tokens = 2026, progress = 0.969378
slot update_slots: id  3 | task 51869 | n_tokens = 2026, memory_seq_rm [2026, end)
slot update_slots: id  3 | task 51869 | prompt processing progress, n_tokens = 2090, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 51869 | prompt done, n_tokens = 2090, batch.n_tokens = 64
slot init_sampler: id  3 | task 51869 | init sampler, took 0.34 ms, tokens: text = 2090, total = 2090
slot update_slots: id  3 | task 51869 | created context checkpoint 1 of 8 (pos_min = 1129, pos_max = 2025, size = 21.034 MiB)
slot print_timing: id  3 | task 51869 | 
prompt eval time =    2802.76 ms /  2090 tokens (    1.34 ms per token,   745.69 tokens per second)
       eval time =    4749.23 ms /   169 tokens (   28.10 ms per token,    35.58 tokens per second)
      total time =    7551.99 ms /  2259 tokens
slot      release: id  3 | task 51869 | stop processing: n_tokens = 2258, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.926
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 52040 | processing task, is_child = 0
slot update_slots: id  3 | task 52040 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2135
slot update_slots: id  3 | task 52040 | n_tokens = 2090, memory_seq_rm [2090, end)
slot update_slots: id  3 | task 52040 | prompt processing progress, n_tokens = 2135, batch.n_tokens = 45, progress = 1.000000
slot update_slots: id  3 | task 52040 | prompt done, n_tokens = 2135, batch.n_tokens = 45
slot init_sampler: id  3 | task 52040 | init sampler, took 0.36 ms, tokens: text = 2135, total = 2135
slot print_timing: id  3 | task 52040 | 
prompt eval time =     146.54 ms /    45 tokens (    3.26 ms per token,   307.08 tokens per second)
       eval time =    4199.87 ms /   150 tokens (   28.00 ms per token,    35.72 tokens per second)
      total time =    4346.41 ms /   195 tokens
slot      release: id  3 | task 52040 | stop processing: n_tokens = 2284, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.935
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 52191 | processing task, is_child = 0
slot update_slots: id  3 | task 52191 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2191
slot update_slots: id  3 | task 52191 | n_tokens = 2135, memory_seq_rm [2135, end)
slot update_slots: id  3 | task 52191 | prompt processing progress, n_tokens = 2191, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  3 | task 52191 | prompt done, n_tokens = 2191, batch.n_tokens = 56
slot init_sampler: id  3 | task 52191 | init sampler, took 0.40 ms, tokens: text = 2191, total = 2191
slot update_slots: id  3 | task 52191 | created context checkpoint 2 of 8 (pos_min = 1387, pos_max = 2134, size = 17.540 MiB)
slot print_timing: id  3 | task 52191 | 
prompt eval time =     198.57 ms /    56 tokens (    3.55 ms per token,   282.02 tokens per second)
       eval time =    3179.00 ms /   112 tokens (   28.38 ms per token,    35.23 tokens per second)
      total time =    3377.57 ms /   168 tokens
slot      release: id  3 | task 52191 | stop processing: n_tokens = 2302, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.952
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 52304 | processing task, is_child = 0
slot update_slots: id  3 | task 52304 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2247
slot update_slots: id  3 | task 52304 | n_tokens = 2191, memory_seq_rm [2191, end)
slot update_slots: id  3 | task 52304 | prompt processing progress, n_tokens = 2247, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  3 | task 52304 | prompt done, n_tokens = 2247, batch.n_tokens = 56
slot init_sampler: id  3 | task 52304 | init sampler, took 0.35 ms, tokens: text = 2247, total = 2247
slot print_timing: id  3 | task 52304 | 
prompt eval time =     152.68 ms /    56 tokens (    2.73 ms per token,   366.78 tokens per second)
       eval time =    5303.48 ms /   186 tokens (   28.51 ms per token,    35.07 tokens per second)
      total time =    5456.16 ms /   242 tokens
slot      release: id  3 | task 52304 | stop processing: n_tokens = 2432, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.565 (> 0.100 thold), f_keep = 0.924
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 52491 | processing task, is_child = 0
slot update_slots: id  3 | task 52491 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3977
slot update_slots: id  3 | task 52491 | n_tokens = 2247, memory_seq_rm [2247, end)
slot update_slots: id  3 | task 52491 | prompt processing progress, n_tokens = 3913, batch.n_tokens = 1666, progress = 0.983907
slot update_slots: id  3 | task 52491 | n_tokens = 3913, memory_seq_rm [3913, end)
slot update_slots: id  3 | task 52491 | prompt processing progress, n_tokens = 3977, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 52491 | prompt done, n_tokens = 3977, batch.n_tokens = 64
slot init_sampler: id  3 | task 52491 | init sampler, took 0.75 ms, tokens: text = 3977, total = 3977
slot update_slots: id  3 | task 52491 | created context checkpoint 3 of 8 (pos_min = 3016, pos_max = 3912, size = 21.034 MiB)
slot print_timing: id  3 | task 52491 | 
prompt eval time =    2468.33 ms /  1730 tokens (    1.43 ms per token,   700.88 tokens per second)
       eval time =    4791.26 ms /   177 tokens (   27.07 ms per token,    36.94 tokens per second)
      total time =    7259.59 ms /  1907 tokens
slot      release: id  3 | task 52491 | stop processing: n_tokens = 4153, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 52670 | processing task, is_child = 0
slot update_slots: id  3 | task 52670 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4032
slot update_slots: id  3 | task 52670 | n_tokens = 3977, memory_seq_rm [3977, end)
slot update_slots: id  3 | task 52670 | prompt processing progress, n_tokens = 4032, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 52670 | prompt done, n_tokens = 4032, batch.n_tokens = 55
slot init_sampler: id  3 | task 52670 | init sampler, took 0.63 ms, tokens: text = 4032, total = 4032
slot print_timing: id  3 | task 52670 | 
prompt eval time =     173.10 ms /    55 tokens (    3.15 ms per token,   317.74 tokens per second)
       eval time =   10701.17 ms /   406 tokens (   26.36 ms per token,    37.94 tokens per second)
      total time =   10874.26 ms /   461 tokens
slot      release: id  3 | task 52670 | stop processing: n_tokens = 4437, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.909
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 53077 | processing task, is_child = 0
slot update_slots: id  3 | task 53077 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4087
slot update_slots: id  3 | task 53077 | n_tokens = 4032, memory_seq_rm [4032, end)
slot update_slots: id  3 | task 53077 | prompt processing progress, n_tokens = 4087, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 53077 | prompt done, n_tokens = 4087, batch.n_tokens = 55
slot init_sampler: id  3 | task 53077 | init sampler, took 0.65 ms, tokens: text = 4087, total = 4087
slot update_slots: id  3 | task 53077 | created context checkpoint 4 of 8 (pos_min = 3540, pos_max = 4031, size = 11.537 MiB)
slot print_timing: id  3 | task 53077 | 
prompt eval time =     176.10 ms /    55 tokens (    3.20 ms per token,   312.33 tokens per second)
       eval time =    5487.65 ms /   213 tokens (   25.76 ms per token,    38.81 tokens per second)
      total time =    5663.74 ms /   268 tokens
slot      release: id  3 | task 53077 | stop processing: n_tokens = 4299, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.951
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 53291 | processing task, is_child = 0
slot update_slots: id  3 | task 53291 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4140
slot update_slots: id  3 | task 53291 | n_tokens = 4087, memory_seq_rm [4087, end)
slot update_slots: id  3 | task 53291 | prompt processing progress, n_tokens = 4140, batch.n_tokens = 53, progress = 1.000000
slot update_slots: id  3 | task 53291 | prompt done, n_tokens = 4140, batch.n_tokens = 53
slot init_sampler: id  3 | task 53291 | init sampler, took 0.66 ms, tokens: text = 4140, total = 4140
slot print_timing: id  3 | task 53291 | 
prompt eval time =     165.72 ms /    53 tokens (    3.13 ms per token,   319.81 tokens per second)
       eval time =    8920.17 ms /   348 tokens (   25.63 ms per token,    39.01 tokens per second)
      total time =    9085.89 ms /   401 tokens
slot      release: id  3 | task 53291 | stop processing: n_tokens = 4487, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.923
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 53640 | processing task, is_child = 0
slot update_slots: id  3 | task 53640 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4193
slot update_slots: id  3 | task 53640 | n_tokens = 4140, memory_seq_rm [4140, end)
slot update_slots: id  3 | task 53640 | prompt processing progress, n_tokens = 4193, batch.n_tokens = 53, progress = 1.000000
slot update_slots: id  3 | task 53640 | prompt done, n_tokens = 4193, batch.n_tokens = 53
slot init_sampler: id  3 | task 53640 | init sampler, took 0.60 ms, tokens: text = 4193, total = 4193
slot update_slots: id  3 | task 53640 | created context checkpoint 5 of 8 (pos_min = 3590, pos_max = 4139, size = 12.897 MiB)
slot print_timing: id  3 | task 53640 | 
prompt eval time =     173.53 ms /    53 tokens (    3.27 ms per token,   305.43 tokens per second)
       eval time =    2601.88 ms /   103 tokens (   25.26 ms per token,    39.59 tokens per second)
      total time =    2775.41 ms /   156 tokens
slot      release: id  3 | task 53640 | stop processing: n_tokens = 4295, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 53744 | processing task, is_child = 0
slot update_slots: id  3 | task 53744 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4247
slot update_slots: id  3 | task 53744 | n_tokens = 4193, memory_seq_rm [4193, end)
slot update_slots: id  3 | task 53744 | prompt processing progress, n_tokens = 4247, batch.n_tokens = 54, progress = 1.000000
slot update_slots: id  3 | task 53744 | prompt done, n_tokens = 4247, batch.n_tokens = 54
slot init_sampler: id  3 | task 53744 | init sampler, took 0.64 ms, tokens: text = 4247, total = 4247
slot print_timing: id  3 | task 53744 | 
prompt eval time =     167.61 ms /    54 tokens (    3.10 ms per token,   322.17 tokens per second)
       eval time =    2349.06 ms /    92 tokens (   25.53 ms per token,    39.16 tokens per second)
      total time =    2516.67 ms /   146 tokens
slot      release: id  3 | task 53744 | stop processing: n_tokens = 4338, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 53837 | processing task, is_child = 0
slot update_slots: id  3 | task 53837 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4295
slot update_slots: id  3 | task 53837 | n_tokens = 4247, memory_seq_rm [4247, end)
slot update_slots: id  3 | task 53837 | prompt processing progress, n_tokens = 4295, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  3 | task 53837 | prompt done, n_tokens = 4295, batch.n_tokens = 48
slot init_sampler: id  3 | task 53837 | init sampler, took 0.75 ms, tokens: text = 4295, total = 4295
slot update_slots: id  3 | task 53837 | created context checkpoint 6 of 8 (pos_min = 3786, pos_max = 4246, size = 10.810 MiB)
slot print_timing: id  3 | task 53837 | 
prompt eval time =     149.31 ms /    48 tokens (    3.11 ms per token,   321.47 tokens per second)
       eval time =   29186.60 ms /  1130 tokens (   25.83 ms per token,    38.72 tokens per second)
      total time =   29335.91 ms /  1178 tokens
slot      release: id  3 | task 53837 | stop processing: n_tokens = 5424, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.402 (> 0.100 thold), f_keep = 0.385
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5424, total state size = 148.221 MiB
srv          load:  - looking for better prompt, base f_keep = 0.385, sim = 0.402
srv        update:  - cache state: 13 prompts, 4732.310 MiB (limits: 8192.000 MiB, 64000 tokens, 207346 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv        update:    - prompt 0x5a0b7e703bc0:    6188 tokens, checkpoints:  4,   244.035 MiB
srv        update:    - prompt 0x5a0b83e530c0:    9513 tokens, checkpoints:  8,   414.791 MiB
srv        update:    - prompt 0x5a0b7f017ff0:    9737 tokens, checkpoints:  8,   413.947 MiB
srv        update:    - prompt 0x5a0ba6c10af0:   10977 tokens, checkpoints:  7,   422.342 MiB
srv        update:    - prompt 0x5a0ba6c62d80:    8696 tokens, checkpoints:  8,   376.945 MiB
srv        update:    - prompt 0x5a0b7e115ed0:   10721 tokens, checkpoints:  2,   314.498 MiB
srv        update:    - prompt 0x5a0b83831940:    9290 tokens, checkpoints:  7,   368.761 MiB
srv        update:    - prompt 0x5a0ba6a9e940:    7437 tokens, checkpoints:  4,   264.365 MiB
srv        update:    - prompt 0x5a0ba48fa680:    9454 tokens, checkpoints:  8,   402.012 MiB
srv        update:    - prompt 0x5a0ba3303800:    5424 tokens, checkpoints:  6,   243.074 MiB
srv  get_availabl: prompt cache update took 176.32 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 54968 | processing task, is_child = 0
slot update_slots: id  3 | task 54968 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5201
slot update_slots: id  3 | task 54968 | n_past = 2090, slot.prompt.tokens.size() = 5424, seq_id = 3, pos_min = 4527, n_swa = 128
slot update_slots: id  3 | task 54968 | restored context checkpoint (pos_min = 1387, pos_max = 2134, size = 17.540 MiB)
slot update_slots: id  3 | task 54968 | erased invalidated context checkpoint (pos_min = 3016, pos_max = 3912, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 54968 | erased invalidated context checkpoint (pos_min = 3540, pos_max = 4031, n_swa = 128, size = 11.537 MiB)
slot update_slots: id  3 | task 54968 | erased invalidated context checkpoint (pos_min = 3590, pos_max = 4139, n_swa = 128, size = 12.897 MiB)
slot update_slots: id  3 | task 54968 | erased invalidated context checkpoint (pos_min = 3786, pos_max = 4246, n_swa = 128, size = 10.810 MiB)
slot update_slots: id  3 | task 54968 | n_tokens = 2090, memory_seq_rm [2090, end)
slot update_slots: id  3 | task 54968 | prompt processing progress, n_tokens = 4138, batch.n_tokens = 2048, progress = 0.795616
slot update_slots: id  3 | task 54968 | n_tokens = 4138, memory_seq_rm [4138, end)
slot update_slots: id  3 | task 54968 | prompt processing progress, n_tokens = 5137, batch.n_tokens = 999, progress = 0.987695
slot update_slots: id  3 | task 54968 | n_tokens = 5137, memory_seq_rm [5137, end)
slot update_slots: id  3 | task 54968 | prompt processing progress, n_tokens = 5201, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 54968 | prompt done, n_tokens = 5201, batch.n_tokens = 64
slot init_sampler: id  3 | task 54968 | init sampler, took 0.79 ms, tokens: text = 5201, total = 5201
slot update_slots: id  3 | task 54968 | created context checkpoint 3 of 8 (pos_min = 4240, pos_max = 5136, size = 21.034 MiB)
slot print_timing: id  3 | task 54968 | 
prompt eval time =    4310.61 ms /  3111 tokens (    1.39 ms per token,   721.71 tokens per second)
       eval time =   30529.31 ms /  1108 tokens (   27.55 ms per token,    36.29 tokens per second)
      total time =   34839.92 ms /  4219 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 54968 | stop processing: n_tokens = 6308, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.825
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56079 | processing task, is_child = 0
slot update_slots: id  3 | task 56079 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5276
slot update_slots: id  3 | task 56079 | n_past = 5201, slot.prompt.tokens.size() = 6308, seq_id = 3, pos_min = 5411, n_swa = 128
slot update_slots: id  3 | task 56079 | restored context checkpoint (pos_min = 4240, pos_max = 5136, size = 21.034 MiB)
slot update_slots: id  3 | task 56079 | n_tokens = 5136, memory_seq_rm [5136, end)
slot update_slots: id  3 | task 56079 | prompt processing progress, n_tokens = 5212, batch.n_tokens = 76, progress = 0.987870
slot update_slots: id  3 | task 56079 | n_tokens = 5212, memory_seq_rm [5212, end)
slot update_slots: id  3 | task 56079 | prompt processing progress, n_tokens = 5276, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 56079 | prompt done, n_tokens = 5276, batch.n_tokens = 64
slot init_sampler: id  3 | task 56079 | init sampler, took 1.15 ms, tokens: text = 5276, total = 5276
slot update_slots: id  3 | task 56079 | created context checkpoint 4 of 8 (pos_min = 4315, pos_max = 5211, size = 21.034 MiB)
slot print_timing: id  3 | task 56079 | 
prompt eval time =     601.09 ms /   140 tokens (    4.29 ms per token,   232.91 tokens per second)
       eval time =    1502.65 ms /    58 tokens (   25.91 ms per token,    38.60 tokens per second)
      total time =    2103.74 ms /   198 tokens
slot      release: id  3 | task 56079 | stop processing: n_tokens = 5333, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56139 | processing task, is_child = 0
slot update_slots: id  3 | task 56139 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5331
slot update_slots: id  3 | task 56139 | n_tokens = 5276, memory_seq_rm [5276, end)
slot update_slots: id  3 | task 56139 | prompt processing progress, n_tokens = 5331, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 56139 | prompt done, n_tokens = 5331, batch.n_tokens = 55
slot init_sampler: id  3 | task 56139 | init sampler, took 0.79 ms, tokens: text = 5331, total = 5331
slot print_timing: id  3 | task 56139 | 
prompt eval time =     175.81 ms /    55 tokens (    3.20 ms per token,   312.83 tokens per second)
       eval time =    2320.17 ms /    90 tokens (   25.78 ms per token,    38.79 tokens per second)
      total time =    2495.98 ms /   145 tokens
slot      release: id  3 | task 56139 | stop processing: n_tokens = 5420, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56230 | processing task, is_child = 0
slot update_slots: id  3 | task 56230 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5386
slot update_slots: id  3 | task 56230 | n_tokens = 5331, memory_seq_rm [5331, end)
slot update_slots: id  3 | task 56230 | prompt processing progress, n_tokens = 5386, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 56230 | prompt done, n_tokens = 5386, batch.n_tokens = 55
slot init_sampler: id  3 | task 56230 | init sampler, took 0.75 ms, tokens: text = 5386, total = 5386
slot update_slots: id  3 | task 56230 | created context checkpoint 5 of 8 (pos_min = 4523, pos_max = 5330, size = 18.947 MiB)
slot print_timing: id  3 | task 56230 | 
prompt eval time =     188.15 ms /    55 tokens (    3.42 ms per token,   292.31 tokens per second)
       eval time =    2064.53 ms /    80 tokens (   25.81 ms per token,    38.75 tokens per second)
      total time =    2252.68 ms /   135 tokens
slot      release: id  3 | task 56230 | stop processing: n_tokens = 5465, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 56311 | processing task, is_child = 0
slot update_slots: id  3 | task 56311 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5442
slot update_slots: id  3 | task 56311 | n_tokens = 5386, memory_seq_rm [5386, end)
slot update_slots: id  3 | task 56311 | prompt processing progress, n_tokens = 5442, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  3 | task 56311 | prompt done, n_tokens = 5442, batch.n_tokens = 56
slot init_sampler: id  3 | task 56311 | init sampler, took 0.75 ms, tokens: text = 5442, total = 5442
slot print_timing: id  3 | task 56311 | 
prompt eval time =     168.18 ms /    56 tokens (    3.00 ms per token,   332.97 tokens per second)
       eval time =   48133.54 ms /  1859 tokens (   25.89 ms per token,    38.62 tokens per second)
      total time =   48301.72 ms /  1915 tokens
slot      release: id  3 | task 56311 | stop processing: n_tokens = 7300, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.476 (> 0.100 thold), f_keep = 0.248
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7300, total state size = 192.212 MiB
srv          load:  - looking for better prompt, base f_keep = 0.248, sim = 0.476
srv        update:  - cache state: 14 prompts, 5024.111 MiB (limits: 8192.000 MiB, 64000 tokens, 207207 est)
srv        update:    - prompt 0x5a0b7b980f70:    4375 tokens, checkpoints:  6,   246.005 MiB
srv        update:    - prompt 0x5a0b7dce79f0:   12639 tokens, checkpoints:  8,   491.352 MiB
srv        update:    - prompt 0x5a0b7c504ab0:   15328 tokens, checkpoints:  7,   530.183 MiB
srv        update:    - prompt 0x5a0b7e703bc0:    6188 tokens, checkpoints:  4,   244.035 MiB
srv        update:    - prompt 0x5a0b83e530c0:    9513 tokens, checkpoints:  8,   414.791 MiB
srv        update:    - prompt 0x5a0b7f017ff0:    9737 tokens, checkpoints:  8,   413.947 MiB
srv        update:    - prompt 0x5a0ba6c10af0:   10977 tokens, checkpoints:  7,   422.342 MiB
srv        update:    - prompt 0x5a0ba6c62d80:    8696 tokens, checkpoints:  8,   376.945 MiB
srv        update:    - prompt 0x5a0b7e115ed0:   10721 tokens, checkpoints:  2,   314.498 MiB
srv        update:    - prompt 0x5a0b83831940:    9290 tokens, checkpoints:  7,   368.761 MiB
srv        update:    - prompt 0x5a0ba6a9e940:    7437 tokens, checkpoints:  4,   264.365 MiB
srv        update:    - prompt 0x5a0ba48fa680:    9454 tokens, checkpoints:  8,   402.012 MiB
srv        update:    - prompt 0x5a0ba3303800:    5424 tokens, checkpoints:  6,   243.074 MiB
srv        update:    - prompt 0x5a0b7d3ca240:    7300 tokens, checkpoints:  5,   291.801 MiB
srv  get_availabl: prompt cache update took 232.48 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 58171 | processing task, is_child = 0
slot update_slots: id  3 | task 58171 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3811
slot update_slots: id  3 | task 58171 | n_past = 1814, slot.prompt.tokens.size() = 7300, seq_id = 3, pos_min = 6403, n_swa = 128
slot update_slots: id  3 | task 58171 | restored context checkpoint (pos_min = 1387, pos_max = 2134, size = 17.540 MiB)
slot update_slots: id  3 | task 58171 | erased invalidated context checkpoint (pos_min = 4240, pos_max = 5136, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 58171 | erased invalidated context checkpoint (pos_min = 4315, pos_max = 5211, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 58171 | erased invalidated context checkpoint (pos_min = 4523, pos_max = 5330, n_swa = 128, size = 18.947 MiB)
slot update_slots: id  3 | task 58171 | n_tokens = 1814, memory_seq_rm [1814, end)
slot update_slots: id  3 | task 58171 | prompt processing progress, n_tokens = 3747, batch.n_tokens = 1933, progress = 0.983207
slot update_slots: id  3 | task 58171 | n_tokens = 3747, memory_seq_rm [3747, end)
slot update_slots: id  3 | task 58171 | prompt processing progress, n_tokens = 3811, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 58171 | prompt done, n_tokens = 3811, batch.n_tokens = 64
slot init_sampler: id  3 | task 58171 | init sampler, took 0.58 ms, tokens: text = 3811, total = 3811
slot update_slots: id  3 | task 58171 | created context checkpoint 3 of 8 (pos_min = 2850, pos_max = 3746, size = 21.034 MiB)
slot print_timing: id  3 | task 58171 | 
prompt eval time =    2818.56 ms /  1997 tokens (    1.41 ms per token,   708.52 tokens per second)
       eval time =    2324.47 ms /    71 tokens (   32.74 ms per token,    30.54 tokens per second)
      total time =    5143.03 ms /  2068 tokens
slot      release: id  3 | task 58171 | stop processing: n_tokens = 3881, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 58244 | processing task, is_child = 0
slot update_slots: id  3 | task 58244 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3883
slot update_slots: id  3 | task 58244 | n_tokens = 3811, memory_seq_rm [3811, end)
slot update_slots: id  3 | task 58244 | prompt processing progress, n_tokens = 3819, batch.n_tokens = 8, progress = 0.983518
slot update_slots: id  3 | task 58244 | n_tokens = 3819, memory_seq_rm [3819, end)
slot update_slots: id  3 | task 58244 | prompt processing progress, n_tokens = 3883, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 58244 | prompt done, n_tokens = 3883, batch.n_tokens = 64
slot init_sampler: id  3 | task 58244 | init sampler, took 0.61 ms, tokens: text = 3883, total = 3883
slot update_slots: id  3 | task 58244 | created context checkpoint 4 of 8 (pos_min = 2984, pos_max = 3818, size = 19.580 MiB)
slot print_timing: id  3 | task 58244 | 
prompt eval time =     258.47 ms /    72 tokens (    3.59 ms per token,   278.56 tokens per second)
       eval time =    2035.40 ms /    73 tokens (   27.88 ms per token,    35.87 tokens per second)
      total time =    2293.87 ms /   145 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 58244 | stop processing: n_tokens = 3955, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 58319 | processing task, is_child = 0
slot update_slots: id  3 | task 58319 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3957
slot update_slots: id  3 | task 58319 | n_tokens = 3883, memory_seq_rm [3883, end)
slot update_slots: id  3 | task 58319 | prompt processing progress, n_tokens = 3893, batch.n_tokens = 10, progress = 0.983826
slot update_slots: id  3 | task 58319 | n_tokens = 3893, memory_seq_rm [3893, end)
slot update_slots: id  3 | task 58319 | prompt processing progress, n_tokens = 3957, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 58319 | prompt done, n_tokens = 3957, batch.n_tokens = 64
slot init_sampler: id  3 | task 58319 | init sampler, took 0.75 ms, tokens: text = 3957, total = 3957
slot update_slots: id  3 | task 58319 | created context checkpoint 5 of 8 (pos_min = 3058, pos_max = 3892, size = 19.580 MiB)
slot print_timing: id  3 | task 58319 | 
prompt eval time =     289.30 ms /    74 tokens (    3.91 ms per token,   255.79 tokens per second)
       eval time =    5029.91 ms /   178 tokens (   28.26 ms per token,    35.39 tokens per second)
      total time =    5319.21 ms /   252 tokens
slot      release: id  3 | task 58319 | stop processing: n_tokens = 4134, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.957
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 58499 | processing task, is_child = 0
slot update_slots: id  3 | task 58499 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4033
slot update_slots: id  3 | task 58499 | n_tokens = 3957, memory_seq_rm [3957, end)
slot update_slots: id  3 | task 58499 | prompt processing progress, n_tokens = 3969, batch.n_tokens = 12, progress = 0.984131
slot update_slots: id  3 | task 58499 | n_tokens = 3969, memory_seq_rm [3969, end)
slot update_slots: id  3 | task 58499 | prompt processing progress, n_tokens = 4033, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 58499 | prompt done, n_tokens = 4033, batch.n_tokens = 64
slot init_sampler: id  3 | task 58499 | init sampler, took 0.61 ms, tokens: text = 4033, total = 4033
slot update_slots: id  3 | task 58499 | created context checkpoint 6 of 8 (pos_min = 3237, pos_max = 3968, size = 17.165 MiB)
slot print_timing: id  3 | task 58499 | 
prompt eval time =     289.54 ms /    76 tokens (    3.81 ms per token,   262.48 tokens per second)
       eval time =    7914.75 ms /   279 tokens (   28.37 ms per token,    35.25 tokens per second)
      total time =    8204.29 ms /   355 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 58499 | stop processing: n_tokens = 4311, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.792 (> 0.100 thold), f_keep = 0.936
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 58780 | processing task, is_child = 0
slot update_slots: id  3 | task 58780 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5094
slot update_slots: id  3 | task 58780 | n_tokens = 4033, memory_seq_rm [4033, end)
slot update_slots: id  3 | task 58780 | prompt processing progress, n_tokens = 5030, batch.n_tokens = 997, progress = 0.987436
slot update_slots: id  3 | task 58780 | n_tokens = 5030, memory_seq_rm [5030, end)
slot update_slots: id  3 | task 58780 | prompt processing progress, n_tokens = 5094, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 58780 | prompt done, n_tokens = 5094, batch.n_tokens = 64
slot init_sampler: id  3 | task 58780 | init sampler, took 0.99 ms, tokens: text = 5094, total = 5094
slot update_slots: id  3 | task 58780 | created context checkpoint 7 of 8 (pos_min = 4133, pos_max = 5029, size = 21.034 MiB)
slot print_timing: id  3 | task 58780 | 
prompt eval time =    1566.05 ms /  1061 tokens (    1.48 ms per token,   677.50 tokens per second)
       eval time =    3882.34 ms /   143 tokens (   27.15 ms per token,    36.83 tokens per second)
      total time =    5448.39 ms /  1204 tokens
slot      release: id  3 | task 58780 | stop processing: n_tokens = 5236, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.912 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 58925 | processing task, is_child = 0
slot update_slots: id  3 | task 58925 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5585
slot update_slots: id  3 | task 58925 | n_tokens = 5094, memory_seq_rm [5094, end)
slot update_slots: id  3 | task 58925 | prompt processing progress, n_tokens = 5521, batch.n_tokens = 427, progress = 0.988541
slot update_slots: id  3 | task 58925 | n_tokens = 5521, memory_seq_rm [5521, end)
slot update_slots: id  3 | task 58925 | prompt processing progress, n_tokens = 5585, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 58925 | prompt done, n_tokens = 5585, batch.n_tokens = 64
slot init_sampler: id  3 | task 58925 | init sampler, took 0.86 ms, tokens: text = 5585, total = 5585
slot update_slots: id  3 | task 58925 | created context checkpoint 8 of 8 (pos_min = 4624, pos_max = 5520, size = 21.034 MiB)
slot print_timing: id  3 | task 58925 | 
prompt eval time =     765.40 ms /   491 tokens (    1.56 ms per token,   641.50 tokens per second)
       eval time =   26944.57 ms /  1038 tokens (   25.96 ms per token,    38.52 tokens per second)
      total time =   27709.97 ms /  1529 tokens
slot      release: id  3 | task 58925 | stop processing: n_tokens = 6622, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.899 (> 0.100 thold), f_keep = 0.843
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 59965 | processing task, is_child = 0
slot update_slots: id  3 | task 59965 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6211
slot update_slots: id  3 | task 59965 | n_past = 5585, slot.prompt.tokens.size() = 6622, seq_id = 3, pos_min = 5725, n_swa = 128
slot update_slots: id  3 | task 59965 | restored context checkpoint (pos_min = 4624, pos_max = 5520, size = 21.034 MiB)
slot update_slots: id  3 | task 59965 | n_tokens = 5520, memory_seq_rm [5520, end)
slot update_slots: id  3 | task 59965 | prompt processing progress, n_tokens = 6147, batch.n_tokens = 627, progress = 0.989696
slot update_slots: id  3 | task 59965 | n_tokens = 6147, memory_seq_rm [6147, end)
slot update_slots: id  3 | task 59965 | prompt processing progress, n_tokens = 6211, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 59965 | prompt done, n_tokens = 6211, batch.n_tokens = 64
slot init_sampler: id  3 | task 59965 | init sampler, took 0.99 ms, tokens: text = 6211, total = 6211
slot update_slots: id  3 | task 59965 | erasing old context checkpoint (pos_min = 1129, pos_max = 2025, size = 21.034 MiB)
slot update_slots: id  3 | task 59965 | created context checkpoint 8 of 8 (pos_min = 5250, pos_max = 6146, size = 21.034 MiB)
slot print_timing: id  3 | task 59965 | 
prompt eval time =    1303.64 ms /   691 tokens (    1.89 ms per token,   530.06 tokens per second)
       eval time =   14927.01 ms /   581 tokens (   25.69 ms per token,    38.92 tokens per second)
      total time =   16230.65 ms /  1272 tokens
slot      release: id  3 | task 59965 | stop processing: n_tokens = 6791, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.915
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 60548 | processing task, is_child = 0
slot update_slots: id  3 | task 60548 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6725
slot update_slots: id  3 | task 60548 | n_tokens = 6211, memory_seq_rm [6211, end)
slot update_slots: id  3 | task 60548 | prompt processing progress, n_tokens = 6661, batch.n_tokens = 450, progress = 0.990483
slot update_slots: id  3 | task 60548 | n_tokens = 6661, memory_seq_rm [6661, end)
slot update_slots: id  3 | task 60548 | prompt processing progress, n_tokens = 6725, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 60548 | prompt done, n_tokens = 6725, batch.n_tokens = 64
slot init_sampler: id  3 | task 60548 | init sampler, took 0.99 ms, tokens: text = 6725, total = 6725
slot update_slots: id  3 | task 60548 | erasing old context checkpoint (pos_min = 1387, pos_max = 2134, size = 17.540 MiB)
slot update_slots: id  3 | task 60548 | created context checkpoint 8 of 8 (pos_min = 5894, pos_max = 6660, size = 17.986 MiB)
slot print_timing: id  3 | task 60548 | 
prompt eval time =     786.22 ms /   514 tokens (    1.53 ms per token,   653.76 tokens per second)
       eval time =   14045.43 ms /   536 tokens (   26.20 ms per token,    38.16 tokens per second)
      total time =   14831.65 ms /  1050 tokens
slot      release: id  3 | task 60548 | stop processing: n_tokens = 7260, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.929 (> 0.100 thold), f_keep = 0.926
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 61086 | processing task, is_child = 0
slot update_slots: id  3 | task 61086 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7239
slot update_slots: id  3 | task 61086 | n_tokens = 6725, memory_seq_rm [6725, end)
slot update_slots: id  3 | task 61086 | prompt processing progress, n_tokens = 7175, batch.n_tokens = 450, progress = 0.991159
slot update_slots: id  3 | task 61086 | n_tokens = 7175, memory_seq_rm [7175, end)
slot update_slots: id  3 | task 61086 | prompt processing progress, n_tokens = 7239, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 61086 | prompt done, n_tokens = 7239, batch.n_tokens = 64
slot init_sampler: id  3 | task 61086 | init sampler, took 1.05 ms, tokens: text = 7239, total = 7239
slot update_slots: id  3 | task 61086 | erasing old context checkpoint (pos_min = 2850, pos_max = 3746, size = 21.034 MiB)
slot update_slots: id  3 | task 61086 | created context checkpoint 8 of 8 (pos_min = 6598, pos_max = 7174, size = 13.530 MiB)
