ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-Q5_K_M.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q5_K_M.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-Q5_K_M.gguf.downloadInProgress (etag:"9c18af17b120dafae04e18aeb2bd8137775abe4b88371ff38e5c282dfa8d3fbd")...
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 4 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-Q5_K_M.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 12334 MiB of device memory vs. 14807 MiB of free device memory
llama_params_fit_impl: will leave 2473 >= 1024 MiB of free device memory, no changes needed
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.10 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14807 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-Q5_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  22:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  23:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  24:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  25: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - kv  36:                          general.file_type u32              = 17
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type q5_1:   61 tensors
llama_model_loader: - type q8_0:   13 tensors
llama_model_loader: - type q5_K:   24 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q5_K - Medium
print_info: file size   = 10.90 GiB (4.48 BPW) 
srv  log_server_r: request: GET /health 127.0.0.1 503
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 10747.93 MiB
load_tensors:    CUDA_Host model buffer size =   414.23 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 49152
llama_context: n_ctx_seq     = 24576
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (24576) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.53 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 24576 cells
llama_kv_cache:      CUDA0 KV buffer size =  1152.00 MiB
llama_kv_cache: size = 1152.00 MiB ( 24576 cells,  12 layers,  2/2 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 768 cells
llama_kv_cache:      CUDA0 KV buffer size =    36.00 MiB
llama_kv_cache: size =   36.00 MiB (   768 cells,  12 layers,  2/2 seqs), K (f16):   18.00 MiB, V (f16):   18.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =    55.15 MiB
sched_reserve: graph nodes  = 1400
sched_reserve: graph splits = 2
sched_reserve: reserve took 28.55 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 2
slot   load_model: id  0 | task -1 | new slot, n_ctx = 24576
slot   load_model: id  1 | task -1 | new slot, n_ctx = 24576
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-25

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 0 | processing task, is_child = 0
slot update_slots: id  1 | task 0 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 1026
slot update_slots: id  1 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 0 | prompt processing progress, n_tokens = 962, batch.n_tokens = 962, progress = 0.937622
slot update_slots: id  1 | task 0 | n_tokens = 962, memory_seq_rm [962, end)
slot update_slots: id  1 | task 0 | prompt processing progress, n_tokens = 1026, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 0 | prompt done, n_tokens = 1026, batch.n_tokens = 64
slot init_sampler: id  1 | task 0 | init sampler, took 0.16 ms, tokens: text = 1026, total = 1026
slot update_slots: id  1 | task 0 | created context checkpoint 1 of 8 (pos_min = 194, pos_max = 961, size = 18.009 MiB)
slot print_timing: id  1 | task 0 | 
prompt eval time =    1346.54 ms /  1026 tokens (    1.31 ms per token,   761.95 tokens per second)
       eval time =    2196.52 ms /   120 tokens (   18.30 ms per token,    54.63 tokens per second)
      total time =    3543.06 ms /  1146 tokens
slot      release: id  1 | task 0 | stop processing: n_tokens = 1145, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.694 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 122 | processing task, is_child = 0
slot update_slots: id  1 | task 122 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 1616
slot update_slots: id  1 | task 122 | n_tokens = 1121, memory_seq_rm [1121, end)
slot update_slots: id  1 | task 122 | prompt processing progress, n_tokens = 1552, batch.n_tokens = 431, progress = 0.960396
slot update_slots: id  1 | task 122 | n_tokens = 1552, memory_seq_rm [1552, end)
slot update_slots: id  1 | task 122 | prompt processing progress, n_tokens = 1616, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 122 | prompt done, n_tokens = 1616, batch.n_tokens = 64
slot init_sampler: id  1 | task 122 | init sampler, took 0.23 ms, tokens: text = 1616, total = 1616
slot update_slots: id  1 | task 122 | created context checkpoint 2 of 8 (pos_min = 784, pos_max = 1551, size = 18.009 MiB)
slot print_timing: id  1 | task 122 | 
prompt eval time =     555.85 ms /   495 tokens (    1.12 ms per token,   890.53 tokens per second)
       eval time =     868.74 ms /    46 tokens (   18.89 ms per token,    52.95 tokens per second)
      total time =    1424.59 ms /   541 tokens
slot      release: id  1 | task 122 | stop processing: n_tokens = 1661, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.489 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 170 | processing task, is_child = 0
slot update_slots: id  1 | task 170 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3332
slot update_slots: id  1 | task 170 | n_tokens = 1630, memory_seq_rm [1630, end)
slot update_slots: id  1 | task 170 | prompt processing progress, n_tokens = 3268, batch.n_tokens = 1638, progress = 0.980792
slot update_slots: id  1 | task 170 | n_tokens = 3268, memory_seq_rm [3268, end)
slot update_slots: id  1 | task 170 | prompt processing progress, n_tokens = 3332, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 170 | prompt done, n_tokens = 3332, batch.n_tokens = 64
slot init_sampler: id  1 | task 170 | init sampler, took 0.47 ms, tokens: text = 3332, total = 3332
slot update_slots: id  1 | task 170 | created context checkpoint 3 of 8 (pos_min = 2500, pos_max = 3267, size = 18.009 MiB)
slot print_timing: id  1 | task 170 | 
prompt eval time =    1896.73 ms /  1702 tokens (    1.11 ms per token,   897.33 tokens per second)
       eval time =     842.02 ms /    44 tokens (   19.14 ms per token,    52.26 tokens per second)
      total time =    2738.76 ms /  1746 tokens
slot      release: id  1 | task 170 | stop processing: n_tokens = 3375, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.197 (> 0.100 thold), f_keep = 0.304
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3375, total state size = 97.150 MiB
srv          load:  - looking for better prompt, base f_keep = 0.304, sim = 0.197
srv        update:  - cache state: 1 prompts, 151.177 MiB (limits: 8192.000 MiB, 49152 tokens, 182885 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv  get_availabl: prompt cache update took 131.71 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 216 | processing task, is_child = 0
slot update_slots: id  1 | task 216 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5214
slot update_slots: id  1 | task 216 | n_past = 1026, slot.prompt.tokens.size() = 3375, seq_id = 1, pos_min = 2607, n_swa = 128
slot update_slots: id  1 | task 216 | restored context checkpoint (pos_min = 784, pos_max = 1551, size = 18.009 MiB)
slot update_slots: id  1 | task 216 | erased invalidated context checkpoint (pos_min = 2500, pos_max = 3267, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 216 | n_tokens = 1026, memory_seq_rm [1026, end)
slot update_slots: id  1 | task 216 | prompt processing progress, n_tokens = 3074, batch.n_tokens = 2048, progress = 0.589567
slot update_slots: id  1 | task 216 | n_tokens = 3074, memory_seq_rm [3074, end)
slot update_slots: id  1 | task 216 | prompt processing progress, n_tokens = 5122, batch.n_tokens = 2048, progress = 0.982355
slot update_slots: id  1 | task 216 | n_tokens = 5122, memory_seq_rm [5122, end)
slot update_slots: id  1 | task 216 | prompt processing progress, n_tokens = 5150, batch.n_tokens = 28, progress = 0.987725
slot update_slots: id  1 | task 216 | n_tokens = 5150, memory_seq_rm [5150, end)
slot update_slots: id  1 | task 216 | prompt processing progress, n_tokens = 5214, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 216 | prompt done, n_tokens = 5214, batch.n_tokens = 64
slot init_sampler: id  1 | task 216 | init sampler, took 0.73 ms, tokens: text = 5214, total = 5214
slot update_slots: id  1 | task 216 | created context checkpoint 3 of 8 (pos_min = 4382, pos_max = 5149, size = 18.009 MiB)
slot print_timing: id  1 | task 216 | 
prompt eval time =    4280.62 ms /  4188 tokens (    1.02 ms per token,   978.36 tokens per second)
       eval time =    3810.53 ms /   196 tokens (   19.44 ms per token,    51.44 tokens per second)
      total time =    8091.15 ms /  4384 tokens
slot      release: id  1 | task 216 | stop processing: n_tokens = 5409, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.859 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 416 | processing task, is_child = 0
slot update_slots: id  1 | task 416 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 6069
slot update_slots: id  1 | task 416 | n_tokens = 5214, memory_seq_rm [5214, end)
slot update_slots: id  1 | task 416 | prompt processing progress, n_tokens = 6005, batch.n_tokens = 791, progress = 0.989455
slot update_slots: id  1 | task 416 | n_tokens = 6005, memory_seq_rm [6005, end)
slot update_slots: id  1 | task 416 | prompt processing progress, n_tokens = 6069, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 416 | prompt done, n_tokens = 6069, batch.n_tokens = 64
slot init_sampler: id  1 | task 416 | init sampler, took 0.84 ms, tokens: text = 6069, total = 6069
slot update_slots: id  1 | task 416 | created context checkpoint 4 of 8 (pos_min = 5237, pos_max = 6004, size = 18.009 MiB)
slot print_timing: id  1 | task 416 | 
prompt eval time =    1053.05 ms /   855 tokens (    1.23 ms per token,   811.93 tokens per second)
       eval time =    1444.51 ms /    73 tokens (   19.79 ms per token,    50.54 tokens per second)
      total time =    2497.56 ms /   928 tokens
slot      release: id  1 | task 416 | stop processing: n_tokens = 6141, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.907 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 491 | processing task, is_child = 0
slot update_slots: id  1 | task 491 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 6694
slot update_slots: id  1 | task 491 | n_tokens = 6069, memory_seq_rm [6069, end)
slot update_slots: id  1 | task 491 | prompt processing progress, n_tokens = 6630, batch.n_tokens = 561, progress = 0.990439
slot update_slots: id  1 | task 491 | n_tokens = 6630, memory_seq_rm [6630, end)
slot update_slots: id  1 | task 491 | prompt processing progress, n_tokens = 6694, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 491 | prompt done, n_tokens = 6694, batch.n_tokens = 64
slot init_sampler: id  1 | task 491 | init sampler, took 0.94 ms, tokens: text = 6694, total = 6694
slot update_slots: id  1 | task 491 | created context checkpoint 5 of 8 (pos_min = 5862, pos_max = 6629, size = 18.009 MiB)
slot print_timing: id  1 | task 491 | 
prompt eval time =     863.59 ms /   625 tokens (    1.38 ms per token,   723.72 tokens per second)
       eval time =    3244.61 ms /   161 tokens (   20.15 ms per token,    49.62 tokens per second)
      total time =    4108.20 ms /   786 tokens
slot      release: id  1 | task 491 | stop processing: n_tokens = 6854, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 654 | processing task, is_child = 0
slot update_slots: id  1 | task 654 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 6757
slot update_slots: id  1 | task 654 | n_tokens = 6694, memory_seq_rm [6694, end)
slot update_slots: id  1 | task 654 | prompt processing progress, n_tokens = 6757, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  1 | task 654 | prompt done, n_tokens = 6757, batch.n_tokens = 63
slot init_sampler: id  1 | task 654 | init sampler, took 1.26 ms, tokens: text = 6757, total = 6757
slot print_timing: id  1 | task 654 | 
prompt eval time =     173.38 ms /    63 tokens (    2.75 ms per token,   363.37 tokens per second)
       eval time =    5322.56 ms /   264 tokens (   20.16 ms per token,    49.60 tokens per second)
      total time =    5495.94 ms /   327 tokens
slot      release: id  1 | task 654 | stop processing: n_tokens = 7020, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.799 (> 0.100 thold), f_keep = 0.963
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 919 | processing task, is_child = 0
slot update_slots: id  1 | task 919 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 8459
slot update_slots: id  1 | task 919 | n_tokens = 6757, memory_seq_rm [6757, end)
slot update_slots: id  1 | task 919 | prompt processing progress, n_tokens = 8395, batch.n_tokens = 1638, progress = 0.992434
slot update_slots: id  1 | task 919 | n_tokens = 8395, memory_seq_rm [8395, end)
slot update_slots: id  1 | task 919 | prompt processing progress, n_tokens = 8459, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 919 | prompt done, n_tokens = 8459, batch.n_tokens = 64
slot init_sampler: id  1 | task 919 | init sampler, took 1.23 ms, tokens: text = 8459, total = 8459
slot update_slots: id  1 | task 919 | created context checkpoint 6 of 8 (pos_min = 7627, pos_max = 8394, size = 18.009 MiB)
slot print_timing: id  1 | task 919 | 
prompt eval time =    2099.10 ms /  1702 tokens (    1.23 ms per token,   810.82 tokens per second)
       eval time =    2303.94 ms /   114 tokens (   20.21 ms per token,    49.48 tokens per second)
      total time =    4403.04 ms /  1816 tokens
slot      release: id  1 | task 919 | stop processing: n_tokens = 8572, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 1035 | processing task, is_child = 0
slot update_slots: id  1 | task 1035 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 8599
slot update_slots: id  1 | task 1035 | n_tokens = 8459, memory_seq_rm [8459, end)
slot update_slots: id  1 | task 1035 | prompt processing progress, n_tokens = 8535, batch.n_tokens = 76, progress = 0.992557
slot update_slots: id  1 | task 1035 | n_tokens = 8535, memory_seq_rm [8535, end)
slot update_slots: id  1 | task 1035 | prompt processing progress, n_tokens = 8599, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 1035 | prompt done, n_tokens = 8599, batch.n_tokens = 64
slot init_sampler: id  1 | task 1035 | init sampler, took 1.19 ms, tokens: text = 8599, total = 8599
slot update_slots: id  1 | task 1035 | created context checkpoint 7 of 8 (pos_min = 7804, pos_max = 8534, size = 17.141 MiB)
slot print_timing: id  1 | task 1035 | 
prompt eval time =     405.14 ms /   140 tokens (    2.89 ms per token,   345.56 tokens per second)
       eval time =    3032.42 ms /   147 tokens (   20.63 ms per token,    48.48 tokens per second)
      total time =    3437.56 ms /   287 tokens
slot      release: id  1 | task 1035 | stop processing: n_tokens = 8745, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.946 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 1184 | processing task, is_child = 0
slot update_slots: id  1 | task 1184 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 9094
slot update_slots: id  1 | task 1184 | n_tokens = 8599, memory_seq_rm [8599, end)
slot update_slots: id  1 | task 1184 | prompt processing progress, n_tokens = 9030, batch.n_tokens = 431, progress = 0.992962
slot update_slots: id  1 | task 1184 | n_tokens = 9030, memory_seq_rm [9030, end)
slot update_slots: id  1 | task 1184 | prompt processing progress, n_tokens = 9094, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 1184 | prompt done, n_tokens = 9094, batch.n_tokens = 64
slot init_sampler: id  1 | task 1184 | init sampler, took 2.21 ms, tokens: text = 9094, total = 9094
slot update_slots: id  1 | task 1184 | created context checkpoint 8 of 8 (pos_min = 8262, pos_max = 9029, size = 18.009 MiB)
slot print_timing: id  1 | task 1184 | 
prompt eval time =     660.93 ms /   495 tokens (    1.34 ms per token,   748.94 tokens per second)
       eval time =    2980.89 ms /   144 tokens (   20.70 ms per token,    48.31 tokens per second)
      total time =    3641.82 ms /   639 tokens
slot      release: id  1 | task 1184 | stop processing: n_tokens = 9237, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 1330 | processing task, is_child = 0
slot update_slots: id  1 | task 1330 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 9736
slot update_slots: id  1 | task 1330 | n_tokens = 9094, memory_seq_rm [9094, end)
slot update_slots: id  1 | task 1330 | prompt processing progress, n_tokens = 9672, batch.n_tokens = 578, progress = 0.993426
slot update_slots: id  1 | task 1330 | n_tokens = 9672, memory_seq_rm [9672, end)
slot update_slots: id  1 | task 1330 | prompt processing progress, n_tokens = 9736, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 1330 | prompt done, n_tokens = 9736, batch.n_tokens = 64
slot init_sampler: id  1 | task 1330 | init sampler, took 1.79 ms, tokens: text = 9736, total = 9736
slot update_slots: id  1 | task 1330 | erasing old context checkpoint (pos_min = 194, pos_max = 961, size = 18.009 MiB)
slot update_slots: id  1 | task 1330 | created context checkpoint 8 of 8 (pos_min = 8904, pos_max = 9671, size = 18.009 MiB)
slot print_timing: id  1 | task 1330 | 
prompt eval time =     978.60 ms /   642 tokens (    1.52 ms per token,   656.04 tokens per second)
       eval time =    2589.89 ms /   124 tokens (   20.89 ms per token,    47.88 tokens per second)
      total time =    3568.49 ms /   766 tokens
slot      release: id  1 | task 1330 | stop processing: n_tokens = 9859, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 1456 | processing task, is_child = 0
slot update_slots: id  1 | task 1456 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 9836
slot update_slots: id  1 | task 1456 | n_tokens = 9736, memory_seq_rm [9736, end)
slot update_slots: id  1 | task 1456 | prompt processing progress, n_tokens = 9772, batch.n_tokens = 36, progress = 0.993493
slot update_slots: id  1 | task 1456 | n_tokens = 9772, memory_seq_rm [9772, end)
slot update_slots: id  1 | task 1456 | prompt processing progress, n_tokens = 9836, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 1456 | prompt done, n_tokens = 9836, batch.n_tokens = 64
slot init_sampler: id  1 | task 1456 | init sampler, took 1.35 ms, tokens: text = 9836, total = 9836
slot update_slots: id  1 | task 1456 | erasing old context checkpoint (pos_min = 784, pos_max = 1551, size = 18.009 MiB)
slot update_slots: id  1 | task 1456 | created context checkpoint 8 of 8 (pos_min = 9091, pos_max = 9771, size = 15.969 MiB)
slot print_timing: id  1 | task 1456 | 
prompt eval time =     295.68 ms /   100 tokens (    2.96 ms per token,   338.20 tokens per second)
       eval time =   58657.14 ms /  2690 tokens (   21.81 ms per token,    45.86 tokens per second)
      total time =   58952.82 ms /  2790 tokens
slot      release: id  1 | task 1456 | stop processing: n_tokens = 12525, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.746 (> 0.100 thold), f_keep = 0.785
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 4148 | processing task, is_child = 0
slot update_slots: id  1 | task 4148 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 13180
slot update_slots: id  1 | task 4148 | n_past = 9836, slot.prompt.tokens.size() = 12525, seq_id = 1, pos_min = 11757, n_swa = 128
slot update_slots: id  1 | task 4148 | restored context checkpoint (pos_min = 9091, pos_max = 9771, size = 15.969 MiB)
slot update_slots: id  1 | task 4148 | n_tokens = 9771, memory_seq_rm [9771, end)
slot update_slots: id  1 | task 4148 | prompt processing progress, n_tokens = 11819, batch.n_tokens = 2048, progress = 0.896737
slot update_slots: id  1 | task 4148 | n_tokens = 11819, memory_seq_rm [11819, end)
slot update_slots: id  1 | task 4148 | prompt processing progress, n_tokens = 13116, batch.n_tokens = 1297, progress = 0.995144
slot update_slots: id  1 | task 4148 | n_tokens = 13116, memory_seq_rm [13116, end)
slot update_slots: id  1 | task 4148 | prompt processing progress, n_tokens = 13180, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 4148 | prompt done, n_tokens = 13180, batch.n_tokens = 64
slot init_sampler: id  1 | task 4148 | init sampler, took 1.82 ms, tokens: text = 13180, total = 13180
slot update_slots: id  1 | task 4148 | erasing old context checkpoint (pos_min = 4382, pos_max = 5149, size = 18.009 MiB)
slot update_slots: id  1 | task 4148 | created context checkpoint 8 of 8 (pos_min = 12348, pos_max = 13115, size = 18.009 MiB)
slot print_timing: id  1 | task 4148 | 
prompt eval time =    4285.82 ms /  3409 tokens (    1.26 ms per token,   795.41 tokens per second)
       eval time =    6908.63 ms /   320 tokens (   21.59 ms per token,    46.32 tokens per second)
      total time =   11194.45 ms /  3729 tokens
slot      release: id  1 | task 4148 | stop processing: n_tokens = 13499, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.881 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 4471 | processing task, is_child = 0
slot update_slots: id  1 | task 4471 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 14952
slot update_slots: id  1 | task 4471 | n_tokens = 13180, memory_seq_rm [13180, end)
slot update_slots: id  1 | task 4471 | prompt processing progress, n_tokens = 14888, batch.n_tokens = 1708, progress = 0.995720
slot update_slots: id  1 | task 4471 | n_tokens = 14888, memory_seq_rm [14888, end)
slot update_slots: id  1 | task 4471 | prompt processing progress, n_tokens = 14952, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 4471 | prompt done, n_tokens = 14952, batch.n_tokens = 64
slot init_sampler: id  1 | task 4471 | init sampler, took 2.79 ms, tokens: text = 14952, total = 14952
slot update_slots: id  1 | task 4471 | erasing old context checkpoint (pos_min = 5237, pos_max = 6004, size = 18.009 MiB)
slot update_slots: id  1 | task 4471 | created context checkpoint 8 of 8 (pos_min = 14120, pos_max = 14887, size = 18.009 MiB)
slot print_timing: id  1 | task 4471 | 
prompt eval time =    2436.97 ms /  1772 tokens (    1.38 ms per token,   727.13 tokens per second)
       eval time =   91732.61 ms /  4096 tokens (   22.40 ms per token,    44.65 tokens per second)
      total time =   94169.58 ms /  5868 tokens
slot      release: id  1 | task 4471 | stop processing: n_tokens = 19047, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 4471
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.785
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 8570 | processing task, is_child = 0
slot update_slots: id  1 | task 8570 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 14966
slot update_slots: id  1 | task 8570 | n_past = 14951, slot.prompt.tokens.size() = 19047, seq_id = 1, pos_min = 18279, n_swa = 128
slot update_slots: id  1 | task 8570 | restored context checkpoint (pos_min = 14120, pos_max = 14887, size = 18.009 MiB)
slot update_slots: id  1 | task 8570 | n_tokens = 14887, memory_seq_rm [14887, end)
slot update_slots: id  1 | task 8570 | prompt processing progress, n_tokens = 14902, batch.n_tokens = 15, progress = 0.995724
slot update_slots: id  1 | task 8570 | n_tokens = 14902, memory_seq_rm [14902, end)
slot update_slots: id  1 | task 8570 | prompt processing progress, n_tokens = 14966, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 8570 | prompt done, n_tokens = 14966, batch.n_tokens = 64
slot init_sampler: id  1 | task 8570 | init sampler, took 2.73 ms, tokens: text = 14966, total = 14966
slot print_timing: id  1 | task 8570 | 
prompt eval time =     374.58 ms /    79 tokens (    4.74 ms per token,   210.90 tokens per second)
       eval time =    4293.25 ms /   201 tokens (   21.36 ms per token,    46.82 tokens per second)
      total time =    4667.83 ms /   280 tokens
slot      release: id  1 | task 8570 | stop processing: n_tokens = 15166, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.846 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 8773 | processing task, is_child = 0
slot update_slots: id  1 | task 8773 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 17697
slot update_slots: id  1 | task 8773 | n_tokens = 14966, memory_seq_rm [14966, end)
slot update_slots: id  1 | task 8773 | prompt processing progress, n_tokens = 17014, batch.n_tokens = 2048, progress = 0.961406
slot update_slots: id  1 | task 8773 | n_tokens = 17014, memory_seq_rm [17014, end)
slot update_slots: id  1 | task 8773 | prompt processing progress, n_tokens = 17633, batch.n_tokens = 619, progress = 0.996384
slot update_slots: id  1 | task 8773 | n_tokens = 17633, memory_seq_rm [17633, end)
slot update_slots: id  1 | task 8773 | prompt processing progress, n_tokens = 17697, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 8773 | prompt done, n_tokens = 17697, batch.n_tokens = 64
slot init_sampler: id  1 | task 8773 | init sampler, took 2.43 ms, tokens: text = 17697, total = 17697
slot update_slots: id  1 | task 8773 | erasing old context checkpoint (pos_min = 5862, pos_max = 6629, size = 18.009 MiB)
slot update_slots: id  1 | task 8773 | created context checkpoint 8 of 8 (pos_min = 16865, pos_max = 17632, size = 18.009 MiB)
slot print_timing: id  1 | task 8773 | 
prompt eval time =    3760.99 ms /  2731 tokens (    1.38 ms per token,   726.14 tokens per second)
       eval time =    2393.13 ms /   106 tokens (   22.58 ms per token,    44.29 tokens per second)
      total time =    6154.12 ms /  2837 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 8773 | stop processing: n_tokens = 17802, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8882 | processing task, is_child = 0
slot update_slots: id  0 | task 8882 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 14521
slot update_slots: id  0 | task 8882 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 8882 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.141037
slot update_slots: id  0 | task 8882 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 8882 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.282074
slot update_slots: id  0 | task 8882 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 8882 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.423111
slot update_slots: id  0 | task 8882 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 8882 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.564148
slot update_slots: id  0 | task 8882 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 8882 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.705186
slot update_slots: id  0 | task 8882 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  0 | task 8882 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.846223
slot update_slots: id  0 | task 8882 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  0 | task 8882 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.987260
slot update_slots: id  0 | task 8882 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  0 | task 8882 | prompt processing progress, n_tokens = 14457, batch.n_tokens = 121, progress = 0.995593
slot update_slots: id  0 | task 8882 | n_tokens = 14457, memory_seq_rm [14457, end)
slot update_slots: id  0 | task 8882 | prompt processing progress, n_tokens = 14521, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8882 | prompt done, n_tokens = 14521, batch.n_tokens = 64
slot init_sampler: id  0 | task 8882 | init sampler, took 2.00 ms, tokens: text = 14521, total = 14521
slot update_slots: id  0 | task 8882 | created context checkpoint 1 of 8 (pos_min = 13689, pos_max = 14456, size = 18.009 MiB)
slot print_timing: id  0 | task 8882 | 
prompt eval time =   17138.76 ms / 14521 tokens (    1.18 ms per token,   847.26 tokens per second)
       eval time =    9397.96 ms /   398 tokens (   23.61 ms per token,    42.35 tokens per second)
      total time =   26536.72 ms / 14919 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 8882 | stop processing: n_tokens = 14918, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.244 (> 0.100 thold), f_keep = 0.051
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 17802, total state size = 435.447 MiB
srv          load:  - looking for better prompt, base f_keep = 0.051, sim = 0.244
srv        update:  - cache state: 2 prompts, 727.789 MiB (limits: 8192.000 MiB, 49152 tokens, 238368 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv  get_availabl: prompt cache update took 440.63 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 9289 | processing task, is_child = 0
slot update_slots: id  1 | task 9289 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3747
slot update_slots: id  1 | task 9289 | n_past = 915, slot.prompt.tokens.size() = 17802, seq_id = 1, pos_min = 17034, n_swa = 128
slot update_slots: id  1 | task 9289 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 9289 | erased invalidated context checkpoint (pos_min = 7627, pos_max = 8394, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 9289 | erased invalidated context checkpoint (pos_min = 7804, pos_max = 8534, n_swa = 128, size = 17.141 MiB)
slot update_slots: id  1 | task 9289 | erased invalidated context checkpoint (pos_min = 8262, pos_max = 9029, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 9289 | erased invalidated context checkpoint (pos_min = 8904, pos_max = 9671, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 9289 | erased invalidated context checkpoint (pos_min = 9091, pos_max = 9771, n_swa = 128, size = 15.969 MiB)
slot update_slots: id  1 | task 9289 | erased invalidated context checkpoint (pos_min = 12348, pos_max = 13115, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 9289 | erased invalidated context checkpoint (pos_min = 14120, pos_max = 14887, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 9289 | erased invalidated context checkpoint (pos_min = 16865, pos_max = 17632, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 9289 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 9289 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.546571
slot update_slots: id  1 | task 9289 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 9289 | prompt processing progress, n_tokens = 3683, batch.n_tokens = 1635, progress = 0.982920
slot update_slots: id  1 | task 9289 | n_tokens = 3683, memory_seq_rm [3683, end)
slot update_slots: id  1 | task 9289 | prompt processing progress, n_tokens = 3747, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 9289 | prompt done, n_tokens = 3747, batch.n_tokens = 64
slot init_sampler: id  1 | task 9289 | init sampler, took 0.53 ms, tokens: text = 3747, total = 3747
slot update_slots: id  1 | task 9289 | created context checkpoint 1 of 8 (pos_min = 2915, pos_max = 3682, size = 18.009 MiB)
slot print_timing: id  1 | task 9289 | 
prompt eval time =    4087.22 ms /  3747 tokens (    1.09 ms per token,   916.76 tokens per second)
       eval time =    4814.75 ms /   239 tokens (   20.15 ms per token,    49.64 tokens per second)
      total time =    8901.96 ms /  3986 tokens
slot      release: id  1 | task 9289 | stop processing: n_tokens = 3985, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 9531 | processing task, is_child = 0
slot update_slots: id  1 | task 9531 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3941
slot update_slots: id  1 | task 9531 | n_tokens = 3748, memory_seq_rm [3748, end)
slot update_slots: id  1 | task 9531 | prompt processing progress, n_tokens = 3877, batch.n_tokens = 129, progress = 0.983760
slot update_slots: id  1 | task 9531 | n_tokens = 3877, memory_seq_rm [3877, end)
slot update_slots: id  1 | task 9531 | prompt processing progress, n_tokens = 3941, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 9531 | prompt done, n_tokens = 3941, batch.n_tokens = 64
slot init_sampler: id  1 | task 9531 | init sampler, took 0.55 ms, tokens: text = 3941, total = 3941
slot update_slots: id  1 | task 9531 | created context checkpoint 2 of 8 (pos_min = 3217, pos_max = 3876, size = 15.477 MiB)
slot print_timing: id  1 | task 9531 | 
prompt eval time =     481.49 ms /   193 tokens (    2.49 ms per token,   400.84 tokens per second)
       eval time =    4056.67 ms /   190 tokens (   21.35 ms per token,    46.84 tokens per second)
      total time =    4538.16 ms /   383 tokens
slot      release: id  1 | task 9531 | stop processing: n_tokens = 4130, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.129 (> 0.100 thold), f_keep = 0.029
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 14918, total state size = 367.821 MiB
srv          load:  - looking for better prompt, base f_keep = 0.029, sim = 0.129
srv        update:  - cache state: 3 prompts, 1113.619 MiB (limits: 8192.000 MiB, 49152 tokens, 265521 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv  get_availabl: prompt cache update took 288.29 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9723 | processing task, is_child = 0
slot update_slots: id  0 | task 9723 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3300
slot update_slots: id  0 | task 9723 | n_past = 427, slot.prompt.tokens.size() = 14918, seq_id = 0, pos_min = 14150, n_swa = 128
slot update_slots: id  0 | task 9723 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 9723 | erased invalidated context checkpoint (pos_min = 13689, pos_max = 14456, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 9723 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 9723 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.620606
slot update_slots: id  0 | task 9723 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 9723 | prompt processing progress, n_tokens = 3236, batch.n_tokens = 1188, progress = 0.980606
slot update_slots: id  0 | task 9723 | n_tokens = 3236, memory_seq_rm [3236, end)
slot update_slots: id  0 | task 9723 | prompt processing progress, n_tokens = 3300, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9723 | prompt done, n_tokens = 3300, batch.n_tokens = 64
slot init_sampler: id  0 | task 9723 | init sampler, took 0.64 ms, tokens: text = 3300, total = 3300
slot update_slots: id  0 | task 9723 | created context checkpoint 1 of 8 (pos_min = 2468, pos_max = 3235, size = 18.009 MiB)
slot print_timing: id  0 | task 9723 | 
prompt eval time =    3594.16 ms /  3300 tokens (    1.09 ms per token,   918.16 tokens per second)
       eval time =    7779.14 ms /   378 tokens (   20.58 ms per token,    48.59 tokens per second)
      total time =   11373.31 ms /  3678 tokens
slot      release: id  0 | task 9723 | stop processing: n_tokens = 3677, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.767 (> 0.100 thold), f_keep = 0.222
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4130, total state size = 112.696 MiB
srv          load:  - looking for better prompt, base f_keep = 0.222, sim = 0.767
srv        update:  - cache state: 4 prompts, 1259.801 MiB (limits: 8192.000 MiB, 49152 tokens, 261567 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv  get_availabl: prompt cache update took 96.93 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 10104 | processing task, is_child = 0
slot update_slots: id  1 | task 10104 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 1193
slot update_slots: id  1 | task 10104 | n_past = 915, slot.prompt.tokens.size() = 4130, seq_id = 1, pos_min = 3454, n_swa = 128
slot update_slots: id  1 | task 10104 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 10104 | erased invalidated context checkpoint (pos_min = 2915, pos_max = 3682, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 10104 | erased invalidated context checkpoint (pos_min = 3217, pos_max = 3876, n_swa = 128, size = 15.477 MiB)
slot update_slots: id  1 | task 10104 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 10104 | prompt processing progress, n_tokens = 1129, batch.n_tokens = 1129, progress = 0.946354
slot update_slots: id  1 | task 10104 | n_tokens = 1129, memory_seq_rm [1129, end)
slot update_slots: id  1 | task 10104 | prompt processing progress, n_tokens = 1193, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 10104 | prompt done, n_tokens = 1193, batch.n_tokens = 64
slot init_sampler: id  1 | task 10104 | init sampler, took 0.17 ms, tokens: text = 1193, total = 1193
slot update_slots: id  1 | task 10104 | created context checkpoint 1 of 8 (pos_min = 361, pos_max = 1128, size = 18.009 MiB)
slot print_timing: id  1 | task 10104 | 
prompt eval time =    1509.19 ms /  1193 tokens (    1.27 ms per token,   790.49 tokens per second)
       eval time =    1933.16 ms /    93 tokens (   20.79 ms per token,    48.11 tokens per second)
      total time =    3442.35 ms /  1286 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 10104 | stop processing: n_tokens = 1285, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.707 (> 0.100 thold), f_keep = 0.116
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3677, total state size = 104.231 MiB
srv          load:  - looking for better prompt, base f_keep = 0.116, sim = 0.707
srv        update:  - cache state: 5 prompts, 1382.041 MiB (limits: 8192.000 MiB, 49152 tokens, 260227 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv  get_availabl: prompt cache update took 119.02 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 10199 | processing task, is_child = 0
slot update_slots: id  0 | task 10199 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 604
slot update_slots: id  0 | task 10199 | n_past = 427, slot.prompt.tokens.size() = 3677, seq_id = 0, pos_min = 2909, n_swa = 128
slot update_slots: id  0 | task 10199 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 10199 | erased invalidated context checkpoint (pos_min = 2468, pos_max = 3235, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 10199 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 10199 | prompt processing progress, n_tokens = 540, batch.n_tokens = 540, progress = 0.894040
slot update_slots: id  0 | task 10199 | n_tokens = 540, memory_seq_rm [540, end)
slot update_slots: id  0 | task 10199 | prompt processing progress, n_tokens = 604, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 10199 | prompt done, n_tokens = 604, batch.n_tokens = 64
slot init_sampler: id  0 | task 10199 | init sampler, took 0.12 ms, tokens: text = 604, total = 604
slot update_slots: id  0 | task 10199 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 539, size = 12.663 MiB)
slot print_timing: id  0 | task 10199 | 
prompt eval time =     831.13 ms /   604 tokens (    1.38 ms per token,   726.72 tokens per second)
       eval time =    5482.81 ms /   256 tokens (   21.42 ms per token,    46.69 tokens per second)
      total time =    6313.93 ms /   860 tokens
slot      release: id  0 | task 10199 | stop processing: n_tokens = 859, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.241 (> 0.100 thold), f_keep = 0.712
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 10457 | processing task, is_child = 0
slot update_slots: id  1 | task 10457 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3803
slot update_slots: id  1 | task 10457 | n_tokens = 915, memory_seq_rm [915, end)
slot update_slots: id  1 | task 10457 | prompt processing progress, n_tokens = 2963, batch.n_tokens = 2048, progress = 0.779122
slot update_slots: id  1 | task 10457 | n_tokens = 2963, memory_seq_rm [2963, end)
slot update_slots: id  1 | task 10457 | prompt processing progress, n_tokens = 3739, batch.n_tokens = 776, progress = 0.983171
slot update_slots: id  1 | task 10457 | n_tokens = 3739, memory_seq_rm [3739, end)
slot update_slots: id  1 | task 10457 | prompt processing progress, n_tokens = 3803, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 10457 | prompt done, n_tokens = 3803, batch.n_tokens = 64
slot init_sampler: id  1 | task 10457 | init sampler, took 0.54 ms, tokens: text = 3803, total = 3803
slot update_slots: id  1 | task 10457 | created context checkpoint 2 of 8 (pos_min = 2971, pos_max = 3738, size = 18.009 MiB)
slot print_timing: id  1 | task 10457 | 
prompt eval time =    3338.99 ms /  2888 tokens (    1.16 ms per token,   864.93 tokens per second)
       eval time =    1761.16 ms /    79 tokens (   22.29 ms per token,    44.86 tokens per second)
      total time =    5100.15 ms /  2967 tokens
slot      release: id  1 | task 10457 | stop processing: n_tokens = 3881, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.883 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 10539 | processing task, is_child = 0
slot update_slots: id  1 | task 10539 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 4306
slot update_slots: id  1 | task 10539 | n_tokens = 3803, memory_seq_rm [3803, end)
slot update_slots: id  1 | task 10539 | prompt processing progress, n_tokens = 4242, batch.n_tokens = 439, progress = 0.985137
slot update_slots: id  1 | task 10539 | n_tokens = 4242, memory_seq_rm [4242, end)
slot update_slots: id  1 | task 10539 | prompt processing progress, n_tokens = 4306, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 10539 | prompt done, n_tokens = 4306, batch.n_tokens = 64
slot init_sampler: id  1 | task 10539 | init sampler, took 0.60 ms, tokens: text = 4306, total = 4306
slot update_slots: id  1 | task 10539 | created context checkpoint 3 of 8 (pos_min = 3511, pos_max = 4241, size = 17.141 MiB)
slot print_timing: id  1 | task 10539 | 
prompt eval time =     674.56 ms /   503 tokens (    1.34 ms per token,   745.67 tokens per second)
       eval time =    6893.00 ms /   312 tokens (   22.09 ms per token,    45.26 tokens per second)
      total time =    7567.56 ms /   815 tokens
slot      release: id  1 | task 10539 | stop processing: n_tokens = 4617, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.583 (> 0.100 thold), f_keep = 0.933
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 10853 | processing task, is_child = 0
slot update_slots: id  1 | task 10853 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 7389
slot update_slots: id  1 | task 10853 | n_tokens = 4306, memory_seq_rm [4306, end)
slot update_slots: id  1 | task 10853 | prompt processing progress, n_tokens = 6354, batch.n_tokens = 2048, progress = 0.859927
slot update_slots: id  1 | task 10853 | n_tokens = 6354, memory_seq_rm [6354, end)
slot update_slots: id  1 | task 10853 | prompt processing progress, n_tokens = 7325, batch.n_tokens = 971, progress = 0.991338
slot update_slots: id  1 | task 10853 | n_tokens = 7325, memory_seq_rm [7325, end)
slot update_slots: id  1 | task 10853 | prompt processing progress, n_tokens = 7389, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 10853 | prompt done, n_tokens = 7389, batch.n_tokens = 64
slot init_sampler: id  1 | task 10853 | init sampler, took 1.01 ms, tokens: text = 7389, total = 7389
slot update_slots: id  1 | task 10853 | created context checkpoint 4 of 8 (pos_min = 6557, pos_max = 7324, size = 18.009 MiB)
slot print_timing: id  1 | task 10853 | 
prompt eval time =    3596.99 ms /  3083 tokens (    1.17 ms per token,   857.11 tokens per second)
       eval time =   12536.37 ms /   590 tokens (   21.25 ms per token,    47.06 tokens per second)
      total time =   16133.36 ms /  3673 tokens
slot      release: id  1 | task 10853 | stop processing: n_tokens = 7978, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.938 (> 0.100 thold), f_keep = 0.926
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 11446 | processing task, is_child = 0
slot update_slots: id  1 | task 11446 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 7875
slot update_slots: id  1 | task 11446 | n_tokens = 7389, memory_seq_rm [7389, end)
slot update_slots: id  1 | task 11446 | prompt processing progress, n_tokens = 7811, batch.n_tokens = 422, progress = 0.991873
slot update_slots: id  1 | task 11446 | n_tokens = 7811, memory_seq_rm [7811, end)
slot update_slots: id  1 | task 11446 | prompt processing progress, n_tokens = 7875, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 11446 | prompt done, n_tokens = 7875, batch.n_tokens = 64
slot init_sampler: id  1 | task 11446 | init sampler, took 1.09 ms, tokens: text = 7875, total = 7875
slot update_slots: id  1 | task 11446 | created context checkpoint 5 of 8 (pos_min = 7210, pos_max = 7810, size = 14.093 MiB)
slot print_timing: id  1 | task 11446 | 
prompt eval time =     699.41 ms /   486 tokens (    1.44 ms per token,   694.87 tokens per second)
       eval time =    1119.90 ms /    53 tokens (   21.13 ms per token,    47.33 tokens per second)
      total time =    1819.32 ms /   539 tokens
slot      release: id  1 | task 11446 | stop processing: n_tokens = 7927, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.796 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 11501 | processing task, is_child = 0
slot update_slots: id  1 | task 11501 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 9894
slot update_slots: id  1 | task 11501 | n_tokens = 7875, memory_seq_rm [7875, end)
slot update_slots: id  1 | task 11501 | prompt processing progress, n_tokens = 9830, batch.n_tokens = 1955, progress = 0.993531
slot update_slots: id  1 | task 11501 | n_tokens = 9830, memory_seq_rm [9830, end)
slot update_slots: id  1 | task 11501 | prompt processing progress, n_tokens = 9894, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 11501 | prompt done, n_tokens = 9894, batch.n_tokens = 64
slot init_sampler: id  1 | task 11501 | init sampler, took 1.54 ms, tokens: text = 9894, total = 9894
slot update_slots: id  1 | task 11501 | created context checkpoint 6 of 8 (pos_min = 9062, pos_max = 9829, size = 18.009 MiB)
slot print_timing: id  1 | task 11501 | 
prompt eval time =    2486.78 ms /  2019 tokens (    1.23 ms per token,   811.89 tokens per second)
       eval time =    1383.39 ms /    64 tokens (   21.62 ms per token,    46.26 tokens per second)
      total time =    3870.18 ms /  2083 tokens
slot      release: id  1 | task 11501 | stop processing: n_tokens = 9957, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.784 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 11567 | processing task, is_child = 0
slot update_slots: id  1 | task 11567 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 12625
slot update_slots: id  1 | task 11567 | n_tokens = 9894, memory_seq_rm [9894, end)
slot update_slots: id  1 | task 11567 | prompt processing progress, n_tokens = 11942, batch.n_tokens = 2048, progress = 0.945901
slot update_slots: id  1 | task 11567 | n_tokens = 11942, memory_seq_rm [11942, end)
slot update_slots: id  1 | task 11567 | prompt processing progress, n_tokens = 12561, batch.n_tokens = 619, progress = 0.994931
slot update_slots: id  1 | task 11567 | n_tokens = 12561, memory_seq_rm [12561, end)
slot update_slots: id  1 | task 11567 | prompt processing progress, n_tokens = 12625, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 11567 | prompt done, n_tokens = 12625, batch.n_tokens = 64
slot init_sampler: id  1 | task 11567 | init sampler, took 1.74 ms, tokens: text = 12625, total = 12625
slot update_slots: id  1 | task 11567 | created context checkpoint 7 of 8 (pos_min = 11793, pos_max = 12560, size = 18.009 MiB)
slot print_timing: id  1 | task 11567 | 
prompt eval time =    3519.67 ms /  2731 tokens (    1.29 ms per token,   775.93 tokens per second)
       eval time =   10659.22 ms /   501 tokens (   21.28 ms per token,    47.00 tokens per second)
      total time =   14178.89 ms /  3232 tokens
slot      release: id  1 | task 11567 | stop processing: n_tokens = 13125, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12071 | processing task, is_child = 0
slot update_slots: id  1 | task 12071 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 13138
slot update_slots: id  1 | task 12071 | n_tokens = 12625, memory_seq_rm [12625, end)
slot update_slots: id  1 | task 12071 | prompt processing progress, n_tokens = 13074, batch.n_tokens = 449, progress = 0.995129
slot update_slots: id  1 | task 12071 | n_tokens = 13074, memory_seq_rm [13074, end)
slot update_slots: id  1 | task 12071 | prompt processing progress, n_tokens = 13138, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12071 | prompt done, n_tokens = 13138, batch.n_tokens = 64
slot init_sampler: id  1 | task 12071 | init sampler, took 2.60 ms, tokens: text = 13138, total = 13138
slot update_slots: id  1 | task 12071 | created context checkpoint 8 of 8 (pos_min = 12357, pos_max = 13073, size = 16.813 MiB)
slot print_timing: id  1 | task 12071 | 
prompt eval time =     815.11 ms /   513 tokens (    1.59 ms per token,   629.36 tokens per second)
       eval time =    1302.07 ms /    60 tokens (   21.70 ms per token,    46.08 tokens per second)
      total time =    2117.18 ms /   573 tokens
slot      release: id  1 | task 12071 | stop processing: n_tokens = 13197, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.867 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12133 | processing task, is_child = 0
slot update_slots: id  1 | task 12133 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 15157
slot update_slots: id  1 | task 12133 | n_tokens = 13138, memory_seq_rm [13138, end)
slot update_slots: id  1 | task 12133 | prompt processing progress, n_tokens = 15093, batch.n_tokens = 1955, progress = 0.995778
slot update_slots: id  1 | task 12133 | n_tokens = 15093, memory_seq_rm [15093, end)
slot update_slots: id  1 | task 12133 | prompt processing progress, n_tokens = 15157, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12133 | prompt done, n_tokens = 15157, batch.n_tokens = 64
slot init_sampler: id  1 | task 12133 | init sampler, took 2.22 ms, tokens: text = 15157, total = 15157
slot update_slots: id  1 | task 12133 | erasing old context checkpoint (pos_min = 361, pos_max = 1128, size = 18.009 MiB)
slot update_slots: id  1 | task 12133 | created context checkpoint 8 of 8 (pos_min = 14325, pos_max = 15092, size = 18.009 MiB)
slot print_timing: id  1 | task 12133 | 
prompt eval time =    2656.74 ms /  2019 tokens (    1.32 ms per token,   759.95 tokens per second)
       eval time =    2088.31 ms /    96 tokens (   21.75 ms per token,    45.97 tokens per second)
      total time =    4745.05 ms /  2115 tokens
slot      release: id  1 | task 12133 | stop processing: n_tokens = 15252, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.847 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12231 | processing task, is_child = 0
slot update_slots: id  1 | task 12231 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 17888
slot update_slots: id  1 | task 12231 | n_tokens = 15157, memory_seq_rm [15157, end)
slot update_slots: id  1 | task 12231 | prompt processing progress, n_tokens = 17205, batch.n_tokens = 2048, progress = 0.961818
slot update_slots: id  1 | task 12231 | n_tokens = 17205, memory_seq_rm [17205, end)
slot update_slots: id  1 | task 12231 | prompt processing progress, n_tokens = 17824, batch.n_tokens = 619, progress = 0.996422
slot update_slots: id  1 | task 12231 | n_tokens = 17824, memory_seq_rm [17824, end)
slot update_slots: id  1 | task 12231 | prompt processing progress, n_tokens = 17888, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12231 | prompt done, n_tokens = 17888, batch.n_tokens = 64
slot init_sampler: id  1 | task 12231 | init sampler, took 2.45 ms, tokens: text = 17888, total = 17888
slot update_slots: id  1 | task 12231 | erasing old context checkpoint (pos_min = 2971, pos_max = 3738, size = 18.009 MiB)
slot update_slots: id  1 | task 12231 | created context checkpoint 8 of 8 (pos_min = 17056, pos_max = 17823, size = 18.009 MiB)
slot print_timing: id  1 | task 12231 | 
prompt eval time =    3807.57 ms /  2731 tokens (    1.39 ms per token,   717.25 tokens per second)
       eval time =    1555.36 ms /    69 tokens (   22.54 ms per token,    44.36 tokens per second)
      total time =    5362.93 ms /  2800 tokens
slot      release: id  1 | task 12231 | stop processing: n_tokens = 17956, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12303 | processing task, is_child = 0
slot update_slots: id  1 | task 12303 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 19369
slot update_slots: id  1 | task 12303 | n_tokens = 17888, memory_seq_rm [17888, end)
slot update_slots: id  1 | task 12303 | prompt processing progress, n_tokens = 19305, batch.n_tokens = 1417, progress = 0.996696
slot update_slots: id  1 | task 12303 | n_tokens = 19305, memory_seq_rm [19305, end)
slot update_slots: id  1 | task 12303 | prompt processing progress, n_tokens = 19369, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12303 | prompt done, n_tokens = 19369, batch.n_tokens = 64
slot init_sampler: id  1 | task 12303 | init sampler, took 3.79 ms, tokens: text = 19369, total = 19369
slot update_slots: id  1 | task 12303 | erasing old context checkpoint (pos_min = 3511, pos_max = 4241, size = 17.141 MiB)
slot update_slots: id  1 | task 12303 | created context checkpoint 8 of 8 (pos_min = 18537, pos_max = 19304, size = 18.009 MiB)
slot print_timing: id  1 | task 12303 | 
prompt eval time =    2162.67 ms /  1481 tokens (    1.46 ms per token,   684.80 tokens per second)
       eval time =    1403.53 ms /    62 tokens (   22.64 ms per token,    44.17 tokens per second)
      total time =    3566.20 ms /  1543 tokens
slot      release: id  1 | task 12303 | stop processing: n_tokens = 19430, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.876 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12367 | processing task, is_child = 0
slot update_slots: id  1 | task 12367 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 22100
slot update_slots: id  1 | task 12367 | n_tokens = 19369, memory_seq_rm [19369, end)
slot update_slots: id  1 | task 12367 | prompt processing progress, n_tokens = 21417, batch.n_tokens = 2048, progress = 0.969095
slot update_slots: id  1 | task 12367 | n_tokens = 21417, memory_seq_rm [21417, end)
slot update_slots: id  1 | task 12367 | prompt processing progress, n_tokens = 22036, batch.n_tokens = 619, progress = 0.997104
slot update_slots: id  1 | task 12367 | n_tokens = 22036, memory_seq_rm [22036, end)
slot update_slots: id  1 | task 12367 | prompt processing progress, n_tokens = 22100, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12367 | prompt done, n_tokens = 22100, batch.n_tokens = 64
slot init_sampler: id  1 | task 12367 | init sampler, took 3.08 ms, tokens: text = 22100, total = 22100
slot update_slots: id  1 | task 12367 | erasing old context checkpoint (pos_min = 6557, pos_max = 7324, size = 18.009 MiB)
slot update_slots: id  1 | task 12367 | created context checkpoint 8 of 8 (pos_min = 21268, pos_max = 22035, size = 18.009 MiB)
slot print_timing: id  1 | task 12367 | 
prompt eval time =    4028.16 ms /  2731 tokens (    1.47 ms per token,   677.98 tokens per second)
       eval time =    1965.31 ms /    85 tokens (   23.12 ms per token,    43.25 tokens per second)
      total time =    5993.47 ms /  2816 tokens
slot      release: id  1 | task 12367 | stop processing: n_tokens = 22184, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.890 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12455 | processing task, is_child = 0
slot update_slots: id  1 | task 12455 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 24831
srv    send_error: task id = 12455, error: request (24831 tokens) exceeds the available context size (24576 tokens), try increasing it
slot      release: id  1 | task 12455 | stop processing: n_tokens = 22184, truncated = 0
srv          stop: cancel task, id_task = 12455
srv  update_slots: no tokens to decode
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 400
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.851 (> 0.100 thold), f_keep = 0.041
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 22184, total state size = 538.201 MiB
srv          load:  - looking for better prompt, base f_keep = 0.041, sim = 0.851
srv        update:  - cache state: 6 prompts, 2059.203 MiB (limits: 8192.000 MiB, 49152 tokens, 262905 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv  get_availabl: prompt cache update took 673.54 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12458 | processing task, is_child = 0
slot update_slots: id  1 | task 12458 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 1075
slot update_slots: id  1 | task 12458 | n_past = 915, slot.prompt.tokens.size() = 22184, seq_id = 1, pos_min = 21416, n_swa = 128
slot update_slots: id  1 | task 12458 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 12458 | erased invalidated context checkpoint (pos_min = 7210, pos_max = 7810, n_swa = 128, size = 14.093 MiB)
slot update_slots: id  1 | task 12458 | erased invalidated context checkpoint (pos_min = 9062, pos_max = 9829, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 12458 | erased invalidated context checkpoint (pos_min = 11793, pos_max = 12560, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 12458 | erased invalidated context checkpoint (pos_min = 12357, pos_max = 13073, n_swa = 128, size = 16.813 MiB)
slot update_slots: id  1 | task 12458 | erased invalidated context checkpoint (pos_min = 14325, pos_max = 15092, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 12458 | erased invalidated context checkpoint (pos_min = 17056, pos_max = 17823, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 12458 | erased invalidated context checkpoint (pos_min = 18537, pos_max = 19304, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 12458 | erased invalidated context checkpoint (pos_min = 21268, pos_max = 22035, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 12458 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 12458 | prompt processing progress, n_tokens = 1011, batch.n_tokens = 1011, progress = 0.940465
slot update_slots: id  1 | task 12458 | n_tokens = 1011, memory_seq_rm [1011, end)
slot update_slots: id  1 | task 12458 | prompt processing progress, n_tokens = 1075, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12458 | prompt done, n_tokens = 1075, batch.n_tokens = 64
slot init_sampler: id  1 | task 12458 | init sampler, took 0.15 ms, tokens: text = 1075, total = 1075
slot update_slots: id  1 | task 12458 | created context checkpoint 1 of 8 (pos_min = 243, pos_max = 1010, size = 18.009 MiB)
slot print_timing: id  1 | task 12458 | 
prompt eval time =    1238.37 ms /  1075 tokens (    1.15 ms per token,   868.08 tokens per second)
       eval time =     755.53 ms /    38 tokens (   19.88 ms per token,    50.30 tokens per second)
      total time =    1993.89 ms /  1113 tokens
slot      release: id  1 | task 12458 | stop processing: n_tokens = 1112, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.685 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12498 | processing task, is_child = 0
slot update_slots: id  1 | task 12498 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 1595
slot update_slots: id  1 | task 12498 | n_tokens = 1092, memory_seq_rm [1092, end)
slot update_slots: id  1 | task 12498 | prompt processing progress, n_tokens = 1531, batch.n_tokens = 439, progress = 0.959875
slot update_slots: id  1 | task 12498 | n_tokens = 1531, memory_seq_rm [1531, end)
slot update_slots: id  1 | task 12498 | prompt processing progress, n_tokens = 1595, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12498 | prompt done, n_tokens = 1595, batch.n_tokens = 64
slot init_sampler: id  1 | task 12498 | init sampler, took 0.23 ms, tokens: text = 1595, total = 1595
slot update_slots: id  1 | task 12498 | created context checkpoint 2 of 8 (pos_min = 763, pos_max = 1530, size = 18.009 MiB)
slot print_timing: id  1 | task 12498 | 
prompt eval time =     596.40 ms /   503 tokens (    1.19 ms per token,   843.40 tokens per second)
       eval time =     887.32 ms /    44 tokens (   20.17 ms per token,    49.59 tokens per second)
      total time =    1483.71 ms /   547 tokens
slot      release: id  1 | task 12498 | stop processing: n_tokens = 1638, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.447 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12544 | processing task, is_child = 0
slot update_slots: id  1 | task 12544 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3598
slot update_slots: id  1 | task 12544 | n_tokens = 1607, memory_seq_rm [1607, end)
slot update_slots: id  1 | task 12544 | prompt processing progress, n_tokens = 3534, batch.n_tokens = 1927, progress = 0.982212
slot update_slots: id  1 | task 12544 | n_tokens = 3534, memory_seq_rm [3534, end)
slot update_slots: id  1 | task 12544 | prompt processing progress, n_tokens = 3598, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12544 | prompt done, n_tokens = 3598, batch.n_tokens = 64
slot init_sampler: id  1 | task 12544 | init sampler, took 0.68 ms, tokens: text = 3598, total = 3598
slot update_slots: id  1 | task 12544 | created context checkpoint 3 of 8 (pos_min = 2766, pos_max = 3533, size = 18.009 MiB)
slot print_timing: id  1 | task 12544 | 
prompt eval time =    2144.84 ms /  1991 tokens (    1.08 ms per token,   928.27 tokens per second)
       eval time =     802.31 ms /    40 tokens (   20.06 ms per token,    49.86 tokens per second)
      total time =    2947.15 ms /  2031 tokens
slot      release: id  1 | task 12544 | stop processing: n_tokens = 3637, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.201 (> 0.100 thold), f_keep = 0.296
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3637, total state size = 103.293 MiB
srv          load:  - looking for better prompt, base f_keep = 0.296, sim = 0.201
srv        update:  - cache state: 7 prompts, 2216.523 MiB (limits: 8192.000 MiB, 49152 tokens, 257687 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv  get_availabl: prompt cache update took 91.94 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12586 | processing task, is_child = 0
slot update_slots: id  1 | task 12586 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5341
slot update_slots: id  1 | task 12586 | n_past = 1075, slot.prompt.tokens.size() = 3637, seq_id = 1, pos_min = 2869, n_swa = 128
slot update_slots: id  1 | task 12586 | restored context checkpoint (pos_min = 763, pos_max = 1530, size = 18.009 MiB)
slot update_slots: id  1 | task 12586 | erased invalidated context checkpoint (pos_min = 2766, pos_max = 3533, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 12586 | n_tokens = 1075, memory_seq_rm [1075, end)
slot update_slots: id  1 | task 12586 | prompt processing progress, n_tokens = 3123, batch.n_tokens = 2048, progress = 0.584722
slot update_slots: id  1 | task 12586 | n_tokens = 3123, memory_seq_rm [3123, end)
slot update_slots: id  1 | task 12586 | prompt processing progress, n_tokens = 5171, batch.n_tokens = 2048, progress = 0.968171
slot update_slots: id  1 | task 12586 | n_tokens = 5171, memory_seq_rm [5171, end)
slot update_slots: id  1 | task 12586 | prompt processing progress, n_tokens = 5277, batch.n_tokens = 106, progress = 0.988017
slot update_slots: id  1 | task 12586 | n_tokens = 5277, memory_seq_rm [5277, end)
slot update_slots: id  1 | task 12586 | prompt processing progress, n_tokens = 5341, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12586 | prompt done, n_tokens = 5341, batch.n_tokens = 64
slot init_sampler: id  1 | task 12586 | init sampler, took 1.00 ms, tokens: text = 5341, total = 5341
slot update_slots: id  1 | task 12586 | created context checkpoint 3 of 8 (pos_min = 4509, pos_max = 5276, size = 18.009 MiB)
slot print_timing: id  1 | task 12586 | 
prompt eval time =    4633.85 ms /  4266 tokens (    1.09 ms per token,   920.62 tokens per second)
       eval time =     731.43 ms /    37 tokens (   19.77 ms per token,    50.59 tokens per second)
      total time =    5365.28 ms /  4303 tokens
slot      release: id  1 | task 12586 | stop processing: n_tokens = 5377, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12627 | processing task, is_child = 0
slot update_slots: id  1 | task 12627 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5506
slot update_slots: id  1 | task 12627 | n_tokens = 5340, memory_seq_rm [5340, end)
slot update_slots: id  1 | task 12627 | prompt processing progress, n_tokens = 5442, batch.n_tokens = 102, progress = 0.988376
slot update_slots: id  1 | task 12627 | n_tokens = 5442, memory_seq_rm [5442, end)
slot update_slots: id  1 | task 12627 | prompt processing progress, n_tokens = 5506, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12627 | prompt done, n_tokens = 5506, batch.n_tokens = 64
slot init_sampler: id  1 | task 12627 | init sampler, took 0.78 ms, tokens: text = 5506, total = 5506
slot update_slots: id  1 | task 12627 | created context checkpoint 4 of 8 (pos_min = 4710, pos_max = 5441, size = 17.165 MiB)
slot print_timing: id  1 | task 12627 | 
prompt eval time =     515.44 ms /   166 tokens (    3.11 ms per token,   322.06 tokens per second)
       eval time =    1335.71 ms /    66 tokens (   20.24 ms per token,    49.41 tokens per second)
      total time =    1851.15 ms /   232 tokens
slot      release: id  1 | task 12627 | stop processing: n_tokens = 5571, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.764 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 12695 | processing task, is_child = 0
slot update_slots: id  1 | task 12695 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 7208
slot update_slots: id  1 | task 12695 | n_tokens = 5506, memory_seq_rm [5506, end)
slot update_slots: id  1 | task 12695 | prompt processing progress, n_tokens = 7144, batch.n_tokens = 1638, progress = 0.991121
slot update_slots: id  1 | task 12695 | n_tokens = 7144, memory_seq_rm [7144, end)
slot update_slots: id  1 | task 12695 | prompt processing progress, n_tokens = 7208, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 12695 | prompt done, n_tokens = 7208, batch.n_tokens = 64
slot init_sampler: id  1 | task 12695 | init sampler, took 1.00 ms, tokens: text = 7208, total = 7208
slot update_slots: id  1 | task 12695 | created context checkpoint 5 of 8 (pos_min = 6376, pos_max = 7143, size = 18.009 MiB)
slot print_timing: id  1 | task 12695 | 
prompt eval time =    2141.49 ms /  1702 tokens (    1.26 ms per token,   794.77 tokens per second)
       eval time =    1498.97 ms /    73 tokens (   20.53 ms per token,    48.70 tokens per second)
      total time =    3640.46 ms /  1775 tokens
slot      release: id  1 | task 12695 | stop processing: n_tokens = 7280, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 1139907731
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 859, total state size = 38.152 MiB
srv          load:  - looking for better prompt, base f_keep = 0.497, sim = 0.087
srv        update:  - cache state: 8 prompts, 2267.338 MiB (limits: 8192.000 MiB, 49152 tokens, 255016 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv  get_availabl: prompt cache update took 35.36 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 12770 | processing task, is_child = 0
slot update_slots: id  0 | task 12770 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 4910
slot update_slots: id  0 | task 12770 | n_tokens = 427, memory_seq_rm [427, end)
slot update_slots: id  0 | task 12770 | prompt processing progress, n_tokens = 2475, batch.n_tokens = 2048, progress = 0.504073
slot update_slots: id  0 | task 12770 | n_tokens = 2475, memory_seq_rm [2475, end)
slot update_slots: id  0 | task 12770 | prompt processing progress, n_tokens = 4523, batch.n_tokens = 2048, progress = 0.921181
slot update_slots: id  0 | task 12770 | n_tokens = 4523, memory_seq_rm [4523, end)
slot update_slots: id  0 | task 12770 | prompt processing progress, n_tokens = 4846, batch.n_tokens = 323, progress = 0.986965
slot update_slots: id  0 | task 12770 | n_tokens = 4846, memory_seq_rm [4846, end)
slot update_slots: id  0 | task 12770 | prompt processing progress, n_tokens = 4910, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 12770 | prompt done, n_tokens = 4910, batch.n_tokens = 64
slot init_sampler: id  0 | task 12770 | init sampler, took 0.92 ms, tokens: text = 4910, total = 4910
slot update_slots: id  0 | task 12770 | created context checkpoint 2 of 8 (pos_min = 4078, pos_max = 4845, size = 18.009 MiB)
slot print_timing: id  0 | task 12770 | 
prompt eval time =    4816.02 ms /  4483 tokens (    1.07 ms per token,   930.85 tokens per second)
       eval time =    8372.36 ms /   404 tokens (   20.72 ms per token,    48.25 tokens per second)
      total time =   13188.38 ms /  4887 tokens
slot      release: id  0 | task 12770 | stop processing: n_tokens = 5313, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.295 (> 0.100 thold), f_keep = 0.126
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7280, total state size = 188.718 MiB
srv          load:  - looking for better prompt, base f_keep = 0.126, sim = 0.295
srv        update:  - cache state: 9 prompts, 2545.257 MiB (limits: 8192.000 MiB, 49152 tokens, 250601 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv  get_availabl: prompt cache update took 203.51 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13178 | processing task, is_child = 0
slot update_slots: id  1 | task 13178 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3114
slot update_slots: id  1 | task 13178 | n_past = 920, slot.prompt.tokens.size() = 7280, seq_id = 1, pos_min = 6512, n_swa = 128
slot update_slots: id  1 | task 13178 | restored context checkpoint (pos_min = 763, pos_max = 1530, size = 18.009 MiB)
slot update_slots: id  1 | task 13178 | erased invalidated context checkpoint (pos_min = 4509, pos_max = 5276, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 13178 | erased invalidated context checkpoint (pos_min = 4710, pos_max = 5441, n_swa = 128, size = 17.165 MiB)
slot update_slots: id  1 | task 13178 | erased invalidated context checkpoint (pos_min = 6376, pos_max = 7143, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 13178 | n_tokens = 920, memory_seq_rm [920, end)
slot update_slots: id  1 | task 13178 | prompt processing progress, n_tokens = 2968, batch.n_tokens = 2048, progress = 0.953115
slot update_slots: id  1 | task 13178 | n_tokens = 2968, memory_seq_rm [2968, end)
slot update_slots: id  1 | task 13178 | prompt processing progress, n_tokens = 3050, batch.n_tokens = 82, progress = 0.979448
slot update_slots: id  1 | task 13178 | n_tokens = 3050, memory_seq_rm [3050, end)
slot update_slots: id  1 | task 13178 | prompt processing progress, n_tokens = 3114, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13178 | prompt done, n_tokens = 3114, batch.n_tokens = 64
slot init_sampler: id  1 | task 13178 | init sampler, took 0.44 ms, tokens: text = 3114, total = 3114
slot update_slots: id  1 | task 13178 | created context checkpoint 3 of 8 (pos_min = 2282, pos_max = 3049, size = 18.009 MiB)
slot print_timing: id  1 | task 13178 | 
prompt eval time =    2574.42 ms /  2194 tokens (    1.17 ms per token,   852.23 tokens per second)
       eval time =    1293.34 ms /    58 tokens (   22.30 ms per token,    44.85 tokens per second)
      total time =    3867.75 ms /  2252 tokens
slot      release: id  1 | task 13178 | stop processing: n_tokens = 3171, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.903 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13239 | processing task, is_child = 0
slot update_slots: id  1 | task 13239 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3483
slot update_slots: id  1 | task 13239 | n_tokens = 3144, memory_seq_rm [3144, end)
slot update_slots: id  1 | task 13239 | prompt processing progress, n_tokens = 3419, batch.n_tokens = 275, progress = 0.981625
slot update_slots: id  1 | task 13239 | n_tokens = 3419, memory_seq_rm [3419, end)
slot update_slots: id  1 | task 13239 | prompt processing progress, n_tokens = 3483, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13239 | prompt done, n_tokens = 3483, batch.n_tokens = 64
slot init_sampler: id  1 | task 13239 | init sampler, took 0.67 ms, tokens: text = 3483, total = 3483
slot update_slots: id  1 | task 13239 | created context checkpoint 4 of 8 (pos_min = 2651, pos_max = 3418, size = 18.009 MiB)
slot print_timing: id  1 | task 13239 | 
prompt eval time =     549.25 ms /   339 tokens (    1.62 ms per token,   617.20 tokens per second)
       eval time =     917.53 ms /    42 tokens (   21.85 ms per token,    45.77 tokens per second)
      total time =    1466.78 ms /   381 tokens
slot      release: id  1 | task 13239 | stop processing: n_tokens = 3524, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.572 (> 0.100 thold), f_keep = 0.884
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13283 | processing task, is_child = 0
slot update_slots: id  1 | task 13283 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5444
slot update_slots: id  1 | task 13283 | n_tokens = 3114, memory_seq_rm [3114, end)
slot update_slots: id  1 | task 13283 | prompt processing progress, n_tokens = 5162, batch.n_tokens = 2048, progress = 0.948200
slot update_slots: id  1 | task 13283 | n_tokens = 5162, memory_seq_rm [5162, end)
slot update_slots: id  1 | task 13283 | prompt processing progress, n_tokens = 5380, batch.n_tokens = 218, progress = 0.988244
slot update_slots: id  1 | task 13283 | n_tokens = 5380, memory_seq_rm [5380, end)
slot update_slots: id  1 | task 13283 | prompt processing progress, n_tokens = 5444, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13283 | prompt done, n_tokens = 5444, batch.n_tokens = 64
slot init_sampler: id  1 | task 13283 | init sampler, took 0.75 ms, tokens: text = 5444, total = 5444
slot update_slots: id  1 | task 13283 | created context checkpoint 5 of 8 (pos_min = 4612, pos_max = 5379, size = 18.009 MiB)
slot print_timing: id  1 | task 13283 | 
prompt eval time =    2783.45 ms /  2330 tokens (    1.19 ms per token,   837.09 tokens per second)
       eval time =    1154.29 ms /    52 tokens (   22.20 ms per token,    45.05 tokens per second)
      total time =    3937.74 ms /  2382 tokens
slot      release: id  1 | task 13283 | stop processing: n_tokens = 5495, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.666 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13338 | processing task, is_child = 0
slot update_slots: id  1 | task 13338 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 8175
slot update_slots: id  1 | task 13338 | n_tokens = 5444, memory_seq_rm [5444, end)
slot update_slots: id  1 | task 13338 | prompt processing progress, n_tokens = 7492, batch.n_tokens = 2048, progress = 0.916453
slot update_slots: id  1 | task 13338 | n_tokens = 7492, memory_seq_rm [7492, end)
slot update_slots: id  1 | task 13338 | prompt processing progress, n_tokens = 8111, batch.n_tokens = 619, progress = 0.992171
slot update_slots: id  1 | task 13338 | n_tokens = 8111, memory_seq_rm [8111, end)
slot update_slots: id  1 | task 13338 | prompt processing progress, n_tokens = 8175, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13338 | prompt done, n_tokens = 8175, batch.n_tokens = 64
slot init_sampler: id  1 | task 13338 | init sampler, took 1.13 ms, tokens: text = 8175, total = 8175
slot update_slots: id  1 | task 13338 | created context checkpoint 6 of 8 (pos_min = 7343, pos_max = 8110, size = 18.009 MiB)
slot print_timing: id  1 | task 13338 | 
prompt eval time =    3490.59 ms /  2731 tokens (    1.28 ms per token,   782.39 tokens per second)
       eval time =    2040.14 ms /    89 tokens (   22.92 ms per token,    43.62 tokens per second)
      total time =    5530.73 ms /  2820 tokens
slot      release: id  1 | task 13338 | stop processing: n_tokens = 8263, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13430 | processing task, is_child = 0
slot update_slots: id  1 | task 13430 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 8289
slot update_slots: id  1 | task 13430 | n_tokens = 8175, memory_seq_rm [8175, end)
slot update_slots: id  1 | task 13430 | prompt processing progress, n_tokens = 8225, batch.n_tokens = 50, progress = 0.992279
slot update_slots: id  1 | task 13430 | n_tokens = 8225, memory_seq_rm [8225, end)
slot update_slots: id  1 | task 13430 | prompt processing progress, n_tokens = 8289, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13430 | prompt done, n_tokens = 8289, batch.n_tokens = 64
slot init_sampler: id  1 | task 13430 | init sampler, took 1.15 ms, tokens: text = 8289, total = 8289
slot update_slots: id  1 | task 13430 | created context checkpoint 7 of 8 (pos_min = 7495, pos_max = 8224, size = 17.118 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 13430
slot      release: id  1 | task 13430 | stop processing: n_tokens = 8434, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.128
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8434, total state size = 215.778 MiB
srv          load:  - looking for better prompt, base f_keep = 0.128, sim = 1.000
srv        update:  - cache state: 10 prompts, 2886.207 MiB (limits: 8192.000 MiB, 49152 tokens, 244936 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv  get_availabl: prompt cache update took 246.12 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13579 | processing task, is_child = 0
slot update_slots: id  1 | task 13579 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 1079
slot update_slots: id  1 | task 13579 | n_past = 1079, slot.prompt.tokens.size() = 8434, seq_id = 1, pos_min = 7666, n_swa = 128
slot update_slots: id  1 | task 13579 | restored context checkpoint (pos_min = 763, pos_max = 1530, size = 18.009 MiB)
slot update_slots: id  1 | task 13579 | erased invalidated context checkpoint (pos_min = 2282, pos_max = 3049, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 13579 | erased invalidated context checkpoint (pos_min = 2651, pos_max = 3418, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 13579 | erased invalidated context checkpoint (pos_min = 4612, pos_max = 5379, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 13579 | erased invalidated context checkpoint (pos_min = 7343, pos_max = 8110, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 13579 | erased invalidated context checkpoint (pos_min = 7495, pos_max = 8224, n_swa = 128, size = 17.118 MiB)
slot update_slots: id  1 | task 13579 | need to evaluate at least 1 token for each active slot (n_past = 1079, task.n_tokens() = 1079)
slot update_slots: id  1 | task 13579 | n_past was set to 1078
slot update_slots: id  1 | task 13579 | n_tokens = 1078, memory_seq_rm [1078, end)
slot update_slots: id  1 | task 13579 | prompt processing progress, n_tokens = 1079, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  1 | task 13579 | prompt done, n_tokens = 1079, batch.n_tokens = 1
slot init_sampler: id  1 | task 13579 | init sampler, took 0.16 ms, tokens: text = 1079, total = 1079
slot print_timing: id  1 | task 13579 | 
prompt eval time =      37.31 ms /     1 tokens (   37.31 ms per token,    26.81 tokens per second)
       eval time =     891.84 ms /    42 tokens (   21.23 ms per token,    47.09 tokens per second)
      total time =     929.15 ms /    43 tokens
slot      release: id  1 | task 13579 | stop processing: n_tokens = 1120, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.686 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13622 | processing task, is_child = 0
slot update_slots: id  1 | task 13622 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 1600
slot update_slots: id  1 | task 13622 | n_tokens = 1097, memory_seq_rm [1097, end)
slot update_slots: id  1 | task 13622 | prompt processing progress, n_tokens = 1536, batch.n_tokens = 439, progress = 0.960000
slot update_slots: id  1 | task 13622 | n_tokens = 1536, memory_seq_rm [1536, end)
slot update_slots: id  1 | task 13622 | prompt processing progress, n_tokens = 1600, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13622 | prompt done, n_tokens = 1600, batch.n_tokens = 64
slot init_sampler: id  1 | task 13622 | init sampler, took 0.24 ms, tokens: text = 1600, total = 1600
slot print_timing: id  1 | task 13622 | 
prompt eval time =     590.66 ms /   503 tokens (    1.17 ms per token,   851.59 tokens per second)
       eval time =     924.19 ms /    47 tokens (   19.66 ms per token,    50.86 tokens per second)
      total time =    1514.86 ms /   550 tokens
slot      release: id  1 | task 13622 | stop processing: n_tokens = 1646, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.486 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13671 | processing task, is_child = 0
slot update_slots: id  1 | task 13671 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3313
slot update_slots: id  1 | task 13671 | n_tokens = 1611, memory_seq_rm [1611, end)
slot update_slots: id  1 | task 13671 | prompt processing progress, n_tokens = 3249, batch.n_tokens = 1638, progress = 0.980682
slot update_slots: id  1 | task 13671 | n_tokens = 3249, memory_seq_rm [3249, end)
slot update_slots: id  1 | task 13671 | prompt processing progress, n_tokens = 3313, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13671 | prompt done, n_tokens = 3313, batch.n_tokens = 64
slot init_sampler: id  1 | task 13671 | init sampler, took 0.50 ms, tokens: text = 3313, total = 3313
slot update_slots: id  1 | task 13671 | created context checkpoint 3 of 8 (pos_min = 2481, pos_max = 3248, size = 18.009 MiB)
slot print_timing: id  1 | task 13671 | 
prompt eval time =    2014.37 ms /  1702 tokens (    1.18 ms per token,   844.93 tokens per second)
       eval time =     947.49 ms /    46 tokens (   20.60 ms per token,    48.55 tokens per second)
      total time =    2961.86 ms /  1748 tokens
slot      release: id  1 | task 13671 | stop processing: n_tokens = 3358, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.625 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13719 | processing task, is_child = 0
slot update_slots: id  1 | task 13719 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5315
slot update_slots: id  1 | task 13719 | n_tokens = 3324, memory_seq_rm [3324, end)
slot update_slots: id  1 | task 13719 | prompt processing progress, n_tokens = 5251, batch.n_tokens = 1927, progress = 0.987959
slot update_slots: id  1 | task 13719 | n_tokens = 5251, memory_seq_rm [5251, end)
slot update_slots: id  1 | task 13719 | prompt processing progress, n_tokens = 5315, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13719 | prompt done, n_tokens = 5315, batch.n_tokens = 64
slot init_sampler: id  1 | task 13719 | init sampler, took 1.03 ms, tokens: text = 5315, total = 5315
slot update_slots: id  1 | task 13719 | created context checkpoint 4 of 8 (pos_min = 4483, pos_max = 5250, size = 18.009 MiB)
slot print_timing: id  1 | task 13719 | 
prompt eval time =    2243.76 ms /  1991 tokens (    1.13 ms per token,   887.35 tokens per second)
       eval time =     863.51 ms /    41 tokens (   21.06 ms per token,    47.48 tokens per second)
      total time =    3107.27 ms /  2032 tokens
slot      release: id  1 | task 13719 | stop processing: n_tokens = 5355, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.153 (> 0.100 thold), f_keep = 0.201
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5355, total state size = 143.578 MiB
srv          load:  - looking for better prompt, base f_keep = 0.201, sim = 0.153
srv        update:  - cache state: 11 prompts, 3101.822 MiB (limits: 8192.000 MiB, 49152 tokens, 242052 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv  get_availabl: prompt cache update took 186.67 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13762 | processing task, is_child = 0
slot update_slots: id  1 | task 13762 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 7047
slot update_slots: id  1 | task 13762 | n_past = 1079, slot.prompt.tokens.size() = 5355, seq_id = 1, pos_min = 4587, n_swa = 128
slot update_slots: id  1 | task 13762 | restored context checkpoint (pos_min = 763, pos_max = 1530, size = 18.009 MiB)
slot update_slots: id  1 | task 13762 | erased invalidated context checkpoint (pos_min = 2481, pos_max = 3248, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 13762 | erased invalidated context checkpoint (pos_min = 4483, pos_max = 5250, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 13762 | n_tokens = 1079, memory_seq_rm [1079, end)
slot update_slots: id  1 | task 13762 | prompt processing progress, n_tokens = 3127, batch.n_tokens = 2048, progress = 0.443735
slot update_slots: id  1 | task 13762 | n_tokens = 3127, memory_seq_rm [3127, end)
slot update_slots: id  1 | task 13762 | prompt processing progress, n_tokens = 5175, batch.n_tokens = 2048, progress = 0.734355
slot update_slots: id  1 | task 13762 | n_tokens = 5175, memory_seq_rm [5175, end)
slot update_slots: id  1 | task 13762 | prompt processing progress, n_tokens = 6983, batch.n_tokens = 1808, progress = 0.990918
slot update_slots: id  1 | task 13762 | n_tokens = 6983, memory_seq_rm [6983, end)
slot update_slots: id  1 | task 13762 | prompt processing progress, n_tokens = 7047, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13762 | prompt done, n_tokens = 7047, batch.n_tokens = 64
slot init_sampler: id  1 | task 13762 | init sampler, took 0.98 ms, tokens: text = 7047, total = 7047
slot update_slots: id  1 | task 13762 | created context checkpoint 3 of 8 (pos_min = 6215, pos_max = 6982, size = 18.009 MiB)
slot print_timing: id  1 | task 13762 | 
prompt eval time =    6553.66 ms /  5968 tokens (    1.10 ms per token,   910.64 tokens per second)
       eval time =    1765.26 ms /    84 tokens (   21.02 ms per token,    47.59 tokens per second)
      total time =    8318.92 ms /  6052 tokens
slot      release: id  1 | task 13762 | stop processing: n_tokens = 7130, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.865 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13850 | processing task, is_child = 0
slot update_slots: id  1 | task 13850 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 8143
slot update_slots: id  1 | task 13850 | n_tokens = 7047, memory_seq_rm [7047, end)
slot update_slots: id  1 | task 13850 | prompt processing progress, n_tokens = 8079, batch.n_tokens = 1032, progress = 0.992140
slot update_slots: id  1 | task 13850 | n_tokens = 8079, memory_seq_rm [8079, end)
slot update_slots: id  1 | task 13850 | prompt processing progress, n_tokens = 8143, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13850 | prompt done, n_tokens = 8143, batch.n_tokens = 64
slot init_sampler: id  1 | task 13850 | init sampler, took 1.16 ms, tokens: text = 8143, total = 8143
slot update_slots: id  1 | task 13850 | created context checkpoint 4 of 8 (pos_min = 7311, pos_max = 8078, size = 18.009 MiB)
slot print_timing: id  1 | task 13850 | 
prompt eval time =    1428.93 ms /  1096 tokens (    1.30 ms per token,   767.01 tokens per second)
       eval time =    1819.25 ms /    83 tokens (   21.92 ms per token,    45.62 tokens per second)
      total time =    3248.17 ms /  1179 tokens
slot      release: id  1 | task 13850 | stop processing: n_tokens = 8225, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.929 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13935 | processing task, is_child = 0
slot update_slots: id  1 | task 13935 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 8768
slot update_slots: id  1 | task 13935 | n_tokens = 8143, memory_seq_rm [8143, end)
slot update_slots: id  1 | task 13935 | prompt processing progress, n_tokens = 8704, batch.n_tokens = 561, progress = 0.992701
slot update_slots: id  1 | task 13935 | n_tokens = 8704, memory_seq_rm [8704, end)
slot update_slots: id  1 | task 13935 | prompt processing progress, n_tokens = 8768, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13935 | prompt done, n_tokens = 8768, batch.n_tokens = 64
slot init_sampler: id  1 | task 13935 | init sampler, took 2.04 ms, tokens: text = 8768, total = 8768
slot update_slots: id  1 | task 13935 | created context checkpoint 5 of 8 (pos_min = 7936, pos_max = 8703, size = 18.009 MiB)
slot print_timing: id  1 | task 13935 | 
prompt eval time =     967.43 ms /   625 tokens (    1.55 ms per token,   646.04 tokens per second)
       eval time =    1095.56 ms /    49 tokens (   22.36 ms per token,    44.73 tokens per second)
      total time =    2062.98 ms /   674 tokens
slot      release: id  1 | task 13935 | stop processing: n_tokens = 8816, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.763 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 13986 | processing task, is_child = 0
slot update_slots: id  1 | task 13986 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 11499
slot update_slots: id  1 | task 13986 | n_tokens = 8768, memory_seq_rm [8768, end)
slot update_slots: id  1 | task 13986 | prompt processing progress, n_tokens = 10816, batch.n_tokens = 2048, progress = 0.940604
slot update_slots: id  1 | task 13986 | n_tokens = 10816, memory_seq_rm [10816, end)
slot update_slots: id  1 | task 13986 | prompt processing progress, n_tokens = 11435, batch.n_tokens = 619, progress = 0.994434
slot update_slots: id  1 | task 13986 | n_tokens = 11435, memory_seq_rm [11435, end)
slot update_slots: id  1 | task 13986 | prompt processing progress, n_tokens = 11499, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 13986 | prompt done, n_tokens = 11499, batch.n_tokens = 64
slot init_sampler: id  1 | task 13986 | init sampler, took 1.61 ms, tokens: text = 11499, total = 11499
slot update_slots: id  1 | task 13986 | created context checkpoint 6 of 8 (pos_min = 10667, pos_max = 11434, size = 18.009 MiB)
slot print_timing: id  1 | task 13986 | 
prompt eval time =    3607.67 ms /  2731 tokens (    1.32 ms per token,   757.00 tokens per second)
       eval time =    3009.81 ms /   132 tokens (   22.80 ms per token,    43.86 tokens per second)
      total time =    6617.49 ms /  2863 tokens
slot      release: id  1 | task 13986 | stop processing: n_tokens = 11630, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.871 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 14121 | processing task, is_child = 0
slot update_slots: id  1 | task 14121 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 13201
slot update_slots: id  1 | task 14121 | n_tokens = 11499, memory_seq_rm [11499, end)
slot update_slots: id  1 | task 14121 | prompt processing progress, n_tokens = 13137, batch.n_tokens = 1638, progress = 0.995152
slot update_slots: id  1 | task 14121 | n_tokens = 13137, memory_seq_rm [13137, end)
slot update_slots: id  1 | task 14121 | prompt processing progress, n_tokens = 13201, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 14121 | prompt done, n_tokens = 13201, batch.n_tokens = 64
slot init_sampler: id  1 | task 14121 | init sampler, took 2.46 ms, tokens: text = 13201, total = 13201
slot update_slots: id  1 | task 14121 | created context checkpoint 7 of 8 (pos_min = 12369, pos_max = 13136, size = 18.009 MiB)
slot print_timing: id  1 | task 14121 | 
prompt eval time =    2486.14 ms /  1702 tokens (    1.46 ms per token,   684.59 tokens per second)
       eval time =    2935.46 ms /   125 tokens (   23.48 ms per token,    42.58 tokens per second)
      total time =    5421.60 ms /  1827 tokens
slot      release: id  1 | task 14121 | stop processing: n_tokens = 13325, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.782 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 14248 | processing task, is_child = 0
slot update_slots: id  1 | task 14248 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 16883
slot update_slots: id  1 | task 14248 | n_tokens = 13201, memory_seq_rm [13201, end)
slot update_slots: id  1 | task 14248 | prompt processing progress, n_tokens = 15249, batch.n_tokens = 2048, progress = 0.903216
slot update_slots: id  1 | task 14248 | n_tokens = 15249, memory_seq_rm [15249, end)
slot update_slots: id  1 | task 14248 | prompt processing progress, n_tokens = 16819, batch.n_tokens = 1570, progress = 0.996209
slot update_slots: id  1 | task 14248 | n_tokens = 16819, memory_seq_rm [16819, end)
slot update_slots: id  1 | task 14248 | prompt processing progress, n_tokens = 16883, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 14248 | prompt done, n_tokens = 16883, batch.n_tokens = 64
slot init_sampler: id  1 | task 14248 | init sampler, took 2.50 ms, tokens: text = 16883, total = 16883
slot update_slots: id  1 | task 14248 | created context checkpoint 8 of 8 (pos_min = 16051, pos_max = 16818, size = 18.009 MiB)
slot print_timing: id  1 | task 14248 | 
prompt eval time =    5117.97 ms /  3682 tokens (    1.39 ms per token,   719.43 tokens per second)
       eval time =   72867.74 ms /  3171 tokens (   22.98 ms per token,    43.52 tokens per second)
      total time =   77985.71 ms /  6853 tokens
slot      release: id  1 | task 14248 | stop processing: n_tokens = 20053, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.941 (> 0.100 thold), f_keep = 0.842
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 17422 | processing task, is_child = 0
slot update_slots: id  1 | task 17422 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 17943
slot update_slots: id  1 | task 17422 | n_past = 16883, slot.prompt.tokens.size() = 20053, seq_id = 1, pos_min = 19285, n_swa = 128
slot update_slots: id  1 | task 17422 | restored context checkpoint (pos_min = 16051, pos_max = 16818, size = 18.009 MiB)
slot update_slots: id  1 | task 17422 | n_tokens = 16818, memory_seq_rm [16818, end)
slot update_slots: id  1 | task 17422 | prompt processing progress, n_tokens = 17879, batch.n_tokens = 1061, progress = 0.996433
slot update_slots: id  1 | task 17422 | n_tokens = 17879, memory_seq_rm [17879, end)
slot update_slots: id  1 | task 17422 | prompt processing progress, n_tokens = 17943, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 17422 | prompt done, n_tokens = 17943, batch.n_tokens = 64
slot init_sampler: id  1 | task 17422 | init sampler, took 2.69 ms, tokens: text = 17943, total = 17943
slot update_slots: id  1 | task 17422 | erasing old context checkpoint (pos_min = 243, pos_max = 1010, size = 18.009 MiB)
slot update_slots: id  1 | task 17422 | created context checkpoint 8 of 8 (pos_min = 17111, pos_max = 17878, size = 18.009 MiB)
slot print_timing: id  1 | task 17422 | 
prompt eval time =    1767.90 ms /  1125 tokens (    1.57 ms per token,   636.35 tokens per second)
       eval time =    1700.27 ms /    74 tokens (   22.98 ms per token,    43.52 tokens per second)
      total time =    3468.18 ms /  1199 tokens
slot      release: id  1 | task 17422 | stop processing: n_tokens = 18016, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.868 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 17498 | processing task, is_child = 0
slot update_slots: id  1 | task 17498 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 20674
slot update_slots: id  1 | task 17498 | n_tokens = 17943, memory_seq_rm [17943, end)
slot update_slots: id  1 | task 17498 | prompt processing progress, n_tokens = 19991, batch.n_tokens = 2048, progress = 0.966963
slot update_slots: id  1 | task 17498 | n_tokens = 19991, memory_seq_rm [19991, end)
slot update_slots: id  1 | task 17498 | prompt processing progress, n_tokens = 20610, batch.n_tokens = 619, progress = 0.996904
slot update_slots: id  1 | task 17498 | n_tokens = 20610, memory_seq_rm [20610, end)
slot update_slots: id  1 | task 17498 | prompt processing progress, n_tokens = 20674, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 17498 | prompt done, n_tokens = 20674, batch.n_tokens = 64
slot init_sampler: id  1 | task 17498 | init sampler, took 2.95 ms, tokens: text = 20674, total = 20674
slot update_slots: id  1 | task 17498 | erasing old context checkpoint (pos_min = 763, pos_max = 1530, size = 18.009 MiB)
slot update_slots: id  1 | task 17498 | created context checkpoint 8 of 8 (pos_min = 19842, pos_max = 20609, size = 18.009 MiB)
slot print_timing: id  1 | task 17498 | 
prompt eval time =    4066.39 ms /  2731 tokens (    1.49 ms per token,   671.60 tokens per second)
       eval time =    1899.64 ms /    79 tokens (   24.05 ms per token,    41.59 tokens per second)
      total time =    5966.04 ms /  2810 tokens
slot      release: id  1 | task 17498 | stop processing: n_tokens = 20752, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 17580 | processing task, is_child = 0
slot update_slots: id  1 | task 17580 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 20738
slot update_slots: id  1 | task 17580 | n_tokens = 20674, memory_seq_rm [20674, end)
slot update_slots: id  1 | task 17580 | prompt processing progress, n_tokens = 20738, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 17580 | prompt done, n_tokens = 20738, batch.n_tokens = 64
slot init_sampler: id  1 | task 17580 | init sampler, took 3.80 ms, tokens: text = 20738, total = 20738
slot print_timing: id  1 | task 17580 | 
prompt eval time =     227.44 ms /    64 tokens (    3.55 ms per token,   281.39 tokens per second)
       eval time =    1640.87 ms /    70 tokens (   23.44 ms per token,    42.66 tokens per second)
      total time =    1868.31 ms /   134 tokens
slot      release: id  1 | task 17580 | stop processing: n_tokens = 20807, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 17651 | processing task, is_child = 0
slot update_slots: id  1 | task 17651 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 20809
slot update_slots: id  1 | task 17651 | n_tokens = 20738, memory_seq_rm [20738, end)
slot update_slots: id  1 | task 17651 | prompt processing progress, n_tokens = 20745, batch.n_tokens = 7, progress = 0.996924
slot update_slots: id  1 | task 17651 | n_tokens = 20745, memory_seq_rm [20745, end)
slot update_slots: id  1 | task 17651 | prompt processing progress, n_tokens = 20809, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 17651 | prompt done, n_tokens = 20809, batch.n_tokens = 64
slot init_sampler: id  1 | task 17651 | init sampler, took 3.60 ms, tokens: text = 20809, total = 20809
slot update_slots: id  1 | task 17651 | erasing old context checkpoint (pos_min = 6215, pos_max = 6982, size = 18.009 MiB)
slot update_slots: id  1 | task 17651 | created context checkpoint 8 of 8 (pos_min = 20080, pos_max = 20744, size = 15.594 MiB)
slot print_timing: id  1 | task 17651 | 
prompt eval time =     281.62 ms /    71 tokens (    3.97 ms per token,   252.11 tokens per second)
       eval time =    8274.99 ms /   359 tokens (   23.05 ms per token,    43.38 tokens per second)
      total time =    8556.61 ms /   430 tokens
slot      release: id  1 | task 17651 | stop processing: n_tokens = 21167, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 18012 | processing task, is_child = 0
slot update_slots: id  1 | task 18012 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 21557
slot update_slots: id  1 | task 18012 | n_tokens = 20809, memory_seq_rm [20809, end)
slot update_slots: id  1 | task 18012 | prompt processing progress, n_tokens = 21493, batch.n_tokens = 684, progress = 0.997031
slot update_slots: id  1 | task 18012 | n_tokens = 21493, memory_seq_rm [21493, end)
slot update_slots: id  1 | task 18012 | prompt processing progress, n_tokens = 21557, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 18012 | prompt done, n_tokens = 21557, batch.n_tokens = 64
slot init_sampler: id  1 | task 18012 | init sampler, took 2.96 ms, tokens: text = 21557, total = 21557
slot update_slots: id  1 | task 18012 | erasing old context checkpoint (pos_min = 7311, pos_max = 8078, size = 18.009 MiB)
slot update_slots: id  1 | task 18012 | created context checkpoint 8 of 8 (pos_min = 20725, pos_max = 21492, size = 18.009 MiB)
slot print_timing: id  1 | task 18012 | 
prompt eval time =    1309.86 ms /   748 tokens (    1.75 ms per token,   571.06 tokens per second)
       eval time =   71922.75 ms /  3019 tokens (   23.82 ms per token,    41.98 tokens per second)
      total time =   73232.61 ms /  3767 tokens
slot      release: id  1 | task 18012 | stop processing: n_tokens = 24575, truncated = 1
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.877
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 21033 | processing task, is_child = 0
slot update_slots: id  1 | task 21033 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 21562
slot update_slots: id  1 | task 21033 | n_past = 21556, slot.prompt.tokens.size() = 24575, seq_id = 1, pos_min = 23807, n_swa = 128
slot update_slots: id  1 | task 21033 | restored context checkpoint (pos_min = 20725, pos_max = 21492, size = 18.009 MiB)
slot update_slots: id  1 | task 21033 | n_tokens = 21492, memory_seq_rm [21492, end)
slot update_slots: id  1 | task 21033 | prompt processing progress, n_tokens = 21498, batch.n_tokens = 6, progress = 0.997032
slot update_slots: id  1 | task 21033 | n_tokens = 21498, memory_seq_rm [21498, end)
slot update_slots: id  1 | task 21033 | prompt processing progress, n_tokens = 21562, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 21033 | prompt done, n_tokens = 21562, batch.n_tokens = 64
slot init_sampler: id  1 | task 21033 | init sampler, took 3.00 ms, tokens: text = 21562, total = 21562
slot print_timing: id  1 | task 21033 | 
prompt eval time =     333.03 ms /    70 tokens (    4.76 ms per token,   210.19 tokens per second)
       eval time =   17278.10 ms /   765 tokens (   22.59 ms per token,    44.28 tokens per second)
      total time =   17611.14 ms /   835 tokens
slot      release: id  1 | task 21033 | stop processing: n_tokens = 22326, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 21800 | processing task, is_child = 0
slot update_slots: id  1 | task 21800 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 22217
slot update_slots: id  1 | task 21800 | n_past = 21562, slot.prompt.tokens.size() = 22326, seq_id = 1, pos_min = 21558, n_swa = 128
slot update_slots: id  1 | task 21800 | restored context checkpoint (pos_min = 20725, pos_max = 21492, size = 18.009 MiB)
slot update_slots: id  1 | task 21800 | n_tokens = 21492, memory_seq_rm [21492, end)
slot update_slots: id  1 | task 21800 | prompt processing progress, n_tokens = 22153, batch.n_tokens = 661, progress = 0.997119
slot update_slots: id  1 | task 21800 | n_tokens = 22153, memory_seq_rm [22153, end)
slot update_slots: id  1 | task 21800 | prompt processing progress, n_tokens = 22217, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 21800 | prompt done, n_tokens = 22217, batch.n_tokens = 64
slot init_sampler: id  1 | task 21800 | init sampler, took 3.04 ms, tokens: text = 22217, total = 22217
slot update_slots: id  1 | task 21800 | erasing old context checkpoint (pos_min = 7936, pos_max = 8703, size = 18.009 MiB)
slot update_slots: id  1 | task 21800 | created context checkpoint 8 of 8 (pos_min = 21385, pos_max = 22152, size = 18.009 MiB)
slot print_timing: id  1 | task 21800 | 
prompt eval time =    1292.57 ms /   725 tokens (    1.78 ms per token,   560.90 tokens per second)
       eval time =   38231.65 ms /  1591 tokens (   24.03 ms per token,    41.61 tokens per second)
      total time =   39524.22 ms /  2316 tokens
slot      release: id  1 | task 21800 | stop processing: n_tokens = 23807, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 1628463768
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5313, total state size = 142.594 MiB
srv          load:  - looking for better prompt, base f_keep = 0.081, sim = 0.020
srv        update:  - cache state: 12 prompts, 3275.087 MiB (limits: 8192.000 MiB, 49152 tokens, 242536 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv  get_availabl: prompt cache update took 126.24 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 23393 | processing task, is_child = 0
slot update_slots: id  0 | task 23393 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 21126
slot update_slots: id  0 | task 23393 | n_past = 432, slot.prompt.tokens.size() = 5313, seq_id = 0, pos_min = 4545, n_swa = 128
slot update_slots: id  0 | task 23393 | restored context checkpoint (pos_min = 0, pos_max = 539, size = 12.663 MiB)
slot update_slots: id  0 | task 23393 | erased invalidated context checkpoint (pos_min = 4078, pos_max = 4845, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 23393 | n_tokens = 432, memory_seq_rm [432, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 2480, batch.n_tokens = 2048, progress = 0.117391
slot update_slots: id  0 | task 23393 | n_tokens = 2480, memory_seq_rm [2480, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 4528, batch.n_tokens = 2048, progress = 0.214333
slot update_slots: id  0 | task 23393 | n_tokens = 4528, memory_seq_rm [4528, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 6576, batch.n_tokens = 2048, progress = 0.311275
slot update_slots: id  0 | task 23393 | n_tokens = 6576, memory_seq_rm [6576, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 8624, batch.n_tokens = 2048, progress = 0.408217
slot update_slots: id  0 | task 23393 | n_tokens = 8624, memory_seq_rm [8624, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 10672, batch.n_tokens = 2048, progress = 0.505159
slot update_slots: id  0 | task 23393 | n_tokens = 10672, memory_seq_rm [10672, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 12720, batch.n_tokens = 2048, progress = 0.602102
slot update_slots: id  0 | task 23393 | n_tokens = 12720, memory_seq_rm [12720, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 14768, batch.n_tokens = 2048, progress = 0.699044
slot update_slots: id  0 | task 23393 | n_tokens = 14768, memory_seq_rm [14768, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 16816, batch.n_tokens = 2048, progress = 0.795986
slot update_slots: id  0 | task 23393 | n_tokens = 16816, memory_seq_rm [16816, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 18864, batch.n_tokens = 2048, progress = 0.892928
slot update_slots: id  0 | task 23393 | n_tokens = 18864, memory_seq_rm [18864, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 20912, batch.n_tokens = 2048, progress = 0.989870
slot update_slots: id  0 | task 23393 | n_tokens = 20912, memory_seq_rm [20912, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 21062, batch.n_tokens = 150, progress = 0.996971
slot update_slots: id  0 | task 23393 | n_tokens = 21062, memory_seq_rm [21062, end)
slot update_slots: id  0 | task 23393 | prompt processing progress, n_tokens = 21126, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 23393 | prompt done, n_tokens = 21126, batch.n_tokens = 64
slot init_sampler: id  0 | task 23393 | init sampler, took 2.93 ms, tokens: text = 21126, total = 21126
slot update_slots: id  0 | task 23393 | created context checkpoint 2 of 8 (pos_min = 20294, pos_max = 21061, size = 18.009 MiB)
slot print_timing: id  0 | task 23393 | 
prompt eval time =   24673.88 ms / 20694 tokens (    1.19 ms per token,   838.70 tokens per second)
       eval time =   16635.69 ms /   734 tokens (   22.66 ms per token,    44.12 tokens per second)
      total time =   41309.58 ms / 21428 tokens
slot      release: id  0 | task 23393 | stop processing: n_tokens = 21859, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.301 (> 0.100 thold), f_keep = 0.038
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 23807, total state size = 576.258 MiB
srv          load:  - looking for better prompt, base f_keep = 0.038, sim = 0.301
srv        update:  - cache state: 13 prompts, 3993.003 MiB (limits: 8192.000 MiB, 49152 tokens, 247772 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv  get_availabl: prompt cache update took 537.89 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 24139 | processing task, is_child = 0
slot update_slots: id  1 | task 24139 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3042
slot update_slots: id  1 | task 24139 | n_past = 915, slot.prompt.tokens.size() = 23807, seq_id = 1, pos_min = 23039, n_swa = 128
slot update_slots: id  1 | task 24139 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 24139 | erased invalidated context checkpoint (pos_min = 10667, pos_max = 11434, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 24139 | erased invalidated context checkpoint (pos_min = 12369, pos_max = 13136, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 24139 | erased invalidated context checkpoint (pos_min = 16051, pos_max = 16818, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 24139 | erased invalidated context checkpoint (pos_min = 17111, pos_max = 17878, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 24139 | erased invalidated context checkpoint (pos_min = 19842, pos_max = 20609, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 24139 | erased invalidated context checkpoint (pos_min = 20080, pos_max = 20744, n_swa = 128, size = 15.594 MiB)
slot update_slots: id  1 | task 24139 | erased invalidated context checkpoint (pos_min = 20725, pos_max = 21492, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 24139 | erased invalidated context checkpoint (pos_min = 21385, pos_max = 22152, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 24139 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 24139 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.673241
slot update_slots: id  1 | task 24139 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 24139 | prompt processing progress, n_tokens = 2978, batch.n_tokens = 930, progress = 0.978961
slot update_slots: id  1 | task 24139 | n_tokens = 2978, memory_seq_rm [2978, end)
slot update_slots: id  1 | task 24139 | prompt processing progress, n_tokens = 3042, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 24139 | prompt done, n_tokens = 3042, batch.n_tokens = 64
slot init_sampler: id  1 | task 24139 | init sampler, took 0.57 ms, tokens: text = 3042, total = 3042
slot update_slots: id  1 | task 24139 | created context checkpoint 1 of 8 (pos_min = 2210, pos_max = 2977, size = 18.009 MiB)
slot print_timing: id  1 | task 24139 | 
prompt eval time =    3260.41 ms /  3042 tokens (    1.07 ms per token,   933.01 tokens per second)
       eval time =   30374.09 ms /  1461 tokens (   20.79 ms per token,    48.10 tokens per second)
      total time =   33634.50 ms /  4503 tokens
slot      release: id  1 | task 24139 | stop processing: n_tokens = 4502, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.673 (> 0.100 thold), f_keep = 0.676
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25603 | processing task, is_child = 0
slot update_slots: id  1 | task 25603 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 4519
slot update_slots: id  1 | task 25603 | n_past = 3042, slot.prompt.tokens.size() = 4502, seq_id = 1, pos_min = 3734, n_swa = 128
slot update_slots: id  1 | task 25603 | restored context checkpoint (pos_min = 2210, pos_max = 2977, size = 18.009 MiB)
slot update_slots: id  1 | task 25603 | n_tokens = 2977, memory_seq_rm [2977, end)
slot update_slots: id  1 | task 25603 | prompt processing progress, n_tokens = 4455, batch.n_tokens = 1478, progress = 0.985838
slot update_slots: id  1 | task 25603 | n_tokens = 4455, memory_seq_rm [4455, end)
slot update_slots: id  1 | task 25603 | prompt processing progress, n_tokens = 4519, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25603 | prompt done, n_tokens = 4519, batch.n_tokens = 64
slot init_sampler: id  1 | task 25603 | init sampler, took 0.66 ms, tokens: text = 4519, total = 4519
slot update_slots: id  1 | task 25603 | created context checkpoint 2 of 8 (pos_min = 3687, pos_max = 4454, size = 18.009 MiB)
slot print_timing: id  1 | task 25603 | 
prompt eval time =    1808.34 ms /  1542 tokens (    1.17 ms per token,   852.72 tokens per second)
       eval time =    1139.92 ms /    55 tokens (   20.73 ms per token,    48.25 tokens per second)
      total time =    2948.25 ms /  1597 tokens
slot      release: id  1 | task 25603 | stop processing: n_tokens = 4573, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.164 (> 0.100 thold), f_keep = 0.020
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 21859, total state size = 530.580 MiB
srv          load:  - looking for better prompt, base f_keep = 0.020, sim = 0.164
srv        update:  - cache state: 14 prompts, 4554.255 MiB (limits: 8192.000 MiB, 49152 tokens, 256556 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv  get_availabl: prompt cache update took 418.08 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 25660 | processing task, is_child = 0
slot update_slots: id  0 | task 25660 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 2601
slot update_slots: id  0 | task 25660 | n_past = 427, slot.prompt.tokens.size() = 21859, seq_id = 0, pos_min = 21091, n_swa = 128
slot update_slots: id  0 | task 25660 | restored context checkpoint (pos_min = 0, pos_max = 539, size = 12.663 MiB)
slot update_slots: id  0 | task 25660 | erased invalidated context checkpoint (pos_min = 20294, pos_max = 21061, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 25660 | n_tokens = 427, memory_seq_rm [427, end)
slot update_slots: id  0 | task 25660 | prompt processing progress, n_tokens = 2475, batch.n_tokens = 2048, progress = 0.951557
slot update_slots: id  0 | task 25660 | n_tokens = 2475, memory_seq_rm [2475, end)
slot update_slots: id  0 | task 25660 | prompt processing progress, n_tokens = 2537, batch.n_tokens = 62, progress = 0.975394
slot update_slots: id  0 | task 25660 | n_tokens = 2537, memory_seq_rm [2537, end)
slot update_slots: id  0 | task 25660 | prompt processing progress, n_tokens = 2601, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 25660 | prompt done, n_tokens = 2601, batch.n_tokens = 64
slot init_sampler: id  0 | task 25660 | init sampler, took 0.37 ms, tokens: text = 2601, total = 2601
slot update_slots: id  0 | task 25660 | created context checkpoint 2 of 8 (pos_min = 1769, pos_max = 2536, size = 18.009 MiB)
slot print_timing: id  0 | task 25660 | 
prompt eval time =    2517.46 ms /  2174 tokens (    1.16 ms per token,   863.57 tokens per second)
       eval time =    6229.93 ms /   307 tokens (   20.29 ms per token,    49.28 tokens per second)
      total time =    8747.39 ms /  2481 tokens
slot      release: id  0 | task 25660 | stop processing: n_tokens = 2907, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.316 (> 0.100 thold), f_keep = 0.200
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4573, total state size = 125.241 MiB
srv          load:  - looking for better prompt, base f_keep = 0.200, sim = 0.316
srv        update:  - cache state: 15 prompts, 4715.514 MiB (limits: 8192.000 MiB, 49152 tokens, 255727 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv  get_availabl: prompt cache update took 143.89 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25970 | processing task, is_child = 0
slot update_slots: id  1 | task 25970 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 2897
slot update_slots: id  1 | task 25970 | n_past = 915, slot.prompt.tokens.size() = 4573, seq_id = 1, pos_min = 3805, n_swa = 128
slot update_slots: id  1 | task 25970 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 25970 | erased invalidated context checkpoint (pos_min = 2210, pos_max = 2977, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 25970 | erased invalidated context checkpoint (pos_min = 3687, pos_max = 4454, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 25970 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 25970 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.706938
slot update_slots: id  1 | task 25970 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 25970 | prompt processing progress, n_tokens = 2833, batch.n_tokens = 785, progress = 0.977908
slot update_slots: id  1 | task 25970 | n_tokens = 2833, memory_seq_rm [2833, end)
slot update_slots: id  1 | task 25970 | prompt processing progress, n_tokens = 2897, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25970 | prompt done, n_tokens = 2897, batch.n_tokens = 64
slot init_sampler: id  1 | task 25970 | init sampler, took 0.40 ms, tokens: text = 2897, total = 2897
slot update_slots: id  1 | task 25970 | created context checkpoint 1 of 8 (pos_min = 2065, pos_max = 2832, size = 18.009 MiB)
slot print_timing: id  1 | task 25970 | 
prompt eval time =    3136.72 ms /  2897 tokens (    1.08 ms per token,   923.58 tokens per second)
       eval time =    1926.16 ms /    91 tokens (   21.17 ms per token,    47.24 tokens per second)
      total time =    5062.88 ms /  2988 tokens
slot      release: id  1 | task 25970 | stop processing: n_tokens = 2987, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26064 | processing task, is_child = 0
slot update_slots: id  1 | task 26064 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3013
slot update_slots: id  1 | task 26064 | n_tokens = 2971, memory_seq_rm [2971, end)
slot update_slots: id  1 | task 26064 | prompt processing progress, n_tokens = 3013, batch.n_tokens = 42, progress = 1.000000
slot update_slots: id  1 | task 26064 | prompt done, n_tokens = 3013, batch.n_tokens = 42
slot init_sampler: id  1 | task 26064 | init sampler, took 0.43 ms, tokens: text = 3013, total = 3013
slot update_slots: id  1 | task 26064 | created context checkpoint 2 of 8 (pos_min = 2219, pos_max = 2970, size = 17.634 MiB)
slot print_timing: id  1 | task 26064 | 
prompt eval time =     149.99 ms /    42 tokens (    3.57 ms per token,   280.02 tokens per second)
       eval time =     945.11 ms /    42 tokens (   22.50 ms per token,    44.44 tokens per second)
      total time =    1095.10 ms /    84 tokens
slot      release: id  1 | task 26064 | stop processing: n_tokens = 3054, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26107 | processing task, is_child = 0
slot update_slots: id  1 | task 26107 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3077
slot update_slots: id  1 | task 26107 | n_tokens = 3029, memory_seq_rm [3029, end)
slot update_slots: id  1 | task 26107 | prompt processing progress, n_tokens = 3077, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  1 | task 26107 | prompt done, n_tokens = 3077, batch.n_tokens = 48
slot init_sampler: id  1 | task 26107 | init sampler, took 0.58 ms, tokens: text = 3077, total = 3077
slot print_timing: id  1 | task 26107 | 
prompt eval time =     146.05 ms /    48 tokens (    3.04 ms per token,   328.65 tokens per second)
       eval time =     793.27 ms /    36 tokens (   22.04 ms per token,    45.38 tokens per second)
      total time =     939.32 ms /    84 tokens
slot      release: id  1 | task 26107 | stop processing: n_tokens = 3112, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26144 | processing task, is_child = 0
slot update_slots: id  1 | task 26144 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3136
slot update_slots: id  1 | task 26144 | n_tokens = 3088, memory_seq_rm [3088, end)
slot update_slots: id  1 | task 26144 | prompt processing progress, n_tokens = 3136, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  1 | task 26144 | prompt done, n_tokens = 3136, batch.n_tokens = 48
slot init_sampler: id  1 | task 26144 | init sampler, took 0.47 ms, tokens: text = 3136, total = 3136
slot update_slots: id  1 | task 26144 | created context checkpoint 3 of 8 (pos_min = 2344, pos_max = 3087, size = 17.446 MiB)
slot print_timing: id  1 | task 26144 | 
prompt eval time =     151.87 ms /    48 tokens (    3.16 ms per token,   316.06 tokens per second)
       eval time =     782.98 ms /    36 tokens (   21.75 ms per token,    45.98 tokens per second)
      total time =     934.85 ms /    84 tokens
slot      release: id  1 | task 26144 | stop processing: n_tokens = 3171, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26181 | processing task, is_child = 0
slot update_slots: id  1 | task 26181 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3195
slot update_slots: id  1 | task 26181 | n_tokens = 3147, memory_seq_rm [3147, end)
slot update_slots: id  1 | task 26181 | prompt processing progress, n_tokens = 3195, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  1 | task 26181 | prompt done, n_tokens = 3195, batch.n_tokens = 48
slot init_sampler: id  1 | task 26181 | init sampler, took 0.44 ms, tokens: text = 3195, total = 3195
slot print_timing: id  1 | task 26181 | 
prompt eval time =     146.31 ms /    48 tokens (    3.05 ms per token,   328.08 tokens per second)
       eval time =     949.14 ms /    45 tokens (   21.09 ms per token,    47.41 tokens per second)
      total time =    1095.45 ms /    93 tokens
slot      release: id  1 | task 26181 | stop processing: n_tokens = 3239, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26227 | processing task, is_child = 0
slot update_slots: id  1 | task 26227 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3297
slot update_slots: id  1 | task 26227 | n_tokens = 3213, memory_seq_rm [3213, end)
slot update_slots: id  1 | task 26227 | prompt processing progress, n_tokens = 3233, batch.n_tokens = 20, progress = 0.980588
slot update_slots: id  1 | task 26227 | n_tokens = 3233, memory_seq_rm [3233, end)
slot update_slots: id  1 | task 26227 | prompt processing progress, n_tokens = 3297, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26227 | prompt done, n_tokens = 3297, batch.n_tokens = 64
slot init_sampler: id  1 | task 26227 | init sampler, took 0.69 ms, tokens: text = 3297, total = 3297
slot update_slots: id  1 | task 26227 | created context checkpoint 4 of 8 (pos_min = 2471, pos_max = 3232, size = 17.868 MiB)
slot print_timing: id  1 | task 26227 | 
prompt eval time =     264.24 ms /    84 tokens (    3.15 ms per token,   317.90 tokens per second)
       eval time =    5290.65 ms /   261 tokens (   20.27 ms per token,    49.33 tokens per second)
      total time =    5554.89 ms /   345 tokens
slot      release: id  1 | task 26227 | stop processing: n_tokens = 3557, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26490 | processing task, is_child = 0
slot update_slots: id  1 | task 26490 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3607
slot update_slots: id  1 | task 26490 | n_tokens = 3525, memory_seq_rm [3525, end)
slot update_slots: id  1 | task 26490 | prompt processing progress, n_tokens = 3543, batch.n_tokens = 18, progress = 0.982257
slot update_slots: id  1 | task 26490 | n_tokens = 3543, memory_seq_rm [3543, end)
slot update_slots: id  1 | task 26490 | prompt processing progress, n_tokens = 3607, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26490 | prompt done, n_tokens = 3607, batch.n_tokens = 64
slot init_sampler: id  1 | task 26490 | init sampler, took 0.50 ms, tokens: text = 3607, total = 3607
slot update_slots: id  1 | task 26490 | created context checkpoint 5 of 8 (pos_min = 2789, pos_max = 3542, size = 17.681 MiB)
slot print_timing: id  1 | task 26490 | 
prompt eval time =     267.17 ms /    82 tokens (    3.26 ms per token,   306.92 tokens per second)
       eval time =    4874.82 ms /   241 tokens (   20.23 ms per token,    49.44 tokens per second)
      total time =    5141.99 ms /   323 tokens
slot      release: id  1 | task 26490 | stop processing: n_tokens = 3847, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.845 (> 0.100 thold), f_keep = 0.753
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26733 | processing task, is_child = 0
slot update_slots: id  1 | task 26733 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3429
slot update_slots: id  1 | task 26733 | n_past = 2897, slot.prompt.tokens.size() = 3847, seq_id = 1, pos_min = 3079, n_swa = 128
slot update_slots: id  1 | task 26733 | restored context checkpoint (pos_min = 2471, pos_max = 3232, size = 17.868 MiB)
slot update_slots: id  1 | task 26733 | erased invalidated context checkpoint (pos_min = 2789, pos_max = 3542, n_swa = 128, size = 17.681 MiB)
slot update_slots: id  1 | task 26733 | n_tokens = 2897, memory_seq_rm [2897, end)
slot update_slots: id  1 | task 26733 | prompt processing progress, n_tokens = 3365, batch.n_tokens = 468, progress = 0.981336
slot update_slots: id  1 | task 26733 | n_tokens = 3365, memory_seq_rm [3365, end)
slot update_slots: id  1 | task 26733 | prompt processing progress, n_tokens = 3429, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26733 | prompt done, n_tokens = 3429, batch.n_tokens = 64
slot init_sampler: id  1 | task 26733 | init sampler, took 0.48 ms, tokens: text = 3429, total = 3429
slot update_slots: id  1 | task 26733 | created context checkpoint 5 of 8 (pos_min = 2770, pos_max = 3364, size = 13.952 MiB)
slot print_timing: id  1 | task 26733 | 
prompt eval time =     787.07 ms /   532 tokens (    1.48 ms per token,   675.92 tokens per second)
       eval time =   30745.66 ms /  1470 tokens (   20.92 ms per token,    47.81 tokens per second)
      total time =   31532.73 ms /  2002 tokens
slot      release: id  1 | task 26733 | stop processing: n_tokens = 4898, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.723 (> 0.100 thold), f_keep = 0.700
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28205 | processing task, is_child = 0
slot update_slots: id  1 | task 28205 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 4744
slot update_slots: id  1 | task 28205 | n_past = 3429, slot.prompt.tokens.size() = 4898, seq_id = 1, pos_min = 4130, n_swa = 128
slot update_slots: id  1 | task 28205 | restored context checkpoint (pos_min = 2770, pos_max = 3364, size = 13.952 MiB)
slot update_slots: id  1 | task 28205 | n_tokens = 3364, memory_seq_rm [3364, end)
slot update_slots: id  1 | task 28205 | prompt processing progress, n_tokens = 4680, batch.n_tokens = 1316, progress = 0.986509
slot update_slots: id  1 | task 28205 | n_tokens = 4680, memory_seq_rm [4680, end)
slot update_slots: id  1 | task 28205 | prompt processing progress, n_tokens = 4744, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28205 | prompt done, n_tokens = 4744, batch.n_tokens = 64
slot init_sampler: id  1 | task 28205 | init sampler, took 0.94 ms, tokens: text = 4744, total = 4744
slot update_slots: id  1 | task 28205 | created context checkpoint 6 of 8 (pos_min = 3912, pos_max = 4679, size = 18.009 MiB)
slot print_timing: id  1 | task 28205 | 
prompt eval time =    1749.56 ms /  1380 tokens (    1.27 ms per token,   788.77 tokens per second)
       eval time =    2031.50 ms /    91 tokens (   22.32 ms per token,    44.79 tokens per second)
      total time =    3781.06 ms /  1471 tokens
slot      release: id  1 | task 28205 | stop processing: n_tokens = 4834, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.144 (> 0.100 thold), f_keep = 0.147
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2907, total state size = 86.175 MiB
srv          load:  - looking for better prompt, base f_keep = 0.147, sim = 0.144
srv        update:  - cache state: 16 prompts, 4832.362 MiB (limits: 8192.000 MiB, 49152 tokens, 254472 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv  get_availabl: prompt cache update took 86.50 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28298 | processing task, is_child = 0
slot update_slots: id  0 | task 28298 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 2973
slot update_slots: id  0 | task 28298 | n_past = 427, slot.prompt.tokens.size() = 2907, seq_id = 0, pos_min = 2139, n_swa = 128
slot update_slots: id  0 | task 28298 | restored context checkpoint (pos_min = 0, pos_max = 539, size = 12.663 MiB)
slot update_slots: id  0 | task 28298 | erased invalidated context checkpoint (pos_min = 1769, pos_max = 2536, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 28298 | n_tokens = 427, memory_seq_rm [427, end)
slot update_slots: id  0 | task 28298 | prompt processing progress, n_tokens = 2475, batch.n_tokens = 2048, progress = 0.832492
slot update_slots: id  0 | task 28298 | n_tokens = 2475, memory_seq_rm [2475, end)
slot update_slots: id  0 | task 28298 | prompt processing progress, n_tokens = 2909, batch.n_tokens = 434, progress = 0.978473
slot update_slots: id  0 | task 28298 | n_tokens = 2909, memory_seq_rm [2909, end)
slot update_slots: id  0 | task 28298 | prompt processing progress, n_tokens = 2973, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28298 | prompt done, n_tokens = 2973, batch.n_tokens = 64
slot init_sampler: id  0 | task 28298 | init sampler, took 0.44 ms, tokens: text = 2973, total = 2973
slot update_slots: id  0 | task 28298 | created context checkpoint 2 of 8 (pos_min = 2141, pos_max = 2908, size = 18.009 MiB)
slot print_timing: id  0 | task 28298 | 
prompt eval time =    2894.66 ms /  2546 tokens (    1.14 ms per token,   879.55 tokens per second)
       eval time =   13426.51 ms /   635 tokens (   21.14 ms per token,    47.29 tokens per second)
      total time =   16321.17 ms /  3181 tokens
slot      release: id  0 | task 28298 | stop processing: n_tokens = 3607, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.392 (> 0.100 thold), f_keep = 0.189
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4834, total state size = 131.362 MiB
srv          load:  - looking for better prompt, base f_keep = 0.189, sim = 0.392
srv        update:  - cache state: 17 prompts, 5066.642 MiB (limits: 8192.000 MiB, 49152 tokens, 250521 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv        update:    - prompt 0x57052ce72cb0:    4834 tokens, checkpoints:  6,   234.281 MiB
srv  get_availabl: prompt cache update took 173.27 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28936 | processing task, is_child = 0
slot update_slots: id  1 | task 28936 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 2339
slot update_slots: id  1 | task 28936 | n_past = 916, slot.prompt.tokens.size() = 4834, seq_id = 1, pos_min = 4066, n_swa = 128
slot update_slots: id  1 | task 28936 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 28936 | erased invalidated context checkpoint (pos_min = 2065, pos_max = 2832, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 28936 | erased invalidated context checkpoint (pos_min = 2219, pos_max = 2970, n_swa = 128, size = 17.634 MiB)
slot update_slots: id  1 | task 28936 | erased invalidated context checkpoint (pos_min = 2344, pos_max = 3087, n_swa = 128, size = 17.446 MiB)
slot update_slots: id  1 | task 28936 | erased invalidated context checkpoint (pos_min = 2471, pos_max = 3232, n_swa = 128, size = 17.868 MiB)
slot update_slots: id  1 | task 28936 | erased invalidated context checkpoint (pos_min = 2770, pos_max = 3364, n_swa = 128, size = 13.952 MiB)
slot update_slots: id  1 | task 28936 | erased invalidated context checkpoint (pos_min = 3912, pos_max = 4679, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 28936 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 28936 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.875588
slot update_slots: id  1 | task 28936 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 28936 | prompt processing progress, n_tokens = 2275, batch.n_tokens = 227, progress = 0.972638
slot update_slots: id  1 | task 28936 | n_tokens = 2275, memory_seq_rm [2275, end)
slot update_slots: id  1 | task 28936 | prompt processing progress, n_tokens = 2339, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28936 | prompt done, n_tokens = 2339, batch.n_tokens = 64
slot init_sampler: id  1 | task 28936 | init sampler, took 0.33 ms, tokens: text = 2339, total = 2339
slot update_slots: id  1 | task 28936 | created context checkpoint 1 of 8 (pos_min = 1507, pos_max = 2274, size = 18.009 MiB)
slot print_timing: id  1 | task 28936 | 
prompt eval time =    2628.01 ms /  2339 tokens (    1.12 ms per token,   890.03 tokens per second)
       eval time =    1036.37 ms /    52 tokens (   19.93 ms per token,    50.18 tokens per second)
      total time =    3664.38 ms /  2391 tokens
slot      release: id  1 | task 28936 | stop processing: n_tokens = 2390, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28991 | processing task, is_child = 0
slot update_slots: id  1 | task 28991 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 2385
slot update_slots: id  1 | task 28991 | n_tokens = 2340, memory_seq_rm [2340, end)
slot update_slots: id  1 | task 28991 | prompt processing progress, n_tokens = 2385, batch.n_tokens = 45, progress = 1.000000
slot update_slots: id  1 | task 28991 | prompt done, n_tokens = 2385, batch.n_tokens = 45
slot init_sampler: id  1 | task 28991 | init sampler, took 0.34 ms, tokens: text = 2385, total = 2385
slot update_slots: id  1 | task 28991 | created context checkpoint 2 of 8 (pos_min = 1622, pos_max = 2339, size = 16.837 MiB)
slot print_timing: id  1 | task 28991 | 
prompt eval time =     211.26 ms /    45 tokens (    4.69 ms per token,   213.00 tokens per second)
       eval time =    1833.54 ms /    91 tokens (   20.15 ms per token,    49.63 tokens per second)
      total time =    2044.80 ms /   136 tokens
slot      release: id  1 | task 28991 | stop processing: n_tokens = 2475, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.838 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29083 | processing task, is_child = 0
slot update_slots: id  1 | task 29083 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 2920
slot update_slots: id  1 | task 29083 | n_tokens = 2446, memory_seq_rm [2446, end)
slot update_slots: id  1 | task 29083 | prompt processing progress, n_tokens = 2856, batch.n_tokens = 410, progress = 0.978082
slot update_slots: id  1 | task 29083 | n_tokens = 2856, memory_seq_rm [2856, end)
slot update_slots: id  1 | task 29083 | prompt processing progress, n_tokens = 2920, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29083 | prompt done, n_tokens = 2920, batch.n_tokens = 64
slot init_sampler: id  1 | task 29083 | init sampler, took 0.41 ms, tokens: text = 2920, total = 2920
slot update_slots: id  1 | task 29083 | created context checkpoint 3 of 8 (pos_min = 2088, pos_max = 2855, size = 18.009 MiB)
slot print_timing: id  1 | task 29083 | 
prompt eval time =     613.99 ms /   474 tokens (    1.30 ms per token,   772.01 tokens per second)
       eval time =     813.38 ms /    42 tokens (   19.37 ms per token,    51.64 tokens per second)
      total time =    1427.37 ms /   516 tokens
slot      release: id  1 | task 29083 | stop processing: n_tokens = 2961, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29127 | processing task, is_child = 0
slot update_slots: id  1 | task 29127 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3001
slot update_slots: id  1 | task 29127 | n_tokens = 2933, memory_seq_rm [2933, end)
slot update_slots: id  1 | task 29127 | prompt processing progress, n_tokens = 2937, batch.n_tokens = 4, progress = 0.978674
slot update_slots: id  1 | task 29127 | n_tokens = 2937, memory_seq_rm [2937, end)
slot update_slots: id  1 | task 29127 | prompt processing progress, n_tokens = 3001, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29127 | prompt done, n_tokens = 3001, batch.n_tokens = 64
slot init_sampler: id  1 | task 29127 | init sampler, took 0.42 ms, tokens: text = 3001, total = 3001
slot update_slots: id  1 | task 29127 | created context checkpoint 4 of 8 (pos_min = 2193, pos_max = 2936, size = 17.446 MiB)
slot print_timing: id  1 | task 29127 | 
prompt eval time =     210.55 ms /    68 tokens (    3.10 ms per token,   322.96 tokens per second)
       eval time =     846.97 ms /    41 tokens (   20.66 ms per token,    48.41 tokens per second)
      total time =    1057.52 ms /   109 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 29127 | stop processing: n_tokens = 3041, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29170 | processing task, is_child = 0
slot update_slots: id  1 | task 29170 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3110
slot update_slots: id  1 | task 29170 | n_tokens = 3018, memory_seq_rm [3018, end)
slot update_slots: id  1 | task 29170 | prompt processing progress, n_tokens = 3046, batch.n_tokens = 28, progress = 0.979421
slot update_slots: id  1 | task 29170 | n_tokens = 3046, memory_seq_rm [3046, end)
slot update_slots: id  1 | task 29170 | prompt processing progress, n_tokens = 3110, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29170 | prompt done, n_tokens = 3110, batch.n_tokens = 64
slot init_sampler: id  1 | task 29170 | init sampler, took 0.61 ms, tokens: text = 3110, total = 3110
slot update_slots: id  1 | task 29170 | created context checkpoint 5 of 8 (pos_min = 2278, pos_max = 3045, size = 18.009 MiB)
slot print_timing: id  1 | task 29170 | 
prompt eval time =     275.27 ms /    92 tokens (    2.99 ms per token,   334.21 tokens per second)
       eval time =     633.83 ms /    30 tokens (   21.13 ms per token,    47.33 tokens per second)
      total time =     909.10 ms /   122 tokens
slot      release: id  1 | task 29170 | stop processing: n_tokens = 3139, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29202 | processing task, is_child = 0
slot update_slots: id  1 | task 29202 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3205
slot update_slots: id  1 | task 29202 | n_tokens = 3121, memory_seq_rm [3121, end)
slot update_slots: id  1 | task 29202 | prompt processing progress, n_tokens = 3141, batch.n_tokens = 20, progress = 0.980031
slot update_slots: id  1 | task 29202 | n_tokens = 3141, memory_seq_rm [3141, end)
slot update_slots: id  1 | task 29202 | prompt processing progress, n_tokens = 3205, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29202 | prompt done, n_tokens = 3205, batch.n_tokens = 64
slot init_sampler: id  1 | task 29202 | init sampler, took 0.65 ms, tokens: text = 3205, total = 3205
slot update_slots: id  1 | task 29202 | created context checkpoint 6 of 8 (pos_min = 2373, pos_max = 3140, size = 18.009 MiB)
slot print_timing: id  1 | task 29202 | 
prompt eval time =     260.39 ms /    84 tokens (    3.10 ms per token,   322.59 tokens per second)
       eval time =     961.73 ms /    49 tokens (   19.63 ms per token,    50.95 tokens per second)
      total time =    1222.12 ms /   133 tokens
slot      release: id  1 | task 29202 | stop processing: n_tokens = 3253, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.221 (> 0.100 thold), f_keep = 0.119
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3607, total state size = 102.590 MiB
srv          load:  - looking for better prompt, base f_keep = 0.119, sim = 0.221
srv        update:  - cache state: 18 prompts, 5199.904 MiB (limits: 8192.000 MiB, 49152 tokens, 249783 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv        update:    - prompt 0x57052ce72cb0:    4834 tokens, checkpoints:  6,   234.281 MiB
srv        update:    - prompt 0x57052cc7a5d0:    3607 tokens, checkpoints:  2,   133.262 MiB
srv  get_availabl: prompt cache update took 102.40 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29253 | processing task, is_child = 0
slot update_slots: id  0 | task 29253 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 1940
slot update_slots: id  0 | task 29253 | n_past = 428, slot.prompt.tokens.size() = 3607, seq_id = 0, pos_min = 2839, n_swa = 128
slot update_slots: id  0 | task 29253 | restored context checkpoint (pos_min = 0, pos_max = 539, size = 12.663 MiB)
slot update_slots: id  0 | task 29253 | erased invalidated context checkpoint (pos_min = 2141, pos_max = 2908, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 29253 | n_tokens = 428, memory_seq_rm [428, end)
slot update_slots: id  0 | task 29253 | prompt processing progress, n_tokens = 1876, batch.n_tokens = 1448, progress = 0.967010
slot update_slots: id  0 | task 29253 | n_tokens = 1876, memory_seq_rm [1876, end)
slot update_slots: id  0 | task 29253 | prompt processing progress, n_tokens = 1940, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29253 | prompt done, n_tokens = 1940, batch.n_tokens = 64
slot init_sampler: id  0 | task 29253 | init sampler, took 0.27 ms, tokens: text = 1940, total = 1940
slot update_slots: id  0 | task 29253 | created context checkpoint 2 of 8 (pos_min = 1108, pos_max = 1875, size = 18.009 MiB)
slot print_timing: id  0 | task 29253 | 
prompt eval time =    1661.58 ms /  1512 tokens (    1.10 ms per token,   909.98 tokens per second)
       eval time =    6522.31 ms /   326 tokens (   20.01 ms per token,    49.98 tokens per second)
      total time =    8183.89 ms /  1838 tokens
slot      release: id  0 | task 29253 | stop processing: n_tokens = 2265, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.255 (> 0.100 thold), f_keep = 0.281
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3253, total state size = 94.289 MiB
srv          load:  - looking for better prompt, base f_keep = 0.281, sim = 0.255
srv        update:  - cache state: 19 prompts, 5400.512 MiB (limits: 8192.000 MiB, 49152 tokens, 245439 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv        update:    - prompt 0x57052ce72cb0:    4834 tokens, checkpoints:  6,   234.281 MiB
srv        update:    - prompt 0x57052cc7a5d0:    3607 tokens, checkpoints:  2,   133.262 MiB
srv        update:    - prompt 0x57052dbadd70:    3253 tokens, checkpoints:  6,   200.608 MiB
srv  get_availabl: prompt cache update took 141.02 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29581 | processing task, is_child = 0
slot update_slots: id  1 | task 29581 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3590
slot update_slots: id  1 | task 29581 | n_past = 915, slot.prompt.tokens.size() = 3253, seq_id = 1, pos_min = 2485, n_swa = 128
slot update_slots: id  1 | task 29581 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 29581 | erased invalidated context checkpoint (pos_min = 1507, pos_max = 2274, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 29581 | erased invalidated context checkpoint (pos_min = 1622, pos_max = 2339, n_swa = 128, size = 16.837 MiB)
slot update_slots: id  1 | task 29581 | erased invalidated context checkpoint (pos_min = 2088, pos_max = 2855, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 29581 | erased invalidated context checkpoint (pos_min = 2193, pos_max = 2936, n_swa = 128, size = 17.446 MiB)
slot update_slots: id  1 | task 29581 | erased invalidated context checkpoint (pos_min = 2278, pos_max = 3045, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 29581 | erased invalidated context checkpoint (pos_min = 2373, pos_max = 3140, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 29581 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 29581 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.570474
slot update_slots: id  1 | task 29581 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 29581 | prompt processing progress, n_tokens = 3526, batch.n_tokens = 1478, progress = 0.982173
slot update_slots: id  1 | task 29581 | n_tokens = 3526, memory_seq_rm [3526, end)
slot update_slots: id  1 | task 29581 | prompt processing progress, n_tokens = 3590, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29581 | prompt done, n_tokens = 3590, batch.n_tokens = 64
slot init_sampler: id  1 | task 29581 | init sampler, took 0.67 ms, tokens: text = 3590, total = 3590
slot update_slots: id  1 | task 29581 | created context checkpoint 1 of 8 (pos_min = 2758, pos_max = 3525, size = 18.009 MiB)
slot print_timing: id  1 | task 29581 | 
prompt eval time =    3838.15 ms /  3590 tokens (    1.07 ms per token,   935.35 tokens per second)
       eval time =    1644.48 ms /    77 tokens (   21.36 ms per token,    46.82 tokens per second)
      total time =    5482.64 ms /  3667 tokens
slot      release: id  1 | task 29581 | stop processing: n_tokens = 3666, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.732 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29661 | processing task, is_child = 0
slot update_slots: id  1 | task 29661 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 4962
slot update_slots: id  1 | task 29661 | n_tokens = 3634, memory_seq_rm [3634, end)
slot update_slots: id  1 | task 29661 | prompt processing progress, n_tokens = 4898, batch.n_tokens = 1264, progress = 0.987102
slot update_slots: id  1 | task 29661 | n_tokens = 4898, memory_seq_rm [4898, end)
slot update_slots: id  1 | task 29661 | prompt processing progress, n_tokens = 4962, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29661 | prompt done, n_tokens = 4962, batch.n_tokens = 64
slot init_sampler: id  1 | task 29661 | init sampler, took 0.69 ms, tokens: text = 4962, total = 4962
slot update_slots: id  1 | task 29661 | created context checkpoint 2 of 8 (pos_min = 4130, pos_max = 4897, size = 18.009 MiB)
slot print_timing: id  1 | task 29661 | 
prompt eval time =    1682.78 ms /  1328 tokens (    1.27 ms per token,   789.17 tokens per second)
       eval time =    3402.66 ms /   162 tokens (   21.00 ms per token,    47.61 tokens per second)
      total time =    5085.44 ms /  1490 tokens
slot      release: id  1 | task 29661 | stop processing: n_tokens = 5123, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29825 | processing task, is_child = 0
slot update_slots: id  1 | task 29825 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5169
slot update_slots: id  1 | task 29825 | n_tokens = 5093, memory_seq_rm [5093, end)
slot update_slots: id  1 | task 29825 | prompt processing progress, n_tokens = 5105, batch.n_tokens = 12, progress = 0.987619
slot update_slots: id  1 | task 29825 | n_tokens = 5105, memory_seq_rm [5105, end)
slot update_slots: id  1 | task 29825 | prompt processing progress, n_tokens = 5169, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29825 | prompt done, n_tokens = 5169, batch.n_tokens = 64
slot init_sampler: id  1 | task 29825 | init sampler, took 0.73 ms, tokens: text = 5169, total = 5169
slot update_slots: id  1 | task 29825 | created context checkpoint 3 of 8 (pos_min = 4355, pos_max = 5104, size = 17.587 MiB)
slot print_timing: id  1 | task 29825 | 
prompt eval time =     255.46 ms /    76 tokens (    3.36 ms per token,   297.50 tokens per second)
       eval time =    1149.79 ms /    54 tokens (   21.29 ms per token,    46.96 tokens per second)
      total time =    1405.25 ms /   130 tokens
slot      release: id  1 | task 29825 | stop processing: n_tokens = 5222, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 29881 | processing task, is_child = 0
slot update_slots: id  1 | task 29881 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5268
slot update_slots: id  1 | task 29881 | n_tokens = 5192, memory_seq_rm [5192, end)
slot update_slots: id  1 | task 29881 | prompt processing progress, n_tokens = 5204, batch.n_tokens = 12, progress = 0.987851
slot update_slots: id  1 | task 29881 | n_tokens = 5204, memory_seq_rm [5204, end)
slot update_slots: id  1 | task 29881 | prompt processing progress, n_tokens = 5268, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 29881 | prompt done, n_tokens = 5268, batch.n_tokens = 64
slot init_sampler: id  1 | task 29881 | init sampler, took 0.72 ms, tokens: text = 5268, total = 5268
slot update_slots: id  1 | task 29881 | created context checkpoint 4 of 8 (pos_min = 4454, pos_max = 5203, size = 17.587 MiB)
slot print_timing: id  1 | task 29881 | 
prompt eval time =     261.50 ms /    76 tokens (    3.44 ms per token,   290.63 tokens per second)
       eval time =    2905.69 ms /   133 tokens (   21.85 ms per token,    45.77 tokens per second)
      total time =    3167.19 ms /   209 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 29881 | stop processing: n_tokens = 5400, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30016 | processing task, is_child = 0
slot update_slots: id  1 | task 30016 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5458
slot update_slots: id  1 | task 30016 | n_tokens = 5372, memory_seq_rm [5372, end)
slot update_slots: id  1 | task 30016 | prompt processing progress, n_tokens = 5394, batch.n_tokens = 22, progress = 0.988274
slot update_slots: id  1 | task 30016 | n_tokens = 5394, memory_seq_rm [5394, end)
slot update_slots: id  1 | task 30016 | prompt processing progress, n_tokens = 5458, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30016 | prompt done, n_tokens = 5458, batch.n_tokens = 64
slot init_sampler: id  1 | task 30016 | init sampler, took 1.07 ms, tokens: text = 5458, total = 5458
slot update_slots: id  1 | task 30016 | created context checkpoint 5 of 8 (pos_min = 4632, pos_max = 5393, size = 17.868 MiB)
slot print_timing: id  1 | task 30016 | 
prompt eval time =     292.87 ms /    86 tokens (    3.41 ms per token,   293.64 tokens per second)
       eval time =    1074.14 ms /    48 tokens (   22.38 ms per token,    44.69 tokens per second)
      total time =    1367.02 ms /   134 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 30016 | stop processing: n_tokens = 5505, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.523 (> 0.100 thold), f_keep = 0.652
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30066 | processing task, is_child = 0
slot update_slots: id  1 | task 30066 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 6859
slot update_slots: id  1 | task 30066 | n_past = 3590, slot.prompt.tokens.size() = 5505, seq_id = 1, pos_min = 4741, n_swa = 128
slot update_slots: id  1 | task 30066 | restored context checkpoint (pos_min = 2758, pos_max = 3525, size = 18.009 MiB)
slot update_slots: id  1 | task 30066 | erased invalidated context checkpoint (pos_min = 4130, pos_max = 4897, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 30066 | erased invalidated context checkpoint (pos_min = 4355, pos_max = 5104, n_swa = 128, size = 17.587 MiB)
slot update_slots: id  1 | task 30066 | erased invalidated context checkpoint (pos_min = 4454, pos_max = 5203, n_swa = 128, size = 17.587 MiB)
slot update_slots: id  1 | task 30066 | erased invalidated context checkpoint (pos_min = 4632, pos_max = 5393, n_swa = 128, size = 17.868 MiB)
slot update_slots: id  1 | task 30066 | n_tokens = 3525, memory_seq_rm [3525, end)
slot update_slots: id  1 | task 30066 | prompt processing progress, n_tokens = 5573, batch.n_tokens = 2048, progress = 0.812509
slot update_slots: id  1 | task 30066 | n_tokens = 5573, memory_seq_rm [5573, end)
slot update_slots: id  1 | task 30066 | prompt processing progress, n_tokens = 6795, batch.n_tokens = 1222, progress = 0.990669
slot update_slots: id  1 | task 30066 | n_tokens = 6795, memory_seq_rm [6795, end)
slot update_slots: id  1 | task 30066 | prompt processing progress, n_tokens = 6859, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30066 | prompt done, n_tokens = 6859, batch.n_tokens = 64
slot init_sampler: id  1 | task 30066 | init sampler, took 0.99 ms, tokens: text = 6859, total = 6859
slot update_slots: id  1 | task 30066 | created context checkpoint 2 of 8 (pos_min = 6027, pos_max = 6794, size = 18.009 MiB)
slot print_timing: id  1 | task 30066 | 
prompt eval time =    4033.01 ms /  3334 tokens (    1.21 ms per token,   826.68 tokens per second)
       eval time =   13400.73 ms /   599 tokens (   22.37 ms per token,    44.70 tokens per second)
      total time =   17433.74 ms /  3933 tokens
slot      release: id  1 | task 30066 | stop processing: n_tokens = 7457, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.923 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30668 | processing task, is_child = 0
slot update_slots: id  1 | task 30668 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 7431
slot update_slots: id  1 | task 30668 | n_tokens = 6859, memory_seq_rm [6859, end)
slot update_slots: id  1 | task 30668 | prompt processing progress, n_tokens = 7367, batch.n_tokens = 508, progress = 0.991387
slot update_slots: id  1 | task 30668 | n_tokens = 7367, memory_seq_rm [7367, end)
slot update_slots: id  1 | task 30668 | prompt processing progress, n_tokens = 7431, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30668 | prompt done, n_tokens = 7431, batch.n_tokens = 64
slot init_sampler: id  1 | task 30668 | init sampler, took 1.02 ms, tokens: text = 7431, total = 7431
slot update_slots: id  1 | task 30668 | created context checkpoint 3 of 8 (pos_min = 6689, pos_max = 7366, size = 15.899 MiB)
slot print_timing: id  1 | task 30668 | 
prompt eval time =     776.39 ms /   572 tokens (    1.36 ms per token,   736.75 tokens per second)
       eval time =    1255.78 ms /    57 tokens (   22.03 ms per token,    45.39 tokens per second)
      total time =    2032.16 ms /   629 tokens
slot      release: id  1 | task 30668 | stop processing: n_tokens = 7487, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.848 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30727 | processing task, is_child = 0
slot update_slots: id  1 | task 30727 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 8759
slot update_slots: id  1 | task 30727 | n_tokens = 7431, memory_seq_rm [7431, end)
slot update_slots: id  1 | task 30727 | prompt processing progress, n_tokens = 8695, batch.n_tokens = 1264, progress = 0.992693
slot update_slots: id  1 | task 30727 | n_tokens = 8695, memory_seq_rm [8695, end)
slot update_slots: id  1 | task 30727 | prompt processing progress, n_tokens = 8759, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30727 | prompt done, n_tokens = 8759, batch.n_tokens = 64
slot init_sampler: id  1 | task 30727 | init sampler, took 1.25 ms, tokens: text = 8759, total = 8759
slot update_slots: id  1 | task 30727 | created context checkpoint 4 of 8 (pos_min = 7927, pos_max = 8694, size = 18.009 MiB)
slot print_timing: id  1 | task 30727 | 
prompt eval time =    1782.04 ms /  1328 tokens (    1.34 ms per token,   745.21 tokens per second)
       eval time =   12730.69 ms /   597 tokens (   21.32 ms per token,    46.89 tokens per second)
      total time =   14512.73 ms /  1925 tokens
slot      release: id  1 | task 30727 | stop processing: n_tokens = 9355, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.936
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31326 | processing task, is_child = 0
slot update_slots: id  1 | task 31326 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 9245
slot update_slots: id  1 | task 31326 | n_tokens = 8759, memory_seq_rm [8759, end)
slot update_slots: id  1 | task 31326 | prompt processing progress, n_tokens = 9181, batch.n_tokens = 422, progress = 0.993077
slot update_slots: id  1 | task 31326 | n_tokens = 9181, memory_seq_rm [9181, end)
slot update_slots: id  1 | task 31326 | prompt processing progress, n_tokens = 9245, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31326 | prompt done, n_tokens = 9245, batch.n_tokens = 64
slot init_sampler: id  1 | task 31326 | init sampler, took 1.29 ms, tokens: text = 9245, total = 9245
slot update_slots: id  1 | task 31326 | created context checkpoint 5 of 8 (pos_min = 8632, pos_max = 9180, size = 12.874 MiB)
slot print_timing: id  1 | task 31326 | 
prompt eval time =     706.63 ms /   486 tokens (    1.45 ms per token,   687.78 tokens per second)
       eval time =   40688.04 ms /  1909 tokens (   21.31 ms per token,    46.92 tokens per second)
      total time =   41394.67 ms /  2395 tokens
slot      release: id  1 | task 31326 | stop processing: n_tokens = 11153, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.829 (> 0.100 thold), f_keep = 0.829
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 33237 | processing task, is_child = 0
slot update_slots: id  1 | task 33237 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 11151
slot update_slots: id  1 | task 33237 | n_past = 9245, slot.prompt.tokens.size() = 11153, seq_id = 1, pos_min = 10385, n_swa = 128
slot update_slots: id  1 | task 33237 | restored context checkpoint (pos_min = 8632, pos_max = 9180, size = 12.874 MiB)
slot update_slots: id  1 | task 33237 | n_tokens = 9180, memory_seq_rm [9180, end)
slot update_slots: id  1 | task 33237 | prompt processing progress, n_tokens = 11087, batch.n_tokens = 1907, progress = 0.994261
slot update_slots: id  1 | task 33237 | n_tokens = 11087, memory_seq_rm [11087, end)
slot update_slots: id  1 | task 33237 | prompt processing progress, n_tokens = 11151, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 33237 | prompt done, n_tokens = 11151, batch.n_tokens = 64
slot init_sampler: id  1 | task 33237 | init sampler, took 1.55 ms, tokens: text = 11151, total = 11151
slot update_slots: id  1 | task 33237 | created context checkpoint 6 of 8 (pos_min = 10319, pos_max = 11086, size = 18.009 MiB)
slot print_timing: id  1 | task 33237 | 
prompt eval time =    2582.41 ms /  1971 tokens (    1.31 ms per token,   763.24 tokens per second)
       eval time =    8297.34 ms /   377 tokens (   22.01 ms per token,    45.44 tokens per second)
      total time =   10879.75 ms /  2348 tokens
slot      release: id  1 | task 33237 | stop processing: n_tokens = 11527, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.894 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 33616 | processing task, is_child = 0
slot update_slots: id  1 | task 33616 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 12479
slot update_slots: id  1 | task 33616 | n_tokens = 11151, memory_seq_rm [11151, end)
slot update_slots: id  1 | task 33616 | prompt processing progress, n_tokens = 12415, batch.n_tokens = 1264, progress = 0.994871
slot update_slots: id  1 | task 33616 | n_tokens = 12415, memory_seq_rm [12415, end)
slot update_slots: id  1 | task 33616 | prompt processing progress, n_tokens = 12479, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 33616 | prompt done, n_tokens = 12479, batch.n_tokens = 64
slot init_sampler: id  1 | task 33616 | init sampler, took 2.03 ms, tokens: text = 12479, total = 12479
slot update_slots: id  1 | task 33616 | created context checkpoint 7 of 8 (pos_min = 11647, pos_max = 12414, size = 18.009 MiB)
slot print_timing: id  1 | task 33616 | 
prompt eval time =    1891.57 ms /  1328 tokens (    1.42 ms per token,   702.06 tokens per second)
       eval time =    8768.41 ms /   392 tokens (   22.37 ms per token,    44.71 tokens per second)
      total time =   10659.99 ms /  1720 tokens
slot      release: id  1 | task 33616 | stop processing: n_tokens = 12870, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 34010 | processing task, is_child = 0
slot update_slots: id  1 | task 34010 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 12565
slot update_slots: id  1 | task 34010 | n_tokens = 12479, memory_seq_rm [12479, end)
slot update_slots: id  1 | task 34010 | prompt processing progress, n_tokens = 12501, batch.n_tokens = 22, progress = 0.994906
slot update_slots: id  1 | task 34010 | n_tokens = 12501, memory_seq_rm [12501, end)
slot update_slots: id  1 | task 34010 | prompt processing progress, n_tokens = 12565, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 34010 | prompt done, n_tokens = 12565, batch.n_tokens = 64
slot init_sampler: id  1 | task 34010 | init sampler, took 2.34 ms, tokens: text = 12565, total = 12565
slot update_slots: id  1 | task 34010 | created context checkpoint 8 of 8 (pos_min = 12102, pos_max = 12500, size = 9.356 MiB)
slot print_timing: id  1 | task 34010 | 
prompt eval time =     302.22 ms /    86 tokens (    3.51 ms per token,   284.56 tokens per second)
       eval time =   36349.24 ms /  1648 tokens (   22.06 ms per token,    45.34 tokens per second)
      total time =   36651.46 ms /  1734 tokens
slot      release: id  1 | task 34010 | stop processing: n_tokens = 14212, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.884 (> 0.100 thold), f_keep = 0.884
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35660 | processing task, is_child = 0
slot update_slots: id  1 | task 35660 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 14211
slot update_slots: id  1 | task 35660 | n_past = 12565, slot.prompt.tokens.size() = 14212, seq_id = 1, pos_min = 13444, n_swa = 128
slot update_slots: id  1 | task 35660 | restored context checkpoint (pos_min = 12102, pos_max = 12500, size = 9.356 MiB)
slot update_slots: id  1 | task 35660 | n_tokens = 12500, memory_seq_rm [12500, end)
slot update_slots: id  1 | task 35660 | prompt processing progress, n_tokens = 14147, batch.n_tokens = 1647, progress = 0.995496
slot update_slots: id  1 | task 35660 | n_tokens = 14147, memory_seq_rm [14147, end)
slot update_slots: id  1 | task 35660 | prompt processing progress, n_tokens = 14211, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35660 | prompt done, n_tokens = 14211, batch.n_tokens = 64
slot init_sampler: id  1 | task 35660 | init sampler, took 2.72 ms, tokens: text = 14211, total = 14211
slot update_slots: id  1 | task 35660 | erasing old context checkpoint (pos_min = 2758, pos_max = 3525, size = 18.009 MiB)
slot update_slots: id  1 | task 35660 | created context checkpoint 8 of 8 (pos_min = 13379, pos_max = 14146, size = 18.009 MiB)
slot print_timing: id  1 | task 35660 | 
prompt eval time =    2402.03 ms /  1711 tokens (    1.40 ms per token,   712.32 tokens per second)
       eval time =    1997.18 ms /    89 tokens (   22.44 ms per token,    44.56 tokens per second)
      total time =    4399.20 ms /  1800 tokens
slot      release: id  1 | task 35660 | stop processing: n_tokens = 14299, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35751 | processing task, is_child = 0
slot update_slots: id  1 | task 35751 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 14279
slot update_slots: id  1 | task 35751 | n_tokens = 14211, memory_seq_rm [14211, end)
slot update_slots: id  1 | task 35751 | prompt processing progress, n_tokens = 14215, batch.n_tokens = 4, progress = 0.995518
slot update_slots: id  1 | task 35751 | n_tokens = 14215, memory_seq_rm [14215, end)
slot update_slots: id  1 | task 35751 | prompt processing progress, n_tokens = 14279, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35751 | prompt done, n_tokens = 14279, batch.n_tokens = 64
slot init_sampler: id  1 | task 35751 | init sampler, took 2.08 ms, tokens: text = 14279, total = 14279
slot update_slots: id  1 | task 35751 | erasing old context checkpoint (pos_min = 6027, pos_max = 6794, size = 18.009 MiB)
slot update_slots: id  1 | task 35751 | created context checkpoint 8 of 8 (pos_min = 13531, pos_max = 14214, size = 16.039 MiB)
slot print_timing: id  1 | task 35751 | 
prompt eval time =     240.93 ms /    68 tokens (    3.54 ms per token,   282.23 tokens per second)
       eval time =    2463.71 ms /   113 tokens (   21.80 ms per token,    45.87 tokens per second)
      total time =    2704.65 ms /   181 tokens
slot      release: id  1 | task 35751 | stop processing: n_tokens = 14391, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35866 | processing task, is_child = 0
slot update_slots: id  1 | task 35866 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 14367
slot update_slots: id  1 | task 35866 | n_tokens = 14280, memory_seq_rm [14280, end)
slot update_slots: id  1 | task 35866 | prompt processing progress, n_tokens = 14303, batch.n_tokens = 23, progress = 0.995545
slot update_slots: id  1 | task 35866 | n_tokens = 14303, memory_seq_rm [14303, end)
slot update_slots: id  1 | task 35866 | prompt processing progress, n_tokens = 14367, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35866 | prompt done, n_tokens = 14367, batch.n_tokens = 64
slot init_sampler: id  1 | task 35866 | init sampler, took 2.81 ms, tokens: text = 14367, total = 14367
slot update_slots: id  1 | task 35866 | erasing old context checkpoint (pos_min = 6689, pos_max = 7366, size = 15.899 MiB)
slot update_slots: id  1 | task 35866 | created context checkpoint 8 of 8 (pos_min = 13623, pos_max = 14302, size = 15.946 MiB)
slot print_timing: id  1 | task 35866 | 
prompt eval time =     391.88 ms /    87 tokens (    4.50 ms per token,   222.01 tokens per second)
       eval time =    2144.36 ms /    98 tokens (   21.88 ms per token,    45.70 tokens per second)
      total time =    2536.24 ms /   185 tokens
slot      release: id  1 | task 35866 | stop processing: n_tokens = 14464, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.894 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 35966 | processing task, is_child = 0
slot update_slots: id  1 | task 35966 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 16070
slot update_slots: id  1 | task 35966 | n_tokens = 14367, memory_seq_rm [14367, end)
slot update_slots: id  1 | task 35966 | prompt processing progress, n_tokens = 16006, batch.n_tokens = 1639, progress = 0.996017
slot update_slots: id  1 | task 35966 | n_tokens = 16006, memory_seq_rm [16006, end)
slot update_slots: id  1 | task 35966 | prompt processing progress, n_tokens = 16070, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 35966 | prompt done, n_tokens = 16070, batch.n_tokens = 64
slot init_sampler: id  1 | task 35966 | init sampler, took 2.21 ms, tokens: text = 16070, total = 16070
slot update_slots: id  1 | task 35966 | erasing old context checkpoint (pos_min = 7927, pos_max = 8694, size = 18.009 MiB)
slot update_slots: id  1 | task 35966 | created context checkpoint 8 of 8 (pos_min = 15238, pos_max = 16005, size = 18.009 MiB)
slot print_timing: id  1 | task 35966 | 
prompt eval time =    2394.87 ms /  1703 tokens (    1.41 ms per token,   711.10 tokens per second)
       eval time =    3120.44 ms /   145 tokens (   21.52 ms per token,    46.47 tokens per second)
      total time =    5515.31 ms /  1848 tokens
slot      release: id  1 | task 35966 | stop processing: n_tokens = 16214, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 2353103748
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2265, total state size = 71.121 MiB
srv          load:  - looking for better prompt, base f_keep = 0.189, sim = 0.031
srv        update:  - cache state: 20 prompts, 5502.305 MiB (limits: 8192.000 MiB, 49152 tokens, 244270 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv        update:    - prompt 0x57052ce72cb0:    4834 tokens, checkpoints:  6,   234.281 MiB
srv        update:    - prompt 0x57052cc7a5d0:    3607 tokens, checkpoints:  2,   133.262 MiB
srv        update:    - prompt 0x57052dbadd70:    3253 tokens, checkpoints:  6,   200.608 MiB
srv        update:    - prompt 0x57052c8492b0:    2265 tokens, checkpoints:  2,   101.793 MiB
srv  get_availabl: prompt cache update took 75.55 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36113 | processing task, is_child = 0
slot update_slots: id  0 | task 36113 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 13903
slot update_slots: id  0 | task 36113 | n_past = 427, slot.prompt.tokens.size() = 2265, seq_id = 0, pos_min = 1497, n_swa = 128
slot update_slots: id  0 | task 36113 | restored context checkpoint (pos_min = 0, pos_max = 539, size = 12.663 MiB)
slot update_slots: id  0 | task 36113 | erased invalidated context checkpoint (pos_min = 1108, pos_max = 1875, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 36113 | n_tokens = 427, memory_seq_rm [427, end)
slot update_slots: id  0 | task 36113 | prompt processing progress, n_tokens = 2475, batch.n_tokens = 2048, progress = 0.178019
slot update_slots: id  0 | task 36113 | n_tokens = 2475, memory_seq_rm [2475, end)
slot update_slots: id  0 | task 36113 | prompt processing progress, n_tokens = 4523, batch.n_tokens = 2048, progress = 0.325325
slot update_slots: id  0 | task 36113 | n_tokens = 4523, memory_seq_rm [4523, end)
slot update_slots: id  0 | task 36113 | prompt processing progress, n_tokens = 6571, batch.n_tokens = 2048, progress = 0.472632
slot update_slots: id  0 | task 36113 | n_tokens = 6571, memory_seq_rm [6571, end)
slot update_slots: id  0 | task 36113 | prompt processing progress, n_tokens = 8619, batch.n_tokens = 2048, progress = 0.619938
slot update_slots: id  0 | task 36113 | n_tokens = 8619, memory_seq_rm [8619, end)
slot update_slots: id  0 | task 36113 | prompt processing progress, n_tokens = 10667, batch.n_tokens = 2048, progress = 0.767244
slot update_slots: id  0 | task 36113 | n_tokens = 10667, memory_seq_rm [10667, end)
slot update_slots: id  0 | task 36113 | prompt processing progress, n_tokens = 12715, batch.n_tokens = 2048, progress = 0.914551
slot update_slots: id  0 | task 36113 | n_tokens = 12715, memory_seq_rm [12715, end)
slot update_slots: id  0 | task 36113 | prompt processing progress, n_tokens = 13839, batch.n_tokens = 1124, progress = 0.995397
slot update_slots: id  0 | task 36113 | n_tokens = 13839, memory_seq_rm [13839, end)
slot update_slots: id  0 | task 36113 | prompt processing progress, n_tokens = 13903, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36113 | prompt done, n_tokens = 13903, batch.n_tokens = 64
slot init_sampler: id  0 | task 36113 | init sampler, took 1.96 ms, tokens: text = 13903, total = 13903
slot update_slots: id  0 | task 36113 | created context checkpoint 2 of 8 (pos_min = 13071, pos_max = 13838, size = 18.009 MiB)
slot print_timing: id  0 | task 36113 | 
prompt eval time =   15490.03 ms / 13476 tokens (    1.15 ms per token,   869.98 tokens per second)
       eval time =   11361.54 ms /   493 tokens (   23.05 ms per token,    43.39 tokens per second)
      total time =   26851.57 ms / 13969 tokens
slot      release: id  0 | task 36113 | stop processing: n_tokens = 14395, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.212 (> 0.100 thold), f_keep = 0.056
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 16214, total state size = 398.211 MiB
srv          load:  - looking for better prompt, base f_keep = 0.056, sim = 0.212
srv        update:  - cache state: 21 prompts, 6026.767 MiB (limits: 8192.000 MiB, 49152 tokens, 245053 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv        update:    - prompt 0x57052ce72cb0:    4834 tokens, checkpoints:  6,   234.281 MiB
srv        update:    - prompt 0x57052cc7a5d0:    3607 tokens, checkpoints:  2,   133.262 MiB
srv        update:    - prompt 0x57052dbadd70:    3253 tokens, checkpoints:  6,   200.608 MiB
srv        update:    - prompt 0x57052c8492b0:    2265 tokens, checkpoints:  2,   101.793 MiB
srv        update:    - prompt 0x57051d4ca2e0:   16214 tokens, checkpoints:  8,   524.462 MiB
srv  get_availabl: prompt cache update took 385.66 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 36614 | processing task, is_child = 0
slot update_slots: id  1 | task 36614 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 4307
slot update_slots: id  1 | task 36614 | n_past = 915, slot.prompt.tokens.size() = 16214, seq_id = 1, pos_min = 15446, n_swa = 128
slot update_slots: id  1 | task 36614 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 36614 | erased invalidated context checkpoint (pos_min = 8632, pos_max = 9180, n_swa = 128, size = 12.874 MiB)
slot update_slots: id  1 | task 36614 | erased invalidated context checkpoint (pos_min = 10319, pos_max = 11086, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 36614 | erased invalidated context checkpoint (pos_min = 11647, pos_max = 12414, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 36614 | erased invalidated context checkpoint (pos_min = 12102, pos_max = 12500, n_swa = 128, size = 9.356 MiB)
slot update_slots: id  1 | task 36614 | erased invalidated context checkpoint (pos_min = 13379, pos_max = 14146, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 36614 | erased invalidated context checkpoint (pos_min = 13531, pos_max = 14214, n_swa = 128, size = 16.039 MiB)
slot update_slots: id  1 | task 36614 | erased invalidated context checkpoint (pos_min = 13623, pos_max = 14302, n_swa = 128, size = 15.946 MiB)
slot update_slots: id  1 | task 36614 | erased invalidated context checkpoint (pos_min = 15238, pos_max = 16005, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 36614 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 36614 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.475505
slot update_slots: id  1 | task 36614 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 36614 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.951010
slot update_slots: id  1 | task 36614 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 36614 | prompt processing progress, n_tokens = 4243, batch.n_tokens = 147, progress = 0.985140
slot update_slots: id  1 | task 36614 | n_tokens = 4243, memory_seq_rm [4243, end)
slot update_slots: id  1 | task 36614 | prompt processing progress, n_tokens = 4307, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 36614 | prompt done, n_tokens = 4307, batch.n_tokens = 64
slot init_sampler: id  1 | task 36614 | init sampler, took 0.82 ms, tokens: text = 4307, total = 4307
slot update_slots: id  1 | task 36614 | created context checkpoint 1 of 8 (pos_min = 3475, pos_max = 4242, size = 18.009 MiB)
slot print_timing: id  1 | task 36614 | 
prompt eval time =    4882.94 ms /  4307 tokens (    1.13 ms per token,   882.05 tokens per second)
       eval time =   20139.87 ms /   950 tokens (   21.20 ms per token,    47.17 tokens per second)
      total time =   25022.81 ms /  5257 tokens
slot      release: id  1 | task 36614 | stop processing: n_tokens = 5256, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.862 (> 0.100 thold), f_keep = 0.819
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 37568 | processing task, is_child = 0
slot update_slots: id  1 | task 37568 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 4994
slot update_slots: id  1 | task 37568 | n_past = 4307, slot.prompt.tokens.size() = 5256, seq_id = 1, pos_min = 4488, n_swa = 128
slot update_slots: id  1 | task 37568 | restored context checkpoint (pos_min = 3475, pos_max = 4242, size = 18.009 MiB)
slot update_slots: id  1 | task 37568 | n_tokens = 4242, memory_seq_rm [4242, end)
slot update_slots: id  1 | task 37568 | prompt processing progress, n_tokens = 4930, batch.n_tokens = 688, progress = 0.987185
slot update_slots: id  1 | task 37568 | n_tokens = 4930, memory_seq_rm [4930, end)
slot update_slots: id  1 | task 37568 | prompt processing progress, n_tokens = 4994, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 37568 | prompt done, n_tokens = 4994, batch.n_tokens = 64
slot init_sampler: id  1 | task 37568 | init sampler, took 0.70 ms, tokens: text = 4994, total = 4994
slot update_slots: id  1 | task 37568 | created context checkpoint 2 of 8 (pos_min = 4162, pos_max = 4929, size = 18.009 MiB)
slot print_timing: id  1 | task 37568 | 
prompt eval time =    1062.08 ms /   752 tokens (    1.41 ms per token,   708.04 tokens per second)
       eval time =    1174.28 ms /    57 tokens (   20.60 ms per token,    48.54 tokens per second)
      total time =    2236.37 ms /   809 tokens
slot      release: id  1 | task 37568 | stop processing: n_tokens = 5050, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 37627 | processing task, is_child = 0
slot update_slots: id  1 | task 37627 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5054
slot update_slots: id  1 | task 37627 | n_tokens = 4994, memory_seq_rm [4994, end)
slot update_slots: id  1 | task 37627 | prompt processing progress, n_tokens = 5054, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  1 | task 37627 | prompt done, n_tokens = 5054, batch.n_tokens = 60
slot init_sampler: id  1 | task 37627 | init sampler, took 0.80 ms, tokens: text = 5054, total = 5054
slot print_timing: id  1 | task 37627 | 
prompt eval time =     238.87 ms /    60 tokens (    3.98 ms per token,   251.18 tokens per second)
       eval time =   10847.76 ms /   513 tokens (   21.15 ms per token,    47.29 tokens per second)
      total time =   11086.63 ms /   573 tokens
slot      release: id  1 | task 37627 | stop processing: n_tokens = 5566, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.902 (> 0.100 thold), f_keep = 0.908
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 38141 | processing task, is_child = 0
slot update_slots: id  1 | task 38141 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5607
slot update_slots: id  1 | task 38141 | n_tokens = 5055, memory_seq_rm [5055, end)
slot update_slots: id  1 | task 38141 | prompt processing progress, n_tokens = 5543, batch.n_tokens = 488, progress = 0.988586
slot update_slots: id  1 | task 38141 | n_tokens = 5543, memory_seq_rm [5543, end)
slot update_slots: id  1 | task 38141 | prompt processing progress, n_tokens = 5607, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 38141 | prompt done, n_tokens = 5607, batch.n_tokens = 64
slot init_sampler: id  1 | task 38141 | init sampler, took 1.14 ms, tokens: text = 5607, total = 5607
slot update_slots: id  1 | task 38141 | created context checkpoint 3 of 8 (pos_min = 4928, pos_max = 5542, size = 14.421 MiB)
slot print_timing: id  1 | task 38141 | 
prompt eval time =     736.77 ms /   552 tokens (    1.33 ms per token,   749.22 tokens per second)
       eval time =    1228.01 ms /    58 tokens (   21.17 ms per token,    47.23 tokens per second)
      total time =    1964.78 ms /   610 tokens
slot      release: id  1 | task 38141 | stop processing: n_tokens = 5664, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.772 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 38201 | processing task, is_child = 0
slot update_slots: id  1 | task 38201 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 7264
slot update_slots: id  1 | task 38201 | n_tokens = 5607, memory_seq_rm [5607, end)
slot update_slots: id  1 | task 38201 | prompt processing progress, n_tokens = 7200, batch.n_tokens = 1593, progress = 0.991189
slot update_slots: id  1 | task 38201 | n_tokens = 7200, memory_seq_rm [7200, end)
slot update_slots: id  1 | task 38201 | prompt processing progress, n_tokens = 7264, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 38201 | prompt done, n_tokens = 7264, batch.n_tokens = 64
slot init_sampler: id  1 | task 38201 | init sampler, took 1.40 ms, tokens: text = 7264, total = 7264
slot update_slots: id  1 | task 38201 | created context checkpoint 4 of 8 (pos_min = 6432, pos_max = 7199, size = 18.009 MiB)
slot print_timing: id  1 | task 38201 | 
prompt eval time =    2072.37 ms /  1657 tokens (    1.25 ms per token,   799.57 tokens per second)
       eval time =   67023.02 ms /  3069 tokens (   21.84 ms per token,    45.79 tokens per second)
      total time =   69095.39 ms /  4726 tokens
slot      release: id  1 | task 38201 | stop processing: n_tokens = 10332, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.712 (> 0.100 thold), f_keep = 0.703
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 41272 | processing task, is_child = 0
slot update_slots: id  1 | task 41272 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 10197
slot update_slots: id  1 | task 41272 | n_past = 7264, slot.prompt.tokens.size() = 10332, seq_id = 1, pos_min = 9564, n_swa = 128
slot update_slots: id  1 | task 41272 | restored context checkpoint (pos_min = 6432, pos_max = 7199, size = 18.009 MiB)
slot update_slots: id  1 | task 41272 | n_tokens = 7199, memory_seq_rm [7199, end)
slot update_slots: id  1 | task 41272 | prompt processing progress, n_tokens = 9247, batch.n_tokens = 2048, progress = 0.906835
slot update_slots: id  1 | task 41272 | n_tokens = 9247, memory_seq_rm [9247, end)
slot update_slots: id  1 | task 41272 | prompt processing progress, n_tokens = 10133, batch.n_tokens = 886, progress = 0.993724
slot update_slots: id  1 | task 41272 | n_tokens = 10133, memory_seq_rm [10133, end)
slot update_slots: id  1 | task 41272 | prompt processing progress, n_tokens = 10197, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 41272 | prompt done, n_tokens = 10197, batch.n_tokens = 64
slot init_sampler: id  1 | task 41272 | init sampler, took 1.40 ms, tokens: text = 10197, total = 10197
slot update_slots: id  1 | task 41272 | created context checkpoint 5 of 8 (pos_min = 9365, pos_max = 10132, size = 18.009 MiB)
slot print_timing: id  1 | task 41272 | 
prompt eval time =    3678.50 ms /  2998 tokens (    1.23 ms per token,   815.01 tokens per second)
       eval time =    1351.77 ms /    64 tokens (   21.12 ms per token,    47.35 tokens per second)
      total time =    5030.27 ms /  3062 tokens
slot      release: id  1 | task 41272 | stop processing: n_tokens = 10260, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.838 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 41339 | processing task, is_child = 0
slot update_slots: id  1 | task 41339 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 12172
slot update_slots: id  1 | task 41339 | n_tokens = 10197, memory_seq_rm [10197, end)
slot update_slots: id  1 | task 41339 | prompt processing progress, n_tokens = 12108, batch.n_tokens = 1911, progress = 0.994742
slot update_slots: id  1 | task 41339 | n_tokens = 12108, memory_seq_rm [12108, end)
slot update_slots: id  1 | task 41339 | prompt processing progress, n_tokens = 12172, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 41339 | prompt done, n_tokens = 12172, batch.n_tokens = 64
slot init_sampler: id  1 | task 41339 | init sampler, took 2.35 ms, tokens: text = 12172, total = 12172
slot update_slots: id  1 | task 41339 | created context checkpoint 6 of 8 (pos_min = 11340, pos_max = 12107, size = 18.009 MiB)
slot print_timing: id  1 | task 41339 | 
prompt eval time =    2560.17 ms /  1975 tokens (    1.30 ms per token,   771.43 tokens per second)
       eval time =    1733.46 ms /    79 tokens (   21.94 ms per token,    45.57 tokens per second)
      total time =    4293.63 ms /  2054 tokens
slot      release: id  1 | task 41339 | stop processing: n_tokens = 12250, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.860 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 41420 | processing task, is_child = 0
slot update_slots: id  1 | task 41420 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 14147
slot update_slots: id  1 | task 41420 | n_tokens = 12172, memory_seq_rm [12172, end)
slot update_slots: id  1 | task 41420 | prompt processing progress, n_tokens = 14083, batch.n_tokens = 1911, progress = 0.995476
slot update_slots: id  1 | task 41420 | n_tokens = 14083, memory_seq_rm [14083, end)
slot update_slots: id  1 | task 41420 | prompt processing progress, n_tokens = 14147, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 41420 | prompt done, n_tokens = 14147, batch.n_tokens = 64
slot init_sampler: id  1 | task 41420 | init sampler, took 2.01 ms, tokens: text = 14147, total = 14147
slot update_slots: id  1 | task 41420 | created context checkpoint 7 of 8 (pos_min = 13315, pos_max = 14082, size = 18.009 MiB)
slot print_timing: id  1 | task 41420 | 
prompt eval time =    2632.24 ms /  1975 tokens (    1.33 ms per token,   750.31 tokens per second)
       eval time =    1327.44 ms /    61 tokens (   21.76 ms per token,    45.95 tokens per second)
      total time =    3959.68 ms /  2036 tokens
slot      release: id  1 | task 41420 | stop processing: n_tokens = 14207, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.877 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 41483 | processing task, is_child = 0
slot update_slots: id  1 | task 41483 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 16122
slot update_slots: id  1 | task 41483 | n_tokens = 14147, memory_seq_rm [14147, end)
slot update_slots: id  1 | task 41483 | prompt processing progress, n_tokens = 16058, batch.n_tokens = 1911, progress = 0.996030
slot update_slots: id  1 | task 41483 | n_tokens = 16058, memory_seq_rm [16058, end)
slot update_slots: id  1 | task 41483 | prompt processing progress, n_tokens = 16122, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 41483 | prompt done, n_tokens = 16122, batch.n_tokens = 64
slot init_sampler: id  1 | task 41483 | init sampler, took 2.22 ms, tokens: text = 16122, total = 16122
slot update_slots: id  1 | task 41483 | created context checkpoint 8 of 8 (pos_min = 15290, pos_max = 16057, size = 18.009 MiB)
slot print_timing: id  1 | task 41483 | 
prompt eval time =    2714.80 ms /  1975 tokens (    1.37 ms per token,   727.49 tokens per second)
       eval time =    1379.84 ms /    62 tokens (   22.26 ms per token,    44.93 tokens per second)
      total time =    4094.64 ms /  2037 tokens
slot      release: id  1 | task 41483 | stop processing: n_tokens = 16183, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 41547 | processing task, is_child = 0
slot update_slots: id  1 | task 41547 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 16382
slot update_slots: id  1 | task 41547 | n_tokens = 16122, memory_seq_rm [16122, end)
slot update_slots: id  1 | task 41547 | prompt processing progress, n_tokens = 16318, batch.n_tokens = 196, progress = 0.996093
slot update_slots: id  1 | task 41547 | n_tokens = 16318, memory_seq_rm [16318, end)
slot update_slots: id  1 | task 41547 | prompt processing progress, n_tokens = 16382, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 41547 | prompt done, n_tokens = 16382, batch.n_tokens = 64
slot init_sampler: id  1 | task 41547 | init sampler, took 2.53 ms, tokens: text = 16382, total = 16382
slot update_slots: id  1 | task 41547 | erasing old context checkpoint (pos_min = 3475, pos_max = 4242, size = 18.009 MiB)
slot update_slots: id  1 | task 41547 | created context checkpoint 8 of 8 (pos_min = 15550, pos_max = 16317, size = 18.009 MiB)
slot print_timing: id  1 | task 41547 | 
prompt eval time =     574.35 ms /   260 tokens (    2.21 ms per token,   452.68 tokens per second)
       eval time =   22105.84 ms /   985 tokens (   22.44 ms per token,    44.56 tokens per second)
      total time =   22680.19 ms /  1245 tokens
slot      release: id  1 | task 41547 | stop processing: n_tokens = 17366, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.943
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42534 | processing task, is_child = 0
slot update_slots: id  1 | task 42534 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 17453
slot update_slots: id  1 | task 42534 | n_past = 16382, slot.prompt.tokens.size() = 17366, seq_id = 1, pos_min = 16598, n_swa = 128
slot update_slots: id  1 | task 42534 | restored context checkpoint (pos_min = 15550, pos_max = 16317, size = 18.009 MiB)
slot update_slots: id  1 | task 42534 | n_tokens = 16317, memory_seq_rm [16317, end)
slot update_slots: id  1 | task 42534 | prompt processing progress, n_tokens = 17389, batch.n_tokens = 1072, progress = 0.996333
slot update_slots: id  1 | task 42534 | n_tokens = 17389, memory_seq_rm [17389, end)
slot update_slots: id  1 | task 42534 | prompt processing progress, n_tokens = 17453, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42534 | prompt done, n_tokens = 17453, batch.n_tokens = 64
slot init_sampler: id  1 | task 42534 | init sampler, took 2.40 ms, tokens: text = 17453, total = 17453
slot update_slots: id  1 | task 42534 | erasing old context checkpoint (pos_min = 4162, pos_max = 4929, size = 18.009 MiB)
slot update_slots: id  1 | task 42534 | created context checkpoint 8 of 8 (pos_min = 16621, pos_max = 17388, size = 18.009 MiB)
slot print_timing: id  1 | task 42534 | 
prompt eval time =    1779.68 ms /  1136 tokens (    1.57 ms per token,   638.32 tokens per second)
       eval time =    1326.71 ms /    58 tokens (   22.87 ms per token,    43.72 tokens per second)
      total time =    3106.39 ms /  1194 tokens
slot      release: id  1 | task 42534 | stop processing: n_tokens = 17510, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42594 | processing task, is_child = 0
slot update_slots: id  1 | task 42594 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 18132
slot update_slots: id  1 | task 42594 | n_tokens = 17453, memory_seq_rm [17453, end)
slot update_slots: id  1 | task 42594 | prompt processing progress, n_tokens = 18068, batch.n_tokens = 615, progress = 0.996470
slot update_slots: id  1 | task 42594 | n_tokens = 18068, memory_seq_rm [18068, end)
slot update_slots: id  1 | task 42594 | prompt processing progress, n_tokens = 18132, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42594 | prompt done, n_tokens = 18132, batch.n_tokens = 64
slot init_sampler: id  1 | task 42594 | init sampler, took 2.48 ms, tokens: text = 18132, total = 18132
slot update_slots: id  1 | task 42594 | erasing old context checkpoint (pos_min = 4928, pos_max = 5542, size = 14.421 MiB)
slot update_slots: id  1 | task 42594 | created context checkpoint 8 of 8 (pos_min = 17300, pos_max = 18067, size = 18.009 MiB)
slot print_timing: id  1 | task 42594 | 
prompt eval time =    1215.62 ms /   679 tokens (    1.79 ms per token,   558.56 tokens per second)
       eval time =    1908.93 ms /    84 tokens (   22.73 ms per token,    44.00 tokens per second)
      total time =    3124.55 ms /   763 tokens
slot      release: id  1 | task 42594 | stop processing: n_tokens = 18215, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.893 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42680 | processing task, is_child = 0
slot update_slots: id  1 | task 42680 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 20295
slot update_slots: id  1 | task 42680 | n_tokens = 18132, memory_seq_rm [18132, end)
slot update_slots: id  1 | task 42680 | prompt processing progress, n_tokens = 20180, batch.n_tokens = 2048, progress = 0.994334
slot update_slots: id  1 | task 42680 | n_tokens = 20180, memory_seq_rm [20180, end)
slot update_slots: id  1 | task 42680 | prompt processing progress, n_tokens = 20231, batch.n_tokens = 51, progress = 0.996846
slot update_slots: id  1 | task 42680 | n_tokens = 20231, memory_seq_rm [20231, end)
slot update_slots: id  1 | task 42680 | prompt processing progress, n_tokens = 20295, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 42680 | prompt done, n_tokens = 20295, batch.n_tokens = 64
slot init_sampler: id  1 | task 42680 | init sampler, took 4.14 ms, tokens: text = 20295, total = 20295
slot update_slots: id  1 | task 42680 | erasing old context checkpoint (pos_min = 6432, pos_max = 7199, size = 18.009 MiB)
slot update_slots: id  1 | task 42680 | created context checkpoint 8 of 8 (pos_min = 19463, pos_max = 20230, size = 18.009 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 42680
slot      release: id  1 | task 42680 | stop processing: n_tokens = 20337, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 42727 | processing task, is_child = 0
slot update_slots: id  1 | task 42727 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 20320
slot update_slots: id  1 | task 42727 | n_tokens = 20294, memory_seq_rm [20294, end)
slot update_slots: id  1 | task 42727 | prompt processing progress, n_tokens = 20320, batch.n_tokens = 26, progress = 1.000000
slot update_slots: id  1 | task 42727 | prompt done, n_tokens = 20320, batch.n_tokens = 26
slot init_sampler: id  1 | task 42727 | init sampler, took 2.79 ms, tokens: text = 20320, total = 20320
slot print_timing: id  1 | task 42727 | 
prompt eval time =     125.93 ms /    26 tokens (    4.84 ms per token,   206.47 tokens per second)
       eval time =   29483.10 ms /  1277 tokens (   23.09 ms per token,    43.31 tokens per second)
      total time =   29609.03 ms /  1303 tokens
slot      release: id  1 | task 42727 | stop processing: n_tokens = 21596, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.927 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 44005 | processing task, is_child = 0
slot update_slots: id  1 | task 44005 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 21918
slot update_slots: id  1 | task 44005 | n_past = 20320, slot.prompt.tokens.size() = 21596, seq_id = 1, pos_min = 20828, n_swa = 128
slot update_slots: id  1 | task 44005 | restored context checkpoint (pos_min = 19463, pos_max = 20230, size = 18.009 MiB)
slot update_slots: id  1 | task 44005 | n_tokens = 20230, memory_seq_rm [20230, end)
slot update_slots: id  1 | task 44005 | prompt processing progress, n_tokens = 21854, batch.n_tokens = 1624, progress = 0.997080
slot update_slots: id  1 | task 44005 | n_tokens = 21854, memory_seq_rm [21854, end)
slot update_slots: id  1 | task 44005 | prompt processing progress, n_tokens = 21918, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 44005 | prompt done, n_tokens = 21918, batch.n_tokens = 64
slot init_sampler: id  1 | task 44005 | init sampler, took 3.01 ms, tokens: text = 21918, total = 21918
slot update_slots: id  1 | task 44005 | erasing old context checkpoint (pos_min = 9365, pos_max = 10132, size = 18.009 MiB)
slot update_slots: id  1 | task 44005 | created context checkpoint 8 of 8 (pos_min = 21086, pos_max = 21853, size = 18.009 MiB)
slot print_timing: id  1 | task 44005 | 
prompt eval time =    2668.20 ms /  1688 tokens (    1.58 ms per token,   632.64 tokens per second)
       eval time =    1621.76 ms /    69 tokens (   23.50 ms per token,    42.55 tokens per second)
      total time =    4289.96 ms /  1757 tokens
slot      release: id  1 | task 44005 | stop processing: n_tokens = 21986, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 2678339824
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 14395, total state size = 355.557 MiB
srv          load:  - looking for better prompt, base f_keep = 0.030, sim = 0.021
srv        update:  - cache state: 22 prompts, 6412.996 MiB (limits: 8192.000 MiB, 49152 tokens, 248682 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv        update:    - prompt 0x57052ce72cb0:    4834 tokens, checkpoints:  6,   234.281 MiB
srv        update:    - prompt 0x57052cc7a5d0:    3607 tokens, checkpoints:  2,   133.262 MiB
srv        update:    - prompt 0x57052dbadd70:    3253 tokens, checkpoints:  6,   200.608 MiB
srv        update:    - prompt 0x57052c8492b0:    2265 tokens, checkpoints:  2,   101.793 MiB
srv        update:    - prompt 0x57051d4ca2e0:   16214 tokens, checkpoints:  8,   524.462 MiB
srv        update:    - prompt 0x57052c5b2040:   14395 tokens, checkpoints:  2,   386.229 MiB
srv  get_availabl: prompt cache update took 339.34 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44076 | processing task, is_child = 0
slot update_slots: id  0 | task 44076 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 19864
slot update_slots: id  0 | task 44076 | n_past = 427, slot.prompt.tokens.size() = 14395, seq_id = 0, pos_min = 13627, n_swa = 128
slot update_slots: id  0 | task 44076 | restored context checkpoint (pos_min = 0, pos_max = 539, size = 12.663 MiB)
slot update_slots: id  0 | task 44076 | erased invalidated context checkpoint (pos_min = 13071, pos_max = 13838, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 44076 | n_tokens = 427, memory_seq_rm [427, end)
slot update_slots: id  0 | task 44076 | prompt processing progress, n_tokens = 2475, batch.n_tokens = 2048, progress = 0.124597
slot update_slots: id  0 | task 44076 | n_tokens = 2475, memory_seq_rm [2475, end)
slot update_slots: id  0 | task 44076 | prompt processing progress, n_tokens = 4523, batch.n_tokens = 2048, progress = 0.227698
slot update_slots: id  0 | task 44076 | n_tokens = 4523, memory_seq_rm [4523, end)
slot update_slots: id  0 | task 44076 | prompt processing progress, n_tokens = 6571, batch.n_tokens = 2048, progress = 0.330799
slot update_slots: id  0 | task 44076 | n_tokens = 6571, memory_seq_rm [6571, end)
slot update_slots: id  0 | task 44076 | prompt processing progress, n_tokens = 8619, batch.n_tokens = 2048, progress = 0.433901
slot update_slots: id  0 | task 44076 | n_tokens = 8619, memory_seq_rm [8619, end)
slot update_slots: id  0 | task 44076 | prompt processing progress, n_tokens = 10667, batch.n_tokens = 2048, progress = 0.537002
slot update_slots: id  0 | task 44076 | n_tokens = 10667, memory_seq_rm [10667, end)
slot update_slots: id  0 | task 44076 | prompt processing progress, n_tokens = 12715, batch.n_tokens = 2048, progress = 0.640103
slot update_slots: id  0 | task 44076 | n_tokens = 12715, memory_seq_rm [12715, end)
slot update_slots: id  0 | task 44076 | prompt processing progress, n_tokens = 14763, batch.n_tokens = 2048, progress = 0.743204
slot update_slots: id  0 | task 44076 | n_tokens = 14763, memory_seq_rm [14763, end)
slot update_slots: id  0 | task 44076 | prompt processing progress, n_tokens = 16811, batch.n_tokens = 2048, progress = 0.846305
slot update_slots: id  0 | task 44076 | n_tokens = 16811, memory_seq_rm [16811, end)
slot update_slots: id  0 | task 44076 | prompt processing progress, n_tokens = 18859, batch.n_tokens = 2048, progress = 0.949406
slot update_slots: id  0 | task 44076 | n_tokens = 18859, memory_seq_rm [18859, end)
slot update_slots: id  0 | task 44076 | prompt processing progress, n_tokens = 19800, batch.n_tokens = 941, progress = 0.996778
slot update_slots: id  0 | task 44076 | n_tokens = 19800, memory_seq_rm [19800, end)
slot update_slots: id  0 | task 44076 | prompt processing progress, n_tokens = 19864, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44076 | prompt done, n_tokens = 19864, batch.n_tokens = 64
slot init_sampler: id  0 | task 44076 | init sampler, took 2.74 ms, tokens: text = 19864, total = 19864
slot update_slots: id  0 | task 44076 | created context checkpoint 2 of 8 (pos_min = 19032, pos_max = 19799, size = 18.009 MiB)
slot print_timing: id  0 | task 44076 | 
prompt eval time =   23315.90 ms / 19437 tokens (    1.20 ms per token,   833.64 tokens per second)
       eval time =    9996.95 ms /   440 tokens (   22.72 ms per token,    44.01 tokens per second)
      total time =   33312.86 ms / 19877 tokens
slot      release: id  0 | task 44076 | stop processing: n_tokens = 20303, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.218 (> 0.100 thold), f_keep = 0.042
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 21986, total state size = 533.558 MiB
srv          load:  - looking for better prompt, base f_keep = 0.042, sim = 0.218
srv        update:  - cache state: 23 prompts, 7090.626 MiB (limits: 8192.000 MiB, 49152 tokens, 250318 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv        update:    - prompt 0x57052ce72cb0:    4834 tokens, checkpoints:  6,   234.281 MiB
srv        update:    - prompt 0x57052cc7a5d0:    3607 tokens, checkpoints:  2,   133.262 MiB
srv        update:    - prompt 0x57052dbadd70:    3253 tokens, checkpoints:  6,   200.608 MiB
srv        update:    - prompt 0x57052c8492b0:    2265 tokens, checkpoints:  2,   101.793 MiB
srv        update:    - prompt 0x57051d4ca2e0:   16214 tokens, checkpoints:  8,   524.462 MiB
srv        update:    - prompt 0x57052c5b2040:   14395 tokens, checkpoints:  2,   386.229 MiB
srv        update:    - prompt 0x57052c6d0e00:   21986 tokens, checkpoints:  8,   677.631 MiB
srv  get_availabl: prompt cache update took 572.95 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 44527 | processing task, is_child = 0
slot update_slots: id  1 | task 44527 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 4195
slot update_slots: id  1 | task 44527 | n_past = 915, slot.prompt.tokens.size() = 21986, seq_id = 1, pos_min = 21218, n_swa = 128
slot update_slots: id  1 | task 44527 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 44527 | erased invalidated context checkpoint (pos_min = 11340, pos_max = 12107, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 44527 | erased invalidated context checkpoint (pos_min = 13315, pos_max = 14082, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 44527 | erased invalidated context checkpoint (pos_min = 15290, pos_max = 16057, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 44527 | erased invalidated context checkpoint (pos_min = 15550, pos_max = 16317, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 44527 | erased invalidated context checkpoint (pos_min = 16621, pos_max = 17388, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 44527 | erased invalidated context checkpoint (pos_min = 17300, pos_max = 18067, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 44527 | erased invalidated context checkpoint (pos_min = 19463, pos_max = 20230, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 44527 | erased invalidated context checkpoint (pos_min = 21086, pos_max = 21853, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 44527 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 44527 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.488200
slot update_slots: id  1 | task 44527 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 44527 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.976400
slot update_slots: id  1 | task 44527 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 44527 | prompt processing progress, n_tokens = 4131, batch.n_tokens = 35, progress = 0.984744
slot update_slots: id  1 | task 44527 | n_tokens = 4131, memory_seq_rm [4131, end)
slot update_slots: id  1 | task 44527 | prompt processing progress, n_tokens = 4195, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 44527 | prompt done, n_tokens = 4195, batch.n_tokens = 64
slot init_sampler: id  1 | task 44527 | init sampler, took 0.60 ms, tokens: text = 4195, total = 4195
slot update_slots: id  1 | task 44527 | created context checkpoint 1 of 8 (pos_min = 3363, pos_max = 4130, size = 18.009 MiB)
slot print_timing: id  1 | task 44527 | 
prompt eval time =    4567.29 ms /  4195 tokens (    1.09 ms per token,   918.49 tokens per second)
       eval time =    2196.98 ms /   107 tokens (   20.53 ms per token,    48.70 tokens per second)
      total time =    6764.27 ms /  4302 tokens
slot      release: id  1 | task 44527 | stop processing: n_tokens = 4301, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.717 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 44638 | processing task, is_child = 0
slot update_slots: id  1 | task 44638 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5853
slot update_slots: id  1 | task 44638 | n_tokens = 4195, memory_seq_rm [4195, end)
slot update_slots: id  1 | task 44638 | prompt processing progress, n_tokens = 5789, batch.n_tokens = 1594, progress = 0.989065
slot update_slots: id  1 | task 44638 | n_tokens = 5789, memory_seq_rm [5789, end)
slot update_slots: id  1 | task 44638 | prompt processing progress, n_tokens = 5853, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 44638 | prompt done, n_tokens = 5853, batch.n_tokens = 64
slot init_sampler: id  1 | task 44638 | init sampler, took 1.16 ms, tokens: text = 5853, total = 5853
slot update_slots: id  1 | task 44638 | created context checkpoint 2 of 8 (pos_min = 5021, pos_max = 5788, size = 18.009 MiB)
slot print_timing: id  1 | task 44638 | 
prompt eval time =    2021.70 ms /  1658 tokens (    1.22 ms per token,   820.10 tokens per second)
       eval time =    2598.32 ms /   125 tokens (   20.79 ms per token,    48.11 tokens per second)
      total time =    4620.02 ms /  1783 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 44638 | stop processing: n_tokens = 5977, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.779 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 44765 | processing task, is_child = 0
slot update_slots: id  1 | task 44765 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 7511
slot update_slots: id  1 | task 44765 | n_tokens = 5853, memory_seq_rm [5853, end)
slot update_slots: id  1 | task 44765 | prompt processing progress, n_tokens = 7447, batch.n_tokens = 1594, progress = 0.991479
slot update_slots: id  1 | task 44765 | n_tokens = 7447, memory_seq_rm [7447, end)
slot update_slots: id  1 | task 44765 | prompt processing progress, n_tokens = 7511, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 44765 | prompt done, n_tokens = 7511, batch.n_tokens = 64
slot init_sampler: id  1 | task 44765 | init sampler, took 1.46 ms, tokens: text = 7511, total = 7511
slot update_slots: id  1 | task 44765 | created context checkpoint 3 of 8 (pos_min = 6679, pos_max = 7446, size = 18.009 MiB)
slot print_timing: id  1 | task 44765 | 
prompt eval time =    2058.30 ms /  1658 tokens (    1.24 ms per token,   805.52 tokens per second)
       eval time =    3027.21 ms /   145 tokens (   20.88 ms per token,    47.90 tokens per second)
      total time =    5085.51 ms /  1803 tokens
slot      release: id  1 | task 44765 | stop processing: n_tokens = 7655, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.819 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 44912 | processing task, is_child = 0
slot update_slots: id  1 | task 44912 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 9169
slot update_slots: id  1 | task 44912 | n_tokens = 7511, memory_seq_rm [7511, end)
slot update_slots: id  1 | task 44912 | prompt processing progress, n_tokens = 9105, batch.n_tokens = 1594, progress = 0.993020
slot update_slots: id  1 | task 44912 | n_tokens = 9105, memory_seq_rm [9105, end)
slot update_slots: id  1 | task 44912 | prompt processing progress, n_tokens = 9169, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 44912 | prompt done, n_tokens = 9169, batch.n_tokens = 64
slot init_sampler: id  1 | task 44912 | init sampler, took 1.26 ms, tokens: text = 9169, total = 9169
slot update_slots: id  1 | task 44912 | created context checkpoint 4 of 8 (pos_min = 8337, pos_max = 9104, size = 18.009 MiB)
slot print_timing: id  1 | task 44912 | 
prompt eval time =    2108.56 ms /  1658 tokens (    1.27 ms per token,   786.32 tokens per second)
       eval time =    2156.88 ms /   102 tokens (   21.15 ms per token,    47.29 tokens per second)
      total time =    4265.43 ms /  1760 tokens
slot      release: id  1 | task 44912 | stop processing: n_tokens = 9270, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.948 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 45016 | processing task, is_child = 0
slot update_slots: id  1 | task 45016 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 9673
slot update_slots: id  1 | task 45016 | n_tokens = 9169, memory_seq_rm [9169, end)
slot update_slots: id  1 | task 45016 | prompt processing progress, n_tokens = 9609, batch.n_tokens = 440, progress = 0.993384
slot update_slots: id  1 | task 45016 | n_tokens = 9609, memory_seq_rm [9609, end)
slot update_slots: id  1 | task 45016 | prompt processing progress, n_tokens = 9673, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 45016 | prompt done, n_tokens = 9673, batch.n_tokens = 64
slot init_sampler: id  1 | task 45016 | init sampler, took 1.36 ms, tokens: text = 9673, total = 9673
slot update_slots: id  1 | task 45016 | created context checkpoint 5 of 8 (pos_min = 8888, pos_max = 9608, size = 16.907 MiB)
slot print_timing: id  1 | task 45016 | 
prompt eval time =     694.22 ms /   504 tokens (    1.38 ms per token,   725.99 tokens per second)
       eval time =    3438.18 ms /   159 tokens (   21.62 ms per token,    46.25 tokens per second)
      total time =    4132.40 ms /   663 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 45016 | stop processing: n_tokens = 9831, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.854 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 45177 | processing task, is_child = 0
slot update_slots: id  1 | task 45177 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 11331
slot update_slots: id  1 | task 45177 | n_tokens = 9673, memory_seq_rm [9673, end)
slot update_slots: id  1 | task 45177 | prompt processing progress, n_tokens = 11267, batch.n_tokens = 1594, progress = 0.994352
slot update_slots: id  1 | task 45177 | n_tokens = 11267, memory_seq_rm [11267, end)
slot update_slots: id  1 | task 45177 | prompt processing progress, n_tokens = 11331, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 45177 | prompt done, n_tokens = 11331, batch.n_tokens = 64
slot init_sampler: id  1 | task 45177 | init sampler, took 1.55 ms, tokens: text = 11331, total = 11331
slot update_slots: id  1 | task 45177 | created context checkpoint 6 of 8 (pos_min = 10499, pos_max = 11266, size = 18.009 MiB)
slot print_timing: id  1 | task 45177 | 
prompt eval time =    2187.15 ms /  1658 tokens (    1.32 ms per token,   758.06 tokens per second)
       eval time =    8876.28 ms /   410 tokens (   21.65 ms per token,    46.19 tokens per second)
      total time =   11063.43 ms /  2068 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 45177 | stop processing: n_tokens = 11740, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.852 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 45589 | processing task, is_child = 0
slot update_slots: id  1 | task 45589 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 13306
slot update_slots: id  1 | task 45589 | n_tokens = 11331, memory_seq_rm [11331, end)
slot update_slots: id  1 | task 45589 | prompt processing progress, n_tokens = 13242, batch.n_tokens = 1911, progress = 0.995190
slot update_slots: id  1 | task 45589 | n_tokens = 13242, memory_seq_rm [13242, end)
slot update_slots: id  1 | task 45589 | prompt processing progress, n_tokens = 13306, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 45589 | prompt done, n_tokens = 13306, batch.n_tokens = 64
slot init_sampler: id  1 | task 45589 | init sampler, took 2.59 ms, tokens: text = 13306, total = 13306
slot update_slots: id  1 | task 45589 | created context checkpoint 7 of 8 (pos_min = 12474, pos_max = 13241, size = 18.009 MiB)
slot print_timing: id  1 | task 45589 | 
prompt eval time =    2645.40 ms /  1975 tokens (    1.34 ms per token,   746.58 tokens per second)
       eval time =   17786.70 ms /   798 tokens (   22.29 ms per token,    44.86 tokens per second)
      total time =   20432.11 ms /  2773 tokens
slot      release: id  1 | task 45589 | stop processing: n_tokens = 14103, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.943
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 46389 | processing task, is_child = 0
slot update_slots: id  1 | task 46389 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 13608
slot update_slots: id  1 | task 46389 | n_past = 13306, slot.prompt.tokens.size() = 14103, seq_id = 1, pos_min = 13335, n_swa = 128
slot update_slots: id  1 | task 46389 | restored context checkpoint (pos_min = 12474, pos_max = 13241, size = 18.009 MiB)
slot update_slots: id  1 | task 46389 | n_tokens = 13241, memory_seq_rm [13241, end)
slot update_slots: id  1 | task 46389 | prompt processing progress, n_tokens = 13544, batch.n_tokens = 303, progress = 0.995297
slot update_slots: id  1 | task 46389 | n_tokens = 13544, memory_seq_rm [13544, end)
slot update_slots: id  1 | task 46389 | prompt processing progress, n_tokens = 13608, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 46389 | prompt done, n_tokens = 13608, batch.n_tokens = 64
slot init_sampler: id  1 | task 46389 | init sampler, took 1.87 ms, tokens: text = 13608, total = 13608
slot update_slots: id  1 | task 46389 | created context checkpoint 8 of 8 (pos_min = 12776, pos_max = 13543, size = 18.009 MiB)
slot print_timing: id  1 | task 46389 | 
prompt eval time =     650.78 ms /   367 tokens (    1.77 ms per token,   563.94 tokens per second)
       eval time =   15880.88 ms /   715 tokens (   22.21 ms per token,    45.02 tokens per second)
      total time =   16531.67 ms /  1082 tokens
slot      release: id  1 | task 46389 | stop processing: n_tokens = 14322, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.950
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 47106 | processing task, is_child = 0
slot update_slots: id  1 | task 47106 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 13672
slot update_slots: id  1 | task 47106 | n_past = 13608, slot.prompt.tokens.size() = 14322, seq_id = 1, pos_min = 13554, n_swa = 128
slot update_slots: id  1 | task 47106 | restored context checkpoint (pos_min = 12776, pos_max = 13543, size = 18.009 MiB)
slot update_slots: id  1 | task 47106 | n_tokens = 13543, memory_seq_rm [13543, end)
slot update_slots: id  1 | task 47106 | prompt processing progress, n_tokens = 13608, batch.n_tokens = 65, progress = 0.995319
slot update_slots: id  1 | task 47106 | n_tokens = 13608, memory_seq_rm [13608, end)
slot update_slots: id  1 | task 47106 | prompt processing progress, n_tokens = 13672, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 47106 | prompt done, n_tokens = 13672, batch.n_tokens = 64
slot init_sampler: id  1 | task 47106 | init sampler, took 1.90 ms, tokens: text = 13672, total = 13672
slot print_timing: id  1 | task 47106 | 
prompt eval time =     416.59 ms /   129 tokens (    3.23 ms per token,   309.65 tokens per second)
       eval time =   10570.82 ms /   475 tokens (   22.25 ms per token,    44.94 tokens per second)
      total time =   10987.42 ms /   604 tokens
slot      release: id  1 | task 47106 | stop processing: n_tokens = 14146, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 47583 | processing task, is_child = 0
slot update_slots: id  1 | task 47583 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 13736
slot update_slots: id  1 | task 47583 | n_tokens = 13672, memory_seq_rm [13672, end)
slot update_slots: id  1 | task 47583 | prompt processing progress, n_tokens = 13736, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 47583 | prompt done, n_tokens = 13736, batch.n_tokens = 64
slot init_sampler: id  1 | task 47583 | init sampler, took 1.88 ms, tokens: text = 13736, total = 13736
slot update_slots: id  1 | task 47583 | erasing old context checkpoint (pos_min = 3363, pos_max = 4130, size = 18.009 MiB)
slot update_slots: id  1 | task 47583 | created context checkpoint 8 of 8 (pos_min = 13378, pos_max = 13671, size = 6.894 MiB)
slot print_timing: id  1 | task 47583 | 
prompt eval time =     207.00 ms /    64 tokens (    3.23 ms per token,   309.18 tokens per second)
       eval time =    5954.43 ms /   272 tokens (   21.89 ms per token,    45.68 tokens per second)
      total time =    6161.43 ms /   336 tokens
slot      release: id  1 | task 47583 | stop processing: n_tokens = 14007, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.892 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 47856 | processing task, is_child = 0
slot update_slots: id  1 | task 47856 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 15394
slot update_slots: id  1 | task 47856 | n_tokens = 13736, memory_seq_rm [13736, end)
slot update_slots: id  1 | task 47856 | prompt processing progress, n_tokens = 15330, batch.n_tokens = 1594, progress = 0.995843
slot update_slots: id  1 | task 47856 | n_tokens = 15330, memory_seq_rm [15330, end)
slot update_slots: id  1 | task 47856 | prompt processing progress, n_tokens = 15394, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 47856 | prompt done, n_tokens = 15394, batch.n_tokens = 64
slot init_sampler: id  1 | task 47856 | init sampler, took 2.52 ms, tokens: text = 15394, total = 15394
slot update_slots: id  1 | task 47856 | erasing old context checkpoint (pos_min = 5021, pos_max = 5788, size = 18.009 MiB)
slot update_slots: id  1 | task 47856 | created context checkpoint 8 of 8 (pos_min = 14562, pos_max = 15329, size = 18.009 MiB)
slot print_timing: id  1 | task 47856 | 
prompt eval time =    2326.03 ms /  1658 tokens (    1.40 ms per token,   712.80 tokens per second)
       eval time =    9909.34 ms /   444 tokens (   22.32 ms per token,    44.81 tokens per second)
      total time =   12235.38 ms /  2102 tokens
slot      release: id  1 | task 47856 | stop processing: n_tokens = 15837, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 48302 | processing task, is_child = 0
slot update_slots: id  1 | task 48302 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 15458
slot update_slots: id  1 | task 48302 | n_tokens = 15394, memory_seq_rm [15394, end)
slot update_slots: id  1 | task 48302 | prompt processing progress, n_tokens = 15458, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 48302 | prompt done, n_tokens = 15458, batch.n_tokens = 64
slot init_sampler: id  1 | task 48302 | init sampler, took 2.37 ms, tokens: text = 15458, total = 15458
slot print_timing: id  1 | task 48302 | 
prompt eval time =     207.85 ms /    64 tokens (    3.25 ms per token,   307.91 tokens per second)
       eval time =   11318.11 ms /   507 tokens (   22.32 ms per token,    44.80 tokens per second)
      total time =   11525.96 ms /   571 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 48302 | stop processing: n_tokens = 15964, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.968
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 48810 | processing task, is_child = 0
slot update_slots: id  1 | task 48810 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 15760
slot update_slots: id  1 | task 48810 | n_past = 15458, slot.prompt.tokens.size() = 15964, seq_id = 1, pos_min = 15394, n_swa = 128
slot update_slots: id  1 | task 48810 | restored context checkpoint (pos_min = 14562, pos_max = 15329, size = 18.009 MiB)
slot update_slots: id  1 | task 48810 | n_tokens = 15329, memory_seq_rm [15329, end)
slot update_slots: id  1 | task 48810 | prompt processing progress, n_tokens = 15696, batch.n_tokens = 367, progress = 0.995939
slot update_slots: id  1 | task 48810 | n_tokens = 15696, memory_seq_rm [15696, end)
slot update_slots: id  1 | task 48810 | prompt processing progress, n_tokens = 15760, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 48810 | prompt done, n_tokens = 15760, batch.n_tokens = 64
slot init_sampler: id  1 | task 48810 | init sampler, took 2.23 ms, tokens: text = 15760, total = 15760
slot update_slots: id  1 | task 48810 | erasing old context checkpoint (pos_min = 6679, pos_max = 7446, size = 18.009 MiB)
slot update_slots: id  1 | task 48810 | created context checkpoint 8 of 8 (pos_min = 14928, pos_max = 15695, size = 18.009 MiB)
slot print_timing: id  1 | task 48810 | 
prompt eval time =     738.43 ms /   431 tokens (    1.71 ms per token,   583.67 tokens per second)
       eval time =    2101.93 ms /    95 tokens (   22.13 ms per token,    45.20 tokens per second)
      total time =    2840.36 ms /   526 tokens
slot      release: id  1 | task 48810 | stop processing: n_tokens = 15854, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 48907 | processing task, is_child = 0
slot update_slots: id  1 | task 48907 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 15824
slot update_slots: id  1 | task 48907 | n_tokens = 15760, memory_seq_rm [15760, end)
slot update_slots: id  1 | task 48907 | prompt processing progress, n_tokens = 15824, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 48907 | prompt done, n_tokens = 15824, batch.n_tokens = 64
slot init_sampler: id  1 | task 48907 | init sampler, took 2.16 ms, tokens: text = 15824, total = 15824
slot print_timing: id  1 | task 48907 | 
prompt eval time =     287.02 ms /    64 tokens (    4.48 ms per token,   222.98 tokens per second)
       eval time =   10956.89 ms /   492 tokens (   22.27 ms per token,    44.90 tokens per second)
      total time =   11243.91 ms /   556 tokens
slot      release: id  1 | task 48907 | stop processing: n_tokens = 16315, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 49400 | processing task, is_child = 0
slot update_slots: id  1 | task 49400 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 15892
slot update_slots: id  1 | task 49400 | n_tokens = 15824, memory_seq_rm [15824, end)
slot update_slots: id  1 | task 49400 | prompt processing progress, n_tokens = 15828, batch.n_tokens = 4, progress = 0.995973
slot update_slots: id  1 | task 49400 | n_tokens = 15828, memory_seq_rm [15828, end)
slot update_slots: id  1 | task 49400 | prompt processing progress, n_tokens = 15892, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 49400 | prompt done, n_tokens = 15892, batch.n_tokens = 64
slot init_sampler: id  1 | task 49400 | init sampler, took 2.18 ms, tokens: text = 15892, total = 15892
slot update_slots: id  1 | task 49400 | erasing old context checkpoint (pos_min = 8337, pos_max = 9104, size = 18.009 MiB)
slot update_slots: id  1 | task 49400 | created context checkpoint 8 of 8 (pos_min = 15599, pos_max = 15827, size = 5.370 MiB)
slot print_timing: id  1 | task 49400 | 
prompt eval time =     246.31 ms /    68 tokens (    3.62 ms per token,   276.07 tokens per second)
       eval time =    3018.15 ms /   136 tokens (   22.19 ms per token,    45.06 tokens per second)
      total time =    3264.46 ms /   204 tokens
slot      release: id  1 | task 49400 | stop processing: n_tokens = 16027, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.900 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 49538 | processing task, is_child = 0
slot update_slots: id  1 | task 49538 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 17652
slot update_slots: id  1 | task 49538 | n_tokens = 15893, memory_seq_rm [15893, end)
slot update_slots: id  1 | task 49538 | prompt processing progress, n_tokens = 17588, batch.n_tokens = 1695, progress = 0.996374
slot update_slots: id  1 | task 49538 | n_tokens = 17588, memory_seq_rm [17588, end)
slot update_slots: id  1 | task 49538 | prompt processing progress, n_tokens = 17652, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 49538 | prompt done, n_tokens = 17652, batch.n_tokens = 64
slot init_sampler: id  1 | task 49538 | init sampler, took 2.44 ms, tokens: text = 17652, total = 17652
slot update_slots: id  1 | task 49538 | erasing old context checkpoint (pos_min = 8888, pos_max = 9608, size = 16.907 MiB)
slot update_slots: id  1 | task 49538 | created context checkpoint 8 of 8 (pos_min = 16820, pos_max = 17587, size = 18.009 MiB)
slot print_timing: id  1 | task 49538 | 
prompt eval time =    2544.67 ms /  1759 tokens (    1.45 ms per token,   691.25 tokens per second)
       eval time =    5220.86 ms /   230 tokens (   22.70 ms per token,    44.05 tokens per second)
      total time =    7765.53 ms /  1989 tokens
slot      release: id  1 | task 49538 | stop processing: n_tokens = 17881, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.899 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 49770 | processing task, is_child = 0
slot update_slots: id  1 | task 49770 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 19627
slot update_slots: id  1 | task 49770 | n_tokens = 17652, memory_seq_rm [17652, end)
slot update_slots: id  1 | task 49770 | prompt processing progress, n_tokens = 19563, batch.n_tokens = 1911, progress = 0.996739
slot update_slots: id  1 | task 49770 | n_tokens = 19563, memory_seq_rm [19563, end)
slot update_slots: id  1 | task 49770 | prompt processing progress, n_tokens = 19627, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 49770 | prompt done, n_tokens = 19627, batch.n_tokens = 64
slot init_sampler: id  1 | task 49770 | init sampler, took 2.69 ms, tokens: text = 19627, total = 19627
slot update_slots: id  1 | task 49770 | erasing old context checkpoint (pos_min = 10499, pos_max = 11266, size = 18.009 MiB)
slot update_slots: id  1 | task 49770 | created context checkpoint 8 of 8 (pos_min = 18795, pos_max = 19562, size = 18.009 MiB)
slot print_timing: id  1 | task 49770 | 
prompt eval time =    2865.38 ms /  1975 tokens (    1.45 ms per token,   689.26 tokens per second)
       eval time =   23414.72 ms /  1015 tokens (   23.07 ms per token,    43.35 tokens per second)
      total time =   26280.10 ms /  2990 tokens
slot      release: id  1 | task 49770 | stop processing: n_tokens = 20641, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.951
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 50787 | processing task, is_child = 0
slot update_slots: id  1 | task 50787 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 19692
slot update_slots: id  1 | task 50787 | n_past = 19627, slot.prompt.tokens.size() = 20641, seq_id = 1, pos_min = 19873, n_swa = 128
slot update_slots: id  1 | task 50787 | restored context checkpoint (pos_min = 18795, pos_max = 19562, size = 18.009 MiB)
slot update_slots: id  1 | task 50787 | n_tokens = 19562, memory_seq_rm [19562, end)
slot update_slots: id  1 | task 50787 | prompt processing progress, n_tokens = 19628, batch.n_tokens = 66, progress = 0.996750
slot update_slots: id  1 | task 50787 | n_tokens = 19628, memory_seq_rm [19628, end)
slot update_slots: id  1 | task 50787 | prompt processing progress, n_tokens = 19692, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 50787 | prompt done, n_tokens = 19692, batch.n_tokens = 64
slot init_sampler: id  1 | task 50787 | init sampler, took 3.99 ms, tokens: text = 19692, total = 19692
slot update_slots: id  1 | task 50787 | erasing old context checkpoint (pos_min = 12474, pos_max = 13241, size = 18.009 MiB)
slot update_slots: id  1 | task 50787 | created context checkpoint 8 of 8 (pos_min = 18860, pos_max = 19627, size = 18.009 MiB)
slot print_timing: id  1 | task 50787 | 
prompt eval time =     470.21 ms /   130 tokens (    3.62 ms per token,   276.47 tokens per second)
       eval time =   25186.83 ms /  1084 tokens (   23.24 ms per token,    43.04 tokens per second)
      total time =   25657.04 ms /  1214 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 50787 | stop processing: n_tokens = 20775, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.948
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 51873 | processing task, is_child = 0
slot update_slots: id  1 | task 51873 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 20548
slot update_slots: id  1 | task 51873 | n_past = 19692, slot.prompt.tokens.size() = 20775, seq_id = 1, pos_min = 20007, n_swa = 128
slot update_slots: id  1 | task 51873 | restored context checkpoint (pos_min = 18860, pos_max = 19627, size = 18.009 MiB)
slot update_slots: id  1 | task 51873 | n_tokens = 19627, memory_seq_rm [19627, end)
slot update_slots: id  1 | task 51873 | prompt processing progress, n_tokens = 20484, batch.n_tokens = 857, progress = 0.996885
slot update_slots: id  1 | task 51873 | n_tokens = 20484, memory_seq_rm [20484, end)
slot update_slots: id  1 | task 51873 | prompt processing progress, n_tokens = 20548, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 51873 | prompt done, n_tokens = 20548, batch.n_tokens = 64
slot init_sampler: id  1 | task 51873 | init sampler, took 2.87 ms, tokens: text = 20548, total = 20548
slot update_slots: id  1 | task 51873 | erasing old context checkpoint (pos_min = 12776, pos_max = 13543, size = 18.009 MiB)
slot update_slots: id  1 | task 51873 | created context checkpoint 8 of 8 (pos_min = 19716, pos_max = 20483, size = 18.009 MiB)
slot print_timing: id  1 | task 51873 | 
prompt eval time =    1466.32 ms /   921 tokens (    1.59 ms per token,   628.10 tokens per second)
       eval time =    7385.05 ms /   337 tokens (   21.91 ms per token,    45.63 tokens per second)
      total time =    8851.38 ms /  1258 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 51873 | stop processing: n_tokens = 20884, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 52212 | processing task, is_child = 0
slot update_slots: id  1 | task 52212 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 20764
slot update_slots: id  1 | task 52212 | n_tokens = 20548, memory_seq_rm [20548, end)
slot update_slots: id  1 | task 52212 | prompt processing progress, n_tokens = 20700, batch.n_tokens = 152, progress = 0.996918
slot update_slots: id  1 | task 52212 | n_tokens = 20700, memory_seq_rm [20700, end)
slot update_slots: id  1 | task 52212 | prompt processing progress, n_tokens = 20764, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 52212 | prompt done, n_tokens = 20764, batch.n_tokens = 64
slot init_sampler: id  1 | task 52212 | init sampler, took 3.96 ms, tokens: text = 20764, total = 20764
slot update_slots: id  1 | task 52212 | erasing old context checkpoint (pos_min = 13378, pos_max = 13671, size = 6.894 MiB)
slot update_slots: id  1 | task 52212 | created context checkpoint 8 of 8 (pos_min = 20116, pos_max = 20699, size = 13.694 MiB)
slot print_timing: id  1 | task 52212 | 
prompt eval time =     531.17 ms /   216 tokens (    2.46 ms per token,   406.65 tokens per second)
       eval time =   16244.38 ms /   713 tokens (   22.78 ms per token,    43.89 tokens per second)
      total time =   16775.55 ms /   929 tokens
slot      release: id  1 | task 52212 | stop processing: n_tokens = 21476, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.043
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 21476, total state size = 521.599 MiB
srv          load:  - looking for better prompt, base f_keep = 0.043, sim = 0.954
srv        update:  - cache state: 24 prompts, 7739.344 MiB (limits: 8192.000 MiB, 49152 tokens, 252068 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv        update:    - prompt 0x57052ce72cb0:    4834 tokens, checkpoints:  6,   234.281 MiB
srv        update:    - prompt 0x57052cc7a5d0:    3607 tokens, checkpoints:  2,   133.262 MiB
srv        update:    - prompt 0x57052dbadd70:    3253 tokens, checkpoints:  6,   200.608 MiB
srv        update:    - prompt 0x57052c8492b0:    2265 tokens, checkpoints:  2,   101.793 MiB
srv        update:    - prompt 0x57051d4ca2e0:   16214 tokens, checkpoints:  8,   524.462 MiB
srv        update:    - prompt 0x57052c5b2040:   14395 tokens, checkpoints:  2,   386.229 MiB
srv        update:    - prompt 0x57052c6d0e00:   21986 tokens, checkpoints:  8,   677.631 MiB
srv        update:    - prompt 0x57052ca6a250:   21476 tokens, checkpoints:  8,   648.718 MiB
srv  get_availabl: prompt cache update took 495.68 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 52927 | processing task, is_child = 0
slot update_slots: id  1 | task 52927 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 959
slot update_slots: id  1 | task 52927 | n_past = 915, slot.prompt.tokens.size() = 21476, seq_id = 1, pos_min = 20708, n_swa = 128
slot update_slots: id  1 | task 52927 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 52927 | erased invalidated context checkpoint (pos_min = 14562, pos_max = 15329, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 52927 | erased invalidated context checkpoint (pos_min = 14928, pos_max = 15695, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 52927 | erased invalidated context checkpoint (pos_min = 15599, pos_max = 15827, n_swa = 128, size = 5.370 MiB)
slot update_slots: id  1 | task 52927 | erased invalidated context checkpoint (pos_min = 16820, pos_max = 17587, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 52927 | erased invalidated context checkpoint (pos_min = 18795, pos_max = 19562, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 52927 | erased invalidated context checkpoint (pos_min = 18860, pos_max = 19627, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 52927 | erased invalidated context checkpoint (pos_min = 19716, pos_max = 20483, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 52927 | erased invalidated context checkpoint (pos_min = 20116, pos_max = 20699, n_swa = 128, size = 13.694 MiB)
slot update_slots: id  1 | task 52927 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 52927 | prompt processing progress, n_tokens = 895, batch.n_tokens = 895, progress = 0.933264
slot update_slots: id  1 | task 52927 | n_tokens = 895, memory_seq_rm [895, end)
slot update_slots: id  1 | task 52927 | prompt processing progress, n_tokens = 959, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 52927 | prompt done, n_tokens = 959, batch.n_tokens = 64
slot init_sampler: id  1 | task 52927 | init sampler, took 0.19 ms, tokens: text = 959, total = 959
slot update_slots: id  1 | task 52927 | created context checkpoint 1 of 8 (pos_min = 127, pos_max = 894, size = 18.009 MiB)
slot print_timing: id  1 | task 52927 | 
prompt eval time =    1142.07 ms /   959 tokens (    1.19 ms per token,   839.71 tokens per second)
       eval time =    1423.80 ms /    62 tokens (   22.96 ms per token,    43.55 tokens per second)
      total time =    2565.87 ms /  1021 tokens
slot      release: id  1 | task 52927 | stop processing: n_tokens = 1020, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.860 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 52991 | processing task, is_child = 0
slot update_slots: id  1 | task 52991 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 1167
slot update_slots: id  1 | task 52991 | n_tokens = 1004, memory_seq_rm [1004, end)
slot update_slots: id  1 | task 52991 | prompt processing progress, n_tokens = 1103, batch.n_tokens = 99, progress = 0.945159
slot update_slots: id  1 | task 52991 | n_tokens = 1103, memory_seq_rm [1103, end)
slot update_slots: id  1 | task 52991 | prompt processing progress, n_tokens = 1167, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 52991 | prompt done, n_tokens = 1167, batch.n_tokens = 64
slot init_sampler: id  1 | task 52991 | init sampler, took 0.17 ms, tokens: text = 1167, total = 1167
slot update_slots: id  1 | task 52991 | created context checkpoint 2 of 8 (pos_min = 335, pos_max = 1102, size = 18.009 MiB)
slot print_timing: id  1 | task 52991 | 
prompt eval time =     411.25 ms /   163 tokens (    2.52 ms per token,   396.35 tokens per second)
       eval time =     764.34 ms /    40 tokens (   19.11 ms per token,    52.33 tokens per second)
      total time =    1175.59 ms /   203 tokens
slot      release: id  1 | task 52991 | stop processing: n_tokens = 1206, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.683 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53033 | processing task, is_child = 0
slot update_slots: id  1 | task 53033 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 1731
slot update_slots: id  1 | task 53033 | n_tokens = 1183, memory_seq_rm [1183, end)
slot update_slots: id  1 | task 53033 | prompt processing progress, n_tokens = 1667, batch.n_tokens = 484, progress = 0.963027
slot update_slots: id  1 | task 53033 | n_tokens = 1667, memory_seq_rm [1667, end)
slot update_slots: id  1 | task 53033 | prompt processing progress, n_tokens = 1731, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 53033 | prompt done, n_tokens = 1731, batch.n_tokens = 64
slot init_sampler: id  1 | task 53033 | init sampler, took 0.25 ms, tokens: text = 1731, total = 1731
slot update_slots: id  1 | task 53033 | created context checkpoint 3 of 8 (pos_min = 899, pos_max = 1666, size = 18.009 MiB)
slot print_timing: id  1 | task 53033 | 
prompt eval time =     635.51 ms /   548 tokens (    1.16 ms per token,   862.29 tokens per second)
       eval time =     950.31 ms /    48 tokens (   19.80 ms per token,    50.51 tokens per second)
      total time =    1585.82 ms /   596 tokens
slot      release: id  1 | task 53033 | stop processing: n_tokens = 1778, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.561 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53083 | processing task, is_child = 0
slot update_slots: id  1 | task 53083 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3107
slot update_slots: id  1 | task 53083 | n_tokens = 1744, memory_seq_rm [1744, end)
slot update_slots: id  1 | task 53083 | prompt processing progress, n_tokens = 3043, batch.n_tokens = 1299, progress = 0.979401
slot update_slots: id  1 | task 53083 | n_tokens = 3043, memory_seq_rm [3043, end)
slot update_slots: id  1 | task 53083 | prompt processing progress, n_tokens = 3107, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 53083 | prompt done, n_tokens = 3107, batch.n_tokens = 64
slot init_sampler: id  1 | task 53083 | init sampler, took 0.43 ms, tokens: text = 3107, total = 3107
slot update_slots: id  1 | task 53083 | created context checkpoint 4 of 8 (pos_min = 2275, pos_max = 3042, size = 18.009 MiB)
slot print_timing: id  1 | task 53083 | 
prompt eval time =    1515.84 ms /  1363 tokens (    1.11 ms per token,   899.17 tokens per second)
       eval time =   12423.14 ms /   624 tokens (   19.91 ms per token,    50.23 tokens per second)
      total time =   13938.98 ms /  1987 tokens
slot      release: id  1 | task 53083 | stop processing: n_tokens = 3730, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.845 (> 0.100 thold), f_keep = 0.853
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53709 | processing task, is_child = 0
slot update_slots: id  1 | task 53709 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3765
slot update_slots: id  1 | task 53709 | n_tokens = 3181, memory_seq_rm [3181, end)
slot update_slots: id  1 | task 53709 | prompt processing progress, n_tokens = 3701, batch.n_tokens = 520, progress = 0.983001
slot update_slots: id  1 | task 53709 | n_tokens = 3701, memory_seq_rm [3701, end)
slot update_slots: id  1 | task 53709 | prompt processing progress, n_tokens = 3765, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 53709 | prompt done, n_tokens = 3765, batch.n_tokens = 64
slot init_sampler: id  1 | task 53709 | init sampler, took 0.53 ms, tokens: text = 3765, total = 3765
slot update_slots: id  1 | task 53709 | created context checkpoint 5 of 8 (pos_min = 2962, pos_max = 3700, size = 17.329 MiB)
slot print_timing: id  1 | task 53709 | 
prompt eval time =     793.62 ms /   584 tokens (    1.36 ms per token,   735.87 tokens per second)
       eval time =    1063.57 ms /    53 tokens (   20.07 ms per token,    49.83 tokens per second)
      total time =    1857.19 ms /   637 tokens
slot      release: id  1 | task 53709 | stop processing: n_tokens = 3817, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53764 | processing task, is_child = 0
slot update_slots: id  1 | task 53764 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3849
slot update_slots: id  1 | task 53764 | n_tokens = 3787, memory_seq_rm [3787, end)
slot update_slots: id  1 | task 53764 | prompt processing progress, n_tokens = 3849, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  1 | task 53764 | prompt done, n_tokens = 3849, batch.n_tokens = 62
slot init_sampler: id  1 | task 53764 | init sampler, took 0.53 ms, tokens: text = 3849, total = 3849
slot update_slots: id  1 | task 53764 | created context checkpoint 6 of 8 (pos_min = 3049, pos_max = 3786, size = 17.306 MiB)
slot print_timing: id  1 | task 53764 | 
prompt eval time =     171.69 ms /    62 tokens (    2.77 ms per token,   361.11 tokens per second)
       eval time =    1000.37 ms /    50 tokens (   20.01 ms per token,    49.98 tokens per second)
      total time =    1172.06 ms /   112 tokens
slot      release: id  1 | task 53764 | stop processing: n_tokens = 3898, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53815 | processing task, is_child = 0
slot update_slots: id  1 | task 53815 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 3930
slot update_slots: id  1 | task 53815 | n_tokens = 3868, memory_seq_rm [3868, end)
slot update_slots: id  1 | task 53815 | prompt processing progress, n_tokens = 3930, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  1 | task 53815 | prompt done, n_tokens = 3930, batch.n_tokens = 62
slot init_sampler: id  1 | task 53815 | init sampler, took 0.56 ms, tokens: text = 3930, total = 3930
slot update_slots: id  1 | task 53815 | created context checkpoint 7 of 8 (pos_min = 3130, pos_max = 3867, size = 17.306 MiB)
slot print_timing: id  1 | task 53815 | 
prompt eval time =     184.01 ms /    62 tokens (    2.97 ms per token,   336.93 tokens per second)
       eval time =    1043.92 ms /    52 tokens (   20.08 ms per token,    49.81 tokens per second)
      total time =    1227.94 ms /   114 tokens
slot      release: id  1 | task 53815 | stop processing: n_tokens = 3981, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.744 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53868 | processing task, is_child = 0
slot update_slots: id  1 | task 53868 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5314
slot update_slots: id  1 | task 53868 | n_tokens = 3951, memory_seq_rm [3951, end)
slot update_slots: id  1 | task 53868 | prompt processing progress, n_tokens = 5250, batch.n_tokens = 1299, progress = 0.987956
slot update_slots: id  1 | task 53868 | n_tokens = 5250, memory_seq_rm [5250, end)
slot update_slots: id  1 | task 53868 | prompt processing progress, n_tokens = 5314, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 53868 | prompt done, n_tokens = 5314, batch.n_tokens = 64
slot init_sampler: id  1 | task 53868 | init sampler, took 0.73 ms, tokens: text = 5314, total = 5314
slot update_slots: id  1 | task 53868 | created context checkpoint 8 of 8 (pos_min = 4482, pos_max = 5249, size = 18.009 MiB)
slot print_timing: id  1 | task 53868 | 
prompt eval time =    1607.99 ms /  1363 tokens (    1.18 ms per token,   847.64 tokens per second)
       eval time =    8897.78 ms /   439 tokens (   20.27 ms per token,    49.34 tokens per second)
      total time =   10505.77 ms /  1802 tokens
slot      release: id  1 | task 53868 | stop processing: n_tokens = 5752, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.173 (> 0.100 thold), f_keep = 0.167
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5752, total state size = 152.888 MiB
srv          load:  - looking for better prompt, base f_keep = 0.167, sim = 0.173
srv        update:  - cache state: 25 prompts, 8034.218 MiB (limits: 8192.000 MiB, 49152 tokens, 248681 est)
srv        update:    - prompt 0x57052cb8e030:    3375 tokens, checkpoints:  3,   151.177 MiB
srv        update:    - prompt 0x57052c261ca0:   17802 tokens, checkpoints:  8,   576.612 MiB
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv        update:    - prompt 0x57052ce72cb0:    4834 tokens, checkpoints:  6,   234.281 MiB
srv        update:    - prompt 0x57052cc7a5d0:    3607 tokens, checkpoints:  2,   133.262 MiB
srv        update:    - prompt 0x57052dbadd70:    3253 tokens, checkpoints:  6,   200.608 MiB
srv        update:    - prompt 0x57052c8492b0:    2265 tokens, checkpoints:  2,   101.793 MiB
srv        update:    - prompt 0x57051d4ca2e0:   16214 tokens, checkpoints:  8,   524.462 MiB
srv        update:    - prompt 0x57052c5b2040:   14395 tokens, checkpoints:  2,   386.229 MiB
srv        update:    - prompt 0x57052c6d0e00:   21986 tokens, checkpoints:  8,   677.631 MiB
srv        update:    - prompt 0x57052ca6a250:   21476 tokens, checkpoints:  8,   648.718 MiB
srv        update:    - prompt 0x57052cd041b0:    5752 tokens, checkpoints:  8,   294.873 MiB
srv  get_availabl: prompt cache update took 235.39 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 54309 | processing task, is_child = 0
slot update_slots: id  1 | task 54309 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5548
slot update_slots: id  1 | task 54309 | n_past = 959, slot.prompt.tokens.size() = 5752, seq_id = 1, pos_min = 4984, n_swa = 128
slot update_slots: id  1 | task 54309 | restored context checkpoint (pos_min = 335, pos_max = 1102, size = 18.009 MiB)
slot update_slots: id  1 | task 54309 | erased invalidated context checkpoint (pos_min = 899, pos_max = 1666, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 54309 | erased invalidated context checkpoint (pos_min = 2275, pos_max = 3042, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 54309 | erased invalidated context checkpoint (pos_min = 2962, pos_max = 3700, n_swa = 128, size = 17.329 MiB)
slot update_slots: id  1 | task 54309 | erased invalidated context checkpoint (pos_min = 3049, pos_max = 3786, n_swa = 128, size = 17.306 MiB)
slot update_slots: id  1 | task 54309 | erased invalidated context checkpoint (pos_min = 3130, pos_max = 3867, n_swa = 128, size = 17.306 MiB)
slot update_slots: id  1 | task 54309 | erased invalidated context checkpoint (pos_min = 4482, pos_max = 5249, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 54309 | n_tokens = 959, memory_seq_rm [959, end)
slot update_slots: id  1 | task 54309 | prompt processing progress, n_tokens = 3007, batch.n_tokens = 2048, progress = 0.541997
slot update_slots: id  1 | task 54309 | n_tokens = 3007, memory_seq_rm [3007, end)
slot update_slots: id  1 | task 54309 | prompt processing progress, n_tokens = 5055, batch.n_tokens = 2048, progress = 0.911139
slot update_slots: id  1 | task 54309 | n_tokens = 5055, memory_seq_rm [5055, end)
slot update_slots: id  1 | task 54309 | prompt processing progress, n_tokens = 5484, batch.n_tokens = 429, progress = 0.988464
slot update_slots: id  1 | task 54309 | n_tokens = 5484, memory_seq_rm [5484, end)
slot update_slots: id  1 | task 54309 | prompt processing progress, n_tokens = 5548, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 54309 | prompt done, n_tokens = 5548, batch.n_tokens = 64
slot init_sampler: id  1 | task 54309 | init sampler, took 0.78 ms, tokens: text = 5548, total = 5548
slot update_slots: id  1 | task 54309 | created context checkpoint 3 of 8 (pos_min = 4716, pos_max = 5483, size = 18.009 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 54309
slot      release: id  1 | task 54309 | stop processing: n_tokens = 5550, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 54317 | processing task, is_child = 0
slot update_slots: id  1 | task 54317 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5558
slot update_slots: id  1 | task 54317 | n_tokens = 5547, memory_seq_rm [5547, end)
slot update_slots: id  1 | task 54317 | prompt processing progress, n_tokens = 5558, batch.n_tokens = 11, progress = 1.000000
slot update_slots: id  1 | task 54317 | prompt done, n_tokens = 5558, batch.n_tokens = 11
slot init_sampler: id  1 | task 54317 | init sampler, took 0.96 ms, tokens: text = 5558, total = 5558
slot print_timing: id  1 | task 54317 | 
prompt eval time =      71.41 ms /    11 tokens (    6.49 ms per token,   154.05 tokens per second)
       eval time =   10960.24 ms /   520 tokens (   21.08 ms per token,    47.44 tokens per second)
      total time =   11031.64 ms /   531 tokens
slot      release: id  1 | task 54317 | stop processing: n_tokens = 6077, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.915
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 54838 | processing task, is_child = 0
slot update_slots: id  1 | task 54838 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5958
slot update_slots: id  1 | task 54838 | n_tokens = 5559, memory_seq_rm [5559, end)
slot update_slots: id  1 | task 54838 | prompt processing progress, n_tokens = 5894, batch.n_tokens = 335, progress = 0.989258
slot update_slots: id  1 | task 54838 | n_tokens = 5894, memory_seq_rm [5894, end)
slot update_slots: id  1 | task 54838 | prompt processing progress, n_tokens = 5958, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 54838 | prompt done, n_tokens = 5958, batch.n_tokens = 64
slot init_sampler: id  1 | task 54838 | init sampler, took 0.82 ms, tokens: text = 5958, total = 5958
slot update_slots: id  1 | task 54838 | created context checkpoint 4 of 8 (pos_min = 5309, pos_max = 5893, size = 13.718 MiB)
slot print_timing: id  1 | task 54838 | 
prompt eval time =     681.82 ms /   399 tokens (    1.71 ms per token,   585.20 tokens per second)
       eval time =   44168.18 ms /  2026 tokens (   21.80 ms per token,    45.87 tokens per second)
      total time =   44850.01 ms /  2425 tokens
slot      release: id  1 | task 54838 | stop processing: n_tokens = 7983, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 2939716643
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 20303, total state size = 494.093 MiB
srv          load:  - looking for better prompt, base f_keep = 0.021, sim = 0.083
srv        update:  - cache size limit reached, removing oldest entry (size = 151.177 MiB)
srv        update:  - cache size limit reached, removing oldest entry (size = 576.612 MiB)
srv        update:  - cache state: 24 prompts, 7831.194 MiB (limits: 8192.000 MiB, 49152 tokens, 254214 est)
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv        update:    - prompt 0x57052ce72cb0:    4834 tokens, checkpoints:  6,   234.281 MiB
srv        update:    - prompt 0x57052cc7a5d0:    3607 tokens, checkpoints:  2,   133.262 MiB
srv        update:    - prompt 0x57052dbadd70:    3253 tokens, checkpoints:  6,   200.608 MiB
srv        update:    - prompt 0x57052c8492b0:    2265 tokens, checkpoints:  2,   101.793 MiB
srv        update:    - prompt 0x57051d4ca2e0:   16214 tokens, checkpoints:  8,   524.462 MiB
srv        update:    - prompt 0x57052c5b2040:   14395 tokens, checkpoints:  2,   386.229 MiB
srv        update:    - prompt 0x57052c6d0e00:   21986 tokens, checkpoints:  8,   677.631 MiB
srv        update:    - prompt 0x57052ca6a250:   21476 tokens, checkpoints:  8,   648.718 MiB
srv        update:    - prompt 0x57052cd041b0:    5752 tokens, checkpoints:  8,   294.873 MiB
srv        update:    - prompt 0x57052c9921f0:   20303 tokens, checkpoints:  2,   524.765 MiB
srv  get_availabl: prompt cache update took 465.75 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 56866 | processing task, is_child = 0
slot update_slots: id  0 | task 56866 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 5117
slot update_slots: id  0 | task 56866 | n_past = 427, slot.prompt.tokens.size() = 20303, seq_id = 0, pos_min = 19535, n_swa = 128
slot update_slots: id  0 | task 56866 | restored context checkpoint (pos_min = 0, pos_max = 539, size = 12.663 MiB)
slot update_slots: id  0 | task 56866 | erased invalidated context checkpoint (pos_min = 19032, pos_max = 19799, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 56866 | n_tokens = 427, memory_seq_rm [427, end)
slot update_slots: id  0 | task 56866 | prompt processing progress, n_tokens = 2475, batch.n_tokens = 2048, progress = 0.483682
slot update_slots: id  0 | task 56866 | n_tokens = 2475, memory_seq_rm [2475, end)
slot update_slots: id  0 | task 56866 | prompt processing progress, n_tokens = 4523, batch.n_tokens = 2048, progress = 0.883916
slot update_slots: id  0 | task 56866 | n_tokens = 4523, memory_seq_rm [4523, end)
slot update_slots: id  0 | task 56866 | prompt processing progress, n_tokens = 5053, batch.n_tokens = 530, progress = 0.987493
slot update_slots: id  0 | task 56866 | n_tokens = 5053, memory_seq_rm [5053, end)
slot update_slots: id  0 | task 56866 | prompt processing progress, n_tokens = 5117, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 56866 | prompt done, n_tokens = 5117, batch.n_tokens = 64
slot init_sampler: id  0 | task 56866 | init sampler, took 1.09 ms, tokens: text = 5117, total = 5117
slot update_slots: id  0 | task 56866 | created context checkpoint 2 of 8 (pos_min = 4285, pos_max = 5052, size = 18.009 MiB)
slot print_timing: id  0 | task 56866 | 
prompt eval time =    5200.27 ms /  4690 tokens (    1.11 ms per token,   901.88 tokens per second)
       eval time =   10627.89 ms /   515 tokens (   20.64 ms per token,    48.46 tokens per second)
      total time =   15828.15 ms /  5205 tokens
slot      release: id  0 | task 56866 | stop processing: n_tokens = 5631, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.359 (> 0.100 thold), f_keep = 0.115
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7983, total state size = 205.202 MiB
srv          load:  - looking for better prompt, base f_keep = 0.115, sim = 0.359
srv        update:  - cache state: 25 prompts, 8104.141 MiB (limits: 8192.000 MiB, 49152 tokens, 253722 est)
srv        update:    - prompt 0x57052c7a0380:   14918 tokens, checkpoints:  1,   385.830 MiB
srv        update:    - prompt 0x57052c78f8a0:    4130 tokens, checkpoints:  2,   146.182 MiB
srv        update:    - prompt 0x57052c835a50:    3677 tokens, checkpoints:  1,   122.240 MiB
srv        update:    - prompt 0x57052cbe5b70:   22184 tokens, checkpoints:  8,   677.162 MiB
srv        update:    - prompt 0x57052dbdc330:    3637 tokens, checkpoints:  3,   157.320 MiB
srv        update:    - prompt 0x57052c59f760:     859 tokens, checkpoints:  1,    50.815 MiB
srv        update:    - prompt 0x57052c79aa30:    7280 tokens, checkpoints:  5,   277.919 MiB
srv        update:    - prompt 0x57052ca6bb90:    8434 tokens, checkpoints:  7,   340.950 MiB
srv        update:    - prompt 0x57052c83e960:    5355 tokens, checkpoints:  4,   215.615 MiB
srv        update:    - prompt 0x57052c8acb50:    5313 tokens, checkpoints:  2,   173.265 MiB
srv        update:    - prompt 0x57052cc5abd0:   23807 tokens, checkpoints:  8,   717.916 MiB
srv        update:    - prompt 0x57052cd7bb60:   21859 tokens, checkpoints:  2,   561.252 MiB
srv        update:    - prompt 0x57052cfe7c00:    4573 tokens, checkpoints:  2,   161.260 MiB
srv        update:    - prompt 0x57052cd69360:    2907 tokens, checkpoints:  2,   116.847 MiB
srv        update:    - prompt 0x57052ce72cb0:    4834 tokens, checkpoints:  6,   234.281 MiB
srv        update:    - prompt 0x57052cc7a5d0:    3607 tokens, checkpoints:  2,   133.262 MiB
srv        update:    - prompt 0x57052dbadd70:    3253 tokens, checkpoints:  6,   200.608 MiB
srv        update:    - prompt 0x57052c8492b0:    2265 tokens, checkpoints:  2,   101.793 MiB
srv        update:    - prompt 0x57051d4ca2e0:   16214 tokens, checkpoints:  8,   524.462 MiB
srv        update:    - prompt 0x57052c5b2040:   14395 tokens, checkpoints:  2,   386.229 MiB
srv        update:    - prompt 0x57052c6d0e00:   21986 tokens, checkpoints:  8,   677.631 MiB
srv        update:    - prompt 0x57052ca6a250:   21476 tokens, checkpoints:  8,   648.718 MiB
srv        update:    - prompt 0x57052cd041b0:    5752 tokens, checkpoints:  8,   294.873 MiB
srv        update:    - prompt 0x57052c9921f0:   20303 tokens, checkpoints:  2,   524.765 MiB
srv        update:    - prompt 0x57052cb8e030:    7983 tokens, checkpoints:  4,   272.947 MiB
srv  get_availabl: prompt cache update took 183.67 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 57385 | processing task, is_child = 0
slot update_slots: id  1 | task 57385 | new prompt, n_ctx_slot = 24576, n_keep = 0, task.n_tokens = 2552
slot update_slots: id  1 | task 57385 | n_past = 915, slot.prompt.tokens.size() = 7983, seq_id = 1, pos_min = 7215, n_swa = 128
slot update_slots: id  1 | task 57385 | restored context checkpoint (pos_min = 335, pos_max = 1102, size = 18.009 MiB)
slot update_slots: id  1 | task 57385 | erased invalidated context checkpoint (pos_min = 4716, pos_max = 5483, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  1 | task 57385 | erased invalidated context checkpoint (pos_min = 5309, pos_max = 5893, n_swa = 128, size = 13.718 MiB)
slot update_slots: id  1 | task 57385 | n_tokens = 915, memory_seq_rm [915, end)
slot update_slots: id  1 | task 57385 | prompt processing progress, n_tokens = 2488, batch.n_tokens = 1573, progress = 0.974922
slot update_slots: id  1 | task 57385 | n_tokens = 2488, memory_seq_rm [2488, end)
slot update_slots: id  1 | task 57385 | prompt processing progress, n_tokens = 2552, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 57385 | prompt done, n_tokens = 2552, batch.n_tokens = 64
slot init_sampler: id  1 | task 57385 | init sampler, took 0.54 ms, tokens: text = 2552, total = 2552
slot update_slots: id  1 | task 57385 | created context checkpoint 3 of 8 (pos_min = 1720, pos_max = 2487, size = 18.009 MiB)
slot print_timing: id  1 | task 57385 | 
prompt eval time =    1826.30 ms /  1637 tokens (    1.12 ms per token,   896.35 tokens per second)
       eval time =    6418.77 ms /   314 tokens (   20.44 ms per token,    48.92 tokens per second)
      total time =    8245.07 ms /  1951 tokens
slot      release: id  1 | task 57385 | stop processing: n_tokens = 2865, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
