ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14992 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1578 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 64000 -> need 1580 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.75 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
srv  log_server_r: request: GET /health 127.0.0.1 503
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 64000
llama_context: n_ctx_seq     = 64000
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (64000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 64000 cells
llama_kv_cache:      CUDA0 KV buffer size =  1500.00 MiB
llama_kv_cache: size = 1500.00 MiB ( 64000 cells,  12 layers,  4/1 seqs), K (f16):  750.00 MiB, V (f16):  750.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   132.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 63.11 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv  log_server_r: request: GET /health 127.0.0.1 503
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  1 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  2 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  3 | task -1 | new slot, n_ctx = 64000
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-07

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 460
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 396, batch.n_tokens = 396, progress = 0.860870
slot update_slots: id  3 | task 0 | n_tokens = 396, memory_seq_rm [396, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 460, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 460, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.08 ms, tokens: text = 460, total = 460
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 395, size = 9.286 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =     824.00 ms /   460 tokens (    1.79 ms per token,   558.25 tokens per second)
       eval time =     690.71 ms /    30 tokens (   23.02 ms per token,    43.43 tokens per second)
      total time =    1514.71 ms /   490 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 489, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.876 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 32 | processing task, is_child = 0
slot update_slots: id  3 | task 32 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 525
slot update_slots: id  3 | task 32 | n_tokens = 460, memory_seq_rm [460, end)
slot update_slots: id  3 | task 32 | prompt processing progress, n_tokens = 461, batch.n_tokens = 1, progress = 0.878095
slot update_slots: id  3 | task 32 | n_tokens = 461, memory_seq_rm [461, end)
slot update_slots: id  3 | task 32 | prompt processing progress, n_tokens = 525, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 32 | prompt done, n_tokens = 525, batch.n_tokens = 64
slot init_sampler: id  3 | task 32 | init sampler, took 0.10 ms, tokens: text = 525, total = 525
slot update_slots: id  3 | task 32 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 460, size = 10.810 MiB)
slot print_timing: id  3 | task 32 | 
prompt eval time =     254.45 ms /    65 tokens (    3.91 ms per token,   255.46 tokens per second)
       eval time =     927.37 ms /    42 tokens (   22.08 ms per token,    45.29 tokens per second)
      total time =    1181.82 ms /   107 tokens
slot      release: id  3 | task 32 | stop processing: n_tokens = 566, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.503 (> 0.100 thold), f_keep = 0.928
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 76 | processing task, is_child = 0
slot update_slots: id  3 | task 76 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1043
slot update_slots: id  3 | task 76 | n_tokens = 525, memory_seq_rm [525, end)
slot update_slots: id  3 | task 76 | prompt processing progress, n_tokens = 979, batch.n_tokens = 454, progress = 0.938639
slot update_slots: id  3 | task 76 | n_tokens = 979, memory_seq_rm [979, end)
slot update_slots: id  3 | task 76 | prompt processing progress, n_tokens = 1043, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 76 | prompt done, n_tokens = 1043, batch.n_tokens = 64
slot init_sampler: id  3 | task 76 | init sampler, took 0.23 ms, tokens: text = 1043, total = 1043
slot update_slots: id  3 | task 76 | created context checkpoint 3 of 8 (pos_min = 0, pos_max = 978, size = 22.957 MiB)
slot print_timing: id  3 | task 76 | 
prompt eval time =     538.28 ms /   518 tokens (    1.04 ms per token,   962.33 tokens per second)
       eval time =    1620.13 ms /    74 tokens (   21.89 ms per token,    45.68 tokens per second)
      total time =    2158.41 ms /   592 tokens
slot      release: id  3 | task 76 | stop processing: n_tokens = 1116, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.896 (> 0.100 thold), f_keep = 0.408
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1116, total state size = 50.181 MiB
srv          load:  - looking for better prompt, base f_keep = 0.408, sim = 0.896
srv        update:  - cache state: 1 prompts, 93.234 MiB (limits: 8192.000 MiB, 64000 tokens, 98056 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv  get_availabl: prompt cache update took 75.13 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 152 | processing task, is_child = 0
slot update_slots: id  3 | task 152 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 508
slot update_slots: id  3 | task 152 | n_tokens = 455, memory_seq_rm [455, end)
slot update_slots: id  3 | task 152 | prompt processing progress, n_tokens = 508, batch.n_tokens = 53, progress = 1.000000
slot update_slots: id  3 | task 152 | prompt done, n_tokens = 508, batch.n_tokens = 53
slot init_sampler: id  3 | task 152 | init sampler, took 0.09 ms, tokens: text = 508, total = 508
slot print_timing: id  3 | task 152 | 
prompt eval time =     256.70 ms /    53 tokens (    4.84 ms per token,   206.47 tokens per second)
       eval time =     827.91 ms /    38 tokens (   21.79 ms per token,    45.90 tokens per second)
      total time =    1084.61 ms /    91 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 152 | stop processing: n_tokens = 545, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.493 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 191 | processing task, is_child = 0
slot update_slots: id  3 | task 191 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1031
slot update_slots: id  3 | task 191 | n_tokens = 508, memory_seq_rm [508, end)
slot update_slots: id  3 | task 191 | prompt processing progress, n_tokens = 967, batch.n_tokens = 459, progress = 0.937924
slot update_slots: id  3 | task 191 | n_tokens = 967, memory_seq_rm [967, end)
slot update_slots: id  3 | task 191 | prompt processing progress, n_tokens = 1031, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 191 | prompt done, n_tokens = 1031, batch.n_tokens = 64
slot init_sampler: id  3 | task 191 | init sampler, took 0.19 ms, tokens: text = 1031, total = 1031
slot print_timing: id  3 | task 191 | 
prompt eval time =     551.78 ms /   523 tokens (    1.06 ms per token,   947.84 tokens per second)
       eval time =    1019.97 ms /    43 tokens (   23.72 ms per token,    42.16 tokens per second)
      total time =    1571.76 ms /   566 tokens
slot      release: id  3 | task 191 | stop processing: n_tokens = 1073, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.797 (> 0.100 thold), f_keep = 0.961
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 236 | processing task, is_child = 0
slot update_slots: id  3 | task 236 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1294
slot update_slots: id  3 | task 236 | n_tokens = 1031, memory_seq_rm [1031, end)
slot update_slots: id  3 | task 236 | prompt processing progress, n_tokens = 1230, batch.n_tokens = 199, progress = 0.950541
slot update_slots: id  3 | task 236 | n_tokens = 1230, memory_seq_rm [1230, end)
slot update_slots: id  3 | task 236 | prompt processing progress, n_tokens = 1294, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 236 | prompt done, n_tokens = 1294, batch.n_tokens = 64
slot init_sampler: id  3 | task 236 | init sampler, took 0.23 ms, tokens: text = 1294, total = 1294
slot update_slots: id  3 | task 236 | created context checkpoint 4 of 8 (pos_min = 381, pos_max = 1229, size = 19.908 MiB)
slot print_timing: id  3 | task 236 | 
prompt eval time =     418.01 ms /   263 tokens (    1.59 ms per token,   629.16 tokens per second)
       eval time =    1087.11 ms /    48 tokens (   22.65 ms per token,    44.15 tokens per second)
      total time =    1505.12 ms /   311 tokens
slot      release: id  3 | task 236 | stop processing: n_tokens = 1341, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 286 | processing task, is_child = 0
slot update_slots: id  3 | task 286 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1358
slot update_slots: id  3 | task 286 | n_tokens = 1294, memory_seq_rm [1294, end)
slot update_slots: id  3 | task 286 | prompt processing progress, n_tokens = 1358, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 286 | prompt done, n_tokens = 1358, batch.n_tokens = 64
slot init_sampler: id  3 | task 286 | init sampler, took 0.22 ms, tokens: text = 1358, total = 1358
slot print_timing: id  3 | task 286 | 
prompt eval time =     150.15 ms /    64 tokens (    2.35 ms per token,   426.23 tokens per second)
       eval time =    1270.87 ms /    56 tokens (   22.69 ms per token,    44.06 tokens per second)
      total time =    1421.02 ms /   120 tokens
slot      release: id  3 | task 286 | stop processing: n_tokens = 1413, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.392 (> 0.100 thold), f_keep = 0.961
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 343 | processing task, is_child = 0
slot update_slots: id  3 | task 343 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3466
slot update_slots: id  3 | task 343 | n_tokens = 1358, memory_seq_rm [1358, end)
slot update_slots: id  3 | task 343 | prompt processing progress, n_tokens = 3402, batch.n_tokens = 2044, progress = 0.981535
slot update_slots: id  3 | task 343 | n_tokens = 3402, memory_seq_rm [3402, end)
slot update_slots: id  3 | task 343 | prompt processing progress, n_tokens = 3466, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 343 | prompt done, n_tokens = 3466, batch.n_tokens = 64
slot init_sampler: id  3 | task 343 | init sampler, took 0.60 ms, tokens: text = 3466, total = 3466
slot update_slots: id  3 | task 343 | created context checkpoint 5 of 8 (pos_min = 2378, pos_max = 3401, size = 24.012 MiB)
slot print_timing: id  3 | task 343 | 
prompt eval time =    2003.19 ms /  2108 tokens (    0.95 ms per token,  1052.32 tokens per second)
       eval time =    1030.66 ms /    46 tokens (   22.41 ms per token,    44.63 tokens per second)
      total time =    3033.84 ms /  2154 tokens
slot      release: id  3 | task 343 | stop processing: n_tokens = 3511, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.774 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 391 | processing task, is_child = 0
slot update_slots: id  3 | task 391 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4476
slot update_slots: id  3 | task 391 | n_tokens = 3466, memory_seq_rm [3466, end)
slot update_slots: id  3 | task 391 | prompt processing progress, n_tokens = 4412, batch.n_tokens = 946, progress = 0.985702
slot update_slots: id  3 | task 391 | n_tokens = 4412, memory_seq_rm [4412, end)
slot update_slots: id  3 | task 391 | prompt processing progress, n_tokens = 4476, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 391 | prompt done, n_tokens = 4476, batch.n_tokens = 64
slot init_sampler: id  3 | task 391 | init sampler, took 0.76 ms, tokens: text = 4476, total = 4476
slot update_slots: id  3 | task 391 | created context checkpoint 6 of 8 (pos_min = 3388, pos_max = 4411, size = 24.012 MiB)
slot print_timing: id  3 | task 391 | 
prompt eval time =    1052.99 ms /  1010 tokens (    1.04 ms per token,   959.18 tokens per second)
       eval time =    1998.69 ms /    86 tokens (   23.24 ms per token,    43.03 tokens per second)
      total time =    3051.68 ms /  1096 tokens
slot      release: id  3 | task 391 | stop processing: n_tokens = 4561, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.929 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 479 | processing task, is_child = 0
slot update_slots: id  3 | task 479 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4820
slot update_slots: id  3 | task 479 | n_tokens = 4476, memory_seq_rm [4476, end)
slot update_slots: id  3 | task 479 | prompt processing progress, n_tokens = 4756, batch.n_tokens = 280, progress = 0.986722
slot update_slots: id  3 | task 479 | n_tokens = 4756, memory_seq_rm [4756, end)
slot update_slots: id  3 | task 479 | prompt processing progress, n_tokens = 4820, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 479 | prompt done, n_tokens = 4820, batch.n_tokens = 64
slot init_sampler: id  3 | task 479 | init sampler, took 0.74 ms, tokens: text = 4820, total = 4820
slot update_slots: id  3 | task 479 | created context checkpoint 7 of 8 (pos_min = 3732, pos_max = 4755, size = 24.012 MiB)
slot print_timing: id  3 | task 479 | 
prompt eval time =     485.01 ms /   344 tokens (    1.41 ms per token,   709.27 tokens per second)
       eval time =    2583.20 ms /   108 tokens (   23.92 ms per token,    41.81 tokens per second)
      total time =    3068.20 ms /   452 tokens
slot      release: id  3 | task 479 | stop processing: n_tokens = 4927, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.828 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 589 | processing task, is_child = 0
slot update_slots: id  3 | task 589 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5818
slot update_slots: id  3 | task 589 | n_tokens = 4820, memory_seq_rm [4820, end)
slot update_slots: id  3 | task 589 | prompt processing progress, n_tokens = 5754, batch.n_tokens = 934, progress = 0.989000
slot update_slots: id  3 | task 589 | n_tokens = 5754, memory_seq_rm [5754, end)
slot update_slots: id  3 | task 589 | prompt processing progress, n_tokens = 5818, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 589 | prompt done, n_tokens = 5818, batch.n_tokens = 64
slot init_sampler: id  3 | task 589 | init sampler, took 1.06 ms, tokens: text = 5818, total = 5818
slot update_slots: id  3 | task 589 | created context checkpoint 8 of 8 (pos_min = 4730, pos_max = 5753, size = 24.012 MiB)
slot print_timing: id  3 | task 589 | 
prompt eval time =    1113.22 ms /   998 tokens (    1.12 ms per token,   896.50 tokens per second)
       eval time =   57127.66 ms /  2373 tokens (   24.07 ms per token,    41.54 tokens per second)
      total time =   58240.88 ms /  3371 tokens
slot      release: id  3 | task 589 | stop processing: n_tokens = 8190, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.835 (> 0.100 thold), f_keep = 0.710
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2964 | processing task, is_child = 0
slot update_slots: id  3 | task 2964 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6971
slot update_slots: id  3 | task 2964 | n_past = 5818, slot.prompt.tokens.size() = 8190, seq_id = 3, pos_min = 7166, n_swa = 128
slot update_slots: id  3 | task 2964 | restored context checkpoint (pos_min = 4730, pos_max = 5753, size = 24.012 MiB)
slot update_slots: id  3 | task 2964 | n_tokens = 5753, memory_seq_rm [5753, end)
slot update_slots: id  3 | task 2964 | prompt processing progress, n_tokens = 6907, batch.n_tokens = 1154, progress = 0.990819
slot update_slots: id  3 | task 2964 | n_tokens = 6907, memory_seq_rm [6907, end)
slot update_slots: id  3 | task 2964 | prompt processing progress, n_tokens = 6971, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2964 | prompt done, n_tokens = 6971, batch.n_tokens = 64
slot init_sampler: id  3 | task 2964 | init sampler, took 1.14 ms, tokens: text = 6971, total = 6971
slot update_slots: id  3 | task 2964 | erasing old context checkpoint (pos_min = 0, pos_max = 395, size = 9.286 MiB)
slot update_slots: id  3 | task 2964 | created context checkpoint 8 of 8 (pos_min = 5883, pos_max = 6906, size = 24.012 MiB)
slot print_timing: id  3 | task 2964 | 
prompt eval time =    1449.23 ms /  1218 tokens (    1.19 ms per token,   840.45 tokens per second)
       eval time =   30866.62 ms /  1294 tokens (   23.85 ms per token,    41.92 tokens per second)
      total time =   32315.85 ms /  2512 tokens
slot      release: id  3 | task 2964 | stop processing: n_tokens = 8264, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.847 (> 0.100 thold), f_keep = 0.844
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4260 | processing task, is_child = 0
slot update_slots: id  3 | task 4260 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8228
slot update_slots: id  3 | task 4260 | n_past = 6971, slot.prompt.tokens.size() = 8264, seq_id = 3, pos_min = 7240, n_swa = 128
slot update_slots: id  3 | task 4260 | restored context checkpoint (pos_min = 5883, pos_max = 6906, size = 24.012 MiB)
slot update_slots: id  3 | task 4260 | n_tokens = 6906, memory_seq_rm [6906, end)
slot update_slots: id  3 | task 4260 | prompt processing progress, n_tokens = 8164, batch.n_tokens = 1258, progress = 0.992222
slot update_slots: id  3 | task 4260 | n_tokens = 8164, memory_seq_rm [8164, end)
slot update_slots: id  3 | task 4260 | prompt processing progress, n_tokens = 8228, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4260 | prompt done, n_tokens = 8228, batch.n_tokens = 64
slot init_sampler: id  3 | task 4260 | init sampler, took 2.35 ms, tokens: text = 8228, total = 8228
slot update_slots: id  3 | task 4260 | erasing old context checkpoint (pos_min = 0, pos_max = 460, size = 10.810 MiB)
slot update_slots: id  3 | task 4260 | created context checkpoint 8 of 8 (pos_min = 7140, pos_max = 8163, size = 24.012 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 4260
slot      release: id  3 | task 4260 | stop processing: n_tokens = 9398, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.054
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9398, total state size = 244.385 MiB
srv          load:  - looking for better prompt, base f_keep = 0.054, sim = 1.000
srv        update:  - cache state: 2 prompts, 524.557 MiB (limits: 8192.000 MiB, 64000 tokens, 164196 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv  get_availabl: prompt cache update took 317.50 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5434 | processing task, is_child = 0
slot update_slots: id  3 | task 5434 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 508
slot update_slots: id  3 | task 5434 | n_past = 508, slot.prompt.tokens.size() = 9398, seq_id = 3, pos_min = 8374, n_swa = 128
slot update_slots: id  3 | task 5434 | restored context checkpoint (pos_min = 0, pos_max = 978, size = 22.957 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 381, pos_max = 1229, n_swa = 128, size = 19.908 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 2378, pos_max = 3401, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 3388, pos_max = 4411, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 3732, pos_max = 4755, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 4730, pos_max = 5753, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 5883, pos_max = 6906, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | erased invalidated context checkpoint (pos_min = 7140, pos_max = 8163, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5434 | need to evaluate at least 1 token for each active slot (n_past = 508, task.n_tokens() = 508)
slot update_slots: id  3 | task 5434 | n_past was set to 507
slot update_slots: id  3 | task 5434 | n_tokens = 507, memory_seq_rm [507, end)
slot update_slots: id  3 | task 5434 | prompt processing progress, n_tokens = 508, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  3 | task 5434 | prompt done, n_tokens = 508, batch.n_tokens = 1
slot init_sampler: id  3 | task 5434 | init sampler, took 0.08 ms, tokens: text = 508, total = 508
slot print_timing: id  3 | task 5434 | 
prompt eval time =      50.10 ms /     1 tokens (   50.10 ms per token,    19.96 tokens per second)
       eval time =    1271.03 ms /    53 tokens (   23.98 ms per token,    41.70 tokens per second)
      total time =    1321.13 ms /    54 tokens
slot      release: id  3 | task 5434 | stop processing: n_tokens = 560, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.887 (> 0.100 thold), f_keep = 0.907
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5488 | processing task, is_child = 0
slot update_slots: id  3 | task 5488 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 573
slot update_slots: id  3 | task 5488 | n_tokens = 508, memory_seq_rm [508, end)
slot update_slots: id  3 | task 5488 | prompt processing progress, n_tokens = 509, batch.n_tokens = 1, progress = 0.888307
slot update_slots: id  3 | task 5488 | n_tokens = 509, memory_seq_rm [509, end)
slot update_slots: id  3 | task 5488 | prompt processing progress, n_tokens = 573, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5488 | prompt done, n_tokens = 573, batch.n_tokens = 64
slot init_sampler: id  3 | task 5488 | init sampler, took 0.14 ms, tokens: text = 573, total = 573
slot print_timing: id  3 | task 5488 | 
prompt eval time =     284.62 ms /    65 tokens (    4.38 ms per token,   228.38 tokens per second)
       eval time =     956.40 ms /    43 tokens (   22.24 ms per token,    44.96 tokens per second)
      total time =    1241.01 ms /   108 tokens
slot      release: id  3 | task 5488 | stop processing: n_tokens = 615, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.526 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5533 | processing task, is_child = 0
slot update_slots: id  3 | task 5533 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1090
slot update_slots: id  3 | task 5533 | n_tokens = 573, memory_seq_rm [573, end)
slot update_slots: id  3 | task 5533 | prompt processing progress, n_tokens = 1026, batch.n_tokens = 453, progress = 0.941284
slot update_slots: id  3 | task 5533 | n_tokens = 1026, memory_seq_rm [1026, end)
slot update_slots: id  3 | task 5533 | prompt processing progress, n_tokens = 1090, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5533 | prompt done, n_tokens = 1090, batch.n_tokens = 64
slot init_sampler: id  3 | task 5533 | init sampler, took 0.18 ms, tokens: text = 1090, total = 1090
slot print_timing: id  3 | task 5533 | 
prompt eval time =     559.43 ms /   517 tokens (    1.08 ms per token,   924.15 tokens per second)
       eval time =    1261.06 ms /    54 tokens (   23.35 ms per token,    42.82 tokens per second)
      total time =    1820.49 ms /   571 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5533 | stop processing: n_tokens = 1143, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.809 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5589 | processing task, is_child = 0
slot update_slots: id  3 | task 5589 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1348
slot update_slots: id  3 | task 5589 | n_tokens = 1090, memory_seq_rm [1090, end)
slot update_slots: id  3 | task 5589 | prompt processing progress, n_tokens = 1284, batch.n_tokens = 194, progress = 0.952522
slot update_slots: id  3 | task 5589 | n_tokens = 1284, memory_seq_rm [1284, end)
slot update_slots: id  3 | task 5589 | prompt processing progress, n_tokens = 1348, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5589 | prompt done, n_tokens = 1348, batch.n_tokens = 64
slot init_sampler: id  3 | task 5589 | init sampler, took 0.25 ms, tokens: text = 1348, total = 1348
slot update_slots: id  3 | task 5589 | created context checkpoint 2 of 8 (pos_min = 260, pos_max = 1283, size = 24.012 MiB)
slot print_timing: id  3 | task 5589 | 
prompt eval time =     431.68 ms /   258 tokens (    1.67 ms per token,   597.66 tokens per second)
       eval time =     690.81 ms /    29 tokens (   23.82 ms per token,    41.98 tokens per second)
      total time =    1122.49 ms /   287 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5589 | stop processing: n_tokens = 1376, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5620 | processing task, is_child = 0
slot update_slots: id  3 | task 5620 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1407
slot update_slots: id  3 | task 5620 | n_tokens = 1348, memory_seq_rm [1348, end)
slot update_slots: id  3 | task 5620 | prompt processing progress, n_tokens = 1407, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 5620 | prompt done, n_tokens = 1407, batch.n_tokens = 59
slot init_sampler: id  3 | task 5620 | init sampler, took 0.27 ms, tokens: text = 1407, total = 1407
slot print_timing: id  3 | task 5620 | 
prompt eval time =     152.63 ms /    59 tokens (    2.59 ms per token,   386.56 tokens per second)
       eval time =    1010.99 ms /    42 tokens (   24.07 ms per token,    41.54 tokens per second)
      total time =    1163.62 ms /   101 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5620 | stop processing: n_tokens = 1448, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.401 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5663 | processing task, is_child = 0
slot update_slots: id  3 | task 5663 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3510
slot update_slots: id  3 | task 5663 | n_tokens = 1407, memory_seq_rm [1407, end)
slot update_slots: id  3 | task 5663 | prompt processing progress, n_tokens = 3446, batch.n_tokens = 2039, progress = 0.981766
slot update_slots: id  3 | task 5663 | n_tokens = 3446, memory_seq_rm [3446, end)
slot update_slots: id  3 | task 5663 | prompt processing progress, n_tokens = 3510, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5663 | prompt done, n_tokens = 3510, batch.n_tokens = 64
slot init_sampler: id  3 | task 5663 | init sampler, took 0.57 ms, tokens: text = 3510, total = 3510
slot update_slots: id  3 | task 5663 | created context checkpoint 3 of 8 (pos_min = 2422, pos_max = 3445, size = 24.012 MiB)
slot print_timing: id  3 | task 5663 | 
prompt eval time =    2082.99 ms /  2103 tokens (    0.99 ms per token,  1009.61 tokens per second)
       eval time =     848.18 ms /    36 tokens (   23.56 ms per token,    42.44 tokens per second)
      total time =    2931.17 ms /  2139 tokens
slot      release: id  3 | task 5663 | stop processing: n_tokens = 3545, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.777 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5701 | processing task, is_child = 0
slot update_slots: id  3 | task 5701 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4515
slot update_slots: id  3 | task 5701 | n_tokens = 3510, memory_seq_rm [3510, end)
slot update_slots: id  3 | task 5701 | prompt processing progress, n_tokens = 4451, batch.n_tokens = 941, progress = 0.985825
slot update_slots: id  3 | task 5701 | n_tokens = 4451, memory_seq_rm [4451, end)
slot update_slots: id  3 | task 5701 | prompt processing progress, n_tokens = 4515, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5701 | prompt done, n_tokens = 4515, batch.n_tokens = 64
slot init_sampler: id  3 | task 5701 | init sampler, took 0.69 ms, tokens: text = 4515, total = 4515
slot update_slots: id  3 | task 5701 | created context checkpoint 4 of 8 (pos_min = 3427, pos_max = 4450, size = 24.012 MiB)
slot print_timing: id  3 | task 5701 | 
prompt eval time =    1090.42 ms /  1005 tokens (    1.08 ms per token,   921.66 tokens per second)
       eval time =    2971.23 ms /   124 tokens (   23.96 ms per token,    41.73 tokens per second)
      total time =    4061.65 ms /  1129 tokens
slot      release: id  3 | task 5701 | stop processing: n_tokens = 4638, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.930 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5827 | processing task, is_child = 0
slot update_slots: id  3 | task 5827 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4854
slot update_slots: id  3 | task 5827 | n_tokens = 4515, memory_seq_rm [4515, end)
slot update_slots: id  3 | task 5827 | prompt processing progress, n_tokens = 4790, batch.n_tokens = 275, progress = 0.986815
slot update_slots: id  3 | task 5827 | n_tokens = 4790, memory_seq_rm [4790, end)
slot update_slots: id  3 | task 5827 | prompt processing progress, n_tokens = 4854, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5827 | prompt done, n_tokens = 4854, batch.n_tokens = 64
slot init_sampler: id  3 | task 5827 | init sampler, took 0.87 ms, tokens: text = 4854, total = 4854
slot update_slots: id  3 | task 5827 | created context checkpoint 5 of 8 (pos_min = 3766, pos_max = 4789, size = 24.012 MiB)
slot print_timing: id  3 | task 5827 | 
prompt eval time =     495.91 ms /   339 tokens (    1.46 ms per token,   683.59 tokens per second)
       eval time =   55053.84 ms /  2302 tokens (   23.92 ms per token,    41.81 tokens per second)
      total time =   55549.75 ms /  2641 tokens
slot      release: id  3 | task 5827 | stop processing: n_tokens = 7155, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.808 (> 0.100 thold), f_keep = 0.678
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8131 | processing task, is_child = 0
slot update_slots: id  3 | task 8131 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6004
slot update_slots: id  3 | task 8131 | n_past = 4854, slot.prompt.tokens.size() = 7155, seq_id = 3, pos_min = 6131, n_swa = 128
slot update_slots: id  3 | task 8131 | restored context checkpoint (pos_min = 3766, pos_max = 4789, size = 24.012 MiB)
slot update_slots: id  3 | task 8131 | n_tokens = 4789, memory_seq_rm [4789, end)
slot update_slots: id  3 | task 8131 | prompt processing progress, n_tokens = 5940, batch.n_tokens = 1151, progress = 0.989340
slot update_slots: id  3 | task 8131 | n_tokens = 5940, memory_seq_rm [5940, end)
slot update_slots: id  3 | task 8131 | prompt processing progress, n_tokens = 6004, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8131 | prompt done, n_tokens = 6004, batch.n_tokens = 64
slot init_sampler: id  3 | task 8131 | init sampler, took 0.94 ms, tokens: text = 6004, total = 6004
slot update_slots: id  3 | task 8131 | created context checkpoint 6 of 8 (pos_min = 4916, pos_max = 5939, size = 24.012 MiB)
slot print_timing: id  3 | task 8131 | 
prompt eval time =    1491.36 ms /  1215 tokens (    1.23 ms per token,   814.69 tokens per second)
       eval time =   35281.53 ms /  1483 tokens (   23.79 ms per token,    42.03 tokens per second)
      total time =   36772.89 ms /  2698 tokens
slot      release: id  3 | task 8131 | stop processing: n_tokens = 7486, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.827 (> 0.100 thold), f_keep = 0.802
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9616 | processing task, is_child = 0
slot update_slots: id  3 | task 9616 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7261
slot update_slots: id  3 | task 9616 | n_past = 6004, slot.prompt.tokens.size() = 7486, seq_id = 3, pos_min = 6462, n_swa = 128
slot update_slots: id  3 | task 9616 | restored context checkpoint (pos_min = 4916, pos_max = 5939, size = 24.012 MiB)
slot update_slots: id  3 | task 9616 | n_tokens = 5939, memory_seq_rm [5939, end)
slot update_slots: id  3 | task 9616 | prompt processing progress, n_tokens = 7197, batch.n_tokens = 1258, progress = 0.991186
slot update_slots: id  3 | task 9616 | n_tokens = 7197, memory_seq_rm [7197, end)
slot update_slots: id  3 | task 9616 | prompt processing progress, n_tokens = 7261, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9616 | prompt done, n_tokens = 7261, batch.n_tokens = 64
slot init_sampler: id  3 | task 9616 | init sampler, took 1.09 ms, tokens: text = 7261, total = 7261
slot update_slots: id  3 | task 9616 | created context checkpoint 7 of 8 (pos_min = 6173, pos_max = 7196, size = 24.012 MiB)
slot print_timing: id  3 | task 9616 | 
prompt eval time =    1556.33 ms /  1322 tokens (    1.18 ms per token,   849.43 tokens per second)
       eval time =   25539.62 ms /  1060 tokens (   24.09 ms per token,    41.50 tokens per second)
      total time =   27095.95 ms /  2382 tokens
slot      release: id  3 | task 9616 | stop processing: n_tokens = 8320, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.873
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 10678 | processing task, is_child = 0
slot update_slots: id  3 | task 10678 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7340
slot update_slots: id  3 | task 10678 | n_past = 7261, slot.prompt.tokens.size() = 8320, seq_id = 3, pos_min = 7296, n_swa = 128
slot update_slots: id  3 | task 10678 | restored context checkpoint (pos_min = 6173, pos_max = 7196, size = 24.012 MiB)
slot update_slots: id  3 | task 10678 | n_tokens = 7196, memory_seq_rm [7196, end)
slot update_slots: id  3 | task 10678 | prompt processing progress, n_tokens = 7276, batch.n_tokens = 80, progress = 0.991281
slot update_slots: id  3 | task 10678 | n_tokens = 7276, memory_seq_rm [7276, end)
slot update_slots: id  3 | task 10678 | prompt processing progress, n_tokens = 7340, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 10678 | prompt done, n_tokens = 7340, batch.n_tokens = 64
slot init_sampler: id  3 | task 10678 | init sampler, took 1.08 ms, tokens: text = 7340, total = 7340
slot update_slots: id  3 | task 10678 | created context checkpoint 8 of 8 (pos_min = 6252, pos_max = 7275, size = 24.012 MiB)
slot print_timing: id  3 | task 10678 | 
prompt eval time =     366.02 ms /   144 tokens (    2.54 ms per token,   393.42 tokens per second)
       eval time =  109835.82 ms /  4482 tokens (   24.51 ms per token,    40.81 tokens per second)
      total time =  110201.83 ms /  4626 tokens
slot      release: id  3 | task 10678 | stop processing: n_tokens = 11821, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.621
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 15162 | processing task, is_child = 0
slot update_slots: id  3 | task 15162 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7598
slot update_slots: id  3 | task 15162 | n_past = 7340, slot.prompt.tokens.size() = 11821, seq_id = 3, pos_min = 10797, n_swa = 128
slot update_slots: id  3 | task 15162 | restored context checkpoint (pos_min = 6252, pos_max = 7275, size = 24.012 MiB)
slot update_slots: id  3 | task 15162 | n_tokens = 7275, memory_seq_rm [7275, end)
slot update_slots: id  3 | task 15162 | prompt processing progress, n_tokens = 7534, batch.n_tokens = 259, progress = 0.991577
slot update_slots: id  3 | task 15162 | n_tokens = 7534, memory_seq_rm [7534, end)
slot update_slots: id  3 | task 15162 | prompt processing progress, n_tokens = 7598, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 15162 | prompt done, n_tokens = 7598, batch.n_tokens = 64
slot init_sampler: id  3 | task 15162 | init sampler, took 1.08 ms, tokens: text = 7598, total = 7598
slot update_slots: id  3 | task 15162 | erasing old context checkpoint (pos_min = 0, pos_max = 978, size = 22.957 MiB)
slot update_slots: id  3 | task 15162 | created context checkpoint 8 of 8 (pos_min = 6510, pos_max = 7533, size = 24.012 MiB)
slot print_timing: id  3 | task 15162 | 
prompt eval time =     488.43 ms /   323 tokens (    1.51 ms per token,   661.30 tokens per second)
       eval time =  152999.24 ms /  6204 tokens (   24.66 ms per token,    40.55 tokens per second)
      total time =  153487.68 ms /  6527 tokens
slot      release: id  3 | task 15162 | stop processing: n_tokens = 13801, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.869 (> 0.100 thold), f_keep = 0.551
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21368 | processing task, is_child = 0
slot update_slots: id  3 | task 21368 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8746
slot update_slots: id  3 | task 21368 | n_past = 7598, slot.prompt.tokens.size() = 13801, seq_id = 3, pos_min = 12777, n_swa = 128
slot update_slots: id  3 | task 21368 | restored context checkpoint (pos_min = 6510, pos_max = 7533, size = 24.012 MiB)
slot update_slots: id  3 | task 21368 | n_tokens = 7533, memory_seq_rm [7533, end)
slot update_slots: id  3 | task 21368 | prompt processing progress, n_tokens = 8682, batch.n_tokens = 1149, progress = 0.992682
slot update_slots: id  3 | task 21368 | n_tokens = 8682, memory_seq_rm [8682, end)
slot update_slots: id  3 | task 21368 | prompt processing progress, n_tokens = 8746, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21368 | prompt done, n_tokens = 8746, batch.n_tokens = 64
slot init_sampler: id  3 | task 21368 | init sampler, took 1.29 ms, tokens: text = 8746, total = 8746
slot update_slots: id  3 | task 21368 | erasing old context checkpoint (pos_min = 260, pos_max = 1283, size = 24.012 MiB)
slot update_slots: id  3 | task 21368 | created context checkpoint 8 of 8 (pos_min = 7658, pos_max = 8681, size = 24.012 MiB)
slot print_timing: id  3 | task 21368 | 
prompt eval time =    1497.03 ms /  1213 tokens (    1.23 ms per token,   810.27 tokens per second)
       eval time =     718.79 ms /    29 tokens (   24.79 ms per token,    40.35 tokens per second)
      total time =    2215.82 ms /  1242 tokens
slot      release: id  3 | task 21368 | stop processing: n_tokens = 8774, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21399 | processing task, is_child = 0
slot update_slots: id  3 | task 21399 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9085
slot update_slots: id  3 | task 21399 | n_tokens = 8746, memory_seq_rm [8746, end)
slot update_slots: id  3 | task 21399 | prompt processing progress, n_tokens = 9021, batch.n_tokens = 275, progress = 0.992955
slot update_slots: id  3 | task 21399 | n_tokens = 9021, memory_seq_rm [9021, end)
slot update_slots: id  3 | task 21399 | prompt processing progress, n_tokens = 9085, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21399 | prompt done, n_tokens = 9085, batch.n_tokens = 64
slot init_sampler: id  3 | task 21399 | init sampler, took 1.38 ms, tokens: text = 9085, total = 9085
slot update_slots: id  3 | task 21399 | erasing old context checkpoint (pos_min = 2422, pos_max = 3445, size = 24.012 MiB)
slot update_slots: id  3 | task 21399 | created context checkpoint 8 of 8 (pos_min = 7997, pos_max = 9020, size = 24.012 MiB)
slot print_timing: id  3 | task 21399 | 
prompt eval time =     564.44 ms /   339 tokens (    1.67 ms per token,   600.59 tokens per second)
       eval time =    7396.41 ms /   307 tokens (   24.09 ms per token,    41.51 tokens per second)
      total time =    7960.85 ms /   646 tokens
slot      release: id  3 | task 21399 | stop processing: n_tokens = 9391, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21708 | processing task, is_child = 0
slot update_slots: id  3 | task 21708 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9240
slot update_slots: id  3 | task 21708 | n_tokens = 9085, memory_seq_rm [9085, end)
slot update_slots: id  3 | task 21708 | prompt processing progress, n_tokens = 9176, batch.n_tokens = 91, progress = 0.993074
slot update_slots: id  3 | task 21708 | n_tokens = 9176, memory_seq_rm [9176, end)
slot update_slots: id  3 | task 21708 | prompt processing progress, n_tokens = 9240, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21708 | prompt done, n_tokens = 9240, batch.n_tokens = 64
slot init_sampler: id  3 | task 21708 | init sampler, took 1.37 ms, tokens: text = 9240, total = 9240
slot update_slots: id  3 | task 21708 | erasing old context checkpoint (pos_min = 3427, pos_max = 4450, size = 24.012 MiB)
slot update_slots: id  3 | task 21708 | created context checkpoint 8 of 8 (pos_min = 8367, pos_max = 9175, size = 18.970 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 21708
slot      release: id  3 | task 21708 | stop processing: n_tokens = 9556, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.948 (> 0.100 thold), f_keep = 0.048
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9556, total state size = 245.019 MiB
srv          load:  - looking for better prompt, base f_keep = 0.048, sim = 0.948
srv        update:  - cache state: 3 prompts, 956.630 MiB (limits: 8192.000 MiB, 64000 tokens, 171867 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv        update:    - prompt 0x5d0e7b4386e0:    9556 tokens, checkpoints:  8,   432.073 MiB
srv  get_availabl: prompt cache update took 328.24 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22028 | processing task, is_child = 0
slot update_slots: id  3 | task 22028 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 480
slot update_slots: id  3 | task 22028 | n_past = 455, slot.prompt.tokens.size() = 9556, seq_id = 3, pos_min = 8663, n_swa = 128
slot update_slots: id  3 | task 22028 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 3766, pos_max = 4789, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 4916, pos_max = 5939, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 6173, pos_max = 7196, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 6252, pos_max = 7275, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 6510, pos_max = 7533, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 7658, pos_max = 8681, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 7997, pos_max = 9020, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 22028 | erased invalidated context checkpoint (pos_min = 8367, pos_max = 9175, n_swa = 128, size = 18.970 MiB)
slot update_slots: id  3 | task 22028 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 22028 | prompt processing progress, n_tokens = 416, batch.n_tokens = 416, progress = 0.866667
slot update_slots: id  3 | task 22028 | n_tokens = 416, memory_seq_rm [416, end)
slot update_slots: id  3 | task 22028 | prompt processing progress, n_tokens = 480, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22028 | prompt done, n_tokens = 480, batch.n_tokens = 64
slot init_sampler: id  3 | task 22028 | init sampler, took 0.08 ms, tokens: text = 480, total = 480
slot update_slots: id  3 | task 22028 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 415, size = 9.755 MiB)
slot print_timing: id  3 | task 22028 | 
prompt eval time =     592.86 ms /   480 tokens (    1.24 ms per token,   809.63 tokens per second)
       eval time =    2048.57 ms /    92 tokens (   22.27 ms per token,    44.91 tokens per second)
      total time =    2641.43 ms /   572 tokens
slot      release: id  3 | task 22028 | stop processing: n_tokens = 571, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.467 (> 0.100 thold), f_keep = 0.841
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22122 | processing task, is_child = 0
slot update_slots: id  3 | task 22122 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1027
slot update_slots: id  3 | task 22122 | n_tokens = 480, memory_seq_rm [480, end)
slot update_slots: id  3 | task 22122 | prompt processing progress, n_tokens = 963, batch.n_tokens = 483, progress = 0.937683
slot update_slots: id  3 | task 22122 | n_tokens = 963, memory_seq_rm [963, end)
slot update_slots: id  3 | task 22122 | prompt processing progress, n_tokens = 1027, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22122 | prompt done, n_tokens = 1027, batch.n_tokens = 64
slot init_sampler: id  3 | task 22122 | init sampler, took 0.18 ms, tokens: text = 1027, total = 1027
slot update_slots: id  3 | task 22122 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 962, size = 22.582 MiB)
slot print_timing: id  3 | task 22122 | 
prompt eval time =     576.50 ms /   547 tokens (    1.05 ms per token,   948.82 tokens per second)
       eval time =    1645.14 ms /    72 tokens (   22.85 ms per token,    43.77 tokens per second)
      total time =    2221.64 ms /   619 tokens
slot      release: id  3 | task 22122 | stop processing: n_tokens = 1098, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.491 (> 0.100 thold), f_keep = 0.935
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22196 | processing task, is_child = 0
slot update_slots: id  3 | task 22196 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2092
slot update_slots: id  3 | task 22196 | n_tokens = 1027, memory_seq_rm [1027, end)
slot update_slots: id  3 | task 22196 | prompt processing progress, n_tokens = 2028, batch.n_tokens = 1001, progress = 0.969407
slot update_slots: id  3 | task 22196 | n_tokens = 2028, memory_seq_rm [2028, end)
slot update_slots: id  3 | task 22196 | prompt processing progress, n_tokens = 2092, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22196 | prompt done, n_tokens = 2092, batch.n_tokens = 64
slot init_sampler: id  3 | task 22196 | init sampler, took 0.41 ms, tokens: text = 2092, total = 2092
slot update_slots: id  3 | task 22196 | created context checkpoint 3 of 8 (pos_min = 1004, pos_max = 2027, size = 24.012 MiB)
slot print_timing: id  3 | task 22196 | 
prompt eval time =    1078.40 ms /  1065 tokens (    1.01 ms per token,   987.57 tokens per second)
       eval time =    4087.54 ms /   176 tokens (   23.22 ms per token,    43.06 tokens per second)
      total time =    5165.94 ms /  1241 tokens
slot      release: id  3 | task 22196 | stop processing: n_tokens = 2267, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.499 (> 0.100 thold), f_keep = 0.923
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22374 | processing task, is_child = 0
slot update_slots: id  3 | task 22374 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4195
slot update_slots: id  3 | task 22374 | n_tokens = 2092, memory_seq_rm [2092, end)
slot update_slots: id  3 | task 22374 | prompt processing progress, n_tokens = 4131, batch.n_tokens = 2039, progress = 0.984744
slot update_slots: id  3 | task 22374 | n_tokens = 4131, memory_seq_rm [4131, end)
slot update_slots: id  3 | task 22374 | prompt processing progress, n_tokens = 4195, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22374 | prompt done, n_tokens = 4195, batch.n_tokens = 64
slot init_sampler: id  3 | task 22374 | init sampler, took 0.67 ms, tokens: text = 4195, total = 4195
slot update_slots: id  3 | task 22374 | created context checkpoint 4 of 8 (pos_min = 3107, pos_max = 4130, size = 24.012 MiB)
slot print_timing: id  3 | task 22374 | 
prompt eval time =    2074.30 ms /  2103 tokens (    0.99 ms per token,  1013.83 tokens per second)
       eval time =     934.42 ms /    40 tokens (   23.36 ms per token,    42.81 tokens per second)
      total time =    3008.72 ms /  2143 tokens
slot      release: id  3 | task 22374 | stop processing: n_tokens = 4234, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.807 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22416 | processing task, is_child = 0
slot update_slots: id  3 | task 22416 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5200
slot update_slots: id  3 | task 22416 | n_tokens = 4195, memory_seq_rm [4195, end)
slot update_slots: id  3 | task 22416 | prompt processing progress, n_tokens = 5136, batch.n_tokens = 941, progress = 0.987692
slot update_slots: id  3 | task 22416 | n_tokens = 5136, memory_seq_rm [5136, end)
slot update_slots: id  3 | task 22416 | prompt processing progress, n_tokens = 5200, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22416 | prompt done, n_tokens = 5200, batch.n_tokens = 64
slot init_sampler: id  3 | task 22416 | init sampler, took 0.81 ms, tokens: text = 5200, total = 5200
slot update_slots: id  3 | task 22416 | created context checkpoint 5 of 8 (pos_min = 4112, pos_max = 5135, size = 24.012 MiB)
slot print_timing: id  3 | task 22416 | 
prompt eval time =    1090.54 ms /  1005 tokens (    1.09 ms per token,   921.56 tokens per second)
       eval time =   47884.35 ms /  1997 tokens (   23.98 ms per token,    41.70 tokens per second)
      total time =   48974.89 ms /  3002 tokens
slot      release: id  3 | task 22416 | stop processing: n_tokens = 7196, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.806 (> 0.100 thold), f_keep = 0.723
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 24415 | processing task, is_child = 0
slot update_slots: id  3 | task 24415 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6455
slot update_slots: id  3 | task 24415 | n_past = 5200, slot.prompt.tokens.size() = 7196, seq_id = 3, pos_min = 6172, n_swa = 128
slot update_slots: id  3 | task 24415 | restored context checkpoint (pos_min = 4112, pos_max = 5135, size = 24.012 MiB)
slot update_slots: id  3 | task 24415 | n_tokens = 5135, memory_seq_rm [5135, end)
slot update_slots: id  3 | task 24415 | prompt processing progress, n_tokens = 6391, batch.n_tokens = 1256, progress = 0.990085
slot update_slots: id  3 | task 24415 | n_tokens = 6391, memory_seq_rm [6391, end)
slot update_slots: id  3 | task 24415 | prompt processing progress, n_tokens = 6455, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 24415 | prompt done, n_tokens = 6455, batch.n_tokens = 64
slot init_sampler: id  3 | task 24415 | init sampler, took 0.95 ms, tokens: text = 6455, total = 6455
slot update_slots: id  3 | task 24415 | created context checkpoint 6 of 8 (pos_min = 5367, pos_max = 6390, size = 24.012 MiB)
slot print_timing: id  3 | task 24415 | 
prompt eval time =    1548.48 ms /  1320 tokens (    1.17 ms per token,   852.45 tokens per second)
       eval time =   35894.03 ms /  1513 tokens (   23.72 ms per token,    42.15 tokens per second)
      total time =   37442.51 ms /  2833 tokens
slot      release: id  3 | task 24415 | stop processing: n_tokens = 7967, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.838 (> 0.100 thold), f_keep = 0.810
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 25930 | processing task, is_child = 0
slot update_slots: id  3 | task 25930 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7703
slot update_slots: id  3 | task 25930 | n_past = 6455, slot.prompt.tokens.size() = 7967, seq_id = 3, pos_min = 6943, n_swa = 128
slot update_slots: id  3 | task 25930 | restored context checkpoint (pos_min = 5367, pos_max = 6390, size = 24.012 MiB)
slot update_slots: id  3 | task 25930 | n_tokens = 6390, memory_seq_rm [6390, end)
slot update_slots: id  3 | task 25930 | prompt processing progress, n_tokens = 7639, batch.n_tokens = 1249, progress = 0.991692
slot update_slots: id  3 | task 25930 | n_tokens = 7639, memory_seq_rm [7639, end)
slot update_slots: id  3 | task 25930 | prompt processing progress, n_tokens = 7703, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 25930 | prompt done, n_tokens = 7703, batch.n_tokens = 64
slot init_sampler: id  3 | task 25930 | init sampler, took 1.46 ms, tokens: text = 7703, total = 7703
slot update_slots: id  3 | task 25930 | created context checkpoint 7 of 8 (pos_min = 6615, pos_max = 7638, size = 24.012 MiB)
slot print_timing: id  3 | task 25930 | 
prompt eval time =    1561.24 ms /  1313 tokens (    1.19 ms per token,   841.00 tokens per second)
       eval time =   39627.47 ms /  1645 tokens (   24.09 ms per token,    41.51 tokens per second)
      total time =   41188.71 ms /  2958 tokens
slot      release: id  3 | task 25930 | stop processing: n_tokens = 9347, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.860 (> 0.100 thold), f_keep = 0.824
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27577 | processing task, is_child = 0
slot update_slots: id  3 | task 27577 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8959
slot update_slots: id  3 | task 27577 | n_past = 7703, slot.prompt.tokens.size() = 9347, seq_id = 3, pos_min = 8323, n_swa = 128
slot update_slots: id  3 | task 27577 | restored context checkpoint (pos_min = 6615, pos_max = 7638, size = 24.012 MiB)
slot update_slots: id  3 | task 27577 | n_tokens = 7638, memory_seq_rm [7638, end)
slot update_slots: id  3 | task 27577 | prompt processing progress, n_tokens = 8895, batch.n_tokens = 1257, progress = 0.992856
slot update_slots: id  3 | task 27577 | n_tokens = 8895, memory_seq_rm [8895, end)
slot update_slots: id  3 | task 27577 | prompt processing progress, n_tokens = 8959, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 27577 | prompt done, n_tokens = 8959, batch.n_tokens = 64
slot init_sampler: id  3 | task 27577 | init sampler, took 1.33 ms, tokens: text = 8959, total = 8959
slot update_slots: id  3 | task 27577 | created context checkpoint 8 of 8 (pos_min = 7871, pos_max = 8894, size = 24.012 MiB)
slot print_timing: id  3 | task 27577 | 
prompt eval time =    1542.67 ms /  1321 tokens (    1.17 ms per token,   856.31 tokens per second)
       eval time =    1150.69 ms /    47 tokens (   24.48 ms per token,    40.84 tokens per second)
      total time =    2693.37 ms /  1368 tokens
slot      release: id  3 | task 27577 | stop processing: n_tokens = 9005, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 27626 | processing task, is_child = 0
slot update_slots: id  3 | task 27626 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9421
slot update_slots: id  3 | task 27626 | n_tokens = 8959, memory_seq_rm [8959, end)
slot update_slots: id  3 | task 27626 | prompt processing progress, n_tokens = 9357, batch.n_tokens = 398, progress = 0.993207
slot update_slots: id  3 | task 27626 | n_tokens = 9357, memory_seq_rm [9357, end)
slot update_slots: id  3 | task 27626 | prompt processing progress, n_tokens = 9421, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 27626 | prompt done, n_tokens = 9421, batch.n_tokens = 64
slot init_sampler: id  3 | task 27626 | init sampler, took 1.37 ms, tokens: text = 9421, total = 9421
slot update_slots: id  3 | task 27626 | erasing old context checkpoint (pos_min = 0, pos_max = 415, size = 9.755 MiB)
slot update_slots: id  3 | task 27626 | created context checkpoint 8 of 8 (pos_min = 8333, pos_max = 9356, size = 24.012 MiB)
slot print_timing: id  3 | task 27626 | 
prompt eval time =     588.80 ms /   462 tokens (    1.27 ms per token,   784.65 tokens per second)
       eval time =   30882.86 ms /  1277 tokens (   24.18 ms per token,    41.35 tokens per second)
      total time =   31471.65 ms /  1739 tokens
slot      release: id  3 | task 27626 | stop processing: n_tokens = 10697, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.883 (> 0.100 thold), f_keep = 0.881
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28905 | processing task, is_child = 0
slot update_slots: id  3 | task 28905 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10674
slot update_slots: id  3 | task 28905 | n_past = 9421, slot.prompt.tokens.size() = 10697, seq_id = 3, pos_min = 9673, n_swa = 128
slot update_slots: id  3 | task 28905 | restored context checkpoint (pos_min = 8333, pos_max = 9356, size = 24.012 MiB)
slot update_slots: id  3 | task 28905 | n_tokens = 9356, memory_seq_rm [9356, end)
slot update_slots: id  3 | task 28905 | prompt processing progress, n_tokens = 10610, batch.n_tokens = 1254, progress = 0.994004
slot update_slots: id  3 | task 28905 | n_tokens = 10610, memory_seq_rm [10610, end)
slot update_slots: id  3 | task 28905 | prompt processing progress, n_tokens = 10674, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 28905 | prompt done, n_tokens = 10674, batch.n_tokens = 64
slot init_sampler: id  3 | task 28905 | init sampler, took 1.55 ms, tokens: text = 10674, total = 10674
slot update_slots: id  3 | task 28905 | erasing old context checkpoint (pos_min = 0, pos_max = 962, size = 22.582 MiB)
slot update_slots: id  3 | task 28905 | created context checkpoint 8 of 8 (pos_min = 9586, pos_max = 10609, size = 24.012 MiB)
slot print_timing: id  3 | task 28905 | 
prompt eval time =    1592.00 ms /  1318 tokens (    1.21 ms per token,   827.89 tokens per second)
       eval time =    1564.04 ms /    63 tokens (   24.83 ms per token,    40.28 tokens per second)
      total time =    3156.04 ms /  1381 tokens
slot      release: id  3 | task 28905 | stop processing: n_tokens = 10736, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.956 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 28970 | processing task, is_child = 0
slot update_slots: id  3 | task 28970 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11164
slot update_slots: id  3 | task 28970 | n_tokens = 10674, memory_seq_rm [10674, end)
slot update_slots: id  3 | task 28970 | prompt processing progress, n_tokens = 11100, batch.n_tokens = 426, progress = 0.994267
slot update_slots: id  3 | task 28970 | n_tokens = 11100, memory_seq_rm [11100, end)
slot update_slots: id  3 | task 28970 | prompt processing progress, n_tokens = 11164, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 28970 | prompt done, n_tokens = 11164, batch.n_tokens = 64
slot init_sampler: id  3 | task 28970 | init sampler, took 1.65 ms, tokens: text = 11164, total = 11164
slot update_slots: id  3 | task 28970 | erasing old context checkpoint (pos_min = 1004, pos_max = 2027, size = 24.012 MiB)
slot update_slots: id  3 | task 28970 | created context checkpoint 8 of 8 (pos_min = 10076, pos_max = 11099, size = 24.012 MiB)
slot print_timing: id  3 | task 28970 | 
prompt eval time =     732.19 ms /   490 tokens (    1.49 ms per token,   669.23 tokens per second)
       eval time =    1762.71 ms /    73 tokens (   24.15 ms per token,    41.41 tokens per second)
      total time =    2494.90 ms /   563 tokens
slot      release: id  3 | task 28970 | stop processing: n_tokens = 11236, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29045 | processing task, is_child = 0
slot update_slots: id  3 | task 29045 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11467
slot update_slots: id  3 | task 29045 | n_tokens = 11164, memory_seq_rm [11164, end)
slot update_slots: id  3 | task 29045 | prompt processing progress, n_tokens = 11403, batch.n_tokens = 239, progress = 0.994419
slot update_slots: id  3 | task 29045 | n_tokens = 11403, memory_seq_rm [11403, end)
slot update_slots: id  3 | task 29045 | prompt processing progress, n_tokens = 11467, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29045 | prompt done, n_tokens = 11467, batch.n_tokens = 64
slot init_sampler: id  3 | task 29045 | init sampler, took 1.63 ms, tokens: text = 11467, total = 11467
slot update_slots: id  3 | task 29045 | erasing old context checkpoint (pos_min = 3107, pos_max = 4130, size = 24.012 MiB)
slot update_slots: id  3 | task 29045 | created context checkpoint 8 of 8 (pos_min = 10379, pos_max = 11402, size = 24.012 MiB)
slot print_timing: id  3 | task 29045 | 
prompt eval time =     538.83 ms /   303 tokens (    1.78 ms per token,   562.33 tokens per second)
       eval time =    2941.68 ms /   122 tokens (   24.11 ms per token,    41.47 tokens per second)
      total time =    3480.51 ms /   425 tokens
slot      release: id  3 | task 29045 | stop processing: n_tokens = 11588, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29169 | processing task, is_child = 0
slot update_slots: id  3 | task 29169 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11580
slot update_slots: id  3 | task 29169 | n_tokens = 11467, memory_seq_rm [11467, end)
slot update_slots: id  3 | task 29169 | prompt processing progress, n_tokens = 11516, batch.n_tokens = 49, progress = 0.994473
slot update_slots: id  3 | task 29169 | n_tokens = 11516, memory_seq_rm [11516, end)
slot update_slots: id  3 | task 29169 | prompt processing progress, n_tokens = 11580, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29169 | prompt done, n_tokens = 11580, batch.n_tokens = 64
slot init_sampler: id  3 | task 29169 | init sampler, took 1.64 ms, tokens: text = 11580, total = 11580
slot update_slots: id  3 | task 29169 | erasing old context checkpoint (pos_min = 4112, pos_max = 5135, size = 24.012 MiB)
slot update_slots: id  3 | task 29169 | created context checkpoint 8 of 8 (pos_min = 10564, pos_max = 11515, size = 22.324 MiB)
slot print_timing: id  3 | task 29169 | 
prompt eval time =     336.06 ms /   113 tokens (    2.97 ms per token,   336.25 tokens per second)
       eval time =    1398.35 ms /    57 tokens (   24.53 ms per token,    40.76 tokens per second)
      total time =    1734.40 ms /   170 tokens
slot      release: id  3 | task 29169 | stop processing: n_tokens = 11636, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.959 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29228 | processing task, is_child = 0
slot update_slots: id  3 | task 29228 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12070
slot update_slots: id  3 | task 29228 | n_tokens = 11580, memory_seq_rm [11580, end)
slot update_slots: id  3 | task 29228 | prompt processing progress, n_tokens = 12006, batch.n_tokens = 426, progress = 0.994698
slot update_slots: id  3 | task 29228 | n_tokens = 12006, memory_seq_rm [12006, end)
slot update_slots: id  3 | task 29228 | prompt processing progress, n_tokens = 12070, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29228 | prompt done, n_tokens = 12070, batch.n_tokens = 64
slot init_sampler: id  3 | task 29228 | init sampler, took 1.72 ms, tokens: text = 12070, total = 12070
slot update_slots: id  3 | task 29228 | erasing old context checkpoint (pos_min = 5367, pos_max = 6390, size = 24.012 MiB)
slot update_slots: id  3 | task 29228 | created context checkpoint 8 of 8 (pos_min = 10982, pos_max = 12005, size = 24.012 MiB)
slot print_timing: id  3 | task 29228 | 
prompt eval time =     637.00 ms /   490 tokens (    1.30 ms per token,   769.24 tokens per second)
       eval time =    1552.89 ms /    62 tokens (   25.05 ms per token,    39.93 tokens per second)
      total time =    2189.88 ms /   552 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 29228 | stop processing: n_tokens = 12131, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29292 | processing task, is_child = 0
slot update_slots: id  3 | task 29292 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12361
slot update_slots: id  3 | task 29292 | n_tokens = 12070, memory_seq_rm [12070, end)
slot update_slots: id  3 | task 29292 | prompt processing progress, n_tokens = 12297, batch.n_tokens = 227, progress = 0.994822
slot update_slots: id  3 | task 29292 | n_tokens = 12297, memory_seq_rm [12297, end)
slot update_slots: id  3 | task 29292 | prompt processing progress, n_tokens = 12361, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29292 | prompt done, n_tokens = 12361, batch.n_tokens = 64
slot init_sampler: id  3 | task 29292 | init sampler, took 2.31 ms, tokens: text = 12361, total = 12361
slot update_slots: id  3 | task 29292 | erasing old context checkpoint (pos_min = 6615, pos_max = 7638, size = 24.012 MiB)
slot update_slots: id  3 | task 29292 | created context checkpoint 8 of 8 (pos_min = 11273, pos_max = 12296, size = 24.012 MiB)
slot print_timing: id  3 | task 29292 | 
prompt eval time =     533.40 ms /   291 tokens (    1.83 ms per token,   545.55 tokens per second)
       eval time =    2226.17 ms /    90 tokens (   24.74 ms per token,    40.43 tokens per second)
      total time =    2759.58 ms /   381 tokens
slot      release: id  3 | task 29292 | stop processing: n_tokens = 12450, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29384 | processing task, is_child = 0
slot update_slots: id  3 | task 29384 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12463
slot update_slots: id  3 | task 29384 | n_tokens = 12361, memory_seq_rm [12361, end)
slot update_slots: id  3 | task 29384 | prompt processing progress, n_tokens = 12399, batch.n_tokens = 38, progress = 0.994865
slot update_slots: id  3 | task 29384 | n_tokens = 12399, memory_seq_rm [12399, end)
slot update_slots: id  3 | task 29384 | prompt processing progress, n_tokens = 12463, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29384 | prompt done, n_tokens = 12463, batch.n_tokens = 64
slot init_sampler: id  3 | task 29384 | init sampler, took 1.76 ms, tokens: text = 12463, total = 12463
slot update_slots: id  3 | task 29384 | erasing old context checkpoint (pos_min = 7871, pos_max = 8894, size = 24.012 MiB)
slot update_slots: id  3 | task 29384 | created context checkpoint 8 of 8 (pos_min = 11426, pos_max = 12398, size = 22.816 MiB)
slot print_timing: id  3 | task 29384 | 
prompt eval time =     315.50 ms /   102 tokens (    3.09 ms per token,   323.29 tokens per second)
       eval time =    1299.30 ms /    53 tokens (   24.52 ms per token,    40.79 tokens per second)
      total time =    1614.81 ms /   155 tokens
slot      release: id  3 | task 29384 | stop processing: n_tokens = 12515, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29439 | processing task, is_child = 0
slot update_slots: id  3 | task 29439 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12523
slot update_slots: id  3 | task 29439 | n_tokens = 12463, memory_seq_rm [12463, end)
slot update_slots: id  3 | task 29439 | prompt processing progress, n_tokens = 12523, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  3 | task 29439 | prompt done, n_tokens = 12523, batch.n_tokens = 60
slot init_sampler: id  3 | task 29439 | init sampler, took 1.80 ms, tokens: text = 12523, total = 12523
slot print_timing: id  3 | task 29439 | 
prompt eval time =     157.91 ms /    60 tokens (    2.63 ms per token,   379.97 tokens per second)
       eval time =    1361.61 ms /    56 tokens (   24.31 ms per token,    41.13 tokens per second)
      total time =    1519.51 ms /   116 tokens
slot      release: id  3 | task 29439 | stop processing: n_tokens = 12578, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 29496 | processing task, is_child = 0
slot update_slots: id  3 | task 29496 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13014
slot update_slots: id  3 | task 29496 | n_tokens = 12523, memory_seq_rm [12523, end)
slot update_slots: id  3 | task 29496 | prompt processing progress, n_tokens = 12950, batch.n_tokens = 427, progress = 0.995082
slot update_slots: id  3 | task 29496 | n_tokens = 12950, memory_seq_rm [12950, end)
slot update_slots: id  3 | task 29496 | prompt processing progress, n_tokens = 13014, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 29496 | prompt done, n_tokens = 13014, batch.n_tokens = 64
slot init_sampler: id  3 | task 29496 | init sampler, took 1.84 ms, tokens: text = 13014, total = 13014
slot update_slots: id  3 | task 29496 | erasing old context checkpoint (pos_min = 8333, pos_max = 9356, size = 24.012 MiB)
slot update_slots: id  3 | task 29496 | created context checkpoint 8 of 8 (pos_min = 11926, pos_max = 12949, size = 24.012 MiB)
slot print_timing: id  3 | task 29496 | 
prompt eval time =     648.23 ms /   491 tokens (    1.32 ms per token,   757.45 tokens per second)
       eval time =   59936.47 ms /  2428 tokens (   24.69 ms per token,    40.51 tokens per second)
      total time =   60584.70 ms /  2919 tokens
slot      release: id  3 | task 29496 | stop processing: n_tokens = 15441, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.843 (> 0.100 thold), f_keep = 0.843
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31926 | processing task, is_child = 0
slot update_slots: id  3 | task 31926 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15437
slot update_slots: id  3 | task 31926 | n_past = 13014, slot.prompt.tokens.size() = 15441, seq_id = 3, pos_min = 14417, n_swa = 128
slot update_slots: id  3 | task 31926 | restored context checkpoint (pos_min = 11926, pos_max = 12949, size = 24.012 MiB)
slot update_slots: id  3 | task 31926 | n_tokens = 12949, memory_seq_rm [12949, end)
slot update_slots: id  3 | task 31926 | prompt processing progress, n_tokens = 14997, batch.n_tokens = 2048, progress = 0.971497
slot update_slots: id  3 | task 31926 | n_tokens = 14997, memory_seq_rm [14997, end)
slot update_slots: id  3 | task 31926 | prompt processing progress, n_tokens = 15373, batch.n_tokens = 376, progress = 0.995854
slot update_slots: id  3 | task 31926 | n_tokens = 15373, memory_seq_rm [15373, end)
slot update_slots: id  3 | task 31926 | prompt processing progress, n_tokens = 15437, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31926 | prompt done, n_tokens = 15437, batch.n_tokens = 64
slot init_sampler: id  3 | task 31926 | init sampler, took 2.23 ms, tokens: text = 15437, total = 15437
slot update_slots: id  3 | task 31926 | erasing old context checkpoint (pos_min = 9586, pos_max = 10609, size = 24.012 MiB)
slot update_slots: id  3 | task 31926 | created context checkpoint 8 of 8 (pos_min = 14349, pos_max = 15372, size = 24.012 MiB)
slot print_timing: id  3 | task 31926 | 
prompt eval time =    2922.05 ms /  2488 tokens (    1.17 ms per token,   851.46 tokens per second)
       eval time =     810.52 ms /    33 tokens (   24.56 ms per token,    40.71 tokens per second)
      total time =    3732.57 ms /  2521 tokens
slot      release: id  3 | task 31926 | stop processing: n_tokens = 15469, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31962 | processing task, is_child = 0
slot update_slots: id  3 | task 31962 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15780
slot update_slots: id  3 | task 31962 | n_tokens = 15437, memory_seq_rm [15437, end)
slot update_slots: id  3 | task 31962 | prompt processing progress, n_tokens = 15716, batch.n_tokens = 279, progress = 0.995944
slot update_slots: id  3 | task 31962 | n_tokens = 15716, memory_seq_rm [15716, end)
slot update_slots: id  3 | task 31962 | prompt processing progress, n_tokens = 15780, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 31962 | prompt done, n_tokens = 15780, batch.n_tokens = 64
slot init_sampler: id  3 | task 31962 | init sampler, took 2.40 ms, tokens: text = 15780, total = 15780
slot update_slots: id  3 | task 31962 | erasing old context checkpoint (pos_min = 10076, pos_max = 11099, size = 24.012 MiB)
slot update_slots: id  3 | task 31962 | created context checkpoint 8 of 8 (pos_min = 14692, pos_max = 15715, size = 24.012 MiB)
slot print_timing: id  3 | task 31962 | 
prompt eval time =     617.46 ms /   343 tokens (    1.80 ms per token,   555.51 tokens per second)
       eval time =   26877.93 ms /  1075 tokens (   25.00 ms per token,    40.00 tokens per second)
      total time =   27495.38 ms /  1418 tokens
slot      release: id  3 | task 31962 | stop processing: n_tokens = 16854, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.936
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33039 | processing task, is_child = 0
slot update_slots: id  3 | task 33039 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16100
slot update_slots: id  3 | task 33039 | n_past = 15780, slot.prompt.tokens.size() = 16854, seq_id = 3, pos_min = 15830, n_swa = 128
slot update_slots: id  3 | task 33039 | restored context checkpoint (pos_min = 14692, pos_max = 15715, size = 24.012 MiB)
slot update_slots: id  3 | task 33039 | n_tokens = 15715, memory_seq_rm [15715, end)
slot update_slots: id  3 | task 33039 | prompt processing progress, n_tokens = 16036, batch.n_tokens = 321, progress = 0.996025
slot update_slots: id  3 | task 33039 | n_tokens = 16036, memory_seq_rm [16036, end)
slot update_slots: id  3 | task 33039 | prompt processing progress, n_tokens = 16100, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33039 | prompt done, n_tokens = 16100, batch.n_tokens = 64
slot init_sampler: id  3 | task 33039 | init sampler, took 2.27 ms, tokens: text = 16100, total = 16100
slot update_slots: id  3 | task 33039 | erasing old context checkpoint (pos_min = 10379, pos_max = 11402, size = 24.012 MiB)
slot update_slots: id  3 | task 33039 | created context checkpoint 8 of 8 (pos_min = 15012, pos_max = 16035, size = 24.012 MiB)
slot print_timing: id  3 | task 33039 | 
prompt eval time =     623.17 ms /   385 tokens (    1.62 ms per token,   617.81 tokens per second)
       eval time =   11186.33 ms /   446 tokens (   25.08 ms per token,    39.87 tokens per second)
      total time =   11809.50 ms /   831 tokens
slot      release: id  3 | task 33039 | stop processing: n_tokens = 16545, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33487 | processing task, is_child = 0
slot update_slots: id  3 | task 33487 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16340
slot update_slots: id  3 | task 33487 | n_tokens = 16100, memory_seq_rm [16100, end)
slot update_slots: id  3 | task 33487 | prompt processing progress, n_tokens = 16276, batch.n_tokens = 176, progress = 0.996083
slot update_slots: id  3 | task 33487 | n_tokens = 16276, memory_seq_rm [16276, end)
slot update_slots: id  3 | task 33487 | prompt processing progress, n_tokens = 16340, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33487 | prompt done, n_tokens = 16340, batch.n_tokens = 64
slot init_sampler: id  3 | task 33487 | init sampler, took 2.27 ms, tokens: text = 16340, total = 16340
slot update_slots: id  3 | task 33487 | erasing old context checkpoint (pos_min = 10564, pos_max = 11515, size = 22.324 MiB)
slot update_slots: id  3 | task 33487 | created context checkpoint 8 of 8 (pos_min = 15536, pos_max = 16275, size = 17.353 MiB)
slot print_timing: id  3 | task 33487 | 
prompt eval time =     488.33 ms /   240 tokens (    2.03 ms per token,   491.47 tokens per second)
       eval time =     903.53 ms /    37 tokens (   24.42 ms per token,    40.95 tokens per second)
      total time =    1391.86 ms /   277 tokens
slot      release: id  3 | task 33487 | stop processing: n_tokens = 16376, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 33526 | processing task, is_child = 0
slot update_slots: id  3 | task 33526 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16420
slot update_slots: id  3 | task 33526 | n_tokens = 16340, memory_seq_rm [16340, end)
slot update_slots: id  3 | task 33526 | prompt processing progress, n_tokens = 16356, batch.n_tokens = 16, progress = 0.996102
slot update_slots: id  3 | task 33526 | n_tokens = 16356, memory_seq_rm [16356, end)
slot update_slots: id  3 | task 33526 | prompt processing progress, n_tokens = 16420, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 33526 | prompt done, n_tokens = 16420, batch.n_tokens = 64
slot init_sampler: id  3 | task 33526 | init sampler, took 3.14 ms, tokens: text = 16420, total = 16420
slot update_slots: id  3 | task 33526 | erasing old context checkpoint (pos_min = 10982, pos_max = 12005, size = 24.012 MiB)
slot update_slots: id  3 | task 33526 | created context checkpoint 8 of 8 (pos_min = 15636, pos_max = 16355, size = 16.884 MiB)
slot print_timing: id  3 | task 33526 | 
prompt eval time =     286.60 ms /    80 tokens (    3.58 ms per token,   279.14 tokens per second)
       eval time =    2032.77 ms /    81 tokens (   25.10 ms per token,    39.85 tokens per second)
      total time =    2319.36 ms /   161 tokens
slot      release: id  3 | task 33526 | stop processing: n_tokens = 16500, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 33609 | processing task, is_child = 0
slot update_slots: id  2 | task 33609 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9340
slot update_slots: id  2 | task 33609 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.219272
slot update_slots: id  2 | task 33609 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.438544
slot update_slots: id  2 | task 33609 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.657816
slot update_slots: id  2 | task 33609 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.877088
slot update_slots: id  2 | task 33609 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 9276, batch.n_tokens = 1084, progress = 0.993148
slot update_slots: id  2 | task 33609 | n_tokens = 9276, memory_seq_rm [9276, end)
slot update_slots: id  2 | task 33609 | prompt processing progress, n_tokens = 9340, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 33609 | prompt done, n_tokens = 9340, batch.n_tokens = 64
slot init_sampler: id  2 | task 33609 | init sampler, took 2.12 ms, tokens: text = 9340, total = 9340
slot update_slots: id  2 | task 33609 | created context checkpoint 1 of 8 (pos_min = 8379, pos_max = 9275, size = 21.034 MiB)
slot print_timing: id  2 | task 33609 | 
prompt eval time =   11372.96 ms /  9340 tokens (    1.22 ms per token,   821.25 tokens per second)
       eval time =    1352.88 ms /    52 tokens (   26.02 ms per token,    38.44 tokens per second)
      total time =   12725.84 ms /  9392 tokens
slot      release: id  2 | task 33609 | stop processing: n_tokens = 9391, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 33667 | processing task, is_child = 0
slot update_slots: id  2 | task 33667 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9446
slot update_slots: id  2 | task 33667 | n_tokens = 9340, memory_seq_rm [9340, end)
slot update_slots: id  2 | task 33667 | prompt processing progress, n_tokens = 9382, batch.n_tokens = 42, progress = 0.993225
slot update_slots: id  2 | task 33667 | n_tokens = 9382, memory_seq_rm [9382, end)
slot update_slots: id  2 | task 33667 | prompt processing progress, n_tokens = 9446, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 33667 | prompt done, n_tokens = 9446, batch.n_tokens = 64
slot init_sampler: id  2 | task 33667 | init sampler, took 1.35 ms, tokens: text = 9446, total = 9446
slot update_slots: id  2 | task 33667 | created context checkpoint 2 of 8 (pos_min = 8494, pos_max = 9381, size = 20.823 MiB)
slot print_timing: id  2 | task 33667 | 
prompt eval time =     349.37 ms /   106 tokens (    3.30 ms per token,   303.41 tokens per second)
       eval time =    2250.61 ms /    87 tokens (   25.87 ms per token,    38.66 tokens per second)
      total time =    2599.98 ms /   193 tokens
slot      release: id  2 | task 33667 | stop processing: n_tokens = 9532, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.811 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 33756 | processing task, is_child = 0
slot update_slots: id  2 | task 33756 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11653
slot update_slots: id  2 | task 33756 | n_tokens = 9446, memory_seq_rm [9446, end)
slot update_slots: id  2 | task 33756 | prompt processing progress, n_tokens = 11494, batch.n_tokens = 2048, progress = 0.986355
slot update_slots: id  2 | task 33756 | n_tokens = 11494, memory_seq_rm [11494, end)
slot update_slots: id  2 | task 33756 | prompt processing progress, n_tokens = 11589, batch.n_tokens = 95, progress = 0.994508
slot update_slots: id  2 | task 33756 | n_tokens = 11589, memory_seq_rm [11589, end)
slot update_slots: id  2 | task 33756 | prompt processing progress, n_tokens = 11653, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 33756 | prompt done, n_tokens = 11653, batch.n_tokens = 64
slot init_sampler: id  2 | task 33756 | init sampler, took 1.73 ms, tokens: text = 11653, total = 11653
slot update_slots: id  2 | task 33756 | created context checkpoint 3 of 8 (pos_min = 10692, pos_max = 11588, size = 21.034 MiB)
slot print_timing: id  2 | task 33756 | 
prompt eval time =    3075.22 ms /  2207 tokens (    1.39 ms per token,   717.67 tokens per second)
       eval time =   37055.03 ms /  1391 tokens (   26.64 ms per token,    37.54 tokens per second)
      total time =   40130.25 ms /  3598 tokens
slot      release: id  2 | task 33756 | stop processing: n_tokens = 13043, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.903 (> 0.100 thold), f_keep = 0.893
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 35150 | processing task, is_child = 0
slot update_slots: id  2 | task 35150 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12899
slot update_slots: id  2 | task 35150 | n_past = 11653, slot.prompt.tokens.size() = 13043, seq_id = 2, pos_min = 12146, n_swa = 128
slot update_slots: id  2 | task 35150 | restored context checkpoint (pos_min = 10692, pos_max = 11588, size = 21.034 MiB)
slot update_slots: id  2 | task 35150 | n_tokens = 11588, memory_seq_rm [11588, end)
slot update_slots: id  2 | task 35150 | prompt processing progress, n_tokens = 12835, batch.n_tokens = 1247, progress = 0.995038
slot update_slots: id  2 | task 35150 | n_tokens = 12835, memory_seq_rm [12835, end)
slot update_slots: id  2 | task 35150 | prompt processing progress, n_tokens = 12899, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 35150 | prompt done, n_tokens = 12899, batch.n_tokens = 64
slot init_sampler: id  2 | task 35150 | init sampler, took 2.05 ms, tokens: text = 12899, total = 12899
slot update_slots: id  2 | task 35150 | created context checkpoint 4 of 8 (pos_min = 11938, pos_max = 12834, size = 21.034 MiB)
slot print_timing: id  2 | task 35150 | 
prompt eval time =    2130.93 ms /  1311 tokens (    1.63 ms per token,   615.23 tokens per second)
       eval time =   34502.73 ms /  1311 tokens (   26.32 ms per token,    38.00 tokens per second)
      total time =   36633.66 ms /  2622 tokens
slot      release: id  2 | task 35150 | stop processing: n_tokens = 14209, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.912 (> 0.100 thold), f_keep = 0.908
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36463 | processing task, is_child = 0
slot update_slots: id  2 | task 36463 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14145
slot update_slots: id  2 | task 36463 | n_past = 12899, slot.prompt.tokens.size() = 14209, seq_id = 2, pos_min = 13312, n_swa = 128
slot update_slots: id  2 | task 36463 | restored context checkpoint (pos_min = 11938, pos_max = 12834, size = 21.034 MiB)
slot update_slots: id  2 | task 36463 | n_tokens = 12834, memory_seq_rm [12834, end)
slot update_slots: id  2 | task 36463 | prompt processing progress, n_tokens = 14081, batch.n_tokens = 1247, progress = 0.995475
slot update_slots: id  2 | task 36463 | n_tokens = 14081, memory_seq_rm [14081, end)
slot update_slots: id  2 | task 36463 | prompt processing progress, n_tokens = 14145, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36463 | prompt done, n_tokens = 14145, batch.n_tokens = 64
slot init_sampler: id  2 | task 36463 | init sampler, took 2.04 ms, tokens: text = 14145, total = 14145
slot update_slots: id  2 | task 36463 | created context checkpoint 5 of 8 (pos_min = 13184, pos_max = 14080, size = 21.034 MiB)
slot print_timing: id  2 | task 36463 | 
prompt eval time =    2119.39 ms /  1311 tokens (    1.62 ms per token,   618.57 tokens per second)
       eval time =    8487.38 ms /   320 tokens (   26.52 ms per token,    37.70 tokens per second)
      total time =   10606.77 ms /  1631 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 36463 | stop processing: n_tokens = 14464, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36785 | processing task, is_child = 0
slot update_slots: id  2 | task 36785 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14222
slot update_slots: id  2 | task 36785 | n_tokens = 14145, memory_seq_rm [14145, end)
slot update_slots: id  2 | task 36785 | prompt processing progress, n_tokens = 14158, batch.n_tokens = 13, progress = 0.995500
slot update_slots: id  2 | task 36785 | n_tokens = 14158, memory_seq_rm [14158, end)
slot update_slots: id  2 | task 36785 | prompt processing progress, n_tokens = 14222, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36785 | prompt done, n_tokens = 14222, batch.n_tokens = 64
slot init_sampler: id  2 | task 36785 | init sampler, took 2.68 ms, tokens: text = 14222, total = 14222
slot update_slots: id  2 | task 36785 | created context checkpoint 6 of 8 (pos_min = 13567, pos_max = 14157, size = 13.859 MiB)
slot print_timing: id  2 | task 36785 | 
prompt eval time =     298.55 ms /    77 tokens (    3.88 ms per token,   257.91 tokens per second)
       eval time =   33543.53 ms /  1255 tokens (   26.73 ms per token,    37.41 tokens per second)
      total time =   33842.08 ms /  1332 tokens
slot      release: id  2 | task 36785 | stop processing: n_tokens = 15476, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.919 (> 0.100 thold), f_keep = 0.919
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 38042 | processing task, is_child = 0
slot update_slots: id  2 | task 38042 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15468
slot update_slots: id  2 | task 38042 | n_past = 14222, slot.prompt.tokens.size() = 15476, seq_id = 2, pos_min = 14579, n_swa = 128
slot update_slots: id  2 | task 38042 | restored context checkpoint (pos_min = 13567, pos_max = 14157, size = 13.859 MiB)
slot update_slots: id  2 | task 38042 | n_tokens = 14157, memory_seq_rm [14157, end)
slot update_slots: id  2 | task 38042 | prompt processing progress, n_tokens = 15404, batch.n_tokens = 1247, progress = 0.995862
slot update_slots: id  2 | task 38042 | n_tokens = 15404, memory_seq_rm [15404, end)
slot update_slots: id  2 | task 38042 | prompt processing progress, n_tokens = 15468, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 38042 | prompt done, n_tokens = 15468, batch.n_tokens = 64
slot init_sampler: id  2 | task 38042 | init sampler, took 2.19 ms, tokens: text = 15468, total = 15468
slot update_slots: id  2 | task 38042 | created context checkpoint 7 of 8 (pos_min = 14556, pos_max = 15403, size = 19.885 MiB)
slot print_timing: id  2 | task 38042 | 
prompt eval time =    2107.21 ms /  1311 tokens (    1.61 ms per token,   622.15 tokens per second)
       eval time =   36164.97 ms /  1348 tokens (   26.83 ms per token,    37.27 tokens per second)
      total time =   38272.18 ms /  2659 tokens
slot      release: id  2 | task 38042 | stop processing: n_tokens = 16815, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 39392 | processing task, is_child = 0
slot update_slots: id  2 | task 39392 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15530
slot update_slots: id  2 | task 39392 | n_past = 15468, slot.prompt.tokens.size() = 16815, seq_id = 2, pos_min = 15918, n_swa = 128
slot update_slots: id  2 | task 39392 | restored context checkpoint (pos_min = 14556, pos_max = 15403, size = 19.885 MiB)
slot update_slots: id  2 | task 39392 | n_tokens = 15403, memory_seq_rm [15403, end)
slot update_slots: id  2 | task 39392 | prompt processing progress, n_tokens = 15466, batch.n_tokens = 63, progress = 0.995879
slot update_slots: id  2 | task 39392 | n_tokens = 15466, memory_seq_rm [15466, end)
slot update_slots: id  2 | task 39392 | prompt processing progress, n_tokens = 15530, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 39392 | prompt done, n_tokens = 15530, batch.n_tokens = 64
slot init_sampler: id  2 | task 39392 | init sampler, took 2.21 ms, tokens: text = 15530, total = 15530
slot print_timing: id  2 | task 39392 | 
prompt eval time =     570.52 ms /   127 tokens (    4.49 ms per token,   222.61 tokens per second)
       eval time =    7473.50 ms /   279 tokens (   26.79 ms per token,    37.33 tokens per second)
      total time =    8044.02 ms /   406 tokens
slot      release: id  2 | task 39392 | stop processing: n_tokens = 15808, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 39673 | processing task, is_child = 0
slot update_slots: id  2 | task 39673 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15626
slot update_slots: id  2 | task 39673 | n_tokens = 15530, memory_seq_rm [15530, end)
slot update_slots: id  2 | task 39673 | prompt processing progress, n_tokens = 15562, batch.n_tokens = 32, progress = 0.995904
slot update_slots: id  2 | task 39673 | n_tokens = 15562, memory_seq_rm [15562, end)
slot update_slots: id  2 | task 39673 | prompt processing progress, n_tokens = 15626, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 39673 | prompt done, n_tokens = 15626, batch.n_tokens = 64
slot init_sampler: id  2 | task 39673 | init sampler, took 2.50 ms, tokens: text = 15626, total = 15626
slot update_slots: id  2 | task 39673 | created context checkpoint 8 of 8 (pos_min = 14927, pos_max = 15561, size = 14.890 MiB)
slot print_timing: id  2 | task 39673 | 
prompt eval time =     410.59 ms /    96 tokens (    4.28 ms per token,   233.81 tokens per second)
       eval time =   78288.55 ms /  2888 tokens (   27.11 ms per token,    36.89 tokens per second)
      total time =   78699.15 ms /  2984 tokens
slot      release: id  2 | task 39673 | stop processing: n_tokens = 18513, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.844
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 42563 | processing task, is_child = 0
slot update_slots: id  2 | task 42563 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15725
slot update_slots: id  2 | task 42563 | n_past = 15626, slot.prompt.tokens.size() = 18513, seq_id = 2, pos_min = 17616, n_swa = 128
slot update_slots: id  2 | task 42563 | restored context checkpoint (pos_min = 14927, pos_max = 15561, size = 14.890 MiB)
slot update_slots: id  2 | task 42563 | n_tokens = 15561, memory_seq_rm [15561, end)
slot update_slots: id  2 | task 42563 | prompt processing progress, n_tokens = 15661, batch.n_tokens = 100, progress = 0.995930
slot update_slots: id  2 | task 42563 | n_tokens = 15661, memory_seq_rm [15661, end)
slot update_slots: id  2 | task 42563 | prompt processing progress, n_tokens = 15725, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 42563 | prompt done, n_tokens = 15725, batch.n_tokens = 64
slot init_sampler: id  2 | task 42563 | init sampler, took 3.05 ms, tokens: text = 15725, total = 15725
slot update_slots: id  2 | task 42563 | erasing old context checkpoint (pos_min = 8379, pos_max = 9275, size = 21.034 MiB)
slot update_slots: id  2 | task 42563 | created context checkpoint 8 of 8 (pos_min = 14927, pos_max = 15660, size = 17.212 MiB)
slot print_timing: id  2 | task 42563 | 
prompt eval time =     627.51 ms /   164 tokens (    3.83 ms per token,   261.35 tokens per second)
       eval time =    3131.90 ms /   118 tokens (   26.54 ms per token,    37.68 tokens per second)
      total time =    3759.41 ms /   282 tokens
slot      release: id  2 | task 42563 | stop processing: n_tokens = 15842, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.937 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 42683 | processing task, is_child = 0
slot update_slots: id  2 | task 42683 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16790
slot update_slots: id  2 | task 42683 | n_tokens = 15725, memory_seq_rm [15725, end)
slot update_slots: id  2 | task 42683 | prompt processing progress, n_tokens = 16726, batch.n_tokens = 1001, progress = 0.996188
slot update_slots: id  2 | task 42683 | n_tokens = 16726, memory_seq_rm [16726, end)
slot update_slots: id  2 | task 42683 | prompt processing progress, n_tokens = 16790, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 42683 | prompt done, n_tokens = 16790, batch.n_tokens = 64
slot init_sampler: id  2 | task 42683 | init sampler, took 2.42 ms, tokens: text = 16790, total = 16790
slot update_slots: id  2 | task 42683 | erasing old context checkpoint (pos_min = 8494, pos_max = 9381, size = 20.823 MiB)
slot update_slots: id  2 | task 42683 | created context checkpoint 8 of 8 (pos_min = 15829, pos_max = 16725, size = 21.034 MiB)
slot print_timing: id  2 | task 42683 | 
prompt eval time =    1664.54 ms /  1065 tokens (    1.56 ms per token,   639.81 tokens per second)
       eval time =    1971.96 ms /    74 tokens (   26.65 ms per token,    37.53 tokens per second)
      total time =    3636.50 ms /  1139 tokens
slot      release: id  2 | task 42683 | stop processing: n_tokens = 16863, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 42759 | processing task, is_child = 0
slot update_slots: id  2 | task 42759 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17048
slot update_slots: id  2 | task 42759 | n_tokens = 16790, memory_seq_rm [16790, end)
slot update_slots: id  2 | task 42759 | prompt processing progress, n_tokens = 16984, batch.n_tokens = 194, progress = 0.996246
slot update_slots: id  2 | task 42759 | n_tokens = 16984, memory_seq_rm [16984, end)
slot update_slots: id  2 | task 42759 | prompt processing progress, n_tokens = 17048, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 42759 | prompt done, n_tokens = 17048, batch.n_tokens = 64
slot init_sampler: id  2 | task 42759 | init sampler, took 2.98 ms, tokens: text = 17048, total = 17048
slot update_slots: id  2 | task 42759 | erasing old context checkpoint (pos_min = 10692, pos_max = 11588, size = 21.034 MiB)
slot update_slots: id  2 | task 42759 | created context checkpoint 8 of 8 (pos_min = 16087, pos_max = 16983, size = 21.034 MiB)
slot print_timing: id  2 | task 42759 | 
prompt eval time =     599.08 ms /   258 tokens (    2.32 ms per token,   430.66 tokens per second)
       eval time =    4095.80 ms /   152 tokens (   26.95 ms per token,    37.11 tokens per second)
      total time =    4694.89 ms /   410 tokens
slot      release: id  2 | task 42759 | stop processing: n_tokens = 17199, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 42913 | processing task, is_child = 0
slot update_slots: id  2 | task 42913 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17108
slot update_slots: id  2 | task 42913 | n_tokens = 17048, memory_seq_rm [17048, end)
slot update_slots: id  2 | task 42913 | prompt processing progress, n_tokens = 17108, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  2 | task 42913 | prompt done, n_tokens = 17108, batch.n_tokens = 60
slot init_sampler: id  2 | task 42913 | init sampler, took 3.26 ms, tokens: text = 17108, total = 17108
slot print_timing: id  2 | task 42913 | 
prompt eval time =     179.81 ms /    60 tokens (    3.00 ms per token,   333.69 tokens per second)
       eval time =    2665.48 ms /   100 tokens (   26.65 ms per token,    37.52 tokens per second)
      total time =    2845.29 ms /   160 tokens
slot      release: id  2 | task 42913 | stop processing: n_tokens = 17207, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43014 | processing task, is_child = 0
slot update_slots: id  2 | task 43014 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17189
slot update_slots: id  2 | task 43014 | n_tokens = 17108, memory_seq_rm [17108, end)
slot update_slots: id  2 | task 43014 | prompt processing progress, n_tokens = 17125, batch.n_tokens = 17, progress = 0.996277
slot update_slots: id  2 | task 43014 | n_tokens = 17125, memory_seq_rm [17125, end)
slot update_slots: id  2 | task 43014 | prompt processing progress, n_tokens = 17189, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 43014 | prompt done, n_tokens = 17189, batch.n_tokens = 64
slot init_sampler: id  2 | task 43014 | init sampler, took 2.51 ms, tokens: text = 17189, total = 17189
slot update_slots: id  2 | task 43014 | erasing old context checkpoint (pos_min = 11938, pos_max = 12834, size = 21.034 MiB)
slot update_slots: id  2 | task 43014 | created context checkpoint 8 of 8 (pos_min = 16310, pos_max = 17124, size = 19.111 MiB)
slot print_timing: id  2 | task 43014 | 
prompt eval time =     317.29 ms /    81 tokens (    3.92 ms per token,   255.29 tokens per second)
       eval time =    3365.90 ms /   126 tokens (   26.71 ms per token,    37.43 tokens per second)
      total time =    3683.19 ms /   207 tokens
slot      release: id  2 | task 43014 | stop processing: n_tokens = 17314, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43142 | processing task, is_child = 0
slot update_slots: id  2 | task 43142 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 17251
slot update_slots: id  2 | task 43142 | n_tokens = 17189, memory_seq_rm [17189, end)
slot update_slots: id  2 | task 43142 | prompt processing progress, n_tokens = 17251, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  2 | task 43142 | prompt done, n_tokens = 17251, batch.n_tokens = 62
slot init_sampler: id  2 | task 43142 | init sampler, took 2.42 ms, tokens: text = 17251, total = 17251
slot print_timing: id  2 | task 43142 | 
prompt eval time =     217.17 ms /    62 tokens (    3.50 ms per token,   285.49 tokens per second)
       eval time =   31529.59 ms /  1169 tokens (   26.97 ms per token,    37.08 tokens per second)
      total time =   31746.76 ms /  1231 tokens
slot      release: id  2 | task 43142 | stop processing: n_tokens = 18419, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.938 (> 0.100 thold), f_keep = 0.937
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44312 | processing task, is_child = 0
slot update_slots: id  2 | task 44312 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18389
slot update_slots: id  2 | task 44312 | n_past = 17251, slot.prompt.tokens.size() = 18419, seq_id = 2, pos_min = 17522, n_swa = 128
slot update_slots: id  2 | task 44312 | restored context checkpoint (pos_min = 16310, pos_max = 17124, size = 19.111 MiB)
slot update_slots: id  2 | task 44312 | n_tokens = 17124, memory_seq_rm [17124, end)
slot update_slots: id  2 | task 44312 | prompt processing progress, n_tokens = 18325, batch.n_tokens = 1201, progress = 0.996520
slot update_slots: id  2 | task 44312 | n_tokens = 18325, memory_seq_rm [18325, end)
slot update_slots: id  2 | task 44312 | prompt processing progress, n_tokens = 18389, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44312 | prompt done, n_tokens = 18389, batch.n_tokens = 64
slot init_sampler: id  2 | task 44312 | init sampler, took 3.23 ms, tokens: text = 18389, total = 18389
slot update_slots: id  2 | task 44312 | erasing old context checkpoint (pos_min = 13184, pos_max = 14080, size = 21.034 MiB)
slot update_slots: id  2 | task 44312 | created context checkpoint 8 of 8 (pos_min = 17428, pos_max = 18324, size = 21.034 MiB)
slot print_timing: id  2 | task 44312 | 
prompt eval time =    2191.41 ms /  1265 tokens (    1.73 ms per token,   577.25 tokens per second)
       eval time =     946.51 ms /    36 tokens (   26.29 ms per token,    38.03 tokens per second)
      total time =    3137.92 ms /  1301 tokens
slot      release: id  2 | task 44312 | stop processing: n_tokens = 18424, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.626 (> 0.100 thold), f_keep = 0.505
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44350 | processing task, is_child = 0
slot update_slots: id  2 | task 44350 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14855
slot update_slots: id  2 | task 44350 | n_past = 9304, slot.prompt.tokens.size() = 18424, seq_id = 2, pos_min = 17527, n_swa = 128
slot update_slots: id  2 | task 44350 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 13567, pos_max = 14157, n_swa = 128, size = 13.859 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 14556, pos_max = 15403, n_swa = 128, size = 19.885 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 14927, pos_max = 15561, n_swa = 128, size = 14.890 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 14927, pos_max = 15660, n_swa = 128, size = 17.212 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 15829, pos_max = 16725, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 16087, pos_max = 16983, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 16310, pos_max = 17124, n_swa = 128, size = 19.111 MiB)
slot update_slots: id  2 | task 44350 | erased invalidated context checkpoint (pos_min = 17428, pos_max = 18324, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 44350 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.137866
slot update_slots: id  2 | task 44350 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.275732
slot update_slots: id  2 | task 44350 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.413598
slot update_slots: id  2 | task 44350 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.551464
slot update_slots: id  2 | task 44350 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.689330
slot update_slots: id  2 | task 44350 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.827196
slot update_slots: id  2 | task 44350 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.965062
slot update_slots: id  2 | task 44350 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 14791, batch.n_tokens = 455, progress = 0.995692
slot update_slots: id  2 | task 44350 | n_tokens = 14791, memory_seq_rm [14791, end)
slot update_slots: id  2 | task 44350 | prompt processing progress, n_tokens = 14855, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44350 | prompt done, n_tokens = 14855, batch.n_tokens = 64
slot init_sampler: id  2 | task 44350 | init sampler, took 2.10 ms, tokens: text = 14855, total = 14855
slot update_slots: id  2 | task 44350 | created context checkpoint 1 of 8 (pos_min = 13894, pos_max = 14790, size = 21.034 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 44350
slot      release: id  2 | task 44350 | stop processing: n_tokens = 14859, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44365 | processing task, is_child = 0
slot update_slots: id  2 | task 44365 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14853
slot update_slots: id  2 | task 44365 | n_tokens = 14845, memory_seq_rm [14845, end)
slot update_slots: id  2 | task 44365 | prompt processing progress, n_tokens = 14853, batch.n_tokens = 8, progress = 1.000000
slot update_slots: id  2 | task 44365 | prompt done, n_tokens = 14853, batch.n_tokens = 8
slot init_sampler: id  2 | task 44365 | init sampler, took 2.06 ms, tokens: text = 14853, total = 14853
slot print_timing: id  2 | task 44365 | 
prompt eval time =     118.85 ms /     8 tokens (   14.86 ms per token,    67.31 tokens per second)
       eval time =    2585.46 ms /    97 tokens (   26.65 ms per token,    37.52 tokens per second)
      total time =    2704.31 ms /   105 tokens
slot      release: id  2 | task 44365 | stop processing: n_tokens = 14949, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44463 | processing task, is_child = 0
slot update_slots: id  2 | task 44463 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 14949
slot update_slots: id  2 | task 44463 | n_tokens = 14853, memory_seq_rm [14853, end)
slot update_slots: id  2 | task 44463 | prompt processing progress, n_tokens = 14885, batch.n_tokens = 32, progress = 0.995719
slot update_slots: id  2 | task 44463 | n_tokens = 14885, memory_seq_rm [14885, end)
slot update_slots: id  2 | task 44463 | prompt processing progress, n_tokens = 14949, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44463 | prompt done, n_tokens = 14949, batch.n_tokens = 64
slot init_sampler: id  2 | task 44463 | init sampler, took 2.32 ms, tokens: text = 14949, total = 14949
slot update_slots: id  2 | task 44463 | created context checkpoint 2 of 8 (pos_min = 14052, pos_max = 14884, size = 19.533 MiB)
slot print_timing: id  2 | task 44463 | 
prompt eval time =     339.12 ms /    96 tokens (    3.53 ms per token,   283.09 tokens per second)
       eval time =    1746.70 ms /    66 tokens (   26.47 ms per token,    37.79 tokens per second)
      total time =    2085.82 ms /   162 tokens
slot      release: id  2 | task 44463 | stop processing: n_tokens = 15014, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44531 | processing task, is_child = 0
slot update_slots: id  2 | task 44531 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15007
slot update_slots: id  2 | task 44531 | n_tokens = 14949, memory_seq_rm [14949, end)
slot update_slots: id  2 | task 44531 | prompt processing progress, n_tokens = 15007, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  2 | task 44531 | prompt done, n_tokens = 15007, batch.n_tokens = 58
slot init_sampler: id  2 | task 44531 | init sampler, took 2.14 ms, tokens: text = 15007, total = 15007
slot print_timing: id  2 | task 44531 | 
prompt eval time =     178.68 ms /    58 tokens (    3.08 ms per token,   324.60 tokens per second)
       eval time =     857.57 ms /    32 tokens (   26.80 ms per token,    37.31 tokens per second)
      total time =    1036.25 ms /    90 tokens
slot      release: id  2 | task 44531 | stop processing: n_tokens = 15038, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.929 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44564 | processing task, is_child = 0
slot update_slots: id  2 | task 44564 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16152
slot update_slots: id  2 | task 44564 | n_tokens = 15007, memory_seq_rm [15007, end)
slot update_slots: id  2 | task 44564 | prompt processing progress, n_tokens = 16088, batch.n_tokens = 1081, progress = 0.996038
slot update_slots: id  2 | task 44564 | n_tokens = 16088, memory_seq_rm [16088, end)
slot update_slots: id  2 | task 44564 | prompt processing progress, n_tokens = 16152, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44564 | prompt done, n_tokens = 16152, batch.n_tokens = 64
slot init_sampler: id  2 | task 44564 | init sampler, took 2.30 ms, tokens: text = 16152, total = 16152
slot update_slots: id  2 | task 44564 | created context checkpoint 3 of 8 (pos_min = 15191, pos_max = 16087, size = 21.034 MiB)
slot print_timing: id  2 | task 44564 | 
prompt eval time =    1812.48 ms /  1145 tokens (    1.58 ms per token,   631.73 tokens per second)
       eval time =    8096.06 ms /   298 tokens (   27.17 ms per token,    36.81 tokens per second)
      total time =    9908.54 ms /  1443 tokens
slot      release: id  2 | task 44564 | stop processing: n_tokens = 16449, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44864 | processing task, is_child = 0
slot update_slots: id  2 | task 44864 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16424
slot update_slots: id  2 | task 44864 | n_tokens = 16152, memory_seq_rm [16152, end)
slot update_slots: id  2 | task 44864 | prompt processing progress, n_tokens = 16360, batch.n_tokens = 208, progress = 0.996103
slot update_slots: id  2 | task 44864 | n_tokens = 16360, memory_seq_rm [16360, end)
slot update_slots: id  2 | task 44864 | prompt processing progress, n_tokens = 16424, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44864 | prompt done, n_tokens = 16424, batch.n_tokens = 64
slot init_sampler: id  2 | task 44864 | init sampler, took 2.37 ms, tokens: text = 16424, total = 16424
slot update_slots: id  2 | task 44864 | created context checkpoint 4 of 8 (pos_min = 15552, pos_max = 16359, size = 18.947 MiB)
slot print_timing: id  2 | task 44864 | 
prompt eval time =     622.74 ms /   272 tokens (    2.29 ms per token,   436.78 tokens per second)
       eval time =    1532.22 ms /    56 tokens (   27.36 ms per token,    36.55 tokens per second)
      total time =    2154.95 ms /   328 tokens
slot      release: id  2 | task 44864 | stop processing: n_tokens = 16479, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44922 | processing task, is_child = 0
slot update_slots: id  2 | task 44922 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 16498
slot update_slots: id  2 | task 44922 | n_tokens = 16424, memory_seq_rm [16424, end)
slot update_slots: id  2 | task 44922 | prompt processing progress, n_tokens = 16434, batch.n_tokens = 10, progress = 0.996121
slot update_slots: id  2 | task 44922 | n_tokens = 16434, memory_seq_rm [16434, end)
slot update_slots: id  2 | task 44922 | prompt processing progress, n_tokens = 16498, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44922 | prompt done, n_tokens = 16498, batch.n_tokens = 64
slot init_sampler: id  2 | task 44922 | init sampler, took 2.41 ms, tokens: text = 16498, total = 16498
slot update_slots: id  2 | task 44922 | created context checkpoint 5 of 8 (pos_min = 15582, pos_max = 16433, size = 19.979 MiB)
slot print_timing: id  2 | task 44922 | 
prompt eval time =     293.73 ms /    74 tokens (    3.97 ms per token,   251.93 tokens per second)
       eval time =    1401.95 ms /    52 tokens (   26.96 ms per token,    37.09 tokens per second)
      total time =    1695.68 ms /   126 tokens
slot      release: id  2 | task 44922 | stop processing: n_tokens = 16549, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.028
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 16500, total state size = 389.886 MiB
srv          load:  - looking for better prompt, base f_keep = 0.028, sim = 0.987
srv          load:  - found better prompt with f_keep = 0.409, sim = 0.989
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
srv          load: failed to restore state with size 52618928
slot  prompt_load: id  3 | task -1 | failed to load prompt from cache
slot prompt_clear: id  3 | task -1 | clearing prompt with 16500 tokens
srv        update:  - cache state: 4 prompts, 1523.629 MiB (limits: 8192.000 MiB, 64000 tokens, 196623 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv        update:    - prompt 0x5d0e7b4386e0:    9556 tokens, checkpoints:  8,   432.073 MiB
srv        update:    - prompt 0x5d0e82e23e50:   16500 tokens, checkpoints:  8,   566.998 MiB
srv  get_availabl: prompt cache update took 418.58 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 44976 | processing task, is_child = 0
slot update_slots: id  3 | task 44976 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 461
slot update_slots: id  3 | task 44976 | erased invalidated context checkpoint (pos_min = 11273, pos_max = 12296, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 44976 | erased invalidated context checkpoint (pos_min = 11426, pos_max = 12398, n_swa = 128, size = 22.816 MiB)
slot update_slots: id  3 | task 44976 | erased invalidated context checkpoint (pos_min = 11926, pos_max = 12949, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 44976 | erased invalidated context checkpoint (pos_min = 14349, pos_max = 15372, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 44976 | erased invalidated context checkpoint (pos_min = 14692, pos_max = 15715, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 44976 | erased invalidated context checkpoint (pos_min = 15012, pos_max = 16035, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 44976 | erased invalidated context checkpoint (pos_min = 15536, pos_max = 16275, n_swa = 128, size = 17.353 MiB)
slot update_slots: id  3 | task 44976 | erased invalidated context checkpoint (pos_min = 15636, pos_max = 16355, n_swa = 128, size = 16.884 MiB)
slot update_slots: id  3 | task 44976 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 44976 | prompt processing progress, n_tokens = 397, batch.n_tokens = 397, progress = 0.861171
slot update_slots: id  3 | task 44976 | n_tokens = 397, memory_seq_rm [397, end)
slot update_slots: id  3 | task 44976 | prompt processing progress, n_tokens = 461, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 44976 | prompt done, n_tokens = 461, batch.n_tokens = 64
slot init_sampler: id  3 | task 44976 | init sampler, took 0.09 ms, tokens: text = 461, total = 461
slot update_slots: id  3 | task 44976 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 396, size = 9.310 MiB)
slot print_timing: id  3 | task 44976 | 
prompt eval time =     833.63 ms /   461 tokens (    1.81 ms per token,   553.00 tokens per second)
       eval time =     782.11 ms /    30 tokens (   26.07 ms per token,    38.36 tokens per second)
      total time =    1615.74 ms /   491 tokens
slot      release: id  3 | task 44976 | stop processing: n_tokens = 490, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.876 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 45008 | processing task, is_child = 0
slot update_slots: id  3 | task 45008 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 526
slot update_slots: id  3 | task 45008 | n_tokens = 461, memory_seq_rm [461, end)
slot update_slots: id  3 | task 45008 | prompt processing progress, n_tokens = 462, batch.n_tokens = 1, progress = 0.878327
slot update_slots: id  3 | task 45008 | n_tokens = 462, memory_seq_rm [462, end)
slot update_slots: id  3 | task 45008 | prompt processing progress, n_tokens = 526, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 45008 | prompt done, n_tokens = 526, batch.n_tokens = 64
slot init_sampler: id  3 | task 45008 | init sampler, took 0.09 ms, tokens: text = 526, total = 526
slot update_slots: id  3 | task 45008 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 461, size = 10.834 MiB)
slot print_timing: id  3 | task 45008 | 
prompt eval time =     305.78 ms /    65 tokens (    4.70 ms per token,   212.57 tokens per second)
       eval time =    1522.76 ms /    57 tokens (   26.72 ms per token,    37.43 tokens per second)
      total time =    1828.54 ms /   122 tokens
slot      release: id  3 | task 45008 | stop processing: n_tokens = 582, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.467 (> 0.100 thold), f_keep = 0.904
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 45067 | processing task, is_child = 0
slot update_slots: id  3 | task 45067 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1126
slot update_slots: id  3 | task 45067 | n_tokens = 526, memory_seq_rm [526, end)
slot update_slots: id  3 | task 45067 | prompt processing progress, n_tokens = 1062, batch.n_tokens = 536, progress = 0.943162
slot update_slots: id  3 | task 45067 | n_tokens = 1062, memory_seq_rm [1062, end)
slot update_slots: id  3 | task 45067 | prompt processing progress, n_tokens = 1126, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 45067 | prompt done, n_tokens = 1126, batch.n_tokens = 64
slot init_sampler: id  3 | task 45067 | init sampler, took 0.21 ms, tokens: text = 1126, total = 1126
slot update_slots: id  3 | task 45067 | created context checkpoint 3 of 8 (pos_min = 392, pos_max = 1061, size = 15.711 MiB)
slot print_timing: id  3 | task 45067 | 
prompt eval time =    1020.01 ms /   600 tokens (    1.70 ms per token,   588.23 tokens per second)
       eval time =    1832.04 ms /    70 tokens (   26.17 ms per token,    38.21 tokens per second)
      total time =    2852.05 ms /   670 tokens
slot      release: id  3 | task 45067 | stop processing: n_tokens = 1195, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.152 (> 0.100 thold), f_keep = 0.381
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1195, total state size = 46.734 MiB
srv          load:  - looking for better prompt, base f_keep = 0.381, sim = 0.152
srv        update:  - cache state: 5 prompts, 1606.217 MiB (limits: 8192.000 MiB, 64000 tokens, 192608 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv        update:    - prompt 0x5d0e7b4386e0:    9556 tokens, checkpoints:  8,   432.073 MiB
srv        update:    - prompt 0x5d0e82e23e50:   16500 tokens, checkpoints:  8,   566.998 MiB
srv        update:    - prompt 0x5d0e803e3740:    1195 tokens, checkpoints:  3,    82.589 MiB
srv  get_availabl: prompt cache update took 33.90 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 45139 | processing task, is_child = 0
slot update_slots: id  3 | task 45139 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2994
slot update_slots: id  3 | task 45139 | n_past = 455, slot.prompt.tokens.size() = 1195, seq_id = 3, pos_min = 397, n_swa = 128
slot update_slots: id  3 | task 45139 | restored context checkpoint (pos_min = 0, pos_max = 461, size = 10.834 MiB)
slot update_slots: id  3 | task 45139 | erased invalidated context checkpoint (pos_min = 392, pos_max = 1061, n_swa = 128, size = 15.711 MiB)
slot update_slots: id  3 | task 45139 | n_tokens = 455, memory_seq_rm [455, end)
slot update_slots: id  3 | task 45139 | prompt processing progress, n_tokens = 2503, batch.n_tokens = 2048, progress = 0.836005
slot update_slots: id  3 | task 45139 | n_tokens = 2503, memory_seq_rm [2503, end)
slot update_slots: id  3 | task 45139 | prompt processing progress, n_tokens = 2930, batch.n_tokens = 427, progress = 0.978624
slot update_slots: id  3 | task 45139 | n_tokens = 2930, memory_seq_rm [2930, end)
slot update_slots: id  3 | task 45139 | prompt processing progress, n_tokens = 2994, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 45139 | prompt done, n_tokens = 2994, batch.n_tokens = 64
slot init_sampler: id  3 | task 45139 | init sampler, took 0.51 ms, tokens: text = 2994, total = 2994
slot update_slots: id  3 | task 45139 | created context checkpoint 3 of 8 (pos_min = 2033, pos_max = 2929, size = 21.034 MiB)
slot print_timing: id  3 | task 45139 | 
prompt eval time =    3803.14 ms /  2539 tokens (    1.50 ms per token,   667.61 tokens per second)
       eval time =    1056.74 ms /    40 tokens (   26.42 ms per token,    37.85 tokens per second)
      total time =    4859.88 ms /  2579 tokens
slot      release: id  3 | task 45139 | stop processing: n_tokens = 3033, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.834 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 45182 | processing task, is_child = 0
slot update_slots: id  3 | task 45182 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3590
slot update_slots: id  3 | task 45182 | n_tokens = 2994, memory_seq_rm [2994, end)
slot update_slots: id  3 | task 45182 | prompt processing progress, n_tokens = 3526, batch.n_tokens = 532, progress = 0.982173
slot update_slots: id  3 | task 45182 | n_tokens = 3526, memory_seq_rm [3526, end)
slot update_slots: id  3 | task 45182 | prompt processing progress, n_tokens = 3590, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 45182 | prompt done, n_tokens = 3590, batch.n_tokens = 64
slot init_sampler: id  3 | task 45182 | init sampler, took 0.57 ms, tokens: text = 3590, total = 3590
slot update_slots: id  3 | task 45182 | created context checkpoint 4 of 8 (pos_min = 2654, pos_max = 3525, size = 20.448 MiB)
slot print_timing: id  3 | task 45182 | 
prompt eval time =    1034.81 ms /   596 tokens (    1.74 ms per token,   575.95 tokens per second)
       eval time =    1992.12 ms /    75 tokens (   26.56 ms per token,    37.65 tokens per second)
      total time =    3026.93 ms /   671 tokens
slot      release: id  3 | task 45182 | stop processing: n_tokens = 3664, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.631 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 45259 | processing task, is_child = 0
slot update_slots: id  3 | task 45259 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5693
slot update_slots: id  3 | task 45259 | n_tokens = 3590, memory_seq_rm [3590, end)
slot update_slots: id  3 | task 45259 | prompt processing progress, n_tokens = 5629, batch.n_tokens = 2039, progress = 0.988758
slot update_slots: id  3 | task 45259 | n_tokens = 5629, memory_seq_rm [5629, end)
slot update_slots: id  3 | task 45259 | prompt processing progress, n_tokens = 5693, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 45259 | prompt done, n_tokens = 5693, batch.n_tokens = 64
slot init_sampler: id  3 | task 45259 | init sampler, took 0.93 ms, tokens: text = 5693, total = 5693
slot update_slots: id  3 | task 45259 | created context checkpoint 5 of 8 (pos_min = 4732, pos_max = 5628, size = 21.034 MiB)
slot print_timing: id  3 | task 45259 | 
prompt eval time =    3087.27 ms /  2103 tokens (    1.47 ms per token,   681.18 tokens per second)
       eval time =    1072.28 ms /    40 tokens (   26.81 ms per token,    37.30 tokens per second)
      total time =    4159.55 ms /  2143 tokens
slot      release: id  3 | task 45259 | stop processing: n_tokens = 5732, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.838 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 45301 | processing task, is_child = 0
slot update_slots: id  3 | task 45301 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6794
slot update_slots: id  3 | task 45301 | n_tokens = 5693, memory_seq_rm [5693, end)
slot update_slots: id  3 | task 45301 | prompt processing progress, n_tokens = 6730, batch.n_tokens = 1037, progress = 0.990580
slot update_slots: id  3 | task 45301 | n_tokens = 6730, memory_seq_rm [6730, end)
slot update_slots: id  3 | task 45301 | prompt processing progress, n_tokens = 6794, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 45301 | prompt done, n_tokens = 6794, batch.n_tokens = 64
slot init_sampler: id  3 | task 45301 | init sampler, took 1.64 ms, tokens: text = 6794, total = 6794
slot update_slots: id  3 | task 45301 | created context checkpoint 6 of 8 (pos_min = 5833, pos_max = 6729, size = 21.034 MiB)
slot print_timing: id  3 | task 45301 | 
prompt eval time =    1781.75 ms /  1101 tokens (    1.62 ms per token,   617.93 tokens per second)
       eval time =   51878.41 ms /  1915 tokens (   27.09 ms per token,    36.91 tokens per second)
      total time =   53660.17 ms /  3016 tokens
slot      release: id  3 | task 45301 | stop processing: n_tokens = 8708, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.300 (> 0.100 thold), f_keep = 0.031
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 16549, total state size = 391.035 MiB
srv          load:  - looking for better prompt, base f_keep = 0.031, sim = 0.300
srv        update:  - cache state: 6 prompts, 2097.780 MiB (limits: 8192.000 MiB, 64000 tokens, 212100 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv        update:    - prompt 0x5d0e7b4386e0:    9556 tokens, checkpoints:  8,   432.073 MiB
srv        update:    - prompt 0x5d0e82e23e50:   16500 tokens, checkpoints:  8,   566.998 MiB
srv        update:    - prompt 0x5d0e803e3740:    1195 tokens, checkpoints:  3,    82.589 MiB
srv        update:    - prompt 0x5d0e9ea7e990:   16549 tokens, checkpoints:  5,   491.562 MiB
srv  get_availabl: prompt cache update took 373.47 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47218 | processing task, is_child = 0
slot update_slots: id  2 | task 47218 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1716
slot update_slots: id  2 | task 47218 | n_past = 514, slot.prompt.tokens.size() = 16549, seq_id = 2, pos_min = 16422, n_swa = 128
slot update_slots: id  2 | task 47218 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 47218 | erased invalidated context checkpoint (pos_min = 13894, pos_max = 14790, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 47218 | erased invalidated context checkpoint (pos_min = 14052, pos_max = 14884, n_swa = 128, size = 19.533 MiB)
slot update_slots: id  2 | task 47218 | erased invalidated context checkpoint (pos_min = 15191, pos_max = 16087, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 47218 | erased invalidated context checkpoint (pos_min = 15552, pos_max = 16359, n_swa = 128, size = 18.947 MiB)
slot update_slots: id  2 | task 47218 | erased invalidated context checkpoint (pos_min = 15582, pos_max = 16433, n_swa = 128, size = 19.979 MiB)
slot update_slots: id  2 | task 47218 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 47218 | prompt processing progress, n_tokens = 1652, batch.n_tokens = 1652, progress = 0.962704
slot update_slots: id  2 | task 47218 | n_tokens = 1652, memory_seq_rm [1652, end)
slot update_slots: id  2 | task 47218 | prompt processing progress, n_tokens = 1716, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47218 | prompt done, n_tokens = 1716, batch.n_tokens = 64
slot init_sampler: id  2 | task 47218 | init sampler, took 0.32 ms, tokens: text = 1716, total = 1716
slot update_slots: id  2 | task 47218 | created context checkpoint 1 of 8 (pos_min = 755, pos_max = 1651, size = 21.034 MiB)
slot print_timing: id  2 | task 47218 | 
prompt eval time =    2095.33 ms /  1716 tokens (    1.22 ms per token,   818.96 tokens per second)
       eval time =   28119.35 ms /  1154 tokens (   24.37 ms per token,    41.04 tokens per second)
      total time =   30214.68 ms /  2870 tokens
slot      release: id  2 | task 47218 | stop processing: n_tokens = 2869, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.579 (> 0.100 thold), f_keep = 0.598
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 48374 | processing task, is_child = 0
slot update_slots: id  2 | task 48374 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2964
slot update_slots: id  2 | task 48374 | n_past = 1716, slot.prompt.tokens.size() = 2869, seq_id = 2, pos_min = 1972, n_swa = 128
slot update_slots: id  2 | task 48374 | restored context checkpoint (pos_min = 755, pos_max = 1651, size = 21.034 MiB)
slot update_slots: id  2 | task 48374 | n_tokens = 1651, memory_seq_rm [1651, end)
slot update_slots: id  2 | task 48374 | prompt processing progress, n_tokens = 2900, batch.n_tokens = 1249, progress = 0.978408
slot update_slots: id  2 | task 48374 | n_tokens = 2900, memory_seq_rm [2900, end)
slot update_slots: id  2 | task 48374 | prompt processing progress, n_tokens = 2964, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 48374 | prompt done, n_tokens = 2964, batch.n_tokens = 64
slot init_sampler: id  2 | task 48374 | init sampler, took 0.51 ms, tokens: text = 2964, total = 2964
slot update_slots: id  2 | task 48374 | created context checkpoint 2 of 8 (pos_min = 2003, pos_max = 2899, size = 21.034 MiB)
slot print_timing: id  2 | task 48374 | 
prompt eval time =    1826.67 ms /  1313 tokens (    1.39 ms per token,   718.79 tokens per second)
       eval time =     473.91 ms /    20 tokens (   23.70 ms per token,    42.20 tokens per second)
      total time =    2300.58 ms /  1333 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 48374 | stop processing: n_tokens = 2983, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 48396 | processing task, is_child = 0
slot update_slots: id  1 | task 48396 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5910
slot update_slots: id  1 | task 48396 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 48396 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.346531
slot update_slots: id  1 | task 48396 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 48396 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.693063
slot update_slots: id  1 | task 48396 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 48396 | prompt processing progress, n_tokens = 5846, batch.n_tokens = 1750, progress = 0.989171
slot update_slots: id  1 | task 48396 | n_tokens = 5846, memory_seq_rm [5846, end)
slot update_slots: id  1 | task 48396 | prompt processing progress, n_tokens = 5910, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 48396 | prompt done, n_tokens = 5910, batch.n_tokens = 64
slot init_sampler: id  1 | task 48396 | init sampler, took 1.57 ms, tokens: text = 5910, total = 5910
slot update_slots: id  1 | task 48396 | created context checkpoint 1 of 8 (pos_min = 5076, pos_max = 5845, size = 18.056 MiB)
slot print_timing: id  1 | task 48396 | 
prompt eval time =    6733.54 ms /  5910 tokens (    1.14 ms per token,   877.70 tokens per second)
       eval time =   32544.16 ms /  1270 tokens (   25.63 ms per token,    39.02 tokens per second)
      total time =   39277.71 ms /  7180 tokens
slot      release: id  1 | task 48396 | stop processing: n_tokens = 7179, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.146 (> 0.100 thold), f_keep = 0.014
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7179, total state size = 186.396 MiB
srv          load:  - looking for better prompt, base f_keep = 0.014, sim = 0.146
srv        update:  - cache state: 7 prompts, 2302.232 MiB (limits: 8192.000 MiB, 64000 tokens, 218809 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv        update:    - prompt 0x5d0e7b4386e0:    9556 tokens, checkpoints:  8,   432.073 MiB
srv        update:    - prompt 0x5d0e82e23e50:   16500 tokens, checkpoints:  8,   566.998 MiB
srv        update:    - prompt 0x5d0e803e3740:    1195 tokens, checkpoints:  3,    82.589 MiB
srv        update:    - prompt 0x5d0e9ea7e990:   16549 tokens, checkpoints:  5,   491.562 MiB
srv        update:    - prompt 0x5d0e98c87f00:    7179 tokens, checkpoints:  1,   204.452 MiB
srv  get_availabl: prompt cache update took 151.84 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 49670 | processing task, is_child = 0
slot update_slots: id  1 | task 49670 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 665
slot update_slots: id  1 | task 49670 | n_past = 97, slot.prompt.tokens.size() = 7179, seq_id = 1, pos_min = 6409, n_swa = 128
slot update_slots: id  1 | task 49670 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 49670 | erased invalidated context checkpoint (pos_min = 5076, pos_max = 5845, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 49670 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 49670 | prompt processing progress, n_tokens = 601, batch.n_tokens = 601, progress = 0.903759
slot update_slots: id  1 | task 49670 | n_tokens = 601, memory_seq_rm [601, end)
slot update_slots: id  1 | task 49670 | prompt processing progress, n_tokens = 665, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 49670 | prompt done, n_tokens = 665, batch.n_tokens = 64
slot init_sampler: id  1 | task 49670 | init sampler, took 0.14 ms, tokens: text = 665, total = 665
slot update_slots: id  1 | task 49670 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 600, size = 14.093 MiB)
slot print_timing: id  1 | task 49670 | 
prompt eval time =    1010.59 ms /   665 tokens (    1.52 ms per token,   658.03 tokens per second)
       eval time =     732.88 ms /    32 tokens (   22.90 ms per token,    43.66 tokens per second)
      total time =    1743.47 ms /   697 tokens
slot      release: id  1 | task 49670 | stop processing: n_tokens = 696, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.524 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 49704 | processing task, is_child = 0
slot update_slots: id  1 | task 49704 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1270
slot update_slots: id  1 | task 49704 | n_tokens = 665, memory_seq_rm [665, end)
slot update_slots: id  1 | task 49704 | prompt processing progress, n_tokens = 1206, batch.n_tokens = 541, progress = 0.949606
slot update_slots: id  1 | task 49704 | n_tokens = 1206, memory_seq_rm [1206, end)
slot update_slots: id  1 | task 49704 | prompt processing progress, n_tokens = 1270, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 49704 | prompt done, n_tokens = 1270, batch.n_tokens = 64
slot init_sampler: id  1 | task 49704 | init sampler, took 0.22 ms, tokens: text = 1270, total = 1270
slot update_slots: id  1 | task 49704 | created context checkpoint 2 of 8 (pos_min = 436, pos_max = 1205, size = 18.056 MiB)
slot print_timing: id  1 | task 49704 | 
prompt eval time =     810.05 ms /   605 tokens (    1.34 ms per token,   746.86 tokens per second)
       eval time =    1242.69 ms /    52 tokens (   23.90 ms per token,    41.84 tokens per second)
      total time =    2052.74 ms /   657 tokens
slot      release: id  1 | task 49704 | stop processing: n_tokens = 1321, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.424 (> 0.100 thold), f_keep = 0.961
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 49758 | processing task, is_child = 0
slot update_slots: id  1 | task 49758 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2992
slot update_slots: id  1 | task 49758 | n_tokens = 1270, memory_seq_rm [1270, end)
slot update_slots: id  1 | task 49758 | prompt processing progress, n_tokens = 2928, batch.n_tokens = 1658, progress = 0.978610
slot update_slots: id  1 | task 49758 | n_tokens = 2928, memory_seq_rm [2928, end)
slot update_slots: id  1 | task 49758 | prompt processing progress, n_tokens = 2992, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 49758 | prompt done, n_tokens = 2992, batch.n_tokens = 64
slot init_sampler: id  1 | task 49758 | init sampler, took 0.45 ms, tokens: text = 2992, total = 2992
slot update_slots: id  1 | task 49758 | created context checkpoint 3 of 8 (pos_min = 2158, pos_max = 2927, size = 18.056 MiB)
slot print_timing: id  1 | task 49758 | 
prompt eval time =    2111.83 ms /  1722 tokens (    1.23 ms per token,   815.40 tokens per second)
       eval time =    1228.14 ms /    50 tokens (   24.56 ms per token,    40.71 tokens per second)
      total time =    3339.97 ms /  1772 tokens
slot      release: id  1 | task 49758 | stop processing: n_tokens = 3041, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.694 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 49810 | processing task, is_child = 0
slot update_slots: id  1 | task 49810 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4313
slot update_slots: id  1 | task 49810 | n_tokens = 2992, memory_seq_rm [2992, end)
slot update_slots: id  1 | task 49810 | prompt processing progress, n_tokens = 4249, batch.n_tokens = 1257, progress = 0.985161
slot update_slots: id  1 | task 49810 | n_tokens = 4249, memory_seq_rm [4249, end)
slot update_slots: id  1 | task 49810 | prompt processing progress, n_tokens = 4313, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 49810 | prompt done, n_tokens = 4313, batch.n_tokens = 64
slot init_sampler: id  1 | task 49810 | init sampler, took 0.81 ms, tokens: text = 4313, total = 4313
slot update_slots: id  1 | task 49810 | created context checkpoint 4 of 8 (pos_min = 3479, pos_max = 4248, size = 18.056 MiB)
slot print_timing: id  1 | task 49810 | 
prompt eval time =    1666.55 ms /  1321 tokens (    1.26 ms per token,   792.66 tokens per second)
       eval time =    1155.37 ms /    46 tokens (   25.12 ms per token,    39.81 tokens per second)
      total time =    2821.92 ms /  1367 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 49810 | stop processing: n_tokens = 4358, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.672 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 49858 | processing task, is_child = 0
slot update_slots: id  1 | task 49858 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6416
slot update_slots: id  1 | task 49858 | n_tokens = 4313, memory_seq_rm [4313, end)
slot update_slots: id  1 | task 49858 | prompt processing progress, n_tokens = 6352, batch.n_tokens = 2039, progress = 0.990025
slot update_slots: id  1 | task 49858 | n_tokens = 6352, memory_seq_rm [6352, end)
slot update_slots: id  1 | task 49858 | prompt processing progress, n_tokens = 6416, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 49858 | prompt done, n_tokens = 6416, batch.n_tokens = 64
slot init_sampler: id  1 | task 49858 | init sampler, took 0.99 ms, tokens: text = 6416, total = 6416
slot update_slots: id  1 | task 49858 | created context checkpoint 5 of 8 (pos_min = 5582, pos_max = 6351, size = 18.056 MiB)
slot print_timing: id  1 | task 49858 | 
prompt eval time =    2481.86 ms /  2103 tokens (    1.18 ms per token,   847.35 tokens per second)
       eval time =     962.11 ms /    39 tokens (   24.67 ms per token,    40.54 tokens per second)
      total time =    3443.97 ms /  2142 tokens
slot      release: id  1 | task 49858 | stop processing: n_tokens = 6454, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.854 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 49899 | processing task, is_child = 0
slot update_slots: id  1 | task 49899 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7517
slot update_slots: id  1 | task 49899 | n_tokens = 6416, memory_seq_rm [6416, end)
slot update_slots: id  1 | task 49899 | prompt processing progress, n_tokens = 7453, batch.n_tokens = 1037, progress = 0.991486
slot update_slots: id  1 | task 49899 | n_tokens = 7453, memory_seq_rm [7453, end)
slot update_slots: id  1 | task 49899 | prompt processing progress, n_tokens = 7517, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 49899 | prompt done, n_tokens = 7517, batch.n_tokens = 64
slot init_sampler: id  1 | task 49899 | init sampler, took 1.11 ms, tokens: text = 7517, total = 7517
slot update_slots: id  1 | task 49899 | created context checkpoint 6 of 8 (pos_min = 6683, pos_max = 7452, size = 18.056 MiB)
slot print_timing: id  1 | task 49899 | 
prompt eval time =    1469.69 ms /  1101 tokens (    1.33 ms per token,   749.14 tokens per second)
       eval time =   19607.90 ms /   773 tokens (   25.37 ms per token,    39.42 tokens per second)
      total time =   21077.60 ms /  1874 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 49899 | stop processing: n_tokens = 8289, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.402 (> 0.100 thold), f_keep = 0.055
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8289, total state size = 212.425 MiB
srv          load:  - looking for better prompt, base f_keep = 0.055, sim = 0.402
srv        update:  - cache state: 8 prompts, 2619.029 MiB (limits: 8192.000 MiB, 64000 tokens, 218269 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv        update:    - prompt 0x5d0e7b4386e0:    9556 tokens, checkpoints:  8,   432.073 MiB
srv        update:    - prompt 0x5d0e82e23e50:   16500 tokens, checkpoints:  8,   566.998 MiB
srv        update:    - prompt 0x5d0e803e3740:    1195 tokens, checkpoints:  3,    82.589 MiB
srv        update:    - prompt 0x5d0e9ea7e990:   16549 tokens, checkpoints:  5,   491.562 MiB
srv        update:    - prompt 0x5d0e98c87f00:    7179 tokens, checkpoints:  1,   204.452 MiB
srv        update:    - prompt 0x5d0e9eb54ef0:    8289 tokens, checkpoints:  6,   316.798 MiB
srv  get_availabl: prompt cache update took 247.23 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 50674 | processing task, is_child = 0
slot update_slots: id  1 | task 50674 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1142
slot update_slots: id  1 | task 50674 | n_past = 459, slot.prompt.tokens.size() = 8289, seq_id = 1, pos_min = 7519, n_swa = 128
slot update_slots: id  1 | task 50674 | restored context checkpoint (pos_min = 0, pos_max = 600, size = 14.093 MiB)
slot update_slots: id  1 | task 50674 | erased invalidated context checkpoint (pos_min = 436, pos_max = 1205, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 50674 | erased invalidated context checkpoint (pos_min = 2158, pos_max = 2927, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 50674 | erased invalidated context checkpoint (pos_min = 3479, pos_max = 4248, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 50674 | erased invalidated context checkpoint (pos_min = 5582, pos_max = 6351, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 50674 | erased invalidated context checkpoint (pos_min = 6683, pos_max = 7452, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 50674 | n_tokens = 459, memory_seq_rm [459, end)
slot update_slots: id  1 | task 50674 | prompt processing progress, n_tokens = 1078, batch.n_tokens = 619, progress = 0.943958
slot update_slots: id  1 | task 50674 | n_tokens = 1078, memory_seq_rm [1078, end)
slot update_slots: id  1 | task 50674 | prompt processing progress, n_tokens = 1142, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 50674 | prompt done, n_tokens = 1142, batch.n_tokens = 64
slot init_sampler: id  1 | task 50674 | init sampler, took 0.21 ms, tokens: text = 1142, total = 1142
slot update_slots: id  1 | task 50674 | created context checkpoint 2 of 8 (pos_min = 308, pos_max = 1077, size = 18.056 MiB)
slot print_timing: id  1 | task 50674 | 
prompt eval time =    1183.95 ms /   683 tokens (    1.73 ms per token,   576.88 tokens per second)
       eval time =   30561.37 ms /  1221 tokens (   25.03 ms per token,    39.95 tokens per second)
      total time =   31745.32 ms /  1904 tokens
slot      release: id  1 | task 50674 | stop processing: n_tokens = 2362, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.557 (> 0.100 thold), f_keep = 0.476
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2362, total state size = 73.443 MiB
srv          load:  - looking for better prompt, base f_keep = 0.476, sim = 0.557
srv        update:  - cache state: 9 prompts, 2724.621 MiB (limits: 8192.000 MiB, 64000 tokens, 216912 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv        update:    - prompt 0x5d0e7b4386e0:    9556 tokens, checkpoints:  8,   432.073 MiB
srv        update:    - prompt 0x5d0e82e23e50:   16500 tokens, checkpoints:  8,   566.998 MiB
srv        update:    - prompt 0x5d0e803e3740:    1195 tokens, checkpoints:  3,    82.589 MiB
srv        update:    - prompt 0x5d0e9ea7e990:   16549 tokens, checkpoints:  5,   491.562 MiB
srv        update:    - prompt 0x5d0e98c87f00:    7179 tokens, checkpoints:  1,   204.452 MiB
srv        update:    - prompt 0x5d0e9eb54ef0:    8289 tokens, checkpoints:  6,   316.798 MiB
srv        update:    - prompt 0x5d0e7bae5850:    2362 tokens, checkpoints:  2,   105.592 MiB
srv  get_availabl: prompt cache update took 71.23 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 51897 | processing task, is_child = 0
slot update_slots: id  1 | task 51897 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2018
slot update_slots: id  1 | task 51897 | n_past = 1124, slot.prompt.tokens.size() = 2362, seq_id = 1, pos_min = 1592, n_swa = 128
slot update_slots: id  1 | task 51897 | restored context checkpoint (pos_min = 308, pos_max = 1077, size = 18.056 MiB)
slot update_slots: id  1 | task 51897 | n_tokens = 1077, memory_seq_rm [1077, end)
slot update_slots: id  1 | task 51897 | prompt processing progress, n_tokens = 1954, batch.n_tokens = 877, progress = 0.968285
slot update_slots: id  1 | task 51897 | n_tokens = 1954, memory_seq_rm [1954, end)
slot update_slots: id  1 | task 51897 | prompt processing progress, n_tokens = 2018, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 51897 | prompt done, n_tokens = 2018, batch.n_tokens = 64
slot init_sampler: id  1 | task 51897 | init sampler, took 0.36 ms, tokens: text = 2018, total = 2018
slot update_slots: id  1 | task 51897 | created context checkpoint 3 of 8 (pos_min = 1184, pos_max = 1953, size = 18.056 MiB)
slot print_timing: id  1 | task 51897 | 
prompt eval time =    1420.64 ms /   941 tokens (    1.51 ms per token,   662.38 tokens per second)
       eval time =    7892.07 ms /   324 tokens (   24.36 ms per token,    41.05 tokens per second)
      total time =    9312.71 ms /  1265 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 51897 | stop processing: n_tokens = 2341, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.490 (> 0.100 thold), f_keep = 0.862
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 52223 | processing task, is_child = 0
slot update_slots: id  1 | task 52223 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4121
slot update_slots: id  1 | task 52223 | n_tokens = 2018, memory_seq_rm [2018, end)
slot update_slots: id  1 | task 52223 | prompt processing progress, n_tokens = 4057, batch.n_tokens = 2039, progress = 0.984470
slot update_slots: id  1 | task 52223 | n_tokens = 4057, memory_seq_rm [4057, end)
slot update_slots: id  1 | task 52223 | prompt processing progress, n_tokens = 4121, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 52223 | prompt done, n_tokens = 4121, batch.n_tokens = 64
slot init_sampler: id  1 | task 52223 | init sampler, took 0.78 ms, tokens: text = 4121, total = 4121
slot update_slots: id  1 | task 52223 | created context checkpoint 4 of 8 (pos_min = 3287, pos_max = 4056, size = 18.056 MiB)
slot print_timing: id  1 | task 52223 | 
prompt eval time =    2531.44 ms /  2103 tokens (    1.20 ms per token,   830.75 tokens per second)
       eval time =   33182.48 ms /  1313 tokens (   25.27 ms per token,    39.57 tokens per second)
      total time =   35713.92 ms /  3416 tokens
slot      release: id  1 | task 52223 | stop processing: n_tokens = 5433, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.809 (> 0.100 thold), f_keep = 0.759
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53538 | processing task, is_child = 0
slot update_slots: id  1 | task 53538 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5092
slot update_slots: id  1 | task 53538 | n_past = 4121, slot.prompt.tokens.size() = 5433, seq_id = 1, pos_min = 4663, n_swa = 128
slot update_slots: id  1 | task 53538 | restored context checkpoint (pos_min = 3287, pos_max = 4056, size = 18.056 MiB)
slot update_slots: id  1 | task 53538 | n_tokens = 4056, memory_seq_rm [4056, end)
slot update_slots: id  1 | task 53538 | prompt processing progress, n_tokens = 5028, batch.n_tokens = 972, progress = 0.987431
slot update_slots: id  1 | task 53538 | n_tokens = 5028, memory_seq_rm [5028, end)
slot update_slots: id  1 | task 53538 | prompt processing progress, n_tokens = 5092, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 53538 | prompt done, n_tokens = 5092, batch.n_tokens = 64
slot init_sampler: id  1 | task 53538 | init sampler, took 0.79 ms, tokens: text = 5092, total = 5092
slot update_slots: id  1 | task 53538 | created context checkpoint 5 of 8 (pos_min = 4258, pos_max = 5027, size = 18.056 MiB)
slot print_timing: id  1 | task 53538 | 
prompt eval time =    1476.66 ms /  1036 tokens (    1.43 ms per token,   701.58 tokens per second)
       eval time =    1635.76 ms /    66 tokens (   24.78 ms per token,    40.35 tokens per second)
      total time =    3112.42 ms /  1102 tokens
slot      release: id  1 | task 53538 | stop processing: n_tokens = 5157, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.911 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53606 | processing task, is_child = 0
slot update_slots: id  1 | task 53606 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5588
slot update_slots: id  1 | task 53606 | n_tokens = 5092, memory_seq_rm [5092, end)
slot update_slots: id  1 | task 53606 | prompt processing progress, n_tokens = 5524, batch.n_tokens = 432, progress = 0.988547
slot update_slots: id  1 | task 53606 | n_tokens = 5524, memory_seq_rm [5524, end)
slot update_slots: id  1 | task 53606 | prompt processing progress, n_tokens = 5588, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 53606 | prompt done, n_tokens = 5588, batch.n_tokens = 64
slot init_sampler: id  1 | task 53606 | init sampler, took 0.84 ms, tokens: text = 5588, total = 5588
slot update_slots: id  1 | task 53606 | created context checkpoint 6 of 8 (pos_min = 4754, pos_max = 5523, size = 18.056 MiB)
slot print_timing: id  1 | task 53606 | 
prompt eval time =     700.97 ms /   496 tokens (    1.41 ms per token,   707.59 tokens per second)
       eval time =    1627.83 ms /    64 tokens (   25.43 ms per token,    39.32 tokens per second)
      total time =    2328.80 ms /   560 tokens
slot      release: id  1 | task 53606 | stop processing: n_tokens = 5651, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.919 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53672 | processing task, is_child = 0
slot update_slots: id  1 | task 53672 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6081
slot update_slots: id  1 | task 53672 | n_tokens = 5588, memory_seq_rm [5588, end)
slot update_slots: id  1 | task 53672 | prompt processing progress, n_tokens = 6017, batch.n_tokens = 429, progress = 0.989475
slot update_slots: id  1 | task 53672 | n_tokens = 6017, memory_seq_rm [6017, end)
slot update_slots: id  1 | task 53672 | prompt processing progress, n_tokens = 6081, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 53672 | prompt done, n_tokens = 6081, batch.n_tokens = 64
slot init_sampler: id  1 | task 53672 | init sampler, took 1.13 ms, tokens: text = 6081, total = 6081
slot update_slots: id  1 | task 53672 | created context checkpoint 7 of 8 (pos_min = 5247, pos_max = 6016, size = 18.056 MiB)
slot print_timing: id  1 | task 53672 | 
prompt eval time =     718.02 ms /   493 tokens (    1.46 ms per token,   686.61 tokens per second)
       eval time =    1503.07 ms /    59 tokens (   25.48 ms per token,    39.25 tokens per second)
      total time =    2221.09 ms /   552 tokens
slot      release: id  1 | task 53672 | stop processing: n_tokens = 6139, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53733 | processing task, is_child = 0
slot update_slots: id  1 | task 53733 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6314
slot update_slots: id  1 | task 53733 | n_tokens = 6081, memory_seq_rm [6081, end)
slot update_slots: id  1 | task 53733 | prompt processing progress, n_tokens = 6250, batch.n_tokens = 169, progress = 0.989864
slot update_slots: id  1 | task 53733 | n_tokens = 6250, memory_seq_rm [6250, end)
slot update_slots: id  1 | task 53733 | prompt processing progress, n_tokens = 6314, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 53733 | prompt done, n_tokens = 6314, batch.n_tokens = 64
slot init_sampler: id  1 | task 53733 | init sampler, took 0.94 ms, tokens: text = 6314, total = 6314
slot update_slots: id  1 | task 53733 | created context checkpoint 8 of 8 (pos_min = 5480, pos_max = 6249, size = 18.056 MiB)
slot print_timing: id  1 | task 53733 | 
prompt eval time =     475.25 ms /   233 tokens (    2.04 ms per token,   490.27 tokens per second)
       eval time =    1526.17 ms /    61 tokens (   25.02 ms per token,    39.97 tokens per second)
      total time =    2001.41 ms /   294 tokens
slot      release: id  1 | task 53733 | stop processing: n_tokens = 6374, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53796 | processing task, is_child = 0
slot update_slots: id  1 | task 53796 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6373
slot update_slots: id  1 | task 53796 | n_tokens = 6314, memory_seq_rm [6314, end)
slot update_slots: id  1 | task 53796 | prompt processing progress, n_tokens = 6373, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  1 | task 53796 | prompt done, n_tokens = 6373, batch.n_tokens = 59
slot init_sampler: id  1 | task 53796 | init sampler, took 0.98 ms, tokens: text = 6373, total = 6373
slot print_timing: id  1 | task 53796 | 
prompt eval time =     166.76 ms /    59 tokens (    2.83 ms per token,   353.81 tokens per second)
       eval time =    1345.01 ms /    54 tokens (   24.91 ms per token,    40.15 tokens per second)
      total time =    1511.76 ms /   113 tokens
slot      release: id  1 | task 53796 | stop processing: n_tokens = 6426, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53851 | processing task, is_child = 0
slot update_slots: id  1 | task 53851 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6462
slot update_slots: id  1 | task 53851 | n_tokens = 6373, memory_seq_rm [6373, end)
slot update_slots: id  1 | task 53851 | prompt processing progress, n_tokens = 6398, batch.n_tokens = 25, progress = 0.990096
slot update_slots: id  1 | task 53851 | n_tokens = 6398, memory_seq_rm [6398, end)
slot update_slots: id  1 | task 53851 | prompt processing progress, n_tokens = 6462, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 53851 | prompt done, n_tokens = 6462, batch.n_tokens = 64
slot init_sampler: id  1 | task 53851 | init sampler, took 1.04 ms, tokens: text = 6462, total = 6462
slot update_slots: id  1 | task 53851 | erasing old context checkpoint (pos_min = 0, pos_max = 600, size = 14.093 MiB)
slot update_slots: id  1 | task 53851 | created context checkpoint 8 of 8 (pos_min = 5656, pos_max = 6397, size = 17.399 MiB)
slot print_timing: id  1 | task 53851 | 
prompt eval time =     308.50 ms /    89 tokens (    3.47 ms per token,   288.49 tokens per second)
       eval time =    1395.14 ms /    56 tokens (   24.91 ms per token,    40.14 tokens per second)
      total time =    1703.64 ms /   145 tokens
slot      release: id  1 | task 53851 | stop processing: n_tokens = 6517, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53909 | processing task, is_child = 0
slot update_slots: id  1 | task 53909 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6995
slot update_slots: id  1 | task 53909 | n_tokens = 6462, memory_seq_rm [6462, end)
slot update_slots: id  1 | task 53909 | prompt processing progress, n_tokens = 6931, batch.n_tokens = 469, progress = 0.990851
slot update_slots: id  1 | task 53909 | n_tokens = 6931, memory_seq_rm [6931, end)
slot update_slots: id  1 | task 53909 | prompt processing progress, n_tokens = 6995, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 53909 | prompt done, n_tokens = 6995, batch.n_tokens = 64
slot init_sampler: id  1 | task 53909 | init sampler, took 1.39 ms, tokens: text = 6995, total = 6995
slot update_slots: id  1 | task 53909 | erasing old context checkpoint (pos_min = 308, pos_max = 1077, size = 18.056 MiB)
slot update_slots: id  1 | task 53909 | created context checkpoint 8 of 8 (pos_min = 6210, pos_max = 6930, size = 16.907 MiB)
slot print_timing: id  1 | task 53909 | 
prompt eval time =     755.99 ms /   533 tokens (    1.42 ms per token,   705.04 tokens per second)
       eval time =    1649.63 ms /    66 tokens (   24.99 ms per token,    40.01 tokens per second)
      total time =    2405.62 ms /   599 tokens
slot      release: id  1 | task 53909 | stop processing: n_tokens = 7060, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.903 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 53977 | processing task, is_child = 0
slot update_slots: id  1 | task 53977 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7743
slot update_slots: id  1 | task 53977 | n_tokens = 6995, memory_seq_rm [6995, end)
slot update_slots: id  1 | task 53977 | prompt processing progress, n_tokens = 7679, batch.n_tokens = 684, progress = 0.991734
slot update_slots: id  1 | task 53977 | n_tokens = 7679, memory_seq_rm [7679, end)
slot update_slots: id  1 | task 53977 | prompt processing progress, n_tokens = 7743, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 53977 | prompt done, n_tokens = 7743, batch.n_tokens = 64
slot init_sampler: id  1 | task 53977 | init sampler, took 1.13 ms, tokens: text = 7743, total = 7743
slot update_slots: id  1 | task 53977 | erasing old context checkpoint (pos_min = 1184, pos_max = 1953, size = 18.056 MiB)
slot update_slots: id  1 | task 53977 | created context checkpoint 8 of 8 (pos_min = 6909, pos_max = 7678, size = 18.056 MiB)
slot print_timing: id  1 | task 53977 | 
prompt eval time =    1096.30 ms /   748 tokens (    1.47 ms per token,   682.30 tokens per second)
       eval time =    1692.78 ms /    66 tokens (   25.65 ms per token,    38.99 tokens per second)
      total time =    2789.07 ms /   814 tokens
slot      release: id  1 | task 53977 | stop processing: n_tokens = 7808, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.927 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 54045 | processing task, is_child = 0
slot update_slots: id  1 | task 54045 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8353
slot update_slots: id  1 | task 54045 | n_tokens = 7743, memory_seq_rm [7743, end)
slot update_slots: id  1 | task 54045 | prompt processing progress, n_tokens = 8289, batch.n_tokens = 546, progress = 0.992338
slot update_slots: id  1 | task 54045 | n_tokens = 8289, memory_seq_rm [8289, end)
slot update_slots: id  1 | task 54045 | prompt processing progress, n_tokens = 8353, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 54045 | prompt done, n_tokens = 8353, batch.n_tokens = 64
slot init_sampler: id  1 | task 54045 | init sampler, took 1.54 ms, tokens: text = 8353, total = 8353
slot update_slots: id  1 | task 54045 | erasing old context checkpoint (pos_min = 3287, pos_max = 4056, size = 18.056 MiB)
slot update_slots: id  1 | task 54045 | created context checkpoint 8 of 8 (pos_min = 7519, pos_max = 8288, size = 18.056 MiB)
slot print_timing: id  1 | task 54045 | 
prompt eval time =     933.38 ms /   610 tokens (    1.53 ms per token,   653.54 tokens per second)
       eval time =    1357.78 ms /    54 tokens (   25.14 ms per token,    39.77 tokens per second)
      total time =    2291.17 ms /   664 tokens
slot      release: id  1 | task 54045 | stop processing: n_tokens = 8406, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 54101 | processing task, is_child = 0
slot update_slots: id  1 | task 54101 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8963
slot update_slots: id  1 | task 54101 | n_tokens = 8353, memory_seq_rm [8353, end)
slot update_slots: id  1 | task 54101 | prompt processing progress, n_tokens = 8899, batch.n_tokens = 546, progress = 0.992860
slot update_slots: id  1 | task 54101 | n_tokens = 8899, memory_seq_rm [8899, end)
slot update_slots: id  1 | task 54101 | prompt processing progress, n_tokens = 8963, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 54101 | prompt done, n_tokens = 8963, batch.n_tokens = 64
slot init_sampler: id  1 | task 54101 | init sampler, took 1.29 ms, tokens: text = 8963, total = 8963
slot update_slots: id  1 | task 54101 | erasing old context checkpoint (pos_min = 4258, pos_max = 5027, size = 18.056 MiB)
slot update_slots: id  1 | task 54101 | created context checkpoint 8 of 8 (pos_min = 8129, pos_max = 8898, size = 18.056 MiB)
slot print_timing: id  1 | task 54101 | 
prompt eval time =     954.04 ms /   610 tokens (    1.56 ms per token,   639.39 tokens per second)
       eval time =    1347.55 ms /    54 tokens (   24.95 ms per token,    40.07 tokens per second)
      total time =    2301.59 ms /   664 tokens
slot      release: id  1 | task 54101 | stop processing: n_tokens = 9016, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.940 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 54157 | processing task, is_child = 0
slot update_slots: id  1 | task 54157 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9534
slot update_slots: id  1 | task 54157 | n_tokens = 8963, memory_seq_rm [8963, end)
slot update_slots: id  1 | task 54157 | prompt processing progress, n_tokens = 9470, batch.n_tokens = 507, progress = 0.993287
slot update_slots: id  1 | task 54157 | n_tokens = 9470, memory_seq_rm [9470, end)
slot update_slots: id  1 | task 54157 | prompt processing progress, n_tokens = 9534, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 54157 | prompt done, n_tokens = 9534, batch.n_tokens = 64
slot init_sampler: id  1 | task 54157 | init sampler, took 1.36 ms, tokens: text = 9534, total = 9534
slot update_slots: id  1 | task 54157 | erasing old context checkpoint (pos_min = 4754, pos_max = 5523, size = 18.056 MiB)
slot update_slots: id  1 | task 54157 | created context checkpoint 8 of 8 (pos_min = 8700, pos_max = 9469, size = 18.056 MiB)
slot print_timing: id  1 | task 54157 | 
prompt eval time =     812.99 ms /   571 tokens (    1.42 ms per token,   702.35 tokens per second)
       eval time =    1381.03 ms /    55 tokens (   25.11 ms per token,    39.83 tokens per second)
      total time =    2194.02 ms /   626 tokens
slot      release: id  1 | task 54157 | stop processing: n_tokens = 9588, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.912 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 54214 | processing task, is_child = 0
slot update_slots: id  1 | task 54214 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 10451
slot update_slots: id  1 | task 54214 | n_tokens = 9534, memory_seq_rm [9534, end)
slot update_slots: id  1 | task 54214 | prompt processing progress, n_tokens = 10387, batch.n_tokens = 853, progress = 0.993876
slot update_slots: id  1 | task 54214 | n_tokens = 10387, memory_seq_rm [10387, end)
slot update_slots: id  1 | task 54214 | prompt processing progress, n_tokens = 10451, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 54214 | prompt done, n_tokens = 10451, batch.n_tokens = 64
slot init_sampler: id  1 | task 54214 | init sampler, took 1.52 ms, tokens: text = 10451, total = 10451
slot update_slots: id  1 | task 54214 | erasing old context checkpoint (pos_min = 5247, pos_max = 6016, size = 18.056 MiB)
slot update_slots: id  1 | task 54214 | created context checkpoint 8 of 8 (pos_min = 9617, pos_max = 10386, size = 18.056 MiB)
slot print_timing: id  1 | task 54214 | 
prompt eval time =    1312.76 ms /   917 tokens (    1.43 ms per token,   698.53 tokens per second)
       eval time =    1716.45 ms /    68 tokens (   25.24 ms per token,    39.62 tokens per second)
      total time =    3029.22 ms /   985 tokens
slot      release: id  1 | task 54214 | stop processing: n_tokens = 10518, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.921 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 54284 | processing task, is_child = 0
slot update_slots: id  1 | task 54284 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11352
slot update_slots: id  1 | task 54284 | n_tokens = 10451, memory_seq_rm [10451, end)
slot update_slots: id  1 | task 54284 | prompt processing progress, n_tokens = 11288, batch.n_tokens = 837, progress = 0.994362
slot update_slots: id  1 | task 54284 | n_tokens = 11288, memory_seq_rm [11288, end)
slot update_slots: id  1 | task 54284 | prompt processing progress, n_tokens = 11352, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 54284 | prompt done, n_tokens = 11352, batch.n_tokens = 64
slot init_sampler: id  1 | task 54284 | init sampler, took 1.99 ms, tokens: text = 11352, total = 11352
slot update_slots: id  1 | task 54284 | erasing old context checkpoint (pos_min = 5480, pos_max = 6249, size = 18.056 MiB)
slot update_slots: id  1 | task 54284 | created context checkpoint 8 of 8 (pos_min = 10518, pos_max = 11287, size = 18.056 MiB)
slot print_timing: id  1 | task 54284 | 
prompt eval time =    1280.54 ms /   901 tokens (    1.42 ms per token,   703.61 tokens per second)
       eval time =   25424.43 ms /   987 tokens (   25.76 ms per token,    38.82 tokens per second)
      total time =   26704.97 ms /  1888 tokens
slot      release: id  1 | task 54284 | stop processing: n_tokens = 12338, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.922 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 55273 | processing task, is_child = 0
slot update_slots: id  1 | task 55273 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12308
slot update_slots: id  1 | task 55273 | n_past = 11352, slot.prompt.tokens.size() = 12338, seq_id = 1, pos_min = 11568, n_swa = 128
slot update_slots: id  1 | task 55273 | restored context checkpoint (pos_min = 10518, pos_max = 11287, size = 18.056 MiB)
slot update_slots: id  1 | task 55273 | n_tokens = 11287, memory_seq_rm [11287, end)
slot update_slots: id  1 | task 55273 | prompt processing progress, n_tokens = 12244, batch.n_tokens = 957, progress = 0.994800
slot update_slots: id  1 | task 55273 | n_tokens = 12244, memory_seq_rm [12244, end)
slot update_slots: id  1 | task 55273 | prompt processing progress, n_tokens = 12308, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 55273 | prompt done, n_tokens = 12308, batch.n_tokens = 64
slot init_sampler: id  1 | task 55273 | init sampler, took 2.47 ms, tokens: text = 12308, total = 12308
slot update_slots: id  1 | task 55273 | erasing old context checkpoint (pos_min = 5656, pos_max = 6397, size = 17.399 MiB)
slot update_slots: id  1 | task 55273 | created context checkpoint 8 of 8 (pos_min = 11474, pos_max = 12243, size = 18.056 MiB)
slot print_timing: id  1 | task 55273 | 
prompt eval time =    1597.85 ms /  1021 tokens (    1.56 ms per token,   638.98 tokens per second)
       eval time =    1176.17 ms /    43 tokens (   27.35 ms per token,    36.56 tokens per second)
      total time =    2774.02 ms /  1064 tokens
slot      release: id  1 | task 55273 | stop processing: n_tokens = 12350, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 55318 | processing task, is_child = 0
slot update_slots: id  1 | task 55318 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12355
slot update_slots: id  1 | task 55318 | n_tokens = 12308, memory_seq_rm [12308, end)
slot update_slots: id  1 | task 55318 | prompt processing progress, n_tokens = 12355, batch.n_tokens = 47, progress = 1.000000
slot update_slots: id  1 | task 55318 | prompt done, n_tokens = 12355, batch.n_tokens = 47
slot init_sampler: id  1 | task 55318 | init sampler, took 1.80 ms, tokens: text = 12355, total = 12355
slot print_timing: id  1 | task 55318 | 
prompt eval time =     237.67 ms /    47 tokens (    5.06 ms per token,   197.76 tokens per second)
       eval time =    1109.35 ms /    42 tokens (   26.41 ms per token,    37.86 tokens per second)
      total time =    1347.02 ms /    89 tokens
slot      release: id  1 | task 55318 | stop processing: n_tokens = 12396, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 55361 | processing task, is_child = 0
slot update_slots: id  1 | task 55361 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 12420
slot update_slots: id  1 | task 55361 | n_tokens = 12355, memory_seq_rm [12355, end)
slot update_slots: id  1 | task 55361 | prompt processing progress, n_tokens = 12356, batch.n_tokens = 1, progress = 0.994847
slot update_slots: id  1 | task 55361 | n_tokens = 12356, memory_seq_rm [12356, end)
slot update_slots: id  1 | task 55361 | prompt processing progress, n_tokens = 12420, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 55361 | prompt done, n_tokens = 12420, batch.n_tokens = 64
slot init_sampler: id  1 | task 55361 | init sampler, took 1.75 ms, tokens: text = 12420, total = 12420
slot update_slots: id  1 | task 55361 | erasing old context checkpoint (pos_min = 6210, pos_max = 6930, size = 16.907 MiB)
slot update_slots: id  1 | task 55361 | created context checkpoint 8 of 8 (pos_min = 11626, pos_max = 12355, size = 17.118 MiB)
slot print_timing: id  1 | task 55361 | 
prompt eval time =     297.17 ms /    65 tokens (    4.57 ms per token,   218.73 tokens per second)
       eval time =    3539.42 ms /   140 tokens (   25.28 ms per token,    39.55 tokens per second)
      total time =    3836.60 ms /   205 tokens
slot      release: id  1 | task 55361 | stop processing: n_tokens = 12559, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.037
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 12559, total state size = 312.552 MiB
srv          load:  - looking for better prompt, base f_keep = 0.037, sim = 0.981
srv        update:  - cache state: 10 prompts, 3180.683 MiB (limits: 8192.000 MiB, 64000 tokens, 218156 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv        update:    - prompt 0x5d0e7b4386e0:    9556 tokens, checkpoints:  8,   432.073 MiB
srv        update:    - prompt 0x5d0e82e23e50:   16500 tokens, checkpoints:  8,   566.998 MiB
srv        update:    - prompt 0x5d0e803e3740:    1195 tokens, checkpoints:  3,    82.589 MiB
srv        update:    - prompt 0x5d0e9ea7e990:   16549 tokens, checkpoints:  5,   491.562 MiB
srv        update:    - prompt 0x5d0e98c87f00:    7179 tokens, checkpoints:  1,   204.452 MiB
srv        update:    - prompt 0x5d0e9eb54ef0:    8289 tokens, checkpoints:  6,   316.798 MiB
srv        update:    - prompt 0x5d0e7bae5850:    2362 tokens, checkpoints:  2,   105.592 MiB
srv        update:    - prompt 0x5d0e82e1e5a0:   12559 tokens, checkpoints:  8,   456.061 MiB
srv  get_availabl: prompt cache update took 435.99 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 55503 | processing task, is_child = 0
slot update_slots: id  1 | task 55503 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 468
slot update_slots: id  1 | task 55503 | n_past = 459, slot.prompt.tokens.size() = 12559, seq_id = 1, pos_min = 11789, n_swa = 128
slot update_slots: id  1 | task 55503 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 55503 | erased invalidated context checkpoint (pos_min = 6909, pos_max = 7678, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 55503 | erased invalidated context checkpoint (pos_min = 7519, pos_max = 8288, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 55503 | erased invalidated context checkpoint (pos_min = 8129, pos_max = 8898, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 55503 | erased invalidated context checkpoint (pos_min = 8700, pos_max = 9469, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 55503 | erased invalidated context checkpoint (pos_min = 9617, pos_max = 10386, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 55503 | erased invalidated context checkpoint (pos_min = 10518, pos_max = 11287, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 55503 | erased invalidated context checkpoint (pos_min = 11474, pos_max = 12243, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 55503 | erased invalidated context checkpoint (pos_min = 11626, pos_max = 12355, n_swa = 128, size = 17.118 MiB)
slot update_slots: id  1 | task 55503 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 55503 | prompt processing progress, n_tokens = 404, batch.n_tokens = 404, progress = 0.863248
slot update_slots: id  1 | task 55503 | n_tokens = 404, memory_seq_rm [404, end)
slot update_slots: id  1 | task 55503 | prompt processing progress, n_tokens = 468, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 55503 | prompt done, n_tokens = 468, batch.n_tokens = 64
slot init_sampler: id  1 | task 55503 | init sampler, took 0.10 ms, tokens: text = 468, total = 468
slot update_slots: id  1 | task 55503 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 403, size = 9.474 MiB)
slot print_timing: id  1 | task 55503 | 
prompt eval time =     676.47 ms /   468 tokens (    1.45 ms per token,   691.83 tokens per second)
       eval time =     819.65 ms /    31 tokens (   26.44 ms per token,    37.82 tokens per second)
      total time =    1496.12 ms /   499 tokens
slot      release: id  1 | task 55503 | stop processing: n_tokens = 498, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.878 (> 0.100 thold), f_keep = 0.940
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 55536 | processing task, is_child = 0
slot update_slots: id  1 | task 55536 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 533
slot update_slots: id  1 | task 55536 | n_tokens = 468, memory_seq_rm [468, end)
slot update_slots: id  1 | task 55536 | prompt processing progress, n_tokens = 469, batch.n_tokens = 1, progress = 0.879925
slot update_slots: id  1 | task 55536 | n_tokens = 469, memory_seq_rm [469, end)
slot update_slots: id  1 | task 55536 | prompt processing progress, n_tokens = 533, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 55536 | prompt done, n_tokens = 533, batch.n_tokens = 64
slot init_sampler: id  1 | task 55536 | init sampler, took 0.09 ms, tokens: text = 533, total = 533
slot update_slots: id  1 | task 55536 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 468, size = 10.998 MiB)
slot print_timing: id  1 | task 55536 | 
prompt eval time =     254.25 ms /    65 tokens (    3.91 ms per token,   255.65 tokens per second)
       eval time =     951.68 ms /    43 tokens (   22.13 ms per token,    45.18 tokens per second)
      total time =    1205.93 ms /   108 tokens
slot      release: id  1 | task 55536 | stop processing: n_tokens = 575, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 55581 | processing task, is_child = 0
slot update_slots: id  0 | task 55581 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4720
slot update_slots: id  0 | task 55581 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 55581 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.433898
slot update_slots: id  0 | task 55581 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 55581 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.867797
slot update_slots: id  0 | task 55581 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 55581 | prompt processing progress, n_tokens = 4656, batch.n_tokens = 560, progress = 0.986441
slot update_slots: id  0 | task 55581 | n_tokens = 4656, memory_seq_rm [4656, end)
slot update_slots: id  0 | task 55581 | prompt processing progress, n_tokens = 4720, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 55581 | prompt done, n_tokens = 4720, batch.n_tokens = 64
slot init_sampler: id  0 | task 55581 | init sampler, took 0.78 ms, tokens: text = 4720, total = 4720
slot update_slots: id  0 | task 55581 | created context checkpoint 1 of 8 (pos_min = 4013, pos_max = 4655, size = 15.078 MiB)
slot print_timing: id  0 | task 55581 | 
prompt eval time =    4997.41 ms /  4720 tokens (    1.06 ms per token,   944.49 tokens per second)
       eval time =   11425.43 ms /   490 tokens (   23.32 ms per token,    42.89 tokens per second)
      total time =   16422.84 ms /  5210 tokens
slot      release: id  0 | task 55581 | stop processing: n_tokens = 5209, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.941 (> 0.100 thold), f_keep = 0.904
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 56075 | processing task, is_child = 0
slot update_slots: id  0 | task 56075 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5004
slot update_slots: id  0 | task 56075 | n_tokens = 4709, memory_seq_rm [4709, end)
slot update_slots: id  0 | task 56075 | prompt processing progress, n_tokens = 4940, batch.n_tokens = 231, progress = 0.987210
slot update_slots: id  0 | task 56075 | n_tokens = 4940, memory_seq_rm [4940, end)
slot update_slots: id  0 | task 56075 | prompt processing progress, n_tokens = 5004, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 56075 | prompt done, n_tokens = 5004, batch.n_tokens = 64
slot init_sampler: id  0 | task 56075 | init sampler, took 1.13 ms, tokens: text = 5004, total = 5004
slot update_slots: id  0 | task 56075 | created context checkpoint 2 of 8 (pos_min = 4566, pos_max = 4939, size = 8.770 MiB)
slot print_timing: id  0 | task 56075 | 
prompt eval time =     662.43 ms /   295 tokens (    2.25 ms per token,   445.33 tokens per second)
       eval time =    1367.33 ms /    57 tokens (   23.99 ms per token,    41.69 tokens per second)
      total time =    2029.76 ms /   352 tokens
slot      release: id  0 | task 56075 | stop processing: n_tokens = 5060, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.703 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 56134 | processing task, is_child = 0
slot update_slots: id  0 | task 56134 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7118
slot update_slots: id  0 | task 56134 | n_tokens = 5004, memory_seq_rm [5004, end)
slot update_slots: id  0 | task 56134 | prompt processing progress, n_tokens = 7052, batch.n_tokens = 2048, progress = 0.990728
slot update_slots: id  0 | task 56134 | n_tokens = 7052, memory_seq_rm [7052, end)
slot update_slots: id  0 | task 56134 | prompt processing progress, n_tokens = 7054, batch.n_tokens = 2, progress = 0.991009
slot update_slots: id  0 | task 56134 | n_tokens = 7054, memory_seq_rm [7054, end)
slot update_slots: id  0 | task 56134 | prompt processing progress, n_tokens = 7118, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 56134 | prompt done, n_tokens = 7118, batch.n_tokens = 64
slot init_sampler: id  0 | task 56134 | init sampler, took 1.03 ms, tokens: text = 7118, total = 7118
slot update_slots: id  0 | task 56134 | created context checkpoint 3 of 8 (pos_min = 6411, pos_max = 7053, size = 15.078 MiB)
slot print_timing: id  0 | task 56134 | 
prompt eval time =    2455.01 ms /  2114 tokens (    1.16 ms per token,   861.10 tokens per second)
       eval time =   17192.17 ms /   730 tokens (   23.55 ms per token,    42.46 tokens per second)
      total time =   19647.18 ms /  2844 tokens
slot      release: id  0 | task 56134 | stop processing: n_tokens = 7847, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.716 (> 0.100 thold), f_keep = 0.802
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 56867 | processing task, is_child = 0
slot update_slots: id  1 | task 56867 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 644
slot update_slots: id  1 | task 56867 | n_past = 461, slot.prompt.tokens.size() = 575, seq_id = 1, pos_min = 448, n_swa = 128
slot update_slots: id  1 | task 56867 | restored context checkpoint (pos_min = 0, pos_max = 468, size = 10.998 MiB)
slot update_slots: id  1 | task 56867 | n_tokens = 461, memory_seq_rm [461, end)
slot update_slots: id  1 | task 56867 | prompt processing progress, n_tokens = 580, batch.n_tokens = 119, progress = 0.900621
slot update_slots: id  1 | task 56867 | n_tokens = 580, memory_seq_rm [580, end)
slot update_slots: id  1 | task 56867 | prompt processing progress, n_tokens = 644, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 56867 | prompt done, n_tokens = 644, batch.n_tokens = 64
slot init_sampler: id  1 | task 56867 | init sampler, took 0.17 ms, tokens: text = 644, total = 644
slot update_slots: id  1 | task 56867 | created context checkpoint 3 of 8 (pos_min = 0, pos_max = 579, size = 13.601 MiB)
slot print_timing: id  1 | task 56867 | 
prompt eval time =     654.58 ms /   183 tokens (    3.58 ms per token,   279.57 tokens per second)
       eval time =     875.62 ms /    37 tokens (   23.67 ms per token,    42.26 tokens per second)
      total time =    1530.20 ms /   220 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 56867 | stop processing: n_tokens = 680, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.516 (> 0.100 thold), f_keep = 0.947
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 56906 | processing task, is_child = 0
slot update_slots: id  1 | task 56906 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1249
slot update_slots: id  1 | task 56906 | n_tokens = 644, memory_seq_rm [644, end)
slot update_slots: id  1 | task 56906 | prompt processing progress, n_tokens = 1185, batch.n_tokens = 541, progress = 0.948759
slot update_slots: id  1 | task 56906 | n_tokens = 1185, memory_seq_rm [1185, end)
slot update_slots: id  1 | task 56906 | prompt processing progress, n_tokens = 1249, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 56906 | prompt done, n_tokens = 1249, batch.n_tokens = 64
slot init_sampler: id  1 | task 56906 | init sampler, took 0.24 ms, tokens: text = 1249, total = 1249
slot update_slots: id  1 | task 56906 | created context checkpoint 4 of 8 (pos_min = 542, pos_max = 1184, size = 15.078 MiB)
slot print_timing: id  1 | task 56906 | 
prompt eval time =     877.67 ms /   605 tokens (    1.45 ms per token,   689.32 tokens per second)
       eval time =    2993.99 ms /   124 tokens (   24.15 ms per token,    41.42 tokens per second)
      total time =    3871.66 ms /   729 tokens
slot      release: id  1 | task 56906 | stop processing: n_tokens = 1372, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.420 (> 0.100 thold), f_keep = 0.910
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 57032 | processing task, is_child = 0
slot update_slots: id  1 | task 57032 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2971
slot update_slots: id  1 | task 57032 | n_tokens = 1249, memory_seq_rm [1249, end)
slot update_slots: id  1 | task 57032 | prompt processing progress, n_tokens = 2907, batch.n_tokens = 1658, progress = 0.978458
slot update_slots: id  1 | task 57032 | n_tokens = 2907, memory_seq_rm [2907, end)
slot update_slots: id  1 | task 57032 | prompt processing progress, n_tokens = 2971, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 57032 | prompt done, n_tokens = 2971, batch.n_tokens = 64
slot init_sampler: id  1 | task 57032 | init sampler, took 0.52 ms, tokens: text = 2971, total = 2971
slot update_slots: id  1 | task 57032 | created context checkpoint 5 of 8 (pos_min = 2264, pos_max = 2906, size = 15.078 MiB)
slot print_timing: id  1 | task 57032 | 
prompt eval time =    2271.89 ms /  1722 tokens (    1.32 ms per token,   757.96 tokens per second)
       eval time =     999.48 ms /    42 tokens (   23.80 ms per token,    42.02 tokens per second)
      total time =    3271.37 ms /  1764 tokens
slot      release: id  1 | task 57032 | stop processing: n_tokens = 3012, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.692 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 57076 | processing task, is_child = 0
slot update_slots: id  1 | task 57076 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4292
slot update_slots: id  1 | task 57076 | n_tokens = 2971, memory_seq_rm [2971, end)
slot update_slots: id  1 | task 57076 | prompt processing progress, n_tokens = 4228, batch.n_tokens = 1257, progress = 0.985089
slot update_slots: id  1 | task 57076 | n_tokens = 4228, memory_seq_rm [4228, end)
slot update_slots: id  1 | task 57076 | prompt processing progress, n_tokens = 4292, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 57076 | prompt done, n_tokens = 4292, batch.n_tokens = 64
slot init_sampler: id  1 | task 57076 | init sampler, took 0.64 ms, tokens: text = 4292, total = 4292
slot update_slots: id  1 | task 57076 | created context checkpoint 6 of 8 (pos_min = 3585, pos_max = 4227, size = 15.078 MiB)
slot print_timing: id  1 | task 57076 | 
prompt eval time =    1770.68 ms /  1321 tokens (    1.34 ms per token,   746.04 tokens per second)
       eval time =    1212.67 ms /    49 tokens (   24.75 ms per token,    40.41 tokens per second)
      total time =    2983.34 ms /  1370 tokens
slot      release: id  1 | task 57076 | stop processing: n_tokens = 4340, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.670 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 57127 | processing task, is_child = 0
slot update_slots: id  1 | task 57127 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6406
slot update_slots: id  1 | task 57127 | n_tokens = 4292, memory_seq_rm [4292, end)
slot update_slots: id  1 | task 57127 | prompt processing progress, n_tokens = 6340, batch.n_tokens = 2048, progress = 0.989697
slot update_slots: id  1 | task 57127 | n_tokens = 6340, memory_seq_rm [6340, end)
slot update_slots: id  1 | task 57127 | prompt processing progress, n_tokens = 6342, batch.n_tokens = 2, progress = 0.990009
slot update_slots: id  1 | task 57127 | n_tokens = 6342, memory_seq_rm [6342, end)
slot update_slots: id  1 | task 57127 | prompt processing progress, n_tokens = 6406, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 57127 | prompt done, n_tokens = 6406, batch.n_tokens = 64
slot init_sampler: id  1 | task 57127 | init sampler, took 1.18 ms, tokens: text = 6406, total = 6406
slot update_slots: id  1 | task 57127 | created context checkpoint 7 of 8 (pos_min = 5699, pos_max = 6341, size = 15.078 MiB)
slot print_timing: id  1 | task 57127 | 
prompt eval time =    2723.05 ms /  2114 tokens (    1.29 ms per token,   776.34 tokens per second)
       eval time =    1141.88 ms /    42 tokens (   27.19 ms per token,    36.78 tokens per second)
      total time =    3864.93 ms /  2156 tokens
slot      release: id  1 | task 57127 | stop processing: n_tokens = 6447, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.837 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 57172 | processing task, is_child = 0
slot update_slots: id  1 | task 57172 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7654
slot update_slots: id  1 | task 57172 | n_tokens = 6406, memory_seq_rm [6406, end)
slot update_slots: id  1 | task 57172 | prompt processing progress, n_tokens = 7590, batch.n_tokens = 1184, progress = 0.991638
slot update_slots: id  1 | task 57172 | n_tokens = 7590, memory_seq_rm [7590, end)
slot update_slots: id  1 | task 57172 | prompt processing progress, n_tokens = 7654, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 57172 | prompt done, n_tokens = 7654, batch.n_tokens = 64
slot init_sampler: id  1 | task 57172 | init sampler, took 2.07 ms, tokens: text = 7654, total = 7654
slot update_slots: id  1 | task 57172 | created context checkpoint 8 of 8 (pos_min = 6947, pos_max = 7589, size = 15.078 MiB)
slot print_timing: id  1 | task 57172 | 
prompt eval time =    1764.11 ms /  1248 tokens (    1.41 ms per token,   707.44 tokens per second)
       eval time =   18210.17 ms /   711 tokens (   25.61 ms per token,    39.04 tokens per second)
      total time =   19974.28 ms /  1959 tokens
slot      release: id  1 | task 57172 | stop processing: n_tokens = 8364, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.599 (> 0.100 thold), f_keep = 0.088
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7847, total state size = 186.982 MiB
srv          load:  - looking for better prompt, base f_keep = 0.088, sim = 0.599
srv        update:  - cache state: 11 prompts, 3406.591 MiB (limits: 8192.000 MiB, 64000 tokens, 222559 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv        update:    - prompt 0x5d0e7b4386e0:    9556 tokens, checkpoints:  8,   432.073 MiB
srv        update:    - prompt 0x5d0e82e23e50:   16500 tokens, checkpoints:  8,   566.998 MiB
srv        update:    - prompt 0x5d0e803e3740:    1195 tokens, checkpoints:  3,    82.589 MiB
srv        update:    - prompt 0x5d0e9ea7e990:   16549 tokens, checkpoints:  5,   491.562 MiB
srv        update:    - prompt 0x5d0e98c87f00:    7179 tokens, checkpoints:  1,   204.452 MiB
srv        update:    - prompt 0x5d0e9eb54ef0:    8289 tokens, checkpoints:  6,   316.798 MiB
srv        update:    - prompt 0x5d0e7bae5850:    2362 tokens, checkpoints:  2,   105.592 MiB
srv        update:    - prompt 0x5d0e82e1e5a0:   12559 tokens, checkpoints:  8,   456.061 MiB
srv        update:    - prompt 0x5d0e9e9ff300:    7847 tokens, checkpoints:  3,   225.909 MiB
srv  get_availabl: prompt cache update took 182.12 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 57885 | processing task, is_child = 0
slot update_slots: id  0 | task 57885 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1155
slot update_slots: id  0 | task 57885 | n_past = 692, slot.prompt.tokens.size() = 7847, seq_id = 0, pos_min = 7720, n_swa = 128
slot update_slots: id  0 | task 57885 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 57885 | erased invalidated context checkpoint (pos_min = 4013, pos_max = 4655, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 57885 | erased invalidated context checkpoint (pos_min = 4566, pos_max = 4939, n_swa = 128, size = 8.770 MiB)
slot update_slots: id  0 | task 57885 | erased invalidated context checkpoint (pos_min = 6411, pos_max = 7053, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 57885 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 57885 | prompt processing progress, n_tokens = 1091, batch.n_tokens = 1091, progress = 0.944589
slot update_slots: id  0 | task 57885 | n_tokens = 1091, memory_seq_rm [1091, end)
slot update_slots: id  0 | task 57885 | prompt processing progress, n_tokens = 1155, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 57885 | prompt done, n_tokens = 1155, batch.n_tokens = 64
slot init_sampler: id  0 | task 57885 | init sampler, took 0.22 ms, tokens: text = 1155, total = 1155
slot update_slots: id  0 | task 57885 | created context checkpoint 1 of 8 (pos_min = 448, pos_max = 1090, size = 15.078 MiB)
slot print_timing: id  0 | task 57885 | 
prompt eval time =    1817.45 ms /  1155 tokens (    1.57 ms per token,   635.51 tokens per second)
       eval time =   24344.91 ms /   930 tokens (   26.18 ms per token,    38.20 tokens per second)
      total time =   26162.36 ms /  2085 tokens
slot      release: id  0 | task 57885 | stop processing: n_tokens = 2084, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.699 (> 0.100 thold), f_keep = 0.546
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 58817 | processing task, is_child = 0
slot update_slots: id  0 | task 58817 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1629
slot update_slots: id  0 | task 58817 | n_past = 1138, slot.prompt.tokens.size() = 2084, seq_id = 0, pos_min = 1441, n_swa = 128
slot update_slots: id  0 | task 58817 | restored context checkpoint (pos_min = 448, pos_max = 1090, size = 15.078 MiB)
slot update_slots: id  0 | task 58817 | n_tokens = 1090, memory_seq_rm [1090, end)
slot update_slots: id  0 | task 58817 | prompt processing progress, n_tokens = 1565, batch.n_tokens = 475, progress = 0.960712
slot update_slots: id  0 | task 58817 | n_tokens = 1565, memory_seq_rm [1565, end)
slot update_slots: id  0 | task 58817 | prompt processing progress, n_tokens = 1629, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 58817 | prompt done, n_tokens = 1629, batch.n_tokens = 64
slot init_sampler: id  0 | task 58817 | init sampler, took 0.30 ms, tokens: text = 1629, total = 1629
slot update_slots: id  0 | task 58817 | created context checkpoint 2 of 8 (pos_min = 922, pos_max = 1564, size = 15.078 MiB)
slot print_timing: id  0 | task 58817 | 
prompt eval time =    1067.18 ms /   539 tokens (    1.98 ms per token,   505.07 tokens per second)
       eval time =   12173.20 ms /   472 tokens (   25.79 ms per token,    38.77 tokens per second)
      total time =   13240.38 ms /  1011 tokens
slot      release: id  0 | task 58817 | stop processing: n_tokens = 2100, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.055
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8364, total state size = 199.106 MiB
srv          load:  - looking for better prompt, base f_keep = 0.055, sim = 0.989
srv        update:  - cache state: 12 prompts, 3715.159 MiB (limits: 8192.000 MiB, 64000 tokens, 222517 est)
srv        update:    - prompt 0x5d0e7c924dc0:    1116 tokens, checkpoints:  3,    93.234 MiB
srv        update:    - prompt 0x5d0e7b46c140:    9398 tokens, checkpoints:  8,   431.323 MiB
srv        update:    - prompt 0x5d0e7b4386e0:    9556 tokens, checkpoints:  8,   432.073 MiB
srv        update:    - prompt 0x5d0e82e23e50:   16500 tokens, checkpoints:  8,   566.998 MiB
srv        update:    - prompt 0x5d0e803e3740:    1195 tokens, checkpoints:  3,    82.589 MiB
srv        update:    - prompt 0x5d0e9ea7e990:   16549 tokens, checkpoints:  5,   491.562 MiB
srv        update:    - prompt 0x5d0e98c87f00:    7179 tokens, checkpoints:  1,   204.452 MiB
srv        update:    - prompt 0x5d0e9eb54ef0:    8289 tokens, checkpoints:  6,   316.798 MiB
srv        update:    - prompt 0x5d0e7bae5850:    2362 tokens, checkpoints:  2,   105.592 MiB
srv        update:    - prompt 0x5d0e82e1e5a0:   12559 tokens, checkpoints:  8,   456.061 MiB
srv        update:    - prompt 0x5d0e9e9ff300:    7847 tokens, checkpoints:  3,   225.909 MiB
srv        update:    - prompt 0x5d0e7a0d2ac0:    8364 tokens, checkpoints:  8,   308.568 MiB
srv  get_availabl: prompt cache update took 293.30 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 59291 | processing task, is_child = 0
slot update_slots: id  1 | task 59291 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 466
slot update_slots: id  1 | task 59291 | n_past = 461, slot.prompt.tokens.size() = 8364, seq_id = 1, pos_min = 8237, n_swa = 128
slot update_slots: id  1 | task 59291 | restored context checkpoint (pos_min = 0, pos_max = 579, size = 13.601 MiB)
slot update_slots: id  1 | task 59291 | erased invalidated context checkpoint (pos_min = 542, pos_max = 1184, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 59291 | erased invalidated context checkpoint (pos_min = 2264, pos_max = 2906, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 59291 | erased invalidated context checkpoint (pos_min = 3585, pos_max = 4227, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 59291 | erased invalidated context checkpoint (pos_min = 5699, pos_max = 6341, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 59291 | erased invalidated context checkpoint (pos_min = 6947, pos_max = 7589, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 59291 | n_tokens = 461, memory_seq_rm [461, end)
slot update_slots: id  1 | task 59291 | prompt processing progress, n_tokens = 466, batch.n_tokens = 5, progress = 1.000000
slot update_slots: id  1 | task 59291 | prompt done, n_tokens = 466, batch.n_tokens = 5
slot init_sampler: id  1 | task 59291 | init sampler, took 0.07 ms, tokens: text = 466, total = 466
slot print_timing: id  1 | task 59291 | 
prompt eval time =     213.93 ms /     5 tokens (   42.79 ms per token,    23.37 tokens per second)
       eval time =     735.12 ms /    30 tokens (   24.50 ms per token,    40.81 tokens per second)
      total time =     949.06 ms /    35 tokens
slot      release: id  1 | task 59291 | stop processing: n_tokens = 495, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.878 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 59322 | processing task, is_child = 0
slot update_slots: id  1 | task 59322 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 531
slot update_slots: id  1 | task 59322 | n_tokens = 466, memory_seq_rm [466, end)
slot update_slots: id  1 | task 59322 | prompt processing progress, n_tokens = 467, batch.n_tokens = 1, progress = 0.879473
slot update_slots: id  1 | task 59322 | n_tokens = 467, memory_seq_rm [467, end)
slot update_slots: id  1 | task 59322 | prompt processing progress, n_tokens = 531, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 59322 | prompt done, n_tokens = 531, batch.n_tokens = 64
slot init_sampler: id  1 | task 59322 | init sampler, took 0.09 ms, tokens: text = 531, total = 531
slot print_timing: id  1 | task 59322 | 
prompt eval time =     294.28 ms /    65 tokens (    4.53 ms per token,   220.88 tokens per second)
       eval time =    1544.12 ms /    65 tokens (   23.76 ms per token,    42.10 tokens per second)
      total time =    1838.40 ms /   130 tokens
slot      release: id  1 | task 59322 | stop processing: n_tokens = 595, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
