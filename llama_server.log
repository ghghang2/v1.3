ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14992 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1578 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 64000 -> need 1580 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 2.41 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 64000
llama_context: n_ctx_seq     = 64000
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (64000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 64000 cells
llama_kv_cache:      CUDA0 KV buffer size =  1500.00 MiB
llama_kv_cache: size = 1500.00 MiB ( 64000 cells,  12 layers,  4/1 seqs), K (f16):  750.00 MiB, V (f16):  750.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   132.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 63.55 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  1 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  2 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  3 | task -1 | new slot, n_ctx = 64000
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-08

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 508
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 444, batch.n_tokens = 444, progress = 0.874016
slot update_slots: id  3 | task 0 | n_tokens = 444, memory_seq_rm [444, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 508, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 508, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.11 ms, tokens: text = 508, total = 508
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 443, size = 10.412 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =     788.98 ms /   508 tokens (    1.55 ms per token,   643.87 tokens per second)
       eval time =    1975.03 ms /    91 tokens (   21.70 ms per token,    46.08 tokens per second)
      total time =    2764.01 ms /   599 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 598, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.528 (> 0.100 thold), f_keep = 0.849
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 93 | processing task, is_child = 0
slot update_slots: id  3 | task 93 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 962
slot update_slots: id  3 | task 93 | n_tokens = 508, memory_seq_rm [508, end)
slot update_slots: id  3 | task 93 | prompt processing progress, n_tokens = 898, batch.n_tokens = 390, progress = 0.933472
slot update_slots: id  3 | task 93 | n_tokens = 898, memory_seq_rm [898, end)
slot update_slots: id  3 | task 93 | prompt processing progress, n_tokens = 962, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 93 | prompt done, n_tokens = 962, batch.n_tokens = 64
slot init_sampler: id  3 | task 93 | init sampler, took 0.17 ms, tokens: text = 962, total = 962
slot update_slots: id  3 | task 93 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 897, size = 21.057 MiB)
slot print_timing: id  3 | task 93 | 
prompt eval time =     493.59 ms /   454 tokens (    1.09 ms per token,   919.79 tokens per second)
       eval time =     919.50 ms /    42 tokens (   21.89 ms per token,    45.68 tokens per second)
      total time =    1413.09 ms /   496 tokens
slot      release: id  3 | task 93 | stop processing: n_tokens = 1003, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.945 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 137 | processing task, is_child = 0
slot update_slots: id  3 | task 137 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1018
slot update_slots: id  3 | task 137 | n_tokens = 962, memory_seq_rm [962, end)
slot update_slots: id  3 | task 137 | prompt processing progress, n_tokens = 1018, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  3 | task 137 | prompt done, n_tokens = 1018, batch.n_tokens = 56
slot init_sampler: id  3 | task 137 | init sampler, took 0.17 ms, tokens: text = 1018, total = 1018
slot print_timing: id  3 | task 137 | 
prompt eval time =     145.03 ms /    56 tokens (    2.59 ms per token,   386.13 tokens per second)
       eval time =    1764.65 ms /    77 tokens (   22.92 ms per token,    43.63 tokens per second)
      total time =    1909.68 ms /   133 tokens
slot      release: id  3 | task 137 | stop processing: n_tokens = 1094, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.909 (> 0.100 thold), f_keep = 0.931
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 215 | processing task, is_child = 0
slot update_slots: id  3 | task 215 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1120
slot update_slots: id  3 | task 215 | n_tokens = 1018, memory_seq_rm [1018, end)
slot update_slots: id  3 | task 215 | prompt processing progress, n_tokens = 1056, batch.n_tokens = 38, progress = 0.942857
slot update_slots: id  3 | task 215 | n_tokens = 1056, memory_seq_rm [1056, end)
slot update_slots: id  3 | task 215 | prompt processing progress, n_tokens = 1120, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 215 | prompt done, n_tokens = 1120, batch.n_tokens = 64
slot init_sampler: id  3 | task 215 | init sampler, took 0.24 ms, tokens: text = 1120, total = 1120
slot update_slots: id  3 | task 215 | created context checkpoint 3 of 8 (pos_min = 70, pos_max = 1055, size = 23.121 MiB)
slot print_timing: id  3 | task 215 | 
prompt eval time =     263.24 ms /   102 tokens (    2.58 ms per token,   387.48 tokens per second)
       eval time =    2189.97 ms /    98 tokens (   22.35 ms per token,    44.75 tokens per second)
      total time =    2453.21 ms /   200 tokens
slot      release: id  3 | task 215 | stop processing: n_tokens = 1217, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 315 | processing task, is_child = 0
slot update_slots: id  3 | task 315 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1183
slot update_slots: id  3 | task 315 | n_tokens = 1120, memory_seq_rm [1120, end)
slot update_slots: id  3 | task 315 | prompt processing progress, n_tokens = 1183, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 315 | prompt done, n_tokens = 1183, batch.n_tokens = 63
slot init_sampler: id  3 | task 315 | init sampler, took 0.22 ms, tokens: text = 1183, total = 1183
slot print_timing: id  3 | task 315 | 
prompt eval time =     277.63 ms /    63 tokens (    4.41 ms per token,   226.92 tokens per second)
       eval time =    1715.65 ms /    68 tokens (   25.23 ms per token,    39.64 tokens per second)
      total time =    1993.28 ms /   131 tokens
slot      release: id  3 | task 315 | stop processing: n_tokens = 1250, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.862 (> 0.100 thold), f_keep = 0.946
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 384 | processing task, is_child = 0
slot update_slots: id  3 | task 384 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1373
slot update_slots: id  3 | task 384 | n_tokens = 1183, memory_seq_rm [1183, end)
slot update_slots: id  3 | task 384 | prompt processing progress, n_tokens = 1309, batch.n_tokens = 126, progress = 0.953387
slot update_slots: id  3 | task 384 | n_tokens = 1309, memory_seq_rm [1309, end)
slot update_slots: id  3 | task 384 | prompt processing progress, n_tokens = 1373, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 384 | prompt done, n_tokens = 1373, batch.n_tokens = 64
slot init_sampler: id  3 | task 384 | init sampler, took 0.65 ms, tokens: text = 1373, total = 1373
slot update_slots: id  3 | task 384 | created context checkpoint 4 of 8 (pos_min = 291, pos_max = 1308, size = 23.871 MiB)
slot print_timing: id  3 | task 384 | 
prompt eval time =     432.20 ms /   190 tokens (    2.27 ms per token,   439.61 tokens per second)
       eval time =    2173.42 ms /    84 tokens (   25.87 ms per token,    38.65 tokens per second)
      total time =    2605.62 ms /   274 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 384 | stop processing: n_tokens = 1456, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.956 (> 0.100 thold), f_keep = 0.943
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 470 | processing task, is_child = 0
slot update_slots: id  3 | task 470 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1436
slot update_slots: id  3 | task 470 | n_tokens = 1373, memory_seq_rm [1373, end)
slot update_slots: id  3 | task 470 | prompt processing progress, n_tokens = 1436, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 470 | prompt done, n_tokens = 1436, batch.n_tokens = 63
slot init_sampler: id  3 | task 470 | init sampler, took 0.27 ms, tokens: text = 1436, total = 1436
slot print_timing: id  3 | task 470 | 
prompt eval time =     296.92 ms /    63 tokens (    4.71 ms per token,   212.18 tokens per second)
       eval time =    1802.43 ms /    62 tokens (   29.07 ms per token,    34.40 tokens per second)
      total time =    2099.35 ms /   125 tokens
slot      release: id  3 | task 470 | stop processing: n_tokens = 1497, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.775 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 533 | processing task, is_child = 0
slot update_slots: id  3 | task 533 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1854
slot update_slots: id  3 | task 533 | n_tokens = 1436, memory_seq_rm [1436, end)
slot update_slots: id  3 | task 533 | prompt processing progress, n_tokens = 1790, batch.n_tokens = 354, progress = 0.965480
slot update_slots: id  3 | task 533 | n_tokens = 1790, memory_seq_rm [1790, end)
slot update_slots: id  3 | task 533 | prompt processing progress, n_tokens = 1854, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 533 | prompt done, n_tokens = 1854, batch.n_tokens = 64
slot init_sampler: id  3 | task 533 | init sampler, took 0.36 ms, tokens: text = 1854, total = 1854
slot update_slots: id  3 | task 533 | created context checkpoint 5 of 8 (pos_min = 772, pos_max = 1789, size = 23.871 MiB)
slot print_timing: id  3 | task 533 | 
prompt eval time =     620.23 ms /   418 tokens (    1.48 ms per token,   673.95 tokens per second)
       eval time =    1597.07 ms /    56 tokens (   28.52 ms per token,    35.06 tokens per second)
      total time =    2217.29 ms /   474 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 533 | stop processing: n_tokens = 1909, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.968 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 591 | processing task, is_child = 0
slot update_slots: id  3 | task 591 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1915
slot update_slots: id  3 | task 591 | n_tokens = 1854, memory_seq_rm [1854, end)
slot update_slots: id  3 | task 591 | prompt processing progress, n_tokens = 1915, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  3 | task 591 | prompt done, n_tokens = 1915, batch.n_tokens = 61
slot init_sampler: id  3 | task 591 | init sampler, took 0.38 ms, tokens: text = 1915, total = 1915
slot print_timing: id  3 | task 591 | 
prompt eval time =     159.84 ms /    61 tokens (    2.62 ms per token,   381.63 tokens per second)
       eval time =    1753.37 ms /    62 tokens (   28.28 ms per token,    35.36 tokens per second)
      total time =    1913.21 ms /   123 tokens
slot      release: id  3 | task 591 | stop processing: n_tokens = 1976, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.870 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 654 | processing task, is_child = 0
slot update_slots: id  3 | task 654 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2202
slot update_slots: id  3 | task 654 | n_tokens = 1915, memory_seq_rm [1915, end)
slot update_slots: id  3 | task 654 | prompt processing progress, n_tokens = 2138, batch.n_tokens = 223, progress = 0.970936
slot update_slots: id  3 | task 654 | n_tokens = 2138, memory_seq_rm [2138, end)
slot update_slots: id  3 | task 654 | prompt processing progress, n_tokens = 2202, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 654 | prompt done, n_tokens = 2202, batch.n_tokens = 64
slot init_sampler: id  3 | task 654 | init sampler, took 0.43 ms, tokens: text = 2202, total = 2202
slot update_slots: id  3 | task 654 | created context checkpoint 6 of 8 (pos_min = 1114, pos_max = 2137, size = 24.012 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 654
slot      release: id  3 | task 654 | stop processing: n_tokens = 2337, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.217
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2337, total state size = 78.812 MiB
srv          load:  - looking for better prompt, base f_keep = 0.217, sim = 1.000
srv        update:  - cache state: 1 prompts, 205.157 MiB (limits: 8192.000 MiB, 64000 tokens, 93317 est)
srv        update:    - prompt 0x559b56192f30:    2337 tokens, checkpoints:  6,   205.157 MiB
srv  get_availabl: prompt cache update took 267.15 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 793 | processing task, is_child = 0
slot update_slots: id  3 | task 793 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 508
slot update_slots: id  3 | task 793 | n_past = 508, slot.prompt.tokens.size() = 2337, seq_id = 3, pos_min = 1313, n_swa = 128
slot update_slots: id  3 | task 793 | restored context checkpoint (pos_min = 291, pos_max = 1308, size = 23.871 MiB)
slot update_slots: id  3 | task 793 | erased invalidated context checkpoint (pos_min = 772, pos_max = 1789, n_swa = 128, size = 23.871 MiB)
slot update_slots: id  3 | task 793 | erased invalidated context checkpoint (pos_min = 1114, pos_max = 2137, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 793 | need to evaluate at least 1 token for each active slot (n_past = 508, task.n_tokens() = 508)
slot update_slots: id  3 | task 793 | n_past was set to 507
slot update_slots: id  3 | task 793 | n_tokens = 507, memory_seq_rm [507, end)
slot update_slots: id  3 | task 793 | prompt processing progress, n_tokens = 508, batch.n_tokens = 1, progress = 1.000000
slot update_slots: id  3 | task 793 | prompt done, n_tokens = 508, batch.n_tokens = 1
slot init_sampler: id  3 | task 793 | init sampler, took 0.09 ms, tokens: text = 508, total = 508
slot print_timing: id  3 | task 793 | 
prompt eval time =      68.52 ms /     1 tokens (   68.52 ms per token,    14.59 tokens per second)
       eval time =    2053.74 ms /    82 tokens (   25.05 ms per token,    39.93 tokens per second)
      total time =    2122.26 ms /    83 tokens
slot      release: id  3 | task 793 | stop processing: n_tokens = 589, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 876 | processing task, is_child = 0
slot update_slots: id  2 | task 876 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 36149
slot update_slots: id  2 | task 876 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.056654
slot update_slots: id  2 | task 876 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.113309
slot update_slots: id  2 | task 876 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.169963
slot update_slots: id  2 | task 876 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.226618
slot update_slots: id  2 | task 876 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.283272
slot update_slots: id  2 | task 876 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.339926
slot update_slots: id  2 | task 876 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.396581
slot update_slots: id  2 | task 876 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.453235
slot update_slots: id  2 | task 876 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.509890
slot update_slots: id  2 | task 876 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 2048, progress = 0.566544
slot update_slots: id  2 | task 876 | n_tokens = 20480, memory_seq_rm [20480, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 22528, batch.n_tokens = 2048, progress = 0.623198
slot update_slots: id  2 | task 876 | n_tokens = 22528, memory_seq_rm [22528, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 24576, batch.n_tokens = 2048, progress = 0.679853
slot update_slots: id  2 | task 876 | n_tokens = 24576, memory_seq_rm [24576, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 26624, batch.n_tokens = 2048, progress = 0.736507
slot update_slots: id  2 | task 876 | n_tokens = 26624, memory_seq_rm [26624, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 28672, batch.n_tokens = 2048, progress = 0.793162
slot update_slots: id  2 | task 876 | n_tokens = 28672, memory_seq_rm [28672, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 30720, batch.n_tokens = 2048, progress = 0.849816
slot update_slots: id  2 | task 876 | n_tokens = 30720, memory_seq_rm [30720, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 32768, batch.n_tokens = 2048, progress = 0.906470
slot update_slots: id  2 | task 876 | n_tokens = 32768, memory_seq_rm [32768, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 34816, batch.n_tokens = 2048, progress = 0.963125
slot update_slots: id  2 | task 876 | n_tokens = 34816, memory_seq_rm [34816, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 36085, batch.n_tokens = 1269, progress = 0.998230
slot update_slots: id  2 | task 876 | n_tokens = 36085, memory_seq_rm [36085, end)
slot update_slots: id  2 | task 876 | prompt processing progress, n_tokens = 36149, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 876 | prompt done, n_tokens = 36149, batch.n_tokens = 64
slot init_sampler: id  2 | task 876 | init sampler, took 17.40 ms, tokens: text = 36149, total = 36149
slot update_slots: id  2 | task 876 | created context checkpoint 1 of 8 (pos_min = 35188, pos_max = 36084, size = 21.034 MiB)
slot print_timing: id  2 | task 876 | 
prompt eval time =   42707.45 ms / 36149 tokens (    1.18 ms per token,   846.43 tokens per second)
       eval time =    3754.62 ms /   111 tokens (   33.83 ms per token,    29.56 tokens per second)
      total time =   46462.07 ms / 36260 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 876 | stop processing: n_tokens = 36259, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1006 | processing task, is_child = 0
slot update_slots: id  2 | task 1006 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 36217
slot update_slots: id  2 | task 1006 | n_tokens = 36149, memory_seq_rm [36149, end)
slot update_slots: id  2 | task 1006 | prompt processing progress, n_tokens = 36153, batch.n_tokens = 4, progress = 0.998233
slot update_slots: id  2 | task 1006 | n_tokens = 36153, memory_seq_rm [36153, end)
slot update_slots: id  2 | task 1006 | prompt processing progress, n_tokens = 36217, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1006 | prompt done, n_tokens = 36217, batch.n_tokens = 64
slot init_sampler: id  2 | task 1006 | init sampler, took 15.15 ms, tokens: text = 36217, total = 36217
slot update_slots: id  2 | task 1006 | created context checkpoint 2 of 8 (pos_min = 35362, pos_max = 36152, size = 18.548 MiB)
slot print_timing: id  2 | task 1006 | 
prompt eval time =     307.39 ms /    68 tokens (    4.52 ms per token,   221.22 tokens per second)
       eval time =    2245.05 ms /    70 tokens (   32.07 ms per token,    31.18 tokens per second)
      total time =    2552.44 ms /   138 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 1006 | stop processing: n_tokens = 36286, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1078 | processing task, is_child = 0
slot update_slots: id  2 | task 1078 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 36538
slot update_slots: id  2 | task 1078 | n_tokens = 36217, memory_seq_rm [36217, end)
slot update_slots: id  2 | task 1078 | prompt processing progress, n_tokens = 36474, batch.n_tokens = 257, progress = 0.998248
slot update_slots: id  2 | task 1078 | n_tokens = 36474, memory_seq_rm [36474, end)
slot update_slots: id  2 | task 1078 | prompt processing progress, n_tokens = 36538, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1078 | prompt done, n_tokens = 36538, batch.n_tokens = 64
slot init_sampler: id  2 | task 1078 | init sampler, took 11.55 ms, tokens: text = 36538, total = 36538
slot update_slots: id  2 | task 1078 | created context checkpoint 3 of 8 (pos_min = 35577, pos_max = 36473, size = 21.034 MiB)
slot print_timing: id  2 | task 1078 | 
prompt eval time =     747.98 ms /   321 tokens (    2.33 ms per token,   429.15 tokens per second)
       eval time =    1678.40 ms /    56 tokens (   29.97 ms per token,    33.37 tokens per second)
      total time =    2426.39 ms /   377 tokens
slot      release: id  2 | task 1078 | stop processing: n_tokens = 36593, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1136 | processing task, is_child = 0
slot update_slots: id  2 | task 1136 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 36616
slot update_slots: id  2 | task 1136 | n_tokens = 36538, memory_seq_rm [36538, end)
slot update_slots: id  2 | task 1136 | prompt processing progress, n_tokens = 36552, batch.n_tokens = 14, progress = 0.998252
slot update_slots: id  2 | task 1136 | n_tokens = 36552, memory_seq_rm [36552, end)
slot update_slots: id  2 | task 1136 | prompt processing progress, n_tokens = 36616, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1136 | prompt done, n_tokens = 36616, batch.n_tokens = 64
slot init_sampler: id  2 | task 1136 | init sampler, took 13.21 ms, tokens: text = 36616, total = 36616
slot update_slots: id  2 | task 1136 | created context checkpoint 4 of 8 (pos_min = 35696, pos_max = 36551, size = 20.073 MiB)
slot print_timing: id  2 | task 1136 | 
prompt eval time =     364.18 ms /    78 tokens (    4.67 ms per token,   214.18 tokens per second)
       eval time =    2020.04 ms /    64 tokens (   31.56 ms per token,    31.68 tokens per second)
      total time =    2384.22 ms /   142 tokens
slot      release: id  2 | task 1136 | stop processing: n_tokens = 36679, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1202 | processing task, is_child = 0
slot update_slots: id  2 | task 1202 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 37086
slot update_slots: id  2 | task 1202 | n_tokens = 36616, memory_seq_rm [36616, end)
slot update_slots: id  2 | task 1202 | prompt processing progress, n_tokens = 37022, batch.n_tokens = 406, progress = 0.998274
slot update_slots: id  2 | task 1202 | n_tokens = 37022, memory_seq_rm [37022, end)
slot update_slots: id  2 | task 1202 | prompt processing progress, n_tokens = 37086, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1202 | prompt done, n_tokens = 37086, batch.n_tokens = 64
slot init_sampler: id  2 | task 1202 | init sampler, took 7.92 ms, tokens: text = 37086, total = 37086
slot update_slots: id  2 | task 1202 | created context checkpoint 5 of 8 (pos_min = 36125, pos_max = 37021, size = 21.034 MiB)
slot print_timing: id  2 | task 1202 | 
prompt eval time =     938.18 ms /   470 tokens (    2.00 ms per token,   500.97 tokens per second)
       eval time =    2326.92 ms /    68 tokens (   34.22 ms per token,    29.22 tokens per second)
      total time =    3265.10 ms /   538 tokens
slot      release: id  2 | task 1202 | stop processing: n_tokens = 37153, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.952 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1272 | processing task, is_child = 0
slot update_slots: id  2 | task 1272 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 38960
slot update_slots: id  2 | task 1272 | n_tokens = 37086, memory_seq_rm [37086, end)
slot update_slots: id  2 | task 1272 | prompt processing progress, n_tokens = 38896, batch.n_tokens = 1810, progress = 0.998357
slot update_slots: id  2 | task 1272 | n_tokens = 38896, memory_seq_rm [38896, end)
slot update_slots: id  2 | task 1272 | prompt processing progress, n_tokens = 38960, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1272 | prompt done, n_tokens = 38960, batch.n_tokens = 64
slot init_sampler: id  2 | task 1272 | init sampler, took 7.57 ms, tokens: text = 38960, total = 38960
slot update_slots: id  2 | task 1272 | created context checkpoint 6 of 8 (pos_min = 37999, pos_max = 38895, size = 21.034 MiB)
slot print_timing: id  2 | task 1272 | 
prompt eval time =    3235.88 ms /  1874 tokens (    1.73 ms per token,   579.13 tokens per second)
       eval time =    1125.76 ms /    36 tokens (   31.27 ms per token,    31.98 tokens per second)
      total time =    4361.64 ms /  1910 tokens
slot      release: id  2 | task 1272 | stop processing: n_tokens = 38995, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1310 | processing task, is_child = 0
slot update_slots: id  2 | task 1310 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 40894
slot update_slots: id  2 | task 1310 | n_tokens = 38960, memory_seq_rm [38960, end)
slot update_slots: id  2 | task 1310 | prompt processing progress, n_tokens = 40830, batch.n_tokens = 1870, progress = 0.998435
slot update_slots: id  2 | task 1310 | n_tokens = 40830, memory_seq_rm [40830, end)
slot update_slots: id  2 | task 1310 | prompt processing progress, n_tokens = 40894, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1310 | prompt done, n_tokens = 40894, batch.n_tokens = 64
slot init_sampler: id  2 | task 1310 | init sampler, took 7.91 ms, tokens: text = 40894, total = 40894
slot update_slots: id  2 | task 1310 | created context checkpoint 7 of 8 (pos_min = 39933, pos_max = 40829, size = 21.034 MiB)
slot print_timing: id  2 | task 1310 | 
prompt eval time =    3400.12 ms /  1934 tokens (    1.76 ms per token,   568.80 tokens per second)
       eval time =    1646.42 ms /    44 tokens (   37.42 ms per token,    26.72 tokens per second)
      total time =    5046.54 ms /  1978 tokens
slot      release: id  2 | task 1310 | stop processing: n_tokens = 40937, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.940 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1356 | processing task, is_child = 0
slot update_slots: id  2 | task 1356 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 43517
slot update_slots: id  2 | task 1356 | n_tokens = 40894, memory_seq_rm [40894, end)
slot update_slots: id  2 | task 1356 | prompt processing progress, n_tokens = 42942, batch.n_tokens = 2048, progress = 0.986787
slot update_slots: id  2 | task 1356 | n_tokens = 42942, memory_seq_rm [42942, end)
slot update_slots: id  2 | task 1356 | prompt processing progress, n_tokens = 43453, batch.n_tokens = 511, progress = 0.998529
slot update_slots: id  2 | task 1356 | n_tokens = 43453, memory_seq_rm [43453, end)
slot update_slots: id  2 | task 1356 | prompt processing progress, n_tokens = 43517, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1356 | prompt done, n_tokens = 43517, batch.n_tokens = 64
slot init_sampler: id  2 | task 1356 | init sampler, took 14.90 ms, tokens: text = 43517, total = 43517
slot update_slots: id  2 | task 1356 | created context checkpoint 8 of 8 (pos_min = 42556, pos_max = 43452, size = 21.034 MiB)
slot print_timing: id  2 | task 1356 | 
prompt eval time =    4472.46 ms /  2623 tokens (    1.71 ms per token,   586.48 tokens per second)
       eval time =    3548.76 ms /   114 tokens (   31.13 ms per token,    32.12 tokens per second)
      total time =    8021.21 ms /  2737 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 1356 | stop processing: n_tokens = 43630, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1473 | processing task, is_child = 0
slot update_slots: id  2 | task 1473 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 43628
slot update_slots: id  2 | task 1473 | n_tokens = 43517, memory_seq_rm [43517, end)
slot update_slots: id  2 | task 1473 | prompt processing progress, n_tokens = 43564, batch.n_tokens = 47, progress = 0.998533
slot update_slots: id  2 | task 1473 | n_tokens = 43564, memory_seq_rm [43564, end)
slot update_slots: id  2 | task 1473 | prompt processing progress, n_tokens = 43628, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1473 | prompt done, n_tokens = 43628, batch.n_tokens = 64
slot init_sampler: id  2 | task 1473 | init sampler, took 16.41 ms, tokens: text = 43628, total = 43628
slot update_slots: id  2 | task 1473 | erasing old context checkpoint (pos_min = 35188, pos_max = 36084, size = 21.034 MiB)
slot update_slots: id  2 | task 1473 | created context checkpoint 8 of 8 (pos_min = 42733, pos_max = 43563, size = 19.486 MiB)
slot print_timing: id  2 | task 1473 | 
prompt eval time =     431.46 ms /   111 tokens (    3.89 ms per token,   257.26 tokens per second)
       eval time =    1132.46 ms /    35 tokens (   32.36 ms per token,    30.91 tokens per second)
      total time =    1563.93 ms /   146 tokens
slot      release: id  2 | task 1473 | stop processing: n_tokens = 43662, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1510 | processing task, is_child = 0
slot update_slots: id  2 | task 1510 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 45206
slot update_slots: id  2 | task 1510 | n_tokens = 43628, memory_seq_rm [43628, end)
slot update_slots: id  2 | task 1510 | prompt processing progress, n_tokens = 45142, batch.n_tokens = 1514, progress = 0.998584
slot update_slots: id  2 | task 1510 | n_tokens = 45142, memory_seq_rm [45142, end)
slot update_slots: id  2 | task 1510 | prompt processing progress, n_tokens = 45206, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1510 | prompt done, n_tokens = 45206, batch.n_tokens = 64
slot init_sampler: id  2 | task 1510 | init sampler, took 9.29 ms, tokens: text = 45206, total = 45206
slot update_slots: id  2 | task 1510 | erasing old context checkpoint (pos_min = 35362, pos_max = 36152, size = 18.548 MiB)
slot update_slots: id  2 | task 1510 | created context checkpoint 8 of 8 (pos_min = 44245, pos_max = 45141, size = 21.034 MiB)
slot print_timing: id  2 | task 1510 | 
prompt eval time =    2766.40 ms /  1578 tokens (    1.75 ms per token,   570.42 tokens per second)
       eval time =    3880.15 ms /   119 tokens (   32.61 ms per token,    30.67 tokens per second)
      total time =    6646.56 ms /  1697 tokens
slot      release: id  2 | task 1510 | stop processing: n_tokens = 45324, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1631 | processing task, is_child = 0
slot update_slots: id  2 | task 1631 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 45268
slot update_slots: id  2 | task 1631 | n_tokens = 45206, memory_seq_rm [45206, end)
slot update_slots: id  2 | task 1631 | prompt processing progress, n_tokens = 45268, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  2 | task 1631 | prompt done, n_tokens = 45268, batch.n_tokens = 62
slot init_sampler: id  2 | task 1631 | init sampler, took 25.08 ms, tokens: text = 45268, total = 45268
slot print_timing: id  2 | task 1631 | 
prompt eval time =     303.33 ms /    62 tokens (    4.89 ms per token,   204.40 tokens per second)
       eval time =    1677.00 ms /    53 tokens (   31.64 ms per token,    31.60 tokens per second)
      total time =    1980.33 ms /   115 tokens
slot      release: id  2 | task 1631 | stop processing: n_tokens = 45320, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1685 | processing task, is_child = 0
slot update_slots: id  2 | task 1685 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 45960
slot update_slots: id  2 | task 1685 | n_tokens = 45268, memory_seq_rm [45268, end)
slot update_slots: id  2 | task 1685 | prompt processing progress, n_tokens = 45896, batch.n_tokens = 628, progress = 0.998607
slot update_slots: id  2 | task 1685 | n_tokens = 45896, memory_seq_rm [45896, end)
slot update_slots: id  2 | task 1685 | prompt processing progress, n_tokens = 45960, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1685 | prompt done, n_tokens = 45960, batch.n_tokens = 64
slot init_sampler: id  2 | task 1685 | init sampler, took 18.84 ms, tokens: text = 45960, total = 45960
slot update_slots: id  2 | task 1685 | erasing old context checkpoint (pos_min = 35577, pos_max = 36473, size = 21.034 MiB)
slot update_slots: id  2 | task 1685 | created context checkpoint 8 of 8 (pos_min = 44999, pos_max = 45895, size = 21.034 MiB)
slot print_timing: id  2 | task 1685 | 
prompt eval time =    1546.79 ms /   692 tokens (    2.24 ms per token,   447.38 tokens per second)
       eval time =    2635.11 ms /    84 tokens (   31.37 ms per token,    31.88 tokens per second)
      total time =    4181.90 ms /   776 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 1685 | stop processing: n_tokens = 46043, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1771 | processing task, is_child = 0
slot update_slots: id  2 | task 1771 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 47996
slot update_slots: id  2 | task 1771 | n_tokens = 45960, memory_seq_rm [45960, end)
slot update_slots: id  2 | task 1771 | prompt processing progress, n_tokens = 47932, batch.n_tokens = 1972, progress = 0.998667
slot update_slots: id  2 | task 1771 | n_tokens = 47932, memory_seq_rm [47932, end)
slot update_slots: id  2 | task 1771 | prompt processing progress, n_tokens = 47996, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 1771 | prompt done, n_tokens = 47996, batch.n_tokens = 64
slot init_sampler: id  2 | task 1771 | init sampler, took 23.40 ms, tokens: text = 47996, total = 47996
slot update_slots: id  2 | task 1771 | erasing old context checkpoint (pos_min = 35696, pos_max = 36551, size = 20.073 MiB)
slot update_slots: id  2 | task 1771 | created context checkpoint 8 of 8 (pos_min = 47035, pos_max = 47931, size = 21.034 MiB)
slot print_timing: id  2 | task 1771 | 
prompt eval time =    3804.77 ms /  2036 tokens (    1.87 ms per token,   535.12 tokens per second)
       eval time =    4540.04 ms /   140 tokens (   32.43 ms per token,    30.84 tokens per second)
      total time =    8344.81 ms /  2176 tokens
slot      release: id  2 | task 1771 | stop processing: n_tokens = 48135, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1913 | processing task, is_child = 0
slot update_slots: id  2 | task 1913 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 48058
slot update_slots: id  2 | task 1913 | n_tokens = 47996, memory_seq_rm [47996, end)
slot update_slots: id  2 | task 1913 | prompt processing progress, n_tokens = 48058, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  2 | task 1913 | prompt done, n_tokens = 48058, batch.n_tokens = 62
slot init_sampler: id  2 | task 1913 | init sampler, took 9.71 ms, tokens: text = 48058, total = 48058
slot print_timing: id  2 | task 1913 | 
prompt eval time =     270.42 ms /    62 tokens (    4.36 ms per token,   229.27 tokens per second)
       eval time =    2143.55 ms /    68 tokens (   31.52 ms per token,    31.72 tokens per second)
      total time =    2413.97 ms /   130 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 1913 | stop processing: n_tokens = 48125, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 1982 | processing task, is_child = 0
slot update_slots: id  2 | task 1982 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 48117
slot update_slots: id  2 | task 1982 | n_tokens = 48058, memory_seq_rm [48058, end)
slot update_slots: id  2 | task 1982 | prompt processing progress, n_tokens = 48117, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  2 | task 1982 | prompt done, n_tokens = 48117, batch.n_tokens = 59
slot init_sampler: id  2 | task 1982 | init sampler, took 20.05 ms, tokens: text = 48117, total = 48117
slot update_slots: id  2 | task 1982 | erasing old context checkpoint (pos_min = 36125, pos_max = 37021, size = 21.034 MiB)
slot update_slots: id  2 | task 1982 | created context checkpoint 8 of 8 (pos_min = 47238, pos_max = 48057, size = 19.228 MiB)
slot print_timing: id  2 | task 1982 | 
prompt eval time =     259.46 ms /    59 tokens (    4.40 ms per token,   227.39 tokens per second)
       eval time =    2531.23 ms /    82 tokens (   30.87 ms per token,    32.40 tokens per second)
      total time =    2790.69 ms /   141 tokens
slot      release: id  2 | task 1982 | stop processing: n_tokens = 48198, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 2065 | processing task, is_child = 0
slot update_slots: id  2 | task 2065 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 48179
slot update_slots: id  2 | task 2065 | n_tokens = 48117, memory_seq_rm [48117, end)
slot update_slots: id  2 | task 2065 | prompt processing progress, n_tokens = 48179, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  2 | task 2065 | prompt done, n_tokens = 48179, batch.n_tokens = 62
slot init_sampler: id  2 | task 2065 | init sampler, took 10.69 ms, tokens: text = 48179, total = 48179
slot print_timing: id  2 | task 2065 | 
prompt eval time =     275.31 ms /    62 tokens (    4.44 ms per token,   225.20 tokens per second)
       eval time =    1929.48 ms /    57 tokens (   33.85 ms per token,    29.54 tokens per second)
      total time =    2204.79 ms /   119 tokens
slot      release: id  2 | task 2065 | stop processing: n_tokens = 48235, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 2123 | processing task, is_child = 0
slot update_slots: id  2 | task 2123 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 48866
slot update_slots: id  2 | task 2123 | n_tokens = 48179, memory_seq_rm [48179, end)
slot update_slots: id  2 | task 2123 | prompt processing progress, n_tokens = 48802, batch.n_tokens = 623, progress = 0.998690
slot update_slots: id  2 | task 2123 | n_tokens = 48802, memory_seq_rm [48802, end)
slot update_slots: id  2 | task 2123 | prompt processing progress, n_tokens = 48866, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 2123 | prompt done, n_tokens = 48866, batch.n_tokens = 64
slot init_sampler: id  2 | task 2123 | init sampler, took 17.59 ms, tokens: text = 48866, total = 48866
slot update_slots: id  2 | task 2123 | erasing old context checkpoint (pos_min = 37999, pos_max = 38895, size = 21.034 MiB)
slot update_slots: id  2 | task 2123 | created context checkpoint 8 of 8 (pos_min = 47905, pos_max = 48801, size = 21.034 MiB)
slot print_timing: id  2 | task 2123 | 
prompt eval time =    1640.21 ms /   687 tokens (    2.39 ms per token,   418.85 tokens per second)
       eval time =   11955.86 ms /   381 tokens (   31.38 ms per token,    31.87 tokens per second)
      total time =   13596.07 ms /  1068 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 2123 | stop processing: n_tokens = 49246, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 2506 | processing task, is_child = 0
slot update_slots: id  2 | task 2506 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 49831
slot update_slots: id  2 | task 2506 | n_tokens = 48866, memory_seq_rm [48866, end)
slot update_slots: id  2 | task 2506 | prompt processing progress, n_tokens = 49767, batch.n_tokens = 901, progress = 0.998716
slot update_slots: id  2 | task 2506 | n_tokens = 49767, memory_seq_rm [49767, end)
slot update_slots: id  2 | task 2506 | prompt processing progress, n_tokens = 49831, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 2506 | prompt done, n_tokens = 49831, batch.n_tokens = 64
slot init_sampler: id  2 | task 2506 | init sampler, took 26.46 ms, tokens: text = 49831, total = 49831
slot update_slots: id  2 | task 2506 | erasing old context checkpoint (pos_min = 39933, pos_max = 40829, size = 21.034 MiB)
slot update_slots: id  2 | task 2506 | created context checkpoint 8 of 8 (pos_min = 48870, pos_max = 49766, size = 21.034 MiB)
slot print_timing: id  2 | task 2506 | 
prompt eval time =    2030.48 ms /   965 tokens (    2.10 ms per token,   475.26 tokens per second)
       eval time =    7658.78 ms /   249 tokens (   30.76 ms per token,    32.51 tokens per second)
      total time =    9689.26 ms /  1214 tokens
slot      release: id  2 | task 2506 | stop processing: n_tokens = 50079, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 2757 | processing task, is_child = 0
slot update_slots: id  2 | task 2757 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 49889
slot update_slots: id  2 | task 2757 | n_tokens = 49831, memory_seq_rm [49831, end)
slot update_slots: id  2 | task 2757 | prompt processing progress, n_tokens = 49889, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  2 | task 2757 | prompt done, n_tokens = 49889, batch.n_tokens = 58
slot init_sampler: id  2 | task 2757 | init sampler, took 8.14 ms, tokens: text = 49889, total = 49889
slot print_timing: id  2 | task 2757 | 
prompt eval time =     221.47 ms /    58 tokens (    3.82 ms per token,   261.88 tokens per second)
       eval time =    4714.91 ms /   148 tokens (   31.86 ms per token,    31.39 tokens per second)
      total time =    4936.39 ms /   206 tokens
slot      release: id  2 | task 2757 | stop processing: n_tokens = 50036, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 2906 | processing task, is_child = 0
slot update_slots: id  2 | task 2906 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 49952
slot update_slots: id  2 | task 2906 | n_tokens = 49889, memory_seq_rm [49889, end)
slot update_slots: id  2 | task 2906 | prompt processing progress, n_tokens = 49952, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  2 | task 2906 | prompt done, n_tokens = 49952, batch.n_tokens = 63
slot init_sampler: id  2 | task 2906 | init sampler, took 17.71 ms, tokens: text = 49952, total = 49952
slot update_slots: id  2 | task 2906 | erasing old context checkpoint (pos_min = 42556, pos_max = 43452, size = 21.034 MiB)
slot update_slots: id  2 | task 2906 | created context checkpoint 8 of 8 (pos_min = 49182, pos_max = 49888, size = 16.579 MiB)
slot print_timing: id  2 | task 2906 | 
prompt eval time =     301.21 ms /    63 tokens (    4.78 ms per token,   209.16 tokens per second)
       eval time =    5797.46 ms /   188 tokens (   30.84 ms per token,    32.43 tokens per second)
      total time =    6098.66 ms /   251 tokens
slot      release: id  2 | task 2906 | stop processing: n_tokens = 50139, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3095 | processing task, is_child = 0
slot update_slots: id  2 | task 3095 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 50142
slot update_slots: id  2 | task 3095 | n_tokens = 49952, memory_seq_rm [49952, end)
slot update_slots: id  2 | task 3095 | prompt processing progress, n_tokens = 50078, batch.n_tokens = 126, progress = 0.998724
slot update_slots: id  2 | task 3095 | n_tokens = 50078, memory_seq_rm [50078, end)
slot update_slots: id  2 | task 3095 | prompt processing progress, n_tokens = 50142, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3095 | prompt done, n_tokens = 50142, batch.n_tokens = 64
slot init_sampler: id  2 | task 3095 | init sampler, took 9.95 ms, tokens: text = 50142, total = 50142
slot update_slots: id  2 | task 3095 | erasing old context checkpoint (pos_min = 42733, pos_max = 43563, size = 19.486 MiB)
slot update_slots: id  2 | task 3095 | created context checkpoint 8 of 8 (pos_min = 49242, pos_max = 50077, size = 19.604 MiB)
slot print_timing: id  2 | task 3095 | 
prompt eval time =     759.05 ms /   190 tokens (    3.99 ms per token,   250.31 tokens per second)
       eval time =    2445.65 ms /    80 tokens (   30.57 ms per token,    32.71 tokens per second)
      total time =    3204.69 ms /   270 tokens
slot      release: id  2 | task 3095 | stop processing: n_tokens = 50221, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3177 | processing task, is_child = 0
slot update_slots: id  2 | task 3177 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 51647
slot update_slots: id  2 | task 3177 | n_tokens = 50142, memory_seq_rm [50142, end)
slot update_slots: id  2 | task 3177 | prompt processing progress, n_tokens = 51583, batch.n_tokens = 1441, progress = 0.998761
slot update_slots: id  2 | task 3177 | n_tokens = 51583, memory_seq_rm [51583, end)
slot update_slots: id  2 | task 3177 | prompt processing progress, n_tokens = 51647, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3177 | prompt done, n_tokens = 51647, batch.n_tokens = 64
slot init_sampler: id  2 | task 3177 | init sampler, took 32.78 ms, tokens: text = 51647, total = 51647
slot update_slots: id  2 | task 3177 | erasing old context checkpoint (pos_min = 44245, pos_max = 45141, size = 21.034 MiB)
slot update_slots: id  2 | task 3177 | created context checkpoint 8 of 8 (pos_min = 50686, pos_max = 51582, size = 21.034 MiB)
slot print_timing: id  2 | task 3177 | 
prompt eval time =    2940.34 ms /  1505 tokens (    1.95 ms per token,   511.85 tokens per second)
       eval time =    2669.27 ms /    86 tokens (   31.04 ms per token,    32.22 tokens per second)
      total time =    5609.61 ms /  1591 tokens
slot      release: id  2 | task 3177 | stop processing: n_tokens = 51732, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3265 | processing task, is_child = 0
slot update_slots: id  2 | task 3265 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 52720
slot update_slots: id  2 | task 3265 | n_tokens = 51647, memory_seq_rm [51647, end)
slot update_slots: id  2 | task 3265 | prompt processing progress, n_tokens = 52656, batch.n_tokens = 1009, progress = 0.998786
slot update_slots: id  2 | task 3265 | n_tokens = 52656, memory_seq_rm [52656, end)
slot update_slots: id  2 | task 3265 | prompt processing progress, n_tokens = 52720, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3265 | prompt done, n_tokens = 52720, batch.n_tokens = 64
slot init_sampler: id  2 | task 3265 | init sampler, took 9.73 ms, tokens: text = 52720, total = 52720
slot update_slots: id  2 | task 3265 | erasing old context checkpoint (pos_min = 44999, pos_max = 45895, size = 21.034 MiB)
slot update_slots: id  2 | task 3265 | created context checkpoint 8 of 8 (pos_min = 51759, pos_max = 52655, size = 21.034 MiB)
slot print_timing: id  2 | task 3265 | 
prompt eval time =    2147.32 ms /  1073 tokens (    2.00 ms per token,   499.69 tokens per second)
       eval time =   12926.97 ms /   404 tokens (   32.00 ms per token,    31.25 tokens per second)
      total time =   15074.29 ms /  1477 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 3265 | stop processing: n_tokens = 53123, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3671 | processing task, is_child = 0
slot update_slots: id  2 | task 3671 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 52781
slot update_slots: id  2 | task 3671 | n_tokens = 52720, memory_seq_rm [52720, end)
slot update_slots: id  2 | task 3671 | prompt processing progress, n_tokens = 52781, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  2 | task 3671 | prompt done, n_tokens = 52781, batch.n_tokens = 61
slot init_sampler: id  2 | task 3671 | init sampler, took 8.97 ms, tokens: text = 52781, total = 52781
slot print_timing: id  2 | task 3671 | 
prompt eval time =     351.20 ms /    61 tokens (    5.76 ms per token,   173.69 tokens per second)
       eval time =    2015.59 ms /    67 tokens (   30.08 ms per token,    33.24 tokens per second)
      total time =    2366.78 ms /   128 tokens
slot      release: id  2 | task 3671 | stop processing: n_tokens = 52847, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3739 | processing task, is_child = 0
slot update_slots: id  2 | task 3739 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 52856
slot update_slots: id  2 | task 3739 | n_tokens = 52781, memory_seq_rm [52781, end)
slot update_slots: id  2 | task 3739 | prompt processing progress, n_tokens = 52792, batch.n_tokens = 11, progress = 0.998789
slot update_slots: id  2 | task 3739 | n_tokens = 52792, memory_seq_rm [52792, end)
slot update_slots: id  2 | task 3739 | prompt processing progress, n_tokens = 52856, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3739 | prompt done, n_tokens = 52856, batch.n_tokens = 64
slot init_sampler: id  2 | task 3739 | init sampler, took 28.08 ms, tokens: text = 52856, total = 52856
slot update_slots: id  2 | task 3739 | erasing old context checkpoint (pos_min = 47035, pos_max = 47931, size = 21.034 MiB)
slot update_slots: id  2 | task 3739 | created context checkpoint 8 of 8 (pos_min = 52226, pos_max = 52791, size = 13.272 MiB)
slot print_timing: id  2 | task 3739 | 
prompt eval time =     441.75 ms /    75 tokens (    5.89 ms per token,   169.78 tokens per second)
       eval time =    2021.20 ms /    65 tokens (   31.10 ms per token,    32.16 tokens per second)
      total time =    2462.94 ms /   140 tokens
slot      release: id  2 | task 3739 | stop processing: n_tokens = 52920, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3806 | processing task, is_child = 0
slot update_slots: id  2 | task 3806 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 52976
slot update_slots: id  2 | task 3806 | n_tokens = 52856, memory_seq_rm [52856, end)
slot update_slots: id  2 | task 3806 | prompt processing progress, n_tokens = 52912, batch.n_tokens = 56, progress = 0.998792
slot update_slots: id  2 | task 3806 | n_tokens = 52912, memory_seq_rm [52912, end)
slot update_slots: id  2 | task 3806 | prompt processing progress, n_tokens = 52976, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3806 | prompt done, n_tokens = 52976, batch.n_tokens = 64
slot init_sampler: id  2 | task 3806 | init sampler, took 10.09 ms, tokens: text = 52976, total = 52976
slot update_slots: id  2 | task 3806 | erasing old context checkpoint (pos_min = 47238, pos_max = 48057, size = 19.228 MiB)
slot update_slots: id  2 | task 3806 | created context checkpoint 8 of 8 (pos_min = 52226, pos_max = 52911, size = 16.086 MiB)
slot print_timing: id  2 | task 3806 | 
prompt eval time =     642.61 ms /   120 tokens (    5.36 ms per token,   186.74 tokens per second)
       eval time =   10822.76 ms /   321 tokens (   33.72 ms per token,    29.66 tokens per second)
      total time =   11465.37 ms /   441 tokens
slot      release: id  2 | task 3806 | stop processing: n_tokens = 53296, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4129 | processing task, is_child = 0
slot update_slots: id  2 | task 4129 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 53537
slot update_slots: id  2 | task 4129 | n_tokens = 52976, memory_seq_rm [52976, end)
slot update_slots: id  2 | task 4129 | prompt processing progress, n_tokens = 53473, batch.n_tokens = 497, progress = 0.998805
slot update_slots: id  2 | task 4129 | n_tokens = 53473, memory_seq_rm [53473, end)
slot update_slots: id  2 | task 4129 | prompt processing progress, n_tokens = 53537, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4129 | prompt done, n_tokens = 53537, batch.n_tokens = 64
slot init_sampler: id  2 | task 4129 | init sampler, took 10.20 ms, tokens: text = 53537, total = 53537
slot update_slots: id  2 | task 4129 | erasing old context checkpoint (pos_min = 47905, pos_max = 48801, size = 21.034 MiB)
slot update_slots: id  2 | task 4129 | created context checkpoint 8 of 8 (pos_min = 52720, pos_max = 53472, size = 17.657 MiB)
slot print_timing: id  2 | task 4129 | 
prompt eval time =    1304.51 ms /   561 tokens (    2.33 ms per token,   430.05 tokens per second)
       eval time =    6268.51 ms /   190 tokens (   32.99 ms per token,    30.31 tokens per second)
      total time =    7573.02 ms /   751 tokens
slot      release: id  2 | task 4129 | stop processing: n_tokens = 53726, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4321 | processing task, is_child = 0
slot update_slots: id  2 | task 4321 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 53602
slot update_slots: id  2 | task 4321 | n_tokens = 53537, memory_seq_rm [53537, end)
slot update_slots: id  2 | task 4321 | prompt processing progress, n_tokens = 53538, batch.n_tokens = 1, progress = 0.998806
slot update_slots: id  2 | task 4321 | n_tokens = 53538, memory_seq_rm [53538, end)
slot update_slots: id  2 | task 4321 | prompt processing progress, n_tokens = 53602, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4321 | prompt done, n_tokens = 53602, batch.n_tokens = 64
slot init_sampler: id  2 | task 4321 | init sampler, took 23.42 ms, tokens: text = 53602, total = 53602
slot update_slots: id  2 | task 4321 | erasing old context checkpoint (pos_min = 48870, pos_max = 49766, size = 21.034 MiB)
slot update_slots: id  2 | task 4321 | created context checkpoint 8 of 8 (pos_min = 52829, pos_max = 53537, size = 16.626 MiB)
slot print_timing: id  2 | task 4321 | 
prompt eval time =     402.13 ms /    65 tokens (    6.19 ms per token,   161.64 tokens per second)
       eval time =    4725.69 ms /   148 tokens (   31.93 ms per token,    31.32 tokens per second)
      total time =    5127.82 ms /   213 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 4321 | stop processing: n_tokens = 53749, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4471 | processing task, is_child = 0
slot update_slots: id  2 | task 4471 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 53665
slot update_slots: id  2 | task 4471 | n_tokens = 53602, memory_seq_rm [53602, end)
slot update_slots: id  2 | task 4471 | prompt processing progress, n_tokens = 53665, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  2 | task 4471 | prompt done, n_tokens = 53665, batch.n_tokens = 63
slot init_sampler: id  2 | task 4471 | init sampler, took 20.41 ms, tokens: text = 53665, total = 53665
slot print_timing: id  2 | task 4471 | 
prompt eval time =     301.36 ms /    63 tokens (    4.78 ms per token,   209.05 tokens per second)
       eval time =    2955.77 ms /    95 tokens (   31.11 ms per token,    32.14 tokens per second)
      total time =    3257.13 ms /   158 tokens
slot      release: id  2 | task 4471 | stop processing: n_tokens = 53759, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4567 | processing task, is_child = 0
slot update_slots: id  2 | task 4567 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 53728
slot update_slots: id  2 | task 4567 | n_tokens = 53665, memory_seq_rm [53665, end)
slot update_slots: id  2 | task 4567 | prompt processing progress, n_tokens = 53728, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  2 | task 4567 | prompt done, n_tokens = 53728, batch.n_tokens = 63
slot init_sampler: id  2 | task 4567 | init sampler, took 10.66 ms, tokens: text = 53728, total = 53728
slot update_slots: id  2 | task 4567 | erasing old context checkpoint (pos_min = 49182, pos_max = 49888, size = 16.579 MiB)
slot update_slots: id  2 | task 4567 | created context checkpoint 8 of 8 (pos_min = 52942, pos_max = 53664, size = 16.954 MiB)
slot print_timing: id  2 | task 4567 | 
prompt eval time =     296.80 ms /    63 tokens (    4.71 ms per token,   212.27 tokens per second)
       eval time =    5879.70 ms /   176 tokens (   33.41 ms per token,    29.93 tokens per second)
      total time =    6176.49 ms /   239 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 4567 | stop processing: n_tokens = 53903, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4744 | processing task, is_child = 0
slot update_slots: id  2 | task 4744 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 53807
slot update_slots: id  2 | task 4744 | n_tokens = 53728, memory_seq_rm [53728, end)
slot update_slots: id  2 | task 4744 | prompt processing progress, n_tokens = 53743, batch.n_tokens = 15, progress = 0.998811
slot update_slots: id  2 | task 4744 | n_tokens = 53743, memory_seq_rm [53743, end)
slot update_slots: id  2 | task 4744 | prompt processing progress, n_tokens = 53807, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4744 | prompt done, n_tokens = 53807, batch.n_tokens = 64
slot init_sampler: id  2 | task 4744 | init sampler, took 10.90 ms, tokens: text = 53807, total = 53807
slot update_slots: id  2 | task 4744 | erasing old context checkpoint (pos_min = 49242, pos_max = 50077, size = 19.604 MiB)
slot update_slots: id  2 | task 4744 | created context checkpoint 8 of 8 (pos_min = 53086, pos_max = 53742, size = 15.406 MiB)
slot print_timing: id  2 | task 4744 | 
prompt eval time =     426.49 ms /    79 tokens (    5.40 ms per token,   185.23 tokens per second)
       eval time =    7234.25 ms /   226 tokens (   32.01 ms per token,    31.24 tokens per second)
      total time =    7660.74 ms /   305 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 4744 | stop processing: n_tokens = 54032, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4972 | processing task, is_child = 0
slot update_slots: id  2 | task 4972 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 53886
slot update_slots: id  2 | task 4972 | n_tokens = 53807, memory_seq_rm [53807, end)
slot update_slots: id  2 | task 4972 | prompt processing progress, n_tokens = 53822, batch.n_tokens = 15, progress = 0.998812
slot update_slots: id  2 | task 4972 | n_tokens = 53822, memory_seq_rm [53822, end)
slot update_slots: id  2 | task 4972 | prompt processing progress, n_tokens = 53886, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4972 | prompt done, n_tokens = 53886, batch.n_tokens = 64
slot init_sampler: id  2 | task 4972 | init sampler, took 31.68 ms, tokens: text = 53886, total = 53886
slot update_slots: id  2 | task 4972 | erasing old context checkpoint (pos_min = 50686, pos_max = 51582, size = 21.034 MiB)
slot update_slots: id  2 | task 4972 | created context checkpoint 8 of 8 (pos_min = 53215, pos_max = 53821, size = 14.234 MiB)
slot print_timing: id  2 | task 4972 | 
prompt eval time =     478.05 ms /    79 tokens (    6.05 ms per token,   165.26 tokens per second)
       eval time =   69741.65 ms /  2121 tokens (   32.88 ms per token,    30.41 tokens per second)
      total time =   70219.69 ms /  2200 tokens
slot      release: id  2 | task 4972 | stop processing: n_tokens = 56006, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.100 (> 0.100 thold), f_keep = 0.008
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 56006, total state size = 1334.316 MiB
srv          load:  - looking for better prompt, base f_keep = 0.008, sim = 0.100
srv        update:  - cache state: 2 prompts, 1670.742 MiB (limits: 8192.000 MiB, 64000 tokens, 286067 est)
srv        update:    - prompt 0x559b56192f30:    2337 tokens, checkpoints:  6,   205.157 MiB
srv        update:    - prompt 0x559b5adabd30:   56006 tokens, checkpoints:  8,  1465.585 MiB
srv  get_availabl: prompt cache update took 1254.12 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 7095 | processing task, is_child = 0
slot update_slots: id  2 | task 7095 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4576
slot update_slots: id  2 | task 7095 | n_past = 459, slot.prompt.tokens.size() = 56006, seq_id = 2, pos_min = 55109, n_swa = 128
slot update_slots: id  2 | task 7095 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 7095 | erased invalidated context checkpoint (pos_min = 51759, pos_max = 52655, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 7095 | erased invalidated context checkpoint (pos_min = 52226, pos_max = 52791, n_swa = 128, size = 13.272 MiB)
slot update_slots: id  2 | task 7095 | erased invalidated context checkpoint (pos_min = 52226, pos_max = 52911, n_swa = 128, size = 16.086 MiB)
slot update_slots: id  2 | task 7095 | erased invalidated context checkpoint (pos_min = 52720, pos_max = 53472, n_swa = 128, size = 17.657 MiB)
slot update_slots: id  2 | task 7095 | erased invalidated context checkpoint (pos_min = 52829, pos_max = 53537, n_swa = 128, size = 16.626 MiB)
slot update_slots: id  2 | task 7095 | erased invalidated context checkpoint (pos_min = 52942, pos_max = 53664, n_swa = 128, size = 16.954 MiB)
slot update_slots: id  2 | task 7095 | erased invalidated context checkpoint (pos_min = 53086, pos_max = 53742, n_swa = 128, size = 15.406 MiB)
slot update_slots: id  2 | task 7095 | erased invalidated context checkpoint (pos_min = 53215, pos_max = 53821, n_swa = 128, size = 14.234 MiB)
slot update_slots: id  2 | task 7095 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 7095 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.447552
slot update_slots: id  2 | task 7095 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 7095 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.895105
slot update_slots: id  2 | task 7095 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 7095 | prompt processing progress, n_tokens = 4512, batch.n_tokens = 416, progress = 0.986014
slot update_slots: id  2 | task 7095 | n_tokens = 4512, memory_seq_rm [4512, end)
slot update_slots: id  2 | task 7095 | prompt processing progress, n_tokens = 4576, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 7095 | prompt done, n_tokens = 4576, batch.n_tokens = 64
slot init_sampler: id  2 | task 7095 | init sampler, took 0.76 ms, tokens: text = 4576, total = 4576
slot update_slots: id  2 | task 7095 | created context checkpoint 1 of 8 (pos_min = 3615, pos_max = 4511, size = 21.034 MiB)
slot print_timing: id  2 | task 7095 | 
prompt eval time =    4514.29 ms /  4576 tokens (    0.99 ms per token,  1013.67 tokens per second)
       eval time =    1063.62 ms /    44 tokens (   24.17 ms per token,    41.37 tokens per second)
      total time =    5577.92 ms /  4620 tokens
slot      release: id  2 | task 7095 | stop processing: n_tokens = 4619, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 7143 | processing task, is_child = 0
slot update_slots: id  2 | task 7143 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4672
slot update_slots: id  2 | task 7143 | n_tokens = 4576, memory_seq_rm [4576, end)
slot update_slots: id  2 | task 7143 | prompt processing progress, n_tokens = 4608, batch.n_tokens = 32, progress = 0.986301
slot update_slots: id  2 | task 7143 | n_tokens = 4608, memory_seq_rm [4608, end)
slot update_slots: id  2 | task 7143 | prompt processing progress, n_tokens = 4672, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 7143 | prompt done, n_tokens = 4672, batch.n_tokens = 64
slot init_sampler: id  2 | task 7143 | init sampler, took 0.85 ms, tokens: text = 4672, total = 4672
slot update_slots: id  2 | task 7143 | created context checkpoint 2 of 8 (pos_min = 3722, pos_max = 4607, size = 20.776 MiB)
slot print_timing: id  2 | task 7143 | 
prompt eval time =     279.01 ms /    96 tokens (    2.91 ms per token,   344.08 tokens per second)
       eval time =    1173.14 ms /    47 tokens (   24.96 ms per token,    40.06 tokens per second)
      total time =    1452.15 ms /   143 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 7143 | stop processing: n_tokens = 4718, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 7192 | processing task, is_child = 0
slot update_slots: id  2 | task 7192 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4756
slot update_slots: id  2 | task 7192 | n_tokens = 4672, memory_seq_rm [4672, end)
slot update_slots: id  2 | task 7192 | prompt processing progress, n_tokens = 4692, batch.n_tokens = 20, progress = 0.986543
slot update_slots: id  2 | task 7192 | n_tokens = 4692, memory_seq_rm [4692, end)
slot update_slots: id  2 | task 7192 | prompt processing progress, n_tokens = 4756, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 7192 | prompt done, n_tokens = 4756, batch.n_tokens = 64
slot init_sampler: id  2 | task 7192 | init sampler, took 1.03 ms, tokens: text = 4756, total = 4756
slot update_slots: id  2 | task 7192 | created context checkpoint 3 of 8 (pos_min = 3821, pos_max = 4691, size = 20.424 MiB)
slot print_timing: id  2 | task 7192 | 
prompt eval time =     256.63 ms /    84 tokens (    3.06 ms per token,   327.31 tokens per second)
       eval time =    2846.23 ms /   116 tokens (   24.54 ms per token,    40.76 tokens per second)
      total time =    3102.87 ms /   200 tokens
slot      release: id  2 | task 7192 | stop processing: n_tokens = 4871, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 7310 | processing task, is_child = 0
slot update_slots: id  2 | task 7310 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5146
slot update_slots: id  2 | task 7310 | n_tokens = 4756, memory_seq_rm [4756, end)
slot update_slots: id  2 | task 7310 | prompt processing progress, n_tokens = 5082, batch.n_tokens = 326, progress = 0.987563
slot update_slots: id  2 | task 7310 | n_tokens = 5082, memory_seq_rm [5082, end)
slot update_slots: id  2 | task 7310 | prompt processing progress, n_tokens = 5146, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 7310 | prompt done, n_tokens = 5146, batch.n_tokens = 64
slot init_sampler: id  2 | task 7310 | init sampler, took 0.76 ms, tokens: text = 5146, total = 5146
slot update_slots: id  2 | task 7310 | created context checkpoint 4 of 8 (pos_min = 4185, pos_max = 5081, size = 21.034 MiB)
slot print_timing: id  2 | task 7310 | 
prompt eval time =     539.64 ms /   390 tokens (    1.38 ms per token,   722.70 tokens per second)
       eval time =    2647.16 ms /   108 tokens (   24.51 ms per token,    40.80 tokens per second)
      total time =    3186.81 ms /   498 tokens
slot      release: id  2 | task 7310 | stop processing: n_tokens = 5253, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 7420 | processing task, is_child = 0
slot update_slots: id  2 | task 7420 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5208
slot update_slots: id  2 | task 7420 | n_tokens = 5146, memory_seq_rm [5146, end)
slot update_slots: id  2 | task 7420 | prompt processing progress, n_tokens = 5208, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  2 | task 7420 | prompt done, n_tokens = 5208, batch.n_tokens = 62
slot init_sampler: id  2 | task 7420 | init sampler, took 1.03 ms, tokens: text = 5208, total = 5208
slot print_timing: id  2 | task 7420 | 
prompt eval time =     249.89 ms /    62 tokens (    4.03 ms per token,   248.11 tokens per second)
       eval time =   16908.96 ms /   658 tokens (   25.70 ms per token,    38.91 tokens per second)
      total time =   17158.85 ms /   720 tokens
slot      release: id  2 | task 7420 | stop processing: n_tokens = 5865, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.888
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 8079 | processing task, is_child = 0
slot update_slots: id  2 | task 8079 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5366
slot update_slots: id  2 | task 8079 | n_tokens = 5208, memory_seq_rm [5208, end)
slot update_slots: id  2 | task 8079 | prompt processing progress, n_tokens = 5302, batch.n_tokens = 94, progress = 0.988073
slot update_slots: id  2 | task 8079 | n_tokens = 5302, memory_seq_rm [5302, end)
slot update_slots: id  2 | task 8079 | prompt processing progress, n_tokens = 5366, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 8079 | prompt done, n_tokens = 5366, batch.n_tokens = 64
slot init_sampler: id  2 | task 8079 | init sampler, took 1.05 ms, tokens: text = 5366, total = 5366
slot update_slots: id  2 | task 8079 | created context checkpoint 5 of 8 (pos_min = 4968, pos_max = 5301, size = 7.832 MiB)
slot print_timing: id  2 | task 8079 | 
prompt eval time =     424.84 ms /   158 tokens (    2.69 ms per token,   371.91 tokens per second)
       eval time =    1671.66 ms /    64 tokens (   26.12 ms per token,    38.29 tokens per second)
      total time =    2096.49 ms /   222 tokens
slot      release: id  2 | task 8079 | stop processing: n_tokens = 5429, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 8145 | processing task, is_child = 0
slot update_slots: id  2 | task 8145 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5533
slot update_slots: id  2 | task 8145 | n_tokens = 5366, memory_seq_rm [5366, end)
slot update_slots: id  2 | task 8145 | prompt processing progress, n_tokens = 5469, batch.n_tokens = 103, progress = 0.988433
slot update_slots: id  2 | task 8145 | n_tokens = 5469, memory_seq_rm [5469, end)
slot update_slots: id  2 | task 8145 | prompt processing progress, n_tokens = 5533, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 8145 | prompt done, n_tokens = 5533, batch.n_tokens = 64
slot init_sampler: id  2 | task 8145 | init sampler, took 1.17 ms, tokens: text = 5533, total = 5533
slot update_slots: id  2 | task 8145 | created context checkpoint 6 of 8 (pos_min = 4968, pos_max = 5468, size = 11.748 MiB)
slot print_timing: id  2 | task 8145 | 
prompt eval time =     445.65 ms /   167 tokens (    2.67 ms per token,   374.73 tokens per second)
       eval time =   21719.76 ms /   832 tokens (   26.11 ms per token,    38.31 tokens per second)
      total time =   22165.41 ms /   999 tokens
slot      release: id  2 | task 8145 | stop processing: n_tokens = 6364, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.935 (> 0.100 thold), f_keep = 0.869
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 8979 | processing task, is_child = 0
slot update_slots: id  2 | task 8979 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 5920
slot update_slots: id  2 | task 8979 | n_past = 5533, slot.prompt.tokens.size() = 6364, seq_id = 2, pos_min = 5467, n_swa = 128
slot update_slots: id  2 | task 8979 | restored context checkpoint (pos_min = 4968, pos_max = 5468, size = 11.748 MiB)
slot update_slots: id  2 | task 8979 | n_tokens = 5468, memory_seq_rm [5468, end)
slot update_slots: id  2 | task 8979 | prompt processing progress, n_tokens = 5856, batch.n_tokens = 388, progress = 0.989189
slot update_slots: id  2 | task 8979 | n_tokens = 5856, memory_seq_rm [5856, end)
slot update_slots: id  2 | task 8979 | prompt processing progress, n_tokens = 5920, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 8979 | prompt done, n_tokens = 5920, batch.n_tokens = 64
slot init_sampler: id  2 | task 8979 | init sampler, took 3.27 ms, tokens: text = 5920, total = 5920
slot update_slots: id  2 | task 8979 | created context checkpoint 7 of 8 (pos_min = 5208, pos_max = 5855, size = 15.195 MiB)
slot print_timing: id  2 | task 8979 | 
prompt eval time =     725.55 ms /   452 tokens (    1.61 ms per token,   622.98 tokens per second)
       eval time =    6949.35 ms /   278 tokens (   25.00 ms per token,    40.00 tokens per second)
      total time =    7674.89 ms /   730 tokens
slot      release: id  2 | task 8979 | stop processing: n_tokens = 6197, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.940 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9259 | processing task, is_child = 0
slot update_slots: id  2 | task 9259 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6301
slot update_slots: id  2 | task 9259 | n_tokens = 5920, memory_seq_rm [5920, end)
slot update_slots: id  2 | task 9259 | prompt processing progress, n_tokens = 6237, batch.n_tokens = 317, progress = 0.989843
slot update_slots: id  2 | task 9259 | n_tokens = 6237, memory_seq_rm [6237, end)
slot update_slots: id  2 | task 9259 | prompt processing progress, n_tokens = 6301, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9259 | prompt done, n_tokens = 6301, batch.n_tokens = 64
slot init_sampler: id  2 | task 9259 | init sampler, took 1.28 ms, tokens: text = 6301, total = 6301
slot update_slots: id  2 | task 9259 | created context checkpoint 8 of 8 (pos_min = 5525, pos_max = 6236, size = 16.696 MiB)
slot print_timing: id  2 | task 9259 | 
prompt eval time =     549.58 ms /   381 tokens (    1.44 ms per token,   693.26 tokens per second)
       eval time =    1428.79 ms /    56 tokens (   25.51 ms per token,    39.19 tokens per second)
      total time =    1978.37 ms /   437 tokens
slot      release: id  2 | task 9259 | stop processing: n_tokens = 6356, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9317 | processing task, is_child = 0
slot update_slots: id  2 | task 9317 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6468
slot update_slots: id  2 | task 9317 | n_tokens = 6301, memory_seq_rm [6301, end)
slot update_slots: id  2 | task 9317 | prompt processing progress, n_tokens = 6404, batch.n_tokens = 103, progress = 0.990105
slot update_slots: id  2 | task 9317 | n_tokens = 6404, memory_seq_rm [6404, end)
slot update_slots: id  2 | task 9317 | prompt processing progress, n_tokens = 6468, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9317 | prompt done, n_tokens = 6468, batch.n_tokens = 64
slot init_sampler: id  2 | task 9317 | init sampler, took 1.45 ms, tokens: text = 6468, total = 6468
slot update_slots: id  2 | task 9317 | erasing old context checkpoint (pos_min = 3615, pos_max = 4511, size = 21.034 MiB)
slot update_slots: id  2 | task 9317 | created context checkpoint 8 of 8 (pos_min = 5692, pos_max = 6403, size = 16.696 MiB)
slot print_timing: id  2 | task 9317 | 
prompt eval time =     439.93 ms /   167 tokens (    2.63 ms per token,   379.61 tokens per second)
       eval time =    7077.25 ms /   276 tokens (   25.64 ms per token,    39.00 tokens per second)
      total time =    7517.18 ms /   443 tokens
slot      release: id  2 | task 9317 | stop processing: n_tokens = 6743, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9595 | processing task, is_child = 0
slot update_slots: id  2 | task 9595 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6849
slot update_slots: id  2 | task 9595 | n_tokens = 6468, memory_seq_rm [6468, end)
slot update_slots: id  2 | task 9595 | prompt processing progress, n_tokens = 6785, batch.n_tokens = 317, progress = 0.990656
slot update_slots: id  2 | task 9595 | n_tokens = 6785, memory_seq_rm [6785, end)
slot update_slots: id  2 | task 9595 | prompt processing progress, n_tokens = 6849, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9595 | prompt done, n_tokens = 6849, batch.n_tokens = 64
slot init_sampler: id  2 | task 9595 | init sampler, took 1.27 ms, tokens: text = 6849, total = 6849
slot update_slots: id  2 | task 9595 | erasing old context checkpoint (pos_min = 3722, pos_max = 4607, size = 20.776 MiB)
slot update_slots: id  2 | task 9595 | created context checkpoint 8 of 8 (pos_min = 5920, pos_max = 6784, size = 20.284 MiB)
slot print_timing: id  2 | task 9595 | 
prompt eval time =     542.54 ms /   381 tokens (    1.42 ms per token,   702.25 tokens per second)
       eval time =    8545.68 ms /   338 tokens (   25.28 ms per token,    39.55 tokens per second)
      total time =    9088.22 ms /   719 tokens
slot      release: id  2 | task 9595 | stop processing: n_tokens = 7186, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9935 | processing task, is_child = 0
slot update_slots: id  2 | task 9935 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7230
slot update_slots: id  2 | task 9935 | n_tokens = 6849, memory_seq_rm [6849, end)
slot update_slots: id  2 | task 9935 | prompt processing progress, n_tokens = 7166, batch.n_tokens = 317, progress = 0.991148
slot update_slots: id  2 | task 9935 | n_tokens = 7166, memory_seq_rm [7166, end)
slot update_slots: id  2 | task 9935 | prompt processing progress, n_tokens = 7230, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9935 | prompt done, n_tokens = 7230, batch.n_tokens = 64
slot init_sampler: id  2 | task 9935 | init sampler, took 1.93 ms, tokens: text = 7230, total = 7230
slot update_slots: id  2 | task 9935 | erasing old context checkpoint (pos_min = 3821, pos_max = 4691, size = 20.424 MiB)
slot update_slots: id  2 | task 9935 | created context checkpoint 8 of 8 (pos_min = 6289, pos_max = 7165, size = 20.565 MiB)
slot print_timing: id  2 | task 9935 | 
prompt eval time =     564.09 ms /   381 tokens (    1.48 ms per token,   675.42 tokens per second)
       eval time =    1217.60 ms /    48 tokens (   25.37 ms per token,    39.42 tokens per second)
      total time =    1781.69 ms /   429 tokens
slot      release: id  2 | task 9935 | stop processing: n_tokens = 7277, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 9985 | processing task, is_child = 0
slot update_slots: id  2 | task 9985 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7397
slot update_slots: id  2 | task 9985 | n_tokens = 7230, memory_seq_rm [7230, end)
slot update_slots: id  2 | task 9985 | prompt processing progress, n_tokens = 7333, batch.n_tokens = 103, progress = 0.991348
slot update_slots: id  2 | task 9985 | n_tokens = 7333, memory_seq_rm [7333, end)
slot update_slots: id  2 | task 9985 | prompt processing progress, n_tokens = 7397, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 9985 | prompt done, n_tokens = 7397, batch.n_tokens = 64
slot init_sampler: id  2 | task 9985 | init sampler, took 1.20 ms, tokens: text = 7397, total = 7397
slot update_slots: id  2 | task 9985 | erasing old context checkpoint (pos_min = 4185, pos_max = 5081, size = 21.034 MiB)
slot update_slots: id  2 | task 9985 | created context checkpoint 8 of 8 (pos_min = 6436, pos_max = 7332, size = 21.034 MiB)
slot print_timing: id  2 | task 9985 | 
prompt eval time =     424.67 ms /   167 tokens (    2.54 ms per token,   393.24 tokens per second)
       eval time =    7604.09 ms /   301 tokens (   25.26 ms per token,    39.58 tokens per second)
      total time =    8028.76 ms /   468 tokens
slot      release: id  2 | task 9985 | stop processing: n_tokens = 7697, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.961
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 10288 | processing task, is_child = 0
slot update_slots: id  2 | task 10288 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7784
slot update_slots: id  2 | task 10288 | n_tokens = 7397, memory_seq_rm [7397, end)
slot update_slots: id  2 | task 10288 | prompt processing progress, n_tokens = 7720, batch.n_tokens = 323, progress = 0.991778
slot update_slots: id  2 | task 10288 | n_tokens = 7720, memory_seq_rm [7720, end)
slot update_slots: id  2 | task 10288 | prompt processing progress, n_tokens = 7784, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 10288 | prompt done, n_tokens = 7784, batch.n_tokens = 64
slot init_sampler: id  2 | task 10288 | init sampler, took 1.54 ms, tokens: text = 7784, total = 7784
slot update_slots: id  2 | task 10288 | erasing old context checkpoint (pos_min = 4968, pos_max = 5301, size = 7.832 MiB)
slot update_slots: id  2 | task 10288 | created context checkpoint 8 of 8 (pos_min = 6823, pos_max = 7719, size = 21.034 MiB)
slot print_timing: id  2 | task 10288 | 
prompt eval time =     586.54 ms /   387 tokens (    1.52 ms per token,   659.80 tokens per second)
       eval time =    7982.63 ms /   310 tokens (   25.75 ms per token,    38.83 tokens per second)
      total time =    8569.17 ms /   697 tokens
slot      release: id  2 | task 10288 | stop processing: n_tokens = 8093, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 10600 | processing task, is_child = 0
slot update_slots: id  2 | task 10600 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8171
slot update_slots: id  2 | task 10600 | n_tokens = 7784, memory_seq_rm [7784, end)
slot update_slots: id  2 | task 10600 | prompt processing progress, n_tokens = 8107, batch.n_tokens = 323, progress = 0.992167
slot update_slots: id  2 | task 10600 | n_tokens = 8107, memory_seq_rm [8107, end)
slot update_slots: id  2 | task 10600 | prompt processing progress, n_tokens = 8171, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 10600 | prompt done, n_tokens = 8171, batch.n_tokens = 64
slot init_sampler: id  2 | task 10600 | init sampler, took 1.91 ms, tokens: text = 8171, total = 8171
slot update_slots: id  2 | task 10600 | erasing old context checkpoint (pos_min = 4968, pos_max = 5468, size = 11.748 MiB)
slot update_slots: id  2 | task 10600 | created context checkpoint 8 of 8 (pos_min = 7210, pos_max = 8106, size = 21.034 MiB)
slot print_timing: id  2 | task 10600 | 
prompt eval time =     570.88 ms /   387 tokens (    1.48 ms per token,   677.90 tokens per second)
       eval time =    1471.74 ms /    56 tokens (   26.28 ms per token,    38.05 tokens per second)
      total time =    2042.62 ms /   443 tokens
slot      release: id  2 | task 10600 | stop processing: n_tokens = 8226, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 10658 | processing task, is_child = 0
slot update_slots: id  2 | task 10658 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8338
slot update_slots: id  2 | task 10658 | n_tokens = 8171, memory_seq_rm [8171, end)
slot update_slots: id  2 | task 10658 | prompt processing progress, n_tokens = 8274, batch.n_tokens = 103, progress = 0.992324
slot update_slots: id  2 | task 10658 | n_tokens = 8274, memory_seq_rm [8274, end)
slot update_slots: id  2 | task 10658 | prompt processing progress, n_tokens = 8338, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 10658 | prompt done, n_tokens = 8338, batch.n_tokens = 64
slot init_sampler: id  2 | task 10658 | init sampler, took 1.69 ms, tokens: text = 8338, total = 8338
slot update_slots: id  2 | task 10658 | erasing old context checkpoint (pos_min = 5208, pos_max = 5855, size = 15.195 MiB)
slot update_slots: id  2 | task 10658 | created context checkpoint 8 of 8 (pos_min = 7377, pos_max = 8273, size = 21.034 MiB)
slot print_timing: id  2 | task 10658 | 
prompt eval time =     441.34 ms /   167 tokens (    2.64 ms per token,   378.40 tokens per second)
       eval time =    2157.36 ms /    80 tokens (   26.97 ms per token,    37.08 tokens per second)
      total time =    2598.70 ms /   247 tokens
slot      release: id  2 | task 10658 | stop processing: n_tokens = 8417, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 10740 | processing task, is_child = 0
slot update_slots: id  2 | task 10740 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8505
slot update_slots: id  2 | task 10740 | n_tokens = 8338, memory_seq_rm [8338, end)
slot update_slots: id  2 | task 10740 | prompt processing progress, n_tokens = 8441, batch.n_tokens = 103, progress = 0.992475
slot update_slots: id  2 | task 10740 | n_tokens = 8441, memory_seq_rm [8441, end)
slot update_slots: id  2 | task 10740 | prompt processing progress, n_tokens = 8505, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 10740 | prompt done, n_tokens = 8505, batch.n_tokens = 64
slot init_sampler: id  2 | task 10740 | init sampler, took 1.97 ms, tokens: text = 8505, total = 8505
slot update_slots: id  2 | task 10740 | erasing old context checkpoint (pos_min = 5525, pos_max = 6236, size = 16.696 MiB)
slot update_slots: id  2 | task 10740 | created context checkpoint 8 of 8 (pos_min = 7544, pos_max = 8440, size = 21.034 MiB)
slot print_timing: id  2 | task 10740 | 
prompt eval time =     446.42 ms /   167 tokens (    2.67 ms per token,   374.09 tokens per second)
       eval time =    8188.84 ms /   310 tokens (   26.42 ms per token,    37.86 tokens per second)
      total time =    8635.25 ms /   477 tokens
slot      release: id  2 | task 10740 | stop processing: n_tokens = 8814, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.956 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11052 | processing task, is_child = 0
slot update_slots: id  2 | task 11052 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8892
slot update_slots: id  2 | task 11052 | n_tokens = 8505, memory_seq_rm [8505, end)
slot update_slots: id  2 | task 11052 | prompt processing progress, n_tokens = 8828, batch.n_tokens = 323, progress = 0.992803
slot update_slots: id  2 | task 11052 | n_tokens = 8828, memory_seq_rm [8828, end)
slot update_slots: id  2 | task 11052 | prompt processing progress, n_tokens = 8892, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11052 | prompt done, n_tokens = 8892, batch.n_tokens = 64
slot init_sampler: id  2 | task 11052 | init sampler, took 1.66 ms, tokens: text = 8892, total = 8892
slot update_slots: id  2 | task 11052 | erasing old context checkpoint (pos_min = 5692, pos_max = 6403, size = 16.696 MiB)
slot update_slots: id  2 | task 11052 | created context checkpoint 8 of 8 (pos_min = 8037, pos_max = 8827, size = 18.548 MiB)
slot print_timing: id  2 | task 11052 | 
prompt eval time =     569.70 ms /   387 tokens (    1.47 ms per token,   679.31 tokens per second)
       eval time =    4289.85 ms /   163 tokens (   26.32 ms per token,    38.00 tokens per second)
      total time =    4859.55 ms /   550 tokens
slot      release: id  2 | task 11052 | stop processing: n_tokens = 9054, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11217 | processing task, is_child = 0
slot update_slots: id  2 | task 11217 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9050
slot update_slots: id  2 | task 11217 | n_tokens = 8892, memory_seq_rm [8892, end)
slot update_slots: id  2 | task 11217 | prompt processing progress, n_tokens = 8986, batch.n_tokens = 94, progress = 0.992928
slot update_slots: id  2 | task 11217 | n_tokens = 8986, memory_seq_rm [8986, end)
slot update_slots: id  2 | task 11217 | prompt processing progress, n_tokens = 9050, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11217 | prompt done, n_tokens = 9050, batch.n_tokens = 64
slot init_sampler: id  2 | task 11217 | init sampler, took 3.94 ms, tokens: text = 9050, total = 9050
slot update_slots: id  2 | task 11217 | erasing old context checkpoint (pos_min = 5920, pos_max = 6784, size = 20.284 MiB)
slot update_slots: id  2 | task 11217 | created context checkpoint 8 of 8 (pos_min = 8263, pos_max = 8985, size = 16.954 MiB)
slot print_timing: id  2 | task 11217 | 
prompt eval time =     429.15 ms /   158 tokens (    2.72 ms per token,   368.17 tokens per second)
       eval time =    4321.43 ms /   163 tokens (   26.51 ms per token,    37.72 tokens per second)
      total time =    4750.58 ms /   321 tokens
slot      release: id  2 | task 11217 | stop processing: n_tokens = 9212, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11382 | processing task, is_child = 0
slot update_slots: id  2 | task 11382 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9208
slot update_slots: id  2 | task 11382 | n_tokens = 9050, memory_seq_rm [9050, end)
slot update_slots: id  2 | task 11382 | prompt processing progress, n_tokens = 9144, batch.n_tokens = 94, progress = 0.993050
slot update_slots: id  2 | task 11382 | n_tokens = 9144, memory_seq_rm [9144, end)
slot update_slots: id  2 | task 11382 | prompt processing progress, n_tokens = 9208, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11382 | prompt done, n_tokens = 9208, batch.n_tokens = 64
slot init_sampler: id  2 | task 11382 | init sampler, took 1.78 ms, tokens: text = 9208, total = 9208
slot update_slots: id  2 | task 11382 | erasing old context checkpoint (pos_min = 6289, pos_max = 7165, size = 20.565 MiB)
slot update_slots: id  2 | task 11382 | created context checkpoint 8 of 8 (pos_min = 8421, pos_max = 9143, size = 16.954 MiB)
slot print_timing: id  2 | task 11382 | 
prompt eval time =     425.06 ms /   158 tokens (    2.69 ms per token,   371.71 tokens per second)
       eval time =    1548.35 ms /    60 tokens (   25.81 ms per token,    38.75 tokens per second)
      total time =    1973.42 ms /   218 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 11382 | stop processing: n_tokens = 9267, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11444 | processing task, is_child = 0
slot update_slots: id  2 | task 11444 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 9375
slot update_slots: id  2 | task 11444 | n_tokens = 9208, memory_seq_rm [9208, end)
slot update_slots: id  2 | task 11444 | prompt processing progress, n_tokens = 9311, batch.n_tokens = 103, progress = 0.993173
slot update_slots: id  2 | task 11444 | n_tokens = 9311, memory_seq_rm [9311, end)
slot update_slots: id  2 | task 11444 | prompt processing progress, n_tokens = 9375, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11444 | prompt done, n_tokens = 9375, batch.n_tokens = 64
slot init_sampler: id  2 | task 11444 | init sampler, took 1.51 ms, tokens: text = 9375, total = 9375
slot update_slots: id  2 | task 11444 | erasing old context checkpoint (pos_min = 6436, pos_max = 7332, size = 21.034 MiB)
slot update_slots: id  2 | task 11444 | created context checkpoint 8 of 8 (pos_min = 8505, pos_max = 9310, size = 18.900 MiB)
slot print_timing: id  2 | task 11444 | 
prompt eval time =     431.22 ms /   167 tokens (    2.58 ms per token,   387.27 tokens per second)
       eval time =   64002.37 ms /  2470 tokens (   25.91 ms per token,    38.59 tokens per second)
      total time =   64433.59 ms /  2637 tokens
slot      release: id  2 | task 11444 | stop processing: n_tokens = 11844, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.811 (> 0.100 thold), f_keep = 0.792
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13916 | processing task, is_child = 0
slot update_slots: id  2 | task 13916 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 11564
slot update_slots: id  2 | task 13916 | n_past = 9375, slot.prompt.tokens.size() = 11844, seq_id = 2, pos_min = 10947, n_swa = 128
slot update_slots: id  2 | task 13916 | restored context checkpoint (pos_min = 8505, pos_max = 9310, size = 18.900 MiB)
slot update_slots: id  2 | task 13916 | n_tokens = 9310, memory_seq_rm [9310, end)
slot update_slots: id  2 | task 13916 | prompt processing progress, n_tokens = 11358, batch.n_tokens = 2048, progress = 0.982186
slot update_slots: id  2 | task 13916 | n_tokens = 11358, memory_seq_rm [11358, end)
slot update_slots: id  2 | task 13916 | prompt processing progress, n_tokens = 11500, batch.n_tokens = 142, progress = 0.994466
slot update_slots: id  2 | task 13916 | n_tokens = 11500, memory_seq_rm [11500, end)
slot update_slots: id  2 | task 13916 | prompt processing progress, n_tokens = 11564, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13916 | prompt done, n_tokens = 11564, batch.n_tokens = 64
slot init_sampler: id  2 | task 13916 | init sampler, took 3.15 ms, tokens: text = 11564, total = 11564
slot update_slots: id  2 | task 13916 | erasing old context checkpoint (pos_min = 6823, pos_max = 7719, size = 21.034 MiB)
slot update_slots: id  2 | task 13916 | created context checkpoint 8 of 8 (pos_min = 10603, pos_max = 11499, size = 21.034 MiB)
slot print_timing: id  2 | task 13916 | 
prompt eval time =    2938.88 ms /  2254 tokens (    1.30 ms per token,   766.96 tokens per second)
       eval time =   56888.21 ms /  2202 tokens (   25.83 ms per token,    38.71 tokens per second)
      total time =   59827.10 ms /  4456 tokens
slot      release: id  2 | task 13916 | stop processing: n_tokens = 13765, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.841 (> 0.100 thold), f_keep = 0.840
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16121 | processing task, is_child = 0
slot update_slots: id  2 | task 16121 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 13753
slot update_slots: id  2 | task 16121 | n_past = 11564, slot.prompt.tokens.size() = 13765, seq_id = 2, pos_min = 12868, n_swa = 128
slot update_slots: id  2 | task 16121 | restored context checkpoint (pos_min = 10603, pos_max = 11499, size = 21.034 MiB)
slot update_slots: id  2 | task 16121 | n_tokens = 11499, memory_seq_rm [11499, end)
slot update_slots: id  2 | task 16121 | prompt processing progress, n_tokens = 13547, batch.n_tokens = 2048, progress = 0.985021
slot update_slots: id  2 | task 16121 | n_tokens = 13547, memory_seq_rm [13547, end)
slot update_slots: id  2 | task 16121 | prompt processing progress, n_tokens = 13689, batch.n_tokens = 142, progress = 0.995346
slot update_slots: id  2 | task 16121 | n_tokens = 13689, memory_seq_rm [13689, end)
slot update_slots: id  2 | task 16121 | prompt processing progress, n_tokens = 13753, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16121 | prompt done, n_tokens = 13753, batch.n_tokens = 64
slot init_sampler: id  2 | task 16121 | init sampler, took 2.50 ms, tokens: text = 13753, total = 13753
slot update_slots: id  2 | task 16121 | erasing old context checkpoint (pos_min = 7210, pos_max = 8106, size = 21.034 MiB)
slot update_slots: id  2 | task 16121 | created context checkpoint 8 of 8 (pos_min = 12792, pos_max = 13688, size = 21.034 MiB)
slot print_timing: id  2 | task 16121 | 
prompt eval time =    2948.18 ms /  2254 tokens (    1.31 ms per token,   764.54 tokens per second)
       eval time =   60113.57 ms /  2306 tokens (   26.07 ms per token,    38.36 tokens per second)
      total time =   63061.74 ms /  4560 tokens
slot      release: id  2 | task 16121 | stop processing: n_tokens = 16058, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.862 (> 0.100 thold), f_keep = 0.856
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18430 | processing task, is_child = 0
slot update_slots: id  2 | task 18430 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 15950
slot update_slots: id  2 | task 18430 | n_past = 13753, slot.prompt.tokens.size() = 16058, seq_id = 2, pos_min = 15161, n_swa = 128
slot update_slots: id  2 | task 18430 | restored context checkpoint (pos_min = 12792, pos_max = 13688, size = 21.034 MiB)
slot update_slots: id  2 | task 18430 | n_tokens = 13688, memory_seq_rm [13688, end)
slot update_slots: id  2 | task 18430 | prompt processing progress, n_tokens = 15736, batch.n_tokens = 2048, progress = 0.986583
slot update_slots: id  2 | task 18430 | n_tokens = 15736, memory_seq_rm [15736, end)
slot update_slots: id  2 | task 18430 | prompt processing progress, n_tokens = 15886, batch.n_tokens = 150, progress = 0.995987
slot update_slots: id  2 | task 18430 | n_tokens = 15886, memory_seq_rm [15886, end)
slot update_slots: id  2 | task 18430 | prompt processing progress, n_tokens = 15950, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 18430 | prompt done, n_tokens = 15950, batch.n_tokens = 64
slot init_sampler: id  2 | task 18430 | init sampler, took 2.97 ms, tokens: text = 15950, total = 15950
slot update_slots: id  2 | task 18430 | erasing old context checkpoint (pos_min = 7377, pos_max = 8273, size = 21.034 MiB)
slot update_slots: id  2 | task 18430 | created context checkpoint 8 of 8 (pos_min = 14989, pos_max = 15885, size = 21.034 MiB)
slot print_timing: id  2 | task 18430 | 
prompt eval time =    3061.21 ms /  2262 tokens (    1.35 ms per token,   738.92 tokens per second)
       eval time =   57780.79 ms /  2200 tokens (   26.26 ms per token,    38.07 tokens per second)
      total time =   60842.00 ms /  4462 tokens
slot      release: id  2 | task 18430 | stop processing: n_tokens = 18149, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.879 (> 0.100 thold), f_keep = 0.879
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20633 | processing task, is_child = 0
slot update_slots: id  2 | task 20633 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18144
slot update_slots: id  2 | task 20633 | n_past = 15950, slot.prompt.tokens.size() = 18149, seq_id = 2, pos_min = 17252, n_swa = 128
slot update_slots: id  2 | task 20633 | restored context checkpoint (pos_min = 14989, pos_max = 15885, size = 21.034 MiB)
slot update_slots: id  2 | task 20633 | n_tokens = 15885, memory_seq_rm [15885, end)
slot update_slots: id  2 | task 20633 | prompt processing progress, n_tokens = 17933, batch.n_tokens = 2048, progress = 0.988371
slot update_slots: id  2 | task 20633 | n_tokens = 17933, memory_seq_rm [17933, end)
slot update_slots: id  2 | task 20633 | prompt processing progress, n_tokens = 18080, batch.n_tokens = 147, progress = 0.996473
slot update_slots: id  2 | task 20633 | n_tokens = 18080, memory_seq_rm [18080, end)
slot update_slots: id  2 | task 20633 | prompt processing progress, n_tokens = 18144, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 20633 | prompt done, n_tokens = 18144, batch.n_tokens = 64
slot init_sampler: id  2 | task 20633 | init sampler, took 2.65 ms, tokens: text = 18144, total = 18144
slot update_slots: id  2 | task 20633 | erasing old context checkpoint (pos_min = 7544, pos_max = 8440, size = 21.034 MiB)
slot update_slots: id  2 | task 20633 | created context checkpoint 8 of 8 (pos_min = 17183, pos_max = 18079, size = 21.034 MiB)
slot print_timing: id  2 | task 20633 | 
prompt eval time =    3097.17 ms /  2259 tokens (    1.37 ms per token,   729.38 tokens per second)
       eval time =   59359.10 ms /  2237 tokens (   26.54 ms per token,    37.69 tokens per second)
      total time =   62456.26 ms /  4496 tokens
slot      release: id  2 | task 20633 | stop processing: n_tokens = 20380, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.892 (> 0.100 thold), f_keep = 0.890
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 22873 | processing task, is_child = 0
slot update_slots: id  2 | task 22873 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20333
slot update_slots: id  2 | task 22873 | n_past = 18144, slot.prompt.tokens.size() = 20380, seq_id = 2, pos_min = 19483, n_swa = 128
slot update_slots: id  2 | task 22873 | restored context checkpoint (pos_min = 17183, pos_max = 18079, size = 21.034 MiB)
slot update_slots: id  2 | task 22873 | n_tokens = 18079, memory_seq_rm [18079, end)
slot update_slots: id  2 | task 22873 | prompt processing progress, n_tokens = 20127, batch.n_tokens = 2048, progress = 0.989869
slot update_slots: id  2 | task 22873 | n_tokens = 20127, memory_seq_rm [20127, end)
slot update_slots: id  2 | task 22873 | prompt processing progress, n_tokens = 20269, batch.n_tokens = 142, progress = 0.996852
slot update_slots: id  2 | task 22873 | n_tokens = 20269, memory_seq_rm [20269, end)
slot update_slots: id  2 | task 22873 | prompt processing progress, n_tokens = 20333, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 22873 | prompt done, n_tokens = 20333, batch.n_tokens = 64
slot init_sampler: id  2 | task 22873 | init sampler, took 3.97 ms, tokens: text = 20333, total = 20333
slot update_slots: id  2 | task 22873 | erasing old context checkpoint (pos_min = 8037, pos_max = 8827, size = 18.548 MiB)
slot update_slots: id  2 | task 22873 | created context checkpoint 8 of 8 (pos_min = 19372, pos_max = 20268, size = 21.034 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 22873
slot      release: id  2 | task 22873 | stop processing: n_tokens = 20641, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.241 (> 0.100 thold), f_keep = 0.213
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 20641, total state size = 505.044 MiB
srv          load:  - looking for better prompt, base f_keep = 0.213, sim = 0.241
srv        update:  - cache state: 3 prompts, 2333.764 MiB (limits: 8192.000 MiB, 64000 tokens, 277250 est)
srv        update:    - prompt 0x559b56192f30:    2337 tokens, checkpoints:  6,   205.157 MiB
srv        update:    - prompt 0x559b5adabd30:   56006 tokens, checkpoints:  8,  1465.585 MiB
srv        update:    - prompt 0x559b5afe3540:   20641 tokens, checkpoints:  8,   663.022 MiB
srv  get_availabl: prompt cache update took 510.85 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 23186 | processing task, is_child = 0
slot update_slots: id  2 | task 23186 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 18217
slot update_slots: id  2 | task 23186 | n_past = 4399, slot.prompt.tokens.size() = 20641, seq_id = 2, pos_min = 19744, n_swa = 128
slot update_slots: id  2 | task 23186 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 23186 | erased invalidated context checkpoint (pos_min = 8263, pos_max = 8985, n_swa = 128, size = 16.954 MiB)
slot update_slots: id  2 | task 23186 | erased invalidated context checkpoint (pos_min = 8421, pos_max = 9143, n_swa = 128, size = 16.954 MiB)
slot update_slots: id  2 | task 23186 | erased invalidated context checkpoint (pos_min = 8505, pos_max = 9310, n_swa = 128, size = 18.900 MiB)
slot update_slots: id  2 | task 23186 | erased invalidated context checkpoint (pos_min = 10603, pos_max = 11499, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 23186 | erased invalidated context checkpoint (pos_min = 12792, pos_max = 13688, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 23186 | erased invalidated context checkpoint (pos_min = 14989, pos_max = 15885, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 23186 | erased invalidated context checkpoint (pos_min = 17183, pos_max = 18079, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 23186 | erased invalidated context checkpoint (pos_min = 19372, pos_max = 20268, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 23186 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 23186 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.112422
slot update_slots: id  2 | task 23186 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 23186 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.224845
slot update_slots: id  2 | task 23186 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 23186 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.337267
slot update_slots: id  2 | task 23186 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 23186 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.449690
slot update_slots: id  2 | task 23186 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 23186 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.562112
slot update_slots: id  2 | task 23186 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 23186 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.674535
slot update_slots: id  2 | task 23186 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  2 | task 23186 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.786957
slot update_slots: id  2 | task 23186 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  2 | task 23186 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.899380
slot update_slots: id  2 | task 23186 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  2 | task 23186 | prompt processing progress, n_tokens = 18153, batch.n_tokens = 1769, progress = 0.996487
slot update_slots: id  2 | task 23186 | n_tokens = 18153, memory_seq_rm [18153, end)
slot update_slots: id  2 | task 23186 | prompt processing progress, n_tokens = 18217, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 23186 | prompt done, n_tokens = 18217, batch.n_tokens = 64
slot init_sampler: id  2 | task 23186 | init sampler, took 3.37 ms, tokens: text = 18217, total = 18217
slot update_slots: id  2 | task 23186 | created context checkpoint 1 of 8 (pos_min = 17256, pos_max = 18152, size = 21.034 MiB)
slot print_timing: id  2 | task 23186 | 
prompt eval time =   19827.36 ms / 18217 tokens (    1.09 ms per token,   918.78 tokens per second)
       eval time =   39671.01 ms /  1476 tokens (   26.88 ms per token,    37.21 tokens per second)
      total time =   59498.37 ms / 19693 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 23186 | stop processing: n_tokens = 19692, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.925 (> 0.100 thold), f_keep = 0.925
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 24672 | processing task, is_child = 0
slot update_slots: id  2 | task 24672 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19702
slot update_slots: id  2 | task 24672 | n_past = 18217, slot.prompt.tokens.size() = 19692, seq_id = 2, pos_min = 18795, n_swa = 128
slot update_slots: id  2 | task 24672 | restored context checkpoint (pos_min = 17256, pos_max = 18152, size = 21.034 MiB)
slot update_slots: id  2 | task 24672 | n_tokens = 18152, memory_seq_rm [18152, end)
slot update_slots: id  2 | task 24672 | prompt processing progress, n_tokens = 19638, batch.n_tokens = 1486, progress = 0.996752
slot update_slots: id  2 | task 24672 | n_tokens = 19638, memory_seq_rm [19638, end)
slot update_slots: id  2 | task 24672 | prompt processing progress, n_tokens = 19702, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 24672 | prompt done, n_tokens = 19702, batch.n_tokens = 64
slot init_sampler: id  2 | task 24672 | init sampler, took 2.84 ms, tokens: text = 19702, total = 19702
slot update_slots: id  2 | task 24672 | created context checkpoint 2 of 8 (pos_min = 18741, pos_max = 19637, size = 21.034 MiB)
slot print_timing: id  2 | task 24672 | 
prompt eval time =    2272.83 ms /  1550 tokens (    1.47 ms per token,   681.97 tokens per second)
       eval time =    1283.40 ms /    50 tokens (   25.67 ms per token,    38.96 tokens per second)
      total time =    3556.23 ms /  1600 tokens
slot      release: id  2 | task 24672 | stop processing: n_tokens = 19751, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.919 (> 0.100 thold), f_keep = 0.921
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 24724 | processing task, is_child = 0
slot update_slots: id  2 | task 24724 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19794
slot update_slots: id  2 | task 24724 | n_past = 18192, slot.prompt.tokens.size() = 19751, seq_id = 2, pos_min = 18854, n_swa = 128
slot update_slots: id  2 | task 24724 | restored context checkpoint (pos_min = 17256, pos_max = 18152, size = 21.034 MiB)
slot update_slots: id  2 | task 24724 | erased invalidated context checkpoint (pos_min = 18741, pos_max = 19637, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 24724 | n_tokens = 18152, memory_seq_rm [18152, end)
slot update_slots: id  2 | task 24724 | prompt processing progress, n_tokens = 19730, batch.n_tokens = 1578, progress = 0.996767
slot update_slots: id  2 | task 24724 | n_tokens = 19730, memory_seq_rm [19730, end)
slot update_slots: id  2 | task 24724 | prompt processing progress, n_tokens = 19794, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 24724 | prompt done, n_tokens = 19794, batch.n_tokens = 64
slot init_sampler: id  2 | task 24724 | init sampler, took 3.80 ms, tokens: text = 19794, total = 19794
slot update_slots: id  2 | task 24724 | created context checkpoint 2 of 8 (pos_min = 18833, pos_max = 19729, size = 21.034 MiB)
slot print_timing: id  2 | task 24724 | 
prompt eval time =    2467.13 ms /  1642 tokens (    1.50 ms per token,   665.55 tokens per second)
       eval time =    1205.12 ms /    47 tokens (   25.64 ms per token,    39.00 tokens per second)
      total time =    3672.25 ms /  1689 tokens
slot      release: id  2 | task 24724 | stop processing: n_tokens = 19840, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 24773 | processing task, is_child = 0
slot update_slots: id  2 | task 24773 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 19881
slot update_slots: id  2 | task 24773 | n_tokens = 19794, memory_seq_rm [19794, end)
slot update_slots: id  2 | task 24773 | prompt processing progress, n_tokens = 19817, batch.n_tokens = 23, progress = 0.996781
slot update_slots: id  2 | task 24773 | n_tokens = 19817, memory_seq_rm [19817, end)
slot update_slots: id  2 | task 24773 | prompt processing progress, n_tokens = 19881, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 24773 | prompt done, n_tokens = 19881, batch.n_tokens = 64
slot init_sampler: id  2 | task 24773 | init sampler, took 2.95 ms, tokens: text = 19881, total = 19881
slot update_slots: id  2 | task 24773 | created context checkpoint 3 of 8 (pos_min = 18943, pos_max = 19816, size = 20.495 MiB)
slot print_timing: id  2 | task 24773 | 
prompt eval time =     281.05 ms /    87 tokens (    3.23 ms per token,   309.55 tokens per second)
       eval time =    1285.01 ms /    50 tokens (   25.70 ms per token,    38.91 tokens per second)
      total time =    1566.06 ms /   137 tokens
slot      release: id  2 | task 24773 | stop processing: n_tokens = 19930, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 24825 | processing task, is_child = 0
slot update_slots: id  2 | task 24825 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 20759
slot update_slots: id  2 | task 24825 | n_tokens = 19881, memory_seq_rm [19881, end)
slot update_slots: id  2 | task 24825 | prompt processing progress, n_tokens = 20695, batch.n_tokens = 814, progress = 0.996917
slot update_slots: id  2 | task 24825 | n_tokens = 20695, memory_seq_rm [20695, end)
slot update_slots: id  2 | task 24825 | prompt processing progress, n_tokens = 20759, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 24825 | prompt done, n_tokens = 20759, batch.n_tokens = 64
slot init_sampler: id  2 | task 24825 | init sampler, took 3.22 ms, tokens: text = 20759, total = 20759
slot update_slots: id  2 | task 24825 | created context checkpoint 4 of 8 (pos_min = 19798, pos_max = 20694, size = 21.034 MiB)
slot print_timing: id  2 | task 24825 | 
prompt eval time =    1316.98 ms /   878 tokens (    1.50 ms per token,   666.68 tokens per second)
       eval time =    2229.69 ms /    86 tokens (   25.93 ms per token,    38.57 tokens per second)
      total time =    3546.67 ms /   964 tokens
slot      release: id  2 | task 24825 | stop processing: n_tokens = 20844, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 24913 | processing task, is_child = 0
slot update_slots: id  2 | task 24913 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 22255
slot update_slots: id  2 | task 24913 | n_tokens = 20759, memory_seq_rm [20759, end)
slot update_slots: id  2 | task 24913 | prompt processing progress, n_tokens = 22191, batch.n_tokens = 1432, progress = 0.997124
slot update_slots: id  2 | task 24913 | n_tokens = 22191, memory_seq_rm [22191, end)
slot update_slots: id  2 | task 24913 | prompt processing progress, n_tokens = 22255, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 24913 | prompt done, n_tokens = 22255, batch.n_tokens = 64
slot init_sampler: id  2 | task 24913 | init sampler, took 3.17 ms, tokens: text = 22255, total = 22255
slot update_slots: id  2 | task 24913 | created context checkpoint 5 of 8 (pos_min = 21294, pos_max = 22190, size = 21.034 MiB)
slot print_timing: id  2 | task 24913 | 
prompt eval time =    2027.26 ms /  1496 tokens (    1.36 ms per token,   737.94 tokens per second)
       eval time =    2980.76 ms /   111 tokens (   26.85 ms per token,    37.24 tokens per second)
      total time =    5008.02 ms /  1607 tokens
slot      release: id  2 | task 24913 | stop processing: n_tokens = 22365, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.945 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 25026 | processing task, is_child = 0
slot update_slots: id  2 | task 25026 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 23554
slot update_slots: id  2 | task 25026 | n_tokens = 22255, memory_seq_rm [22255, end)
slot update_slots: id  2 | task 25026 | prompt processing progress, n_tokens = 23490, batch.n_tokens = 1235, progress = 0.997283
slot update_slots: id  2 | task 25026 | n_tokens = 23490, memory_seq_rm [23490, end)
slot update_slots: id  2 | task 25026 | prompt processing progress, n_tokens = 23554, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 25026 | prompt done, n_tokens = 23554, batch.n_tokens = 64
slot init_sampler: id  2 | task 25026 | init sampler, took 3.54 ms, tokens: text = 23554, total = 23554
slot update_slots: id  2 | task 25026 | created context checkpoint 6 of 8 (pos_min = 22593, pos_max = 23489, size = 21.034 MiB)
slot print_timing: id  2 | task 25026 | 
prompt eval time =    1986.02 ms /  1299 tokens (    1.53 ms per token,   654.07 tokens per second)
       eval time =   59129.76 ms /  2163 tokens (   27.34 ms per token,    36.58 tokens per second)
      total time =   61115.78 ms /  3462 tokens
slot      release: id  2 | task 25026 | stop processing: n_tokens = 25716, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.927 (> 0.100 thold), f_keep = 0.916
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 27191 | processing task, is_child = 0
slot update_slots: id  2 | task 27191 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 25420
slot update_slots: id  2 | task 27191 | n_past = 23554, slot.prompt.tokens.size() = 25716, seq_id = 2, pos_min = 24819, n_swa = 128
slot update_slots: id  2 | task 27191 | restored context checkpoint (pos_min = 22593, pos_max = 23489, size = 21.034 MiB)
slot update_slots: id  2 | task 27191 | n_tokens = 23489, memory_seq_rm [23489, end)
slot update_slots: id  2 | task 27191 | prompt processing progress, n_tokens = 25356, batch.n_tokens = 1867, progress = 0.997482
slot update_slots: id  2 | task 27191 | n_tokens = 25356, memory_seq_rm [25356, end)
slot update_slots: id  2 | task 27191 | prompt processing progress, n_tokens = 25420, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 27191 | prompt done, n_tokens = 25420, batch.n_tokens = 64
slot init_sampler: id  2 | task 27191 | init sampler, took 3.92 ms, tokens: text = 25420, total = 25420
slot update_slots: id  2 | task 27191 | created context checkpoint 7 of 8 (pos_min = 24459, pos_max = 25355, size = 21.034 MiB)
slot print_timing: id  2 | task 27191 | 
prompt eval time =    2941.93 ms /  1931 tokens (    1.52 ms per token,   656.37 tokens per second)
       eval time =    1601.43 ms /    59 tokens (   27.14 ms per token,    36.84 tokens per second)
      total time =    4543.36 ms /  1990 tokens
slot      release: id  2 | task 27191 | stop processing: n_tokens = 25478, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 27252 | processing task, is_child = 0
slot update_slots: id  2 | task 27252 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 26916
slot update_slots: id  2 | task 27252 | n_tokens = 25420, memory_seq_rm [25420, end)
slot update_slots: id  2 | task 27252 | prompt processing progress, n_tokens = 26852, batch.n_tokens = 1432, progress = 0.997622
slot update_slots: id  2 | task 27252 | n_tokens = 26852, memory_seq_rm [26852, end)
slot update_slots: id  2 | task 27252 | prompt processing progress, n_tokens = 26916, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 27252 | prompt done, n_tokens = 26916, batch.n_tokens = 64
slot init_sampler: id  2 | task 27252 | init sampler, took 3.87 ms, tokens: text = 26916, total = 26916
slot update_slots: id  2 | task 27252 | created context checkpoint 8 of 8 (pos_min = 25955, pos_max = 26851, size = 21.034 MiB)
slot print_timing: id  2 | task 27252 | 
prompt eval time =    2177.76 ms /  1496 tokens (    1.46 ms per token,   686.94 tokens per second)
       eval time =   57335.14 ms /  2082 tokens (   27.54 ms per token,    36.31 tokens per second)
      total time =   59512.91 ms /  3578 tokens
slot      release: id  2 | task 27252 | stop processing: n_tokens = 28997, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.928 (> 0.100 thold), f_keep = 0.928
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 29336 | processing task, is_child = 0
slot update_slots: id  2 | task 29336 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 28999
slot update_slots: id  2 | task 29336 | n_past = 26916, slot.prompt.tokens.size() = 28997, seq_id = 2, pos_min = 28100, n_swa = 128
slot update_slots: id  2 | task 29336 | restored context checkpoint (pos_min = 25955, pos_max = 26851, size = 21.034 MiB)
slot update_slots: id  2 | task 29336 | n_tokens = 26851, memory_seq_rm [26851, end)
slot update_slots: id  2 | task 29336 | prompt processing progress, n_tokens = 28899, batch.n_tokens = 2048, progress = 0.996552
slot update_slots: id  2 | task 29336 | n_tokens = 28899, memory_seq_rm [28899, end)
slot update_slots: id  2 | task 29336 | prompt processing progress, n_tokens = 28935, batch.n_tokens = 36, progress = 0.997793
slot update_slots: id  2 | task 29336 | n_tokens = 28935, memory_seq_rm [28935, end)
slot update_slots: id  2 | task 29336 | prompt processing progress, n_tokens = 28999, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 29336 | prompt done, n_tokens = 28999, batch.n_tokens = 64
slot init_sampler: id  2 | task 29336 | init sampler, took 4.78 ms, tokens: text = 28999, total = 28999
slot update_slots: id  2 | task 29336 | erasing old context checkpoint (pos_min = 17256, pos_max = 18152, size = 21.034 MiB)
slot update_slots: id  2 | task 29336 | created context checkpoint 8 of 8 (pos_min = 28038, pos_max = 28934, size = 21.034 MiB)
slot print_timing: id  2 | task 29336 | 
prompt eval time =    3440.18 ms /  2148 tokens (    1.60 ms per token,   624.39 tokens per second)
       eval time =   20347.98 ms /   738 tokens (   27.57 ms per token,    36.27 tokens per second)
      total time =   23788.16 ms /  2886 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 29336 | stop processing: n_tokens = 29736, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.811 (> 0.100 thold), f_keep = 0.663
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 30077 | processing task, is_child = 0
slot update_slots: id  2 | task 30077 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24321
slot update_slots: id  2 | task 30077 | n_past = 19718, slot.prompt.tokens.size() = 29736, seq_id = 2, pos_min = 28839, n_swa = 128
slot update_slots: id  2 | task 30077 | restored context checkpoint (pos_min = 18943, pos_max = 19816, size = 20.495 MiB)
slot update_slots: id  2 | task 30077 | erased invalidated context checkpoint (pos_min = 19798, pos_max = 20694, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 30077 | erased invalidated context checkpoint (pos_min = 21294, pos_max = 22190, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 30077 | erased invalidated context checkpoint (pos_min = 22593, pos_max = 23489, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 30077 | erased invalidated context checkpoint (pos_min = 24459, pos_max = 25355, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 30077 | erased invalidated context checkpoint (pos_min = 25955, pos_max = 26851, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 30077 | erased invalidated context checkpoint (pos_min = 28038, pos_max = 28934, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 30077 | n_tokens = 19718, memory_seq_rm [19718, end)
slot update_slots: id  2 | task 30077 | prompt processing progress, n_tokens = 21766, batch.n_tokens = 2048, progress = 0.894947
slot update_slots: id  2 | task 30077 | n_tokens = 21766, memory_seq_rm [21766, end)
slot update_slots: id  2 | task 30077 | prompt processing progress, n_tokens = 23814, batch.n_tokens = 2048, progress = 0.979154
slot update_slots: id  2 | task 30077 | n_tokens = 23814, memory_seq_rm [23814, end)
slot update_slots: id  2 | task 30077 | prompt processing progress, n_tokens = 24257, batch.n_tokens = 443, progress = 0.997369
slot update_slots: id  2 | task 30077 | n_tokens = 24257, memory_seq_rm [24257, end)
slot update_slots: id  2 | task 30077 | prompt processing progress, n_tokens = 24321, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 30077 | prompt done, n_tokens = 24321, batch.n_tokens = 64
slot init_sampler: id  2 | task 30077 | init sampler, took 3.59 ms, tokens: text = 24321, total = 24321
slot update_slots: id  2 | task 30077 | created context checkpoint 3 of 8 (pos_min = 23360, pos_max = 24256, size = 21.034 MiB)
slot print_timing: id  2 | task 30077 | 
prompt eval time =    6349.39 ms /  4603 tokens (    1.38 ms per token,   724.95 tokens per second)
       eval time =    3255.59 ms /   122 tokens (   26.69 ms per token,    37.47 tokens per second)
      total time =    9604.98 ms /  4725 tokens
slot      release: id  2 | task 30077 | stop processing: n_tokens = 24442, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 30203 | processing task, is_child = 0
slot update_slots: id  2 | task 30203 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24394
slot update_slots: id  2 | task 30203 | n_tokens = 24321, memory_seq_rm [24321, end)
slot update_slots: id  2 | task 30203 | prompt processing progress, n_tokens = 24330, batch.n_tokens = 9, progress = 0.997376
slot update_slots: id  2 | task 30203 | n_tokens = 24330, memory_seq_rm [24330, end)
slot update_slots: id  2 | task 30203 | prompt processing progress, n_tokens = 24394, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 30203 | prompt done, n_tokens = 24394, batch.n_tokens = 64
slot init_sampler: id  2 | task 30203 | init sampler, took 3.40 ms, tokens: text = 24394, total = 24394
slot update_slots: id  2 | task 30203 | created context checkpoint 4 of 8 (pos_min = 23545, pos_max = 24329, size = 18.408 MiB)
slot print_timing: id  2 | task 30203 | 
prompt eval time =     316.18 ms /    73 tokens (    4.33 ms per token,   230.88 tokens per second)
       eval time =     785.68 ms /    29 tokens (   27.09 ms per token,    36.91 tokens per second)
      total time =    1101.86 ms /   102 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 30203 | stop processing: n_tokens = 24422, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 30234 | processing task, is_child = 0
slot update_slots: id  2 | task 30234 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24482
slot update_slots: id  2 | task 30234 | n_tokens = 24394, memory_seq_rm [24394, end)
slot update_slots: id  2 | task 30234 | prompt processing progress, n_tokens = 24418, batch.n_tokens = 24, progress = 0.997386
slot update_slots: id  2 | task 30234 | n_tokens = 24418, memory_seq_rm [24418, end)
slot update_slots: id  2 | task 30234 | prompt processing progress, n_tokens = 24482, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 30234 | prompt done, n_tokens = 24482, batch.n_tokens = 64
slot init_sampler: id  2 | task 30234 | init sampler, took 4.76 ms, tokens: text = 24482, total = 24482
slot update_slots: id  2 | task 30234 | created context checkpoint 5 of 8 (pos_min = 23545, pos_max = 24417, size = 20.471 MiB)
slot print_timing: id  2 | task 30234 | 
prompt eval time =     314.20 ms /    88 tokens (    3.57 ms per token,   280.08 tokens per second)
       eval time =    1388.37 ms /    50 tokens (   27.77 ms per token,    36.01 tokens per second)
      total time =    1702.57 ms /   138 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 30234 | stop processing: n_tokens = 24531, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 30286 | processing task, is_child = 0
slot update_slots: id  2 | task 30286 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24543
slot update_slots: id  2 | task 30286 | n_tokens = 24482, memory_seq_rm [24482, end)
slot update_slots: id  2 | task 30286 | prompt processing progress, n_tokens = 24543, batch.n_tokens = 61, progress = 1.000000
slot update_slots: id  2 | task 30286 | prompt done, n_tokens = 24543, batch.n_tokens = 61
slot init_sampler: id  2 | task 30286 | init sampler, took 10.90 ms, tokens: text = 24543, total = 24543
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 30286
slot      release: id  2 | task 30286 | stop processing: n_tokens = 24556, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 30302 | processing task, is_child = 0
slot update_slots: id  2 | task 30302 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24456
slot update_slots: id  2 | task 30302 | n_tokens = 24307, memory_seq_rm [24307, end)
slot update_slots: id  2 | task 30302 | prompt processing progress, n_tokens = 24392, batch.n_tokens = 85, progress = 0.997383
slot update_slots: id  2 | task 30302 | n_tokens = 24392, memory_seq_rm [24392, end)
slot update_slots: id  2 | task 30302 | prompt processing progress, n_tokens = 24456, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 30302 | prompt done, n_tokens = 24456, batch.n_tokens = 64
slot init_sampler: id  2 | task 30302 | init sampler, took 4.60 ms, tokens: text = 24456, total = 24456
slot print_timing: id  2 | task 30302 | 
prompt eval time =     554.85 ms /   149 tokens (    3.72 ms per token,   268.54 tokens per second)
       eval time =    8239.57 ms /   295 tokens (   27.93 ms per token,    35.80 tokens per second)
      total time =    8794.42 ms /   444 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 30302 | stop processing: n_tokens = 24750, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 30599 | processing task, is_child = 0
slot update_slots: id  2 | task 30599 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24655
slot update_slots: id  2 | task 30599 | n_tokens = 24436, memory_seq_rm [24436, end)
slot update_slots: id  2 | task 30599 | prompt processing progress, n_tokens = 24591, batch.n_tokens = 155, progress = 0.997404
slot update_slots: id  2 | task 30599 | n_tokens = 24591, memory_seq_rm [24591, end)
slot update_slots: id  2 | task 30599 | prompt processing progress, n_tokens = 24655, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 30599 | prompt done, n_tokens = 24655, batch.n_tokens = 64
slot init_sampler: id  2 | task 30599 | init sampler, took 3.45 ms, tokens: text = 24655, total = 24655
slot update_slots: id  2 | task 30599 | created context checkpoint 6 of 8 (pos_min = 23853, pos_max = 24590, size = 17.306 MiB)
slot print_timing: id  2 | task 30599 | 
prompt eval time =     572.16 ms /   219 tokens (    2.61 ms per token,   382.76 tokens per second)
       eval time =    6414.77 ms /   238 tokens (   26.95 ms per token,    37.10 tokens per second)
      total time =    6986.93 ms /   457 tokens
slot      release: id  2 | task 30599 | stop processing: n_tokens = 24892, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 30839 | processing task, is_child = 0
slot update_slots: id  2 | task 30839 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24770
slot update_slots: id  2 | task 30839 | n_tokens = 24641, memory_seq_rm [24641, end)
slot update_slots: id  2 | task 30839 | prompt processing progress, n_tokens = 24706, batch.n_tokens = 65, progress = 0.997416
slot update_slots: id  2 | task 30839 | n_tokens = 24706, memory_seq_rm [24706, end)
slot update_slots: id  2 | task 30839 | prompt processing progress, n_tokens = 24770, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 30839 | prompt done, n_tokens = 24770, batch.n_tokens = 64
slot init_sampler: id  2 | task 30839 | init sampler, took 4.66 ms, tokens: text = 24770, total = 24770
slot update_slots: id  2 | task 30839 | created context checkpoint 7 of 8 (pos_min = 23995, pos_max = 24705, size = 16.672 MiB)
slot print_timing: id  2 | task 30839 | 
prompt eval time =     532.53 ms /   129 tokens (    4.13 ms per token,   242.24 tokens per second)
       eval time =    5107.18 ms /   187 tokens (   27.31 ms per token,    36.62 tokens per second)
      total time =    5639.71 ms /   316 tokens
slot      release: id  2 | task 30839 | stop processing: n_tokens = 24956, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 31028 | processing task, is_child = 0
slot update_slots: id  2 | task 31028 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24843
slot update_slots: id  2 | task 31028 | n_tokens = 24770, memory_seq_rm [24770, end)
slot update_slots: id  2 | task 31028 | prompt processing progress, n_tokens = 24779, batch.n_tokens = 9, progress = 0.997424
slot update_slots: id  2 | task 31028 | n_tokens = 24779, memory_seq_rm [24779, end)
slot update_slots: id  2 | task 31028 | prompt processing progress, n_tokens = 24843, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 31028 | prompt done, n_tokens = 24843, batch.n_tokens = 64
slot init_sampler: id  2 | task 31028 | init sampler, took 3.42 ms, tokens: text = 24843, total = 24843
slot update_slots: id  2 | task 31028 | created context checkpoint 8 of 8 (pos_min = 24059, pos_max = 24778, size = 16.884 MiB)
slot print_timing: id  2 | task 31028 | 
prompt eval time =     277.40 ms /    73 tokens (    3.80 ms per token,   263.16 tokens per second)
       eval time =    1034.54 ms /    38 tokens (   27.22 ms per token,    36.73 tokens per second)
      total time =    1311.93 ms /   111 tokens
slot      release: id  2 | task 31028 | stop processing: n_tokens = 24880, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 31068 | processing task, is_child = 0
slot update_slots: id  2 | task 31068 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 24987
slot update_slots: id  2 | task 31068 | n_tokens = 24843, memory_seq_rm [24843, end)
slot update_slots: id  2 | task 31068 | prompt processing progress, n_tokens = 24923, batch.n_tokens = 80, progress = 0.997439
slot update_slots: id  2 | task 31068 | n_tokens = 24923, memory_seq_rm [24923, end)
slot update_slots: id  2 | task 31068 | prompt processing progress, n_tokens = 24987, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 31068 | prompt done, n_tokens = 24987, batch.n_tokens = 64
slot init_sampler: id  2 | task 31068 | init sampler, took 3.52 ms, tokens: text = 24987, total = 24987
slot update_slots: id  2 | task 31068 | erasing old context checkpoint (pos_min = 18833, pos_max = 19729, size = 21.034 MiB)
slot update_slots: id  2 | task 31068 | created context checkpoint 8 of 8 (pos_min = 24059, pos_max = 24922, size = 20.260 MiB)
slot print_timing: id  2 | task 31068 | 
prompt eval time =     427.37 ms /   144 tokens (    2.97 ms per token,   336.95 tokens per second)
       eval time =    1349.30 ms /    49 tokens (   27.54 ms per token,    36.32 tokens per second)
      total time =    1776.66 ms /   193 tokens
slot      release: id  2 | task 31068 | stop processing: n_tokens = 25035, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.702 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 31119 | processing task, is_child = 0
slot update_slots: id  2 | task 31119 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 35614
slot update_slots: id  2 | task 31119 | n_tokens = 24987, memory_seq_rm [24987, end)
slot update_slots: id  2 | task 31119 | prompt processing progress, n_tokens = 27035, batch.n_tokens = 2048, progress = 0.759112
slot update_slots: id  2 | task 31119 | n_tokens = 27035, memory_seq_rm [27035, end)
slot update_slots: id  2 | task 31119 | prompt processing progress, n_tokens = 29083, batch.n_tokens = 2048, progress = 0.816617
slot update_slots: id  2 | task 31119 | n_tokens = 29083, memory_seq_rm [29083, end)
slot update_slots: id  2 | task 31119 | prompt processing progress, n_tokens = 31131, batch.n_tokens = 2048, progress = 0.874123
slot update_slots: id  2 | task 31119 | n_tokens = 31131, memory_seq_rm [31131, end)
slot update_slots: id  2 | task 31119 | prompt processing progress, n_tokens = 33179, batch.n_tokens = 2048, progress = 0.931628
slot update_slots: id  2 | task 31119 | n_tokens = 33179, memory_seq_rm [33179, end)
slot update_slots: id  2 | task 31119 | prompt processing progress, n_tokens = 35227, batch.n_tokens = 2048, progress = 0.989133
slot update_slots: id  2 | task 31119 | n_tokens = 35227, memory_seq_rm [35227, end)
slot update_slots: id  2 | task 31119 | prompt processing progress, n_tokens = 35550, batch.n_tokens = 323, progress = 0.998203
slot update_slots: id  2 | task 31119 | n_tokens = 35550, memory_seq_rm [35550, end)
slot update_slots: id  2 | task 31119 | prompt processing progress, n_tokens = 35614, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 31119 | prompt done, n_tokens = 35614, batch.n_tokens = 64
slot init_sampler: id  2 | task 31119 | init sampler, took 5.03 ms, tokens: text = 35614, total = 35614
slot update_slots: id  2 | task 31119 | erasing old context checkpoint (pos_min = 18943, pos_max = 19816, size = 20.495 MiB)
slot update_slots: id  2 | task 31119 | created context checkpoint 8 of 8 (pos_min = 34653, pos_max = 35549, size = 21.034 MiB)
slot print_timing: id  2 | task 31119 | 
prompt eval time =   15975.79 ms / 10627 tokens (    1.50 ms per token,   665.19 tokens per second)
       eval time =    3020.43 ms /   101 tokens (   29.91 ms per token,    33.44 tokens per second)
      total time =   18996.22 ms / 10728 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 31119 | stop processing: n_tokens = 35714, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 31227 | processing task, is_child = 0
slot update_slots: id  2 | task 31227 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 35705
slot update_slots: id  2 | task 31227 | n_tokens = 35614, memory_seq_rm [35614, end)
slot update_slots: id  2 | task 31227 | prompt processing progress, n_tokens = 35641, batch.n_tokens = 27, progress = 0.998208
slot update_slots: id  2 | task 31227 | n_tokens = 35641, memory_seq_rm [35641, end)
slot update_slots: id  2 | task 31227 | prompt processing progress, n_tokens = 35705, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 31227 | prompt done, n_tokens = 35705, batch.n_tokens = 64
slot init_sampler: id  2 | task 31227 | init sampler, took 5.07 ms, tokens: text = 35705, total = 35705
slot update_slots: id  2 | task 31227 | erasing old context checkpoint (pos_min = 23360, pos_max = 24256, size = 21.034 MiB)
slot update_slots: id  2 | task 31227 | created context checkpoint 8 of 8 (pos_min = 34817, pos_max = 35640, size = 19.322 MiB)
slot print_timing: id  2 | task 31227 | 
prompt eval time =     383.29 ms /    91 tokens (    4.21 ms per token,   237.42 tokens per second)
       eval time =    1963.39 ms /    68 tokens (   28.87 ms per token,    34.63 tokens per second)
      total time =    2346.68 ms /   159 tokens
slot      release: id  2 | task 31227 | stop processing: n_tokens = 35772, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 31297 | processing task, is_child = 0
slot update_slots: id  2 | task 31297 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 35832
slot update_slots: id  2 | task 31297 | n_tokens = 35705, memory_seq_rm [35705, end)
slot update_slots: id  2 | task 31297 | prompt processing progress, n_tokens = 35768, batch.n_tokens = 63, progress = 0.998214
slot update_slots: id  2 | task 31297 | n_tokens = 35768, memory_seq_rm [35768, end)
slot update_slots: id  2 | task 31297 | prompt processing progress, n_tokens = 35832, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 31297 | prompt done, n_tokens = 35832, batch.n_tokens = 64
slot init_sampler: id  2 | task 31297 | init sampler, took 4.98 ms, tokens: text = 35832, total = 35832
slot update_slots: id  2 | task 31297 | erasing old context checkpoint (pos_min = 23545, pos_max = 24329, size = 18.408 MiB)
slot update_slots: id  2 | task 31297 | created context checkpoint 8 of 8 (pos_min = 34875, pos_max = 35767, size = 20.940 MiB)
slot print_timing: id  2 | task 31297 | 
prompt eval time =     478.97 ms /   127 tokens (    3.77 ms per token,   265.15 tokens per second)
       eval time =    1732.90 ms /    60 tokens (   28.88 ms per token,    34.62 tokens per second)
      total time =    2211.87 ms /   187 tokens
slot      release: id  2 | task 31297 | stop processing: n_tokens = 35891, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 31359 | processing task, is_child = 0
slot update_slots: id  2 | task 31359 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 36148
slot update_slots: id  2 | task 31359 | n_tokens = 35832, memory_seq_rm [35832, end)
slot update_slots: id  2 | task 31359 | prompt processing progress, n_tokens = 36084, batch.n_tokens = 252, progress = 0.998230
slot update_slots: id  2 | task 31359 | n_tokens = 36084, memory_seq_rm [36084, end)
slot update_slots: id  2 | task 31359 | prompt processing progress, n_tokens = 36148, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 31359 | prompt done, n_tokens = 36148, batch.n_tokens = 64
slot init_sampler: id  2 | task 31359 | init sampler, took 5.03 ms, tokens: text = 36148, total = 36148
slot update_slots: id  2 | task 31359 | erasing old context checkpoint (pos_min = 23545, pos_max = 24417, size = 20.471 MiB)
slot update_slots: id  2 | task 31359 | created context checkpoint 8 of 8 (pos_min = 35219, pos_max = 36083, size = 20.284 MiB)
slot print_timing: id  2 | task 31359 | 
prompt eval time =     713.93 ms /   316 tokens (    2.26 ms per token,   442.62 tokens per second)
       eval time =   53241.66 ms /  1859 tokens (   28.64 ms per token,    34.92 tokens per second)
      total time =   53955.60 ms /  2175 tokens
slot      release: id  2 | task 31359 | stop processing: n_tokens = 38006, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.951
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 33220 | processing task, is_child = 0
slot update_slots: id  2 | task 33220 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 37005
slot update_slots: id  2 | task 33220 | n_past = 36148, slot.prompt.tokens.size() = 38006, seq_id = 2, pos_min = 37109, n_swa = 128
slot update_slots: id  2 | task 33220 | restored context checkpoint (pos_min = 35219, pos_max = 36083, size = 20.284 MiB)
slot update_slots: id  2 | task 33220 | n_tokens = 36083, memory_seq_rm [36083, end)
slot update_slots: id  2 | task 33220 | prompt processing progress, n_tokens = 36941, batch.n_tokens = 858, progress = 0.998271
slot update_slots: id  2 | task 33220 | n_tokens = 36941, memory_seq_rm [36941, end)
slot update_slots: id  2 | task 33220 | prompt processing progress, n_tokens = 37005, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 33220 | prompt done, n_tokens = 37005, batch.n_tokens = 64
slot init_sampler: id  2 | task 33220 | init sampler, took 5.43 ms, tokens: text = 37005, total = 37005
slot update_slots: id  2 | task 33220 | erasing old context checkpoint (pos_min = 23853, pos_max = 24590, size = 17.306 MiB)
slot update_slots: id  2 | task 33220 | created context checkpoint 8 of 8 (pos_min = 36044, pos_max = 36940, size = 21.034 MiB)
slot print_timing: id  2 | task 33220 | 
prompt eval time =    1857.07 ms /   922 tokens (    2.01 ms per token,   496.48 tokens per second)
       eval time =   24814.22 ms /   860 tokens (   28.85 ms per token,    34.66 tokens per second)
      total time =   26671.29 ms /  1782 tokens
slot      release: id  2 | task 33220 | stop processing: n_tokens = 37864, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 34082 | processing task, is_child = 0
slot update_slots: id  2 | task 34082 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 37860
slot update_slots: id  2 | task 34082 | n_past = 37005, slot.prompt.tokens.size() = 37864, seq_id = 2, pos_min = 36967, n_swa = 128
slot update_slots: id  2 | task 34082 | restored context checkpoint (pos_min = 36044, pos_max = 36940, size = 21.034 MiB)
slot update_slots: id  2 | task 34082 | n_tokens = 36940, memory_seq_rm [36940, end)
slot update_slots: id  2 | task 34082 | prompt processing progress, n_tokens = 37796, batch.n_tokens = 856, progress = 0.998310
slot update_slots: id  2 | task 34082 | n_tokens = 37796, memory_seq_rm [37796, end)
slot update_slots: id  2 | task 34082 | prompt processing progress, n_tokens = 37860, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 34082 | prompt done, n_tokens = 37860, batch.n_tokens = 64
slot init_sampler: id  2 | task 34082 | init sampler, took 5.37 ms, tokens: text = 37860, total = 37860
slot update_slots: id  2 | task 34082 | erasing old context checkpoint (pos_min = 23995, pos_max = 24705, size = 16.672 MiB)
slot update_slots: id  2 | task 34082 | created context checkpoint 8 of 8 (pos_min = 36899, pos_max = 37795, size = 21.034 MiB)
slot print_timing: id  2 | task 34082 | 
prompt eval time =    1834.46 ms /   920 tokens (    1.99 ms per token,   501.51 tokens per second)
       eval time =   24643.49 ms /   859 tokens (   28.69 ms per token,    34.86 tokens per second)
      total time =   26477.94 ms /  1779 tokens
slot      release: id  2 | task 34082 | stop processing: n_tokens = 38718, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 34943 | processing task, is_child = 0
slot update_slots: id  2 | task 34943 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 38698
slot update_slots: id  2 | task 34943 | n_past = 37860, slot.prompt.tokens.size() = 38718, seq_id = 2, pos_min = 37821, n_swa = 128
slot update_slots: id  2 | task 34943 | restored context checkpoint (pos_min = 36899, pos_max = 37795, size = 21.034 MiB)
slot update_slots: id  2 | task 34943 | n_tokens = 37795, memory_seq_rm [37795, end)
slot update_slots: id  2 | task 34943 | prompt processing progress, n_tokens = 38634, batch.n_tokens = 839, progress = 0.998346
slot update_slots: id  2 | task 34943 | n_tokens = 38634, memory_seq_rm [38634, end)
slot update_slots: id  2 | task 34943 | prompt processing progress, n_tokens = 38698, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 34943 | prompt done, n_tokens = 38698, batch.n_tokens = 64
slot init_sampler: id  2 | task 34943 | init sampler, took 5.41 ms, tokens: text = 38698, total = 38698
slot update_slots: id  2 | task 34943 | erasing old context checkpoint (pos_min = 24059, pos_max = 24778, size = 16.884 MiB)
slot update_slots: id  2 | task 34943 | created context checkpoint 8 of 8 (pos_min = 37737, pos_max = 38633, size = 21.034 MiB)
slot print_timing: id  2 | task 34943 | 
prompt eval time =    1793.15 ms /   903 tokens (    1.99 ms per token,   503.58 tokens per second)
       eval time =    1000.63 ms /    36 tokens (   27.80 ms per token,    35.98 tokens per second)
      total time =    2793.78 ms /   939 tokens
slot      release: id  2 | task 34943 | stop processing: n_tokens = 38733, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
