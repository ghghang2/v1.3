ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 3 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 13395 MiB of device memory vs. 14807 MiB of free device memory
llama_params_fit_impl: will leave 1412 >= 1024 MiB of free device memory, no changes needed
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.43 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14807 MiB free
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
warning: failed to mlock 1158266880-byte buffer (after previously locking 0 bytes): Cannot allocate memory
Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
.srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 40192
llama_context: n_ctx_seq     = 40192
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (40192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.77 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 40192 cells
llama_kv_cache:      CUDA0 KV buffer size =   942.00 MiB
llama_kv_cache: size =  942.00 MiB ( 40192 cells,  12 layers,  1/1 seqs), K (f16):  471.00 MiB, V (f16):  471.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 768 cells
llama_kv_cache:      CUDA0 KV buffer size =    18.00 MiB
llama_kv_cache: size =   18.00 MiB (   768 cells,  12 layers,  1/1 seqs), K (f16):    9.00 MiB, V (f16):    9.00 MiB
sched_reserve: reserving ...
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =    85.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 57.72 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 1
slot   load_model: id  0 | task -1 | new slot, n_ctx = 40192
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-28

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 0 | processing task, is_child = 0
slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 740
slot update_slots: id  0 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 0 | prompt processing progress, n_tokens = 676, batch.n_tokens = 676, progress = 0.913514
slot update_slots: id  0 | task 0 | n_tokens = 676, memory_seq_rm [676, end)
slot update_slots: id  0 | task 0 | prompt processing progress, n_tokens = 740, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 0 | prompt done, n_tokens = 740, batch.n_tokens = 64
slot init_sampler: id  0 | task 0 | init sampler, took 0.12 ms, tokens: text = 740, total = 740
slot update_slots: id  0 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 675, size = 15.852 MiB)
slot print_timing: id  0 | task 0 | 
prompt eval time =    1166.14 ms /   740 tokens (    1.58 ms per token,   634.57 tokens per second)
       eval time =    1360.78 ms /    63 tokens (   21.60 ms per token,    46.30 tokens per second)
      total time =    2526.92 ms /   803 tokens
slot      release: id  0 | task 0 | stop processing: n_tokens = 802, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.600 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 65 | processing task, is_child = 0
slot update_slots: id  0 | task 65 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1306
slot update_slots: id  0 | task 65 | n_tokens = 783, memory_seq_rm [783, end)
slot update_slots: id  0 | task 65 | prompt processing progress, n_tokens = 1242, batch.n_tokens = 459, progress = 0.950995
slot update_slots: id  0 | task 65 | n_tokens = 1242, memory_seq_rm [1242, end)
slot update_slots: id  0 | task 65 | prompt processing progress, n_tokens = 1306, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 65 | prompt done, n_tokens = 1306, batch.n_tokens = 64
slot init_sampler: id  0 | task 65 | init sampler, took 0.28 ms, tokens: text = 1306, total = 1306
slot update_slots: id  0 | task 65 | created context checkpoint 2 of 8 (pos_min = 474, pos_max = 1241, size = 18.009 MiB)
slot print_timing: id  0 | task 65 | 
prompt eval time =     563.51 ms /   523 tokens (    1.08 ms per token,   928.11 tokens per second)
       eval time =     971.08 ms /    43 tokens (   22.58 ms per token,    44.28 tokens per second)
      total time =    1534.59 ms /   566 tokens
slot      release: id  0 | task 65 | stop processing: n_tokens = 1348, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.402 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 110 | processing task, is_child = 0
slot update_slots: id  0 | task 110 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3280
slot update_slots: id  0 | task 110 | n_tokens = 1317, memory_seq_rm [1317, end)
slot update_slots: id  0 | task 110 | prompt processing progress, n_tokens = 3216, batch.n_tokens = 1899, progress = 0.980488
slot update_slots: id  0 | task 110 | n_tokens = 3216, memory_seq_rm [3216, end)
slot update_slots: id  0 | task 110 | prompt processing progress, n_tokens = 3280, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 110 | prompt done, n_tokens = 3280, batch.n_tokens = 64
slot init_sampler: id  0 | task 110 | init sampler, took 0.47 ms, tokens: text = 3280, total = 3280
slot update_slots: id  0 | task 110 | created context checkpoint 3 of 8 (pos_min = 2448, pos_max = 3215, size = 18.009 MiB)
slot print_timing: id  0 | task 110 | 
prompt eval time =    1935.64 ms /  1963 tokens (    0.99 ms per token,  1014.13 tokens per second)
       eval time =     636.46 ms /    28 tokens (   22.73 ms per token,    43.99 tokens per second)
      total time =    2572.10 ms /  1991 tokens
slot      release: id  0 | task 110 | stop processing: n_tokens = 3307, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.903 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 140 | processing task, is_child = 0
slot update_slots: id  0 | task 140 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3631
slot update_slots: id  0 | task 140 | n_tokens = 3280, memory_seq_rm [3280, end)
slot update_slots: id  0 | task 140 | prompt processing progress, n_tokens = 3567, batch.n_tokens = 287, progress = 0.982374
slot update_slots: id  0 | task 140 | n_tokens = 3567, memory_seq_rm [3567, end)
slot update_slots: id  0 | task 140 | prompt processing progress, n_tokens = 3631, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 140 | prompt done, n_tokens = 3631, batch.n_tokens = 64
slot init_sampler: id  0 | task 140 | init sampler, took 0.54 ms, tokens: text = 3631, total = 3631
slot update_slots: id  0 | task 140 | created context checkpoint 4 of 8 (pos_min = 2799, pos_max = 3566, size = 18.009 MiB)
slot print_timing: id  0 | task 140 | 
prompt eval time =     463.87 ms /   351 tokens (    1.32 ms per token,   756.68 tokens per second)
       eval time =     914.34 ms /    40 tokens (   22.86 ms per token,    43.75 tokens per second)
      total time =    1378.21 ms /   391 tokens
slot      release: id  0 | task 140 | stop processing: n_tokens = 3670, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.854 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 182 | processing task, is_child = 0
slot update_slots: id  0 | task 182 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4264
slot update_slots: id  0 | task 182 | n_tokens = 3643, memory_seq_rm [3643, end)
slot update_slots: id  0 | task 182 | prompt processing progress, n_tokens = 4200, batch.n_tokens = 557, progress = 0.984991
slot update_slots: id  0 | task 182 | n_tokens = 4200, memory_seq_rm [4200, end)
slot update_slots: id  0 | task 182 | prompt processing progress, n_tokens = 4264, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 182 | prompt done, n_tokens = 4264, batch.n_tokens = 64
slot init_sampler: id  0 | task 182 | init sampler, took 1.86 ms, tokens: text = 4264, total = 4264
slot update_slots: id  0 | task 182 | created context checkpoint 5 of 8 (pos_min = 3432, pos_max = 4199, size = 18.009 MiB)
slot print_timing: id  0 | task 182 | 
prompt eval time =     768.69 ms /   621 tokens (    1.24 ms per token,   807.87 tokens per second)
       eval time =     923.91 ms /    39 tokens (   23.69 ms per token,    42.21 tokens per second)
      total time =    1692.60 ms /   660 tokens
slot      release: id  0 | task 182 | stop processing: n_tokens = 4302, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 223 | processing task, is_child = 0
slot update_slots: id  0 | task 223 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4412
slot update_slots: id  0 | task 223 | n_tokens = 4276, memory_seq_rm [4276, end)
slot update_slots: id  0 | task 223 | prompt processing progress, n_tokens = 4348, batch.n_tokens = 72, progress = 0.985494
slot update_slots: id  0 | task 223 | n_tokens = 4348, memory_seq_rm [4348, end)
slot update_slots: id  0 | task 223 | prompt processing progress, n_tokens = 4412, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 223 | prompt done, n_tokens = 4412, batch.n_tokens = 64
slot init_sampler: id  0 | task 223 | init sampler, took 0.88 ms, tokens: text = 4412, total = 4412
slot update_slots: id  0 | task 223 | created context checkpoint 6 of 8 (pos_min = 3580, pos_max = 4347, size = 18.009 MiB)
slot print_timing: id  0 | task 223 | 
prompt eval time =     345.08 ms /   136 tokens (    2.54 ms per token,   394.11 tokens per second)
       eval time =     886.59 ms /    37 tokens (   23.96 ms per token,    41.73 tokens per second)
      total time =    1231.67 ms /   173 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 223 | stop processing: n_tokens = 4448, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.803 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 262 | processing task, is_child = 0
slot update_slots: id  0 | task 262 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5509
slot update_slots: id  0 | task 262 | n_tokens = 4422, memory_seq_rm [4422, end)
slot update_slots: id  0 | task 262 | prompt processing progress, n_tokens = 5445, batch.n_tokens = 1023, progress = 0.988383
slot update_slots: id  0 | task 262 | n_tokens = 5445, memory_seq_rm [5445, end)
slot update_slots: id  0 | task 262 | prompt processing progress, n_tokens = 5509, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 262 | prompt done, n_tokens = 5509, batch.n_tokens = 64
slot init_sampler: id  0 | task 262 | init sampler, took 0.78 ms, tokens: text = 5509, total = 5509
slot update_slots: id  0 | task 262 | created context checkpoint 7 of 8 (pos_min = 4677, pos_max = 5444, size = 18.009 MiB)
slot print_timing: id  0 | task 262 | 
prompt eval time =    1123.96 ms /  1087 tokens (    1.03 ms per token,   967.11 tokens per second)
       eval time =    1059.06 ms /    46 tokens (   23.02 ms per token,    43.43 tokens per second)
      total time =    2183.02 ms /  1133 tokens
slot      release: id  0 | task 262 | stop processing: n_tokens = 5554, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 310 | processing task, is_child = 0
slot update_slots: id  0 | task 310 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5583
slot update_slots: id  0 | task 310 | n_tokens = 5526, memory_seq_rm [5526, end)
slot update_slots: id  0 | task 310 | prompt processing progress, n_tokens = 5583, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  0 | task 310 | prompt done, n_tokens = 5583, batch.n_tokens = 57
slot init_sampler: id  0 | task 310 | init sampler, took 1.08 ms, tokens: text = 5583, total = 5583
slot update_slots: id  0 | task 310 | created context checkpoint 8 of 8 (pos_min = 4786, pos_max = 5525, size = 17.353 MiB)
slot print_timing: id  0 | task 310 | 
prompt eval time =     266.34 ms /    57 tokens (    4.67 ms per token,   214.01 tokens per second)
       eval time =    1080.15 ms /    45 tokens (   24.00 ms per token,    41.66 tokens per second)
      total time =    1346.49 ms /   102 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 310 | stop processing: n_tokens = 5627, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 356 | processing task, is_child = 0
slot update_slots: id  0 | task 356 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5820
slot update_slots: id  0 | task 356 | n_tokens = 5601, memory_seq_rm [5601, end)
slot update_slots: id  0 | task 356 | prompt processing progress, n_tokens = 5756, batch.n_tokens = 155, progress = 0.989003
slot update_slots: id  0 | task 356 | n_tokens = 5756, memory_seq_rm [5756, end)
slot update_slots: id  0 | task 356 | prompt processing progress, n_tokens = 5820, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 356 | prompt done, n_tokens = 5820, batch.n_tokens = 64
slot init_sampler: id  0 | task 356 | init sampler, took 1.14 ms, tokens: text = 5820, total = 5820
slot update_slots: id  0 | task 356 | erasing old context checkpoint (pos_min = 0, pos_max = 675, size = 15.852 MiB)
slot update_slots: id  0 | task 356 | created context checkpoint 8 of 8 (pos_min = 4988, pos_max = 5755, size = 18.009 MiB)
slot print_timing: id  0 | task 356 | 
prompt eval time =     397.54 ms /   219 tokens (    1.82 ms per token,   550.88 tokens per second)
       eval time =    1000.14 ms /    41 tokens (   24.39 ms per token,    40.99 tokens per second)
      total time =    1397.68 ms /   260 tokens
slot      release: id  0 | task 356 | stop processing: n_tokens = 5860, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.746 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 399 | processing task, is_child = 0
slot update_slots: id  0 | task 399 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7817
slot update_slots: id  0 | task 399 | n_tokens = 5830, memory_seq_rm [5830, end)
slot update_slots: id  0 | task 399 | prompt processing progress, n_tokens = 7753, batch.n_tokens = 1923, progress = 0.991813
slot update_slots: id  0 | task 399 | n_tokens = 7753, memory_seq_rm [7753, end)
slot update_slots: id  0 | task 399 | prompt processing progress, n_tokens = 7817, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 399 | prompt done, n_tokens = 7817, batch.n_tokens = 64
slot init_sampler: id  0 | task 399 | init sampler, took 1.49 ms, tokens: text = 7817, total = 7817
slot update_slots: id  0 | task 399 | erasing old context checkpoint (pos_min = 474, pos_max = 1241, size = 18.009 MiB)
slot update_slots: id  0 | task 399 | created context checkpoint 8 of 8 (pos_min = 6985, pos_max = 7752, size = 18.009 MiB)
slot print_timing: id  0 | task 399 | 
prompt eval time =    2039.88 ms /  1987 tokens (    1.03 ms per token,   974.08 tokens per second)
       eval time =    1180.38 ms /    48 tokens (   24.59 ms per token,    40.66 tokens per second)
      total time =    3220.26 ms /  2035 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 399 | stop processing: n_tokens = 7864, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 449 | processing task, is_child = 0
slot update_slots: id  0 | task 449 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8042
slot update_slots: id  0 | task 449 | n_tokens = 7839, memory_seq_rm [7839, end)
slot update_slots: id  0 | task 449 | prompt processing progress, n_tokens = 7978, batch.n_tokens = 139, progress = 0.992042
slot update_slots: id  0 | task 449 | n_tokens = 7978, memory_seq_rm [7978, end)
slot update_slots: id  0 | task 449 | prompt processing progress, n_tokens = 8042, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 449 | prompt done, n_tokens = 8042, batch.n_tokens = 64
slot init_sampler: id  0 | task 449 | init sampler, took 1.48 ms, tokens: text = 8042, total = 8042
slot update_slots: id  0 | task 449 | erasing old context checkpoint (pos_min = 2448, pos_max = 3215, size = 18.009 MiB)
slot update_slots: id  0 | task 449 | created context checkpoint 8 of 8 (pos_min = 7210, pos_max = 7977, size = 18.009 MiB)
slot print_timing: id  0 | task 449 | 
prompt eval time =     414.97 ms /   203 tokens (    2.04 ms per token,   489.19 tokens per second)
       eval time =     873.84 ms /    28 tokens (   31.21 ms per token,    32.04 tokens per second)
      total time =    1288.81 ms /   231 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 449 | stop processing: n_tokens = 8069, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.935 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 479 | processing task, is_child = 0
slot update_slots: id  0 | task 479 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8604
slot update_slots: id  0 | task 479 | n_tokens = 8042, memory_seq_rm [8042, end)
slot update_slots: id  0 | task 479 | prompt processing progress, n_tokens = 8540, batch.n_tokens = 498, progress = 0.992562
slot update_slots: id  0 | task 479 | n_tokens = 8540, memory_seq_rm [8540, end)
slot update_slots: id  0 | task 479 | prompt processing progress, n_tokens = 8604, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 479 | prompt done, n_tokens = 8604, batch.n_tokens = 64
slot init_sampler: id  0 | task 479 | init sampler, took 1.67 ms, tokens: text = 8604, total = 8604
slot update_slots: id  0 | task 479 | erasing old context checkpoint (pos_min = 2799, pos_max = 3566, size = 18.009 MiB)
slot update_slots: id  0 | task 479 | created context checkpoint 8 of 8 (pos_min = 7772, pos_max = 8539, size = 18.009 MiB)
slot print_timing: id  0 | task 479 | 
prompt eval time =     713.65 ms /   562 tokens (    1.27 ms per token,   787.51 tokens per second)
       eval time =    1028.82 ms /    39 tokens (   26.38 ms per token,    37.91 tokens per second)
      total time =    1742.47 ms /   601 tokens
slot      release: id  0 | task 479 | stop processing: n_tokens = 8642, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.901 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 520 | processing task, is_child = 0
slot update_slots: id  0 | task 520 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 9564
slot update_slots: id  0 | task 520 | n_tokens = 8615, memory_seq_rm [8615, end)
slot update_slots: id  0 | task 520 | prompt processing progress, n_tokens = 9500, batch.n_tokens = 885, progress = 0.993308
slot update_slots: id  0 | task 520 | n_tokens = 9500, memory_seq_rm [9500, end)
slot update_slots: id  0 | task 520 | prompt processing progress, n_tokens = 9564, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 520 | prompt done, n_tokens = 9564, batch.n_tokens = 64
slot init_sampler: id  0 | task 520 | init sampler, took 1.86 ms, tokens: text = 9564, total = 9564
slot update_slots: id  0 | task 520 | erasing old context checkpoint (pos_min = 3432, pos_max = 4199, size = 18.009 MiB)
slot update_slots: id  0 | task 520 | created context checkpoint 8 of 8 (pos_min = 8732, pos_max = 9499, size = 18.009 MiB)
slot print_timing: id  0 | task 520 | 
prompt eval time =    1164.34 ms /   949 tokens (    1.23 ms per token,   815.05 tokens per second)
       eval time =  109340.42 ms /  3958 tokens (   27.63 ms per token,    36.20 tokens per second)
      total time =  110504.76 ms /  4907 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 520 | stop processing: n_tokens = 13521, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 968366755
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 13521, total state size = 335.063 MiB
srv          load:  - looking for better prompt, base f_keep = 0.004, sim = 0.007
srv        update:  - cache state: 1 prompts, 478.479 MiB (limits: 8192.000 MiB, 40192 tokens, 231492 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv  get_availabl: prompt cache update took 929.06 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 4480 | processing task, is_child = 0
slot update_slots: id  0 | task 4480 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8109
slot update_slots: id  0 | task 4480 | n_past = 59, slot.prompt.tokens.size() = 13521, seq_id = 0, pos_min = 12753, n_swa = 128
slot update_slots: id  0 | task 4480 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 3580, pos_max = 4347, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 4677, pos_max = 5444, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 4786, pos_max = 5525, n_swa = 128, size = 17.353 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 4988, pos_max = 5755, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 6985, pos_max = 7752, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 7210, pos_max = 7977, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 7772, pos_max = 8539, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | erased invalidated context checkpoint (pos_min = 8732, pos_max = 9499, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 4480 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 4480 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.252559
slot update_slots: id  0 | task 4480 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 4480 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.505118
slot update_slots: id  0 | task 4480 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 4480 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.757677
slot update_slots: id  0 | task 4480 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 4480 | prompt processing progress, n_tokens = 8045, batch.n_tokens = 1901, progress = 0.992108
slot update_slots: id  0 | task 4480 | n_tokens = 8045, memory_seq_rm [8045, end)
slot update_slots: id  0 | task 4480 | prompt processing progress, n_tokens = 8109, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 4480 | prompt done, n_tokens = 8109, batch.n_tokens = 64
slot init_sampler: id  0 | task 4480 | init sampler, took 1.62 ms, tokens: text = 8109, total = 8109
slot update_slots: id  0 | task 4480 | created context checkpoint 1 of 8 (pos_min = 7277, pos_max = 8044, size = 18.009 MiB)
slot print_timing: id  0 | task 4480 | 
prompt eval time =    8384.61 ms /  8109 tokens (    1.03 ms per token,   967.13 tokens per second)
       eval time =  116798.06 ms /  4096 tokens (   28.52 ms per token,    35.07 tokens per second)
      total time =  125182.67 ms / 12205 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 4480 | stop processing: n_tokens = 12204, truncated = 0
srv  update_slots: all slots are idle
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 1112026055
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 12204, total state size = 304.180 MiB
srv          load:  - looking for better prompt, base f_keep = 0.005, sim = 0.072
srv        update:  - cache state: 2 prompts, 800.668 MiB (limits: 8192.000 MiB, 40192 tokens, 263204 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv  get_availabl: prompt cache update took 333.32 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8581 | processing task, is_child = 0
slot update_slots: id  0 | task 8581 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 822
slot update_slots: id  0 | task 8581 | n_past = 59, slot.prompt.tokens.size() = 12204, seq_id = 0, pos_min = 11436, n_swa = 128
slot update_slots: id  0 | task 8581 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 8581 | erased invalidated context checkpoint (pos_min = 7277, pos_max = 8044, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 8581 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 8581 | prompt processing progress, n_tokens = 758, batch.n_tokens = 758, progress = 0.922141
slot update_slots: id  0 | task 8581 | n_tokens = 758, memory_seq_rm [758, end)
slot update_slots: id  0 | task 8581 | prompt processing progress, n_tokens = 822, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8581 | prompt done, n_tokens = 822, batch.n_tokens = 64
slot init_sampler: id  0 | task 8581 | init sampler, took 0.16 ms, tokens: text = 822, total = 822
slot update_slots: id  0 | task 8581 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 757, size = 17.775 MiB)
slot print_timing: id  0 | task 8581 | 
prompt eval time =     987.10 ms /   822 tokens (    1.20 ms per token,   832.74 tokens per second)
       eval time =    3251.90 ms /   141 tokens (   23.06 ms per token,    43.36 tokens per second)
      total time =    4238.99 ms /   963 tokens
slot      release: id  0 | task 8581 | stop processing: n_tokens = 962, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.643 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8724 | processing task, is_child = 0
slot update_slots: id  0 | task 8724 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1466
slot update_slots: id  0 | task 8724 | n_tokens = 943, memory_seq_rm [943, end)
slot update_slots: id  0 | task 8724 | prompt processing progress, n_tokens = 1402, batch.n_tokens = 459, progress = 0.956344
slot update_slots: id  0 | task 8724 | n_tokens = 1402, memory_seq_rm [1402, end)
slot update_slots: id  0 | task 8724 | prompt processing progress, n_tokens = 1466, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8724 | prompt done, n_tokens = 1466, batch.n_tokens = 64
slot init_sampler: id  0 | task 8724 | init sampler, took 0.27 ms, tokens: text = 1466, total = 1466
slot update_slots: id  0 | task 8724 | created context checkpoint 2 of 8 (pos_min = 634, pos_max = 1401, size = 18.009 MiB)
slot print_timing: id  0 | task 8724 | 
prompt eval time =     591.61 ms /   523 tokens (    1.13 ms per token,   884.02 tokens per second)
       eval time =     926.09 ms /    39 tokens (   23.75 ms per token,    42.11 tokens per second)
      total time =    1517.70 ms /   562 tokens
slot      release: id  0 | task 8724 | stop processing: n_tokens = 1504, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8765 | processing task, is_child = 0
slot update_slots: id  0 | task 8765 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1559
slot update_slots: id  0 | task 8765 | n_tokens = 1477, memory_seq_rm [1477, end)
slot update_slots: id  0 | task 8765 | prompt processing progress, n_tokens = 1495, batch.n_tokens = 18, progress = 0.958948
slot update_slots: id  0 | task 8765 | n_tokens = 1495, memory_seq_rm [1495, end)
slot update_slots: id  0 | task 8765 | prompt processing progress, n_tokens = 1559, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8765 | prompt done, n_tokens = 1559, batch.n_tokens = 64
slot init_sampler: id  0 | task 8765 | init sampler, took 0.33 ms, tokens: text = 1559, total = 1559
slot update_slots: id  0 | task 8765 | created context checkpoint 3 of 8 (pos_min = 736, pos_max = 1494, size = 17.798 MiB)
slot print_timing: id  0 | task 8765 | 
prompt eval time =     316.37 ms /    82 tokens (    3.86 ms per token,   259.19 tokens per second)
       eval time =    3209.01 ms /   129 tokens (   24.88 ms per token,    40.20 tokens per second)
      total time =    3525.38 ms /   211 tokens
slot      release: id  0 | task 8765 | stop processing: n_tokens = 1687, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.458 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8896 | processing task, is_child = 0
slot update_slots: id  0 | task 8896 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3619
slot update_slots: id  0 | task 8896 | n_tokens = 1656, memory_seq_rm [1656, end)
slot update_slots: id  0 | task 8896 | prompt processing progress, n_tokens = 3555, batch.n_tokens = 1899, progress = 0.982316
slot update_slots: id  0 | task 8896 | n_tokens = 3555, memory_seq_rm [3555, end)
slot update_slots: id  0 | task 8896 | prompt processing progress, n_tokens = 3619, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8896 | prompt done, n_tokens = 3619, batch.n_tokens = 64
slot init_sampler: id  0 | task 8896 | init sampler, took 0.68 ms, tokens: text = 3619, total = 3619
slot update_slots: id  0 | task 8896 | created context checkpoint 4 of 8 (pos_min = 2787, pos_max = 3554, size = 18.009 MiB)
slot print_timing: id  0 | task 8896 | 
prompt eval time =    2046.18 ms /  1963 tokens (    1.04 ms per token,   959.35 tokens per second)
       eval time =     970.28 ms /    40 tokens (   24.26 ms per token,    41.23 tokens per second)
      total time =    3016.46 ms /  2003 tokens
slot      release: id  0 | task 8896 | stop processing: n_tokens = 3658, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.912 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8938 | processing task, is_child = 0
slot update_slots: id  0 | task 8938 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3978
slot update_slots: id  0 | task 8938 | n_tokens = 3627, memory_seq_rm [3627, end)
slot update_slots: id  0 | task 8938 | prompt processing progress, n_tokens = 3914, batch.n_tokens = 287, progress = 0.983912
slot update_slots: id  0 | task 8938 | n_tokens = 3914, memory_seq_rm [3914, end)
slot update_slots: id  0 | task 8938 | prompt processing progress, n_tokens = 3978, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8938 | prompt done, n_tokens = 3978, batch.n_tokens = 64
slot init_sampler: id  0 | task 8938 | init sampler, took 0.79 ms, tokens: text = 3978, total = 3978
slot update_slots: id  0 | task 8938 | created context checkpoint 5 of 8 (pos_min = 3146, pos_max = 3913, size = 18.009 MiB)
slot print_timing: id  0 | task 8938 | 
prompt eval time =     494.36 ms /   351 tokens (    1.41 ms per token,   710.02 tokens per second)
       eval time =    3653.84 ms /   153 tokens (   23.88 ms per token,    41.87 tokens per second)
      total time =    4148.19 ms /   504 tokens
slot      release: id  0 | task 8938 | stop processing: n_tokens = 4130, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9093 | processing task, is_child = 0
slot update_slots: id  0 | task 9093 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4162
slot update_slots: id  0 | task 9093 | n_tokens = 4106, memory_seq_rm [4106, end)
slot update_slots: id  0 | task 9093 | prompt processing progress, n_tokens = 4162, batch.n_tokens = 56, progress = 1.000000
slot update_slots: id  0 | task 9093 | prompt done, n_tokens = 4162, batch.n_tokens = 56
slot init_sampler: id  0 | task 9093 | init sampler, took 0.81 ms, tokens: text = 4162, total = 4162
slot update_slots: id  0 | task 9093 | created context checkpoint 6 of 8 (pos_min = 3362, pos_max = 4105, size = 17.446 MiB)
slot print_timing: id  0 | task 9093 | 
prompt eval time =     255.67 ms /    56 tokens (    4.57 ms per token,   219.03 tokens per second)
       eval time =    1177.93 ms /    48 tokens (   24.54 ms per token,    40.75 tokens per second)
      total time =    1433.60 ms /   104 tokens
slot      release: id  0 | task 9093 | stop processing: n_tokens = 4209, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9142 | processing task, is_child = 0
slot update_slots: id  0 | task 9142 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4350
slot update_slots: id  0 | task 9142 | n_tokens = 4182, memory_seq_rm [4182, end)
slot update_slots: id  0 | task 9142 | prompt processing progress, n_tokens = 4286, batch.n_tokens = 104, progress = 0.985287
slot update_slots: id  0 | task 9142 | n_tokens = 4286, memory_seq_rm [4286, end)
slot update_slots: id  0 | task 9142 | prompt processing progress, n_tokens = 4350, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9142 | prompt done, n_tokens = 4350, batch.n_tokens = 64
slot init_sampler: id  0 | task 9142 | init sampler, took 0.82 ms, tokens: text = 4350, total = 4350
slot update_slots: id  0 | task 9142 | created context checkpoint 7 of 8 (pos_min = 3518, pos_max = 4285, size = 18.009 MiB)
slot print_timing: id  0 | task 9142 | 
prompt eval time =     418.19 ms /   168 tokens (    2.49 ms per token,   401.73 tokens per second)
       eval time =    1061.07 ms /    36 tokens (   29.47 ms per token,    33.93 tokens per second)
      total time =    1479.26 ms /   204 tokens
slot      release: id  0 | task 9142 | stop processing: n_tokens = 4385, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9180 | processing task, is_child = 0
slot update_slots: id  0 | task 9180 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4450
slot update_slots: id  0 | task 9180 | n_tokens = 4360, memory_seq_rm [4360, end)
slot update_slots: id  0 | task 9180 | prompt processing progress, n_tokens = 4386, batch.n_tokens = 26, progress = 0.985618
slot update_slots: id  0 | task 9180 | n_tokens = 4386, memory_seq_rm [4386, end)
slot update_slots: id  0 | task 9180 | prompt processing progress, n_tokens = 4450, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9180 | prompt done, n_tokens = 4450, batch.n_tokens = 64
slot init_sampler: id  0 | task 9180 | init sampler, took 0.90 ms, tokens: text = 4450, total = 4450
slot update_slots: id  0 | task 9180 | created context checkpoint 8 of 8 (pos_min = 3618, pos_max = 4385, size = 18.009 MiB)
slot print_timing: id  0 | task 9180 | 
prompt eval time =     348.40 ms /    90 tokens (    3.87 ms per token,   258.32 tokens per second)
       eval time =     971.52 ms /    40 tokens (   24.29 ms per token,    41.17 tokens per second)
      total time =    1319.92 ms /   130 tokens
slot      release: id  0 | task 9180 | stop processing: n_tokens = 4489, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.875 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9222 | processing task, is_child = 0
slot update_slots: id  0 | task 9222 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5100
slot update_slots: id  0 | task 9222 | n_tokens = 4462, memory_seq_rm [4462, end)
slot update_slots: id  0 | task 9222 | prompt processing progress, n_tokens = 5036, batch.n_tokens = 574, progress = 0.987451
slot update_slots: id  0 | task 9222 | n_tokens = 5036, memory_seq_rm [5036, end)
slot update_slots: id  0 | task 9222 | prompt processing progress, n_tokens = 5100, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9222 | prompt done, n_tokens = 5100, batch.n_tokens = 64
slot init_sampler: id  0 | task 9222 | init sampler, took 0.98 ms, tokens: text = 5100, total = 5100
slot update_slots: id  0 | task 9222 | erasing old context checkpoint (pos_min = 0, pos_max = 757, size = 17.775 MiB)
slot update_slots: id  0 | task 9222 | created context checkpoint 8 of 8 (pos_min = 4268, pos_max = 5035, size = 18.009 MiB)
slot print_timing: id  0 | task 9222 | 
prompt eval time =     861.73 ms /   638 tokens (    1.35 ms per token,   740.37 tokens per second)
       eval time =    1460.56 ms /    56 tokens (   26.08 ms per token,    38.34 tokens per second)
      total time =    2322.30 ms /   694 tokens
slot      release: id  0 | task 9222 | stop processing: n_tokens = 5155, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.740 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9280 | processing task, is_child = 0
slot update_slots: id  0 | task 9280 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6929
slot update_slots: id  0 | task 9280 | n_tokens = 5129, memory_seq_rm [5129, end)
slot update_slots: id  0 | task 9280 | prompt processing progress, n_tokens = 6865, batch.n_tokens = 1736, progress = 0.990763
slot update_slots: id  0 | task 9280 | n_tokens = 6865, memory_seq_rm [6865, end)
slot update_slots: id  0 | task 9280 | prompt processing progress, n_tokens = 6929, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9280 | prompt done, n_tokens = 6929, batch.n_tokens = 64
slot init_sampler: id  0 | task 9280 | init sampler, took 4.54 ms, tokens: text = 6929, total = 6929
slot update_slots: id  0 | task 9280 | erasing old context checkpoint (pos_min = 634, pos_max = 1401, size = 18.009 MiB)
slot update_slots: id  0 | task 9280 | created context checkpoint 8 of 8 (pos_min = 6097, pos_max = 6864, size = 18.009 MiB)
slot print_timing: id  0 | task 9280 | 
prompt eval time =    2135.56 ms /  1800 tokens (    1.19 ms per token,   842.87 tokens per second)
       eval time =    1548.15 ms /    55 tokens (   28.15 ms per token,    35.53 tokens per second)
      total time =    3683.71 ms /  1855 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 9280 | stop processing: n_tokens = 6983, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.918 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9337 | processing task, is_child = 0
slot update_slots: id  0 | task 9337 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7577
slot update_slots: id  0 | task 9337 | n_tokens = 6956, memory_seq_rm [6956, end)
slot update_slots: id  0 | task 9337 | prompt processing progress, n_tokens = 7513, batch.n_tokens = 557, progress = 0.991553
slot update_slots: id  0 | task 9337 | n_tokens = 7513, memory_seq_rm [7513, end)
slot update_slots: id  0 | task 9337 | prompt processing progress, n_tokens = 7577, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9337 | prompt done, n_tokens = 7577, batch.n_tokens = 64
slot init_sampler: id  0 | task 9337 | init sampler, took 1.47 ms, tokens: text = 7577, total = 7577
slot update_slots: id  0 | task 9337 | erasing old context checkpoint (pos_min = 736, pos_max = 1494, size = 17.798 MiB)
slot update_slots: id  0 | task 9337 | created context checkpoint 8 of 8 (pos_min = 6745, pos_max = 7512, size = 18.009 MiB)
slot print_timing: id  0 | task 9337 | 
prompt eval time =     879.03 ms /   621 tokens (    1.42 ms per token,   706.46 tokens per second)
       eval time =    1044.89 ms /    39 tokens (   26.79 ms per token,    37.32 tokens per second)
      total time =    1923.92 ms /   660 tokens
slot      release: id  0 | task 9337 | stop processing: n_tokens = 7615, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9378 | processing task, is_child = 0
slot update_slots: id  0 | task 9378 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7659
slot update_slots: id  0 | task 9378 | n_tokens = 7591, memory_seq_rm [7591, end)
slot update_slots: id  0 | task 9378 | prompt processing progress, n_tokens = 7595, batch.n_tokens = 4, progress = 0.991644
slot update_slots: id  0 | task 9378 | n_tokens = 7595, memory_seq_rm [7595, end)
slot update_slots: id  0 | task 9378 | prompt processing progress, n_tokens = 7659, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9378 | prompt done, n_tokens = 7659, batch.n_tokens = 64
slot init_sampler: id  0 | task 9378 | init sampler, took 2.61 ms, tokens: text = 7659, total = 7659
slot update_slots: id  0 | task 9378 | erasing old context checkpoint (pos_min = 2787, pos_max = 3554, size = 18.009 MiB)
slot update_slots: id  0 | task 9378 | created context checkpoint 8 of 8 (pos_min = 6847, pos_max = 7594, size = 17.540 MiB)
slot print_timing: id  0 | task 9378 | 
prompt eval time =     240.73 ms /    68 tokens (    3.54 ms per token,   282.48 tokens per second)
       eval time =    1233.46 ms /    49 tokens (   25.17 ms per token,    39.73 tokens per second)
      total time =    1474.18 ms /   117 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 9378 | stop processing: n_tokens = 7707, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9429 | processing task, is_child = 0
slot update_slots: id  0 | task 9429 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7753
slot update_slots: id  0 | task 9429 | n_tokens = 7682, memory_seq_rm [7682, end)
slot update_slots: id  0 | task 9429 | prompt processing progress, n_tokens = 7689, batch.n_tokens = 7, progress = 0.991745
slot update_slots: id  0 | task 9429 | n_tokens = 7689, memory_seq_rm [7689, end)
slot update_slots: id  0 | task 9429 | prompt processing progress, n_tokens = 7753, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9429 | prompt done, n_tokens = 7753, batch.n_tokens = 64
slot init_sampler: id  0 | task 9429 | init sampler, took 3.15 ms, tokens: text = 7753, total = 7753
slot update_slots: id  0 | task 9429 | erasing old context checkpoint (pos_min = 3146, pos_max = 3913, size = 18.009 MiB)
slot update_slots: id  0 | task 9429 | created context checkpoint 8 of 8 (pos_min = 6939, pos_max = 7688, size = 17.587 MiB)
slot print_timing: id  0 | task 9429 | 
prompt eval time =     271.52 ms /    71 tokens (    3.82 ms per token,   261.49 tokens per second)
       eval time =     893.38 ms /    32 tokens (   27.92 ms per token,    35.82 tokens per second)
      total time =    1164.90 ms /   103 tokens
slot      release: id  0 | task 9429 | stop processing: n_tokens = 7784, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9463 | processing task, is_child = 0
slot update_slots: id  0 | task 9463 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7821
slot update_slots: id  0 | task 9463 | n_tokens = 7769, memory_seq_rm [7769, end)
slot update_slots: id  0 | task 9463 | prompt processing progress, n_tokens = 7821, batch.n_tokens = 52, progress = 1.000000
slot update_slots: id  0 | task 9463 | prompt done, n_tokens = 7821, batch.n_tokens = 52
slot init_sampler: id  0 | task 9463 | init sampler, took 1.48 ms, tokens: text = 7821, total = 7821
slot update_slots: id  0 | task 9463 | erasing old context checkpoint (pos_min = 3362, pos_max = 4105, size = 17.446 MiB)
slot update_slots: id  0 | task 9463 | created context checkpoint 8 of 8 (pos_min = 7016, pos_max = 7768, size = 17.657 MiB)
slot print_timing: id  0 | task 9463 | 
prompt eval time =     185.83 ms /    52 tokens (    3.57 ms per token,   279.83 tokens per second)
       eval time =    1183.59 ms /    45 tokens (   26.30 ms per token,    38.02 tokens per second)
      total time =    1369.41 ms /    97 tokens
slot      release: id  0 | task 9463 | stop processing: n_tokens = 7865, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9509 | processing task, is_child = 0
slot update_slots: id  0 | task 9509 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7913
slot update_slots: id  0 | task 9509 | n_tokens = 7838, memory_seq_rm [7838, end)
slot update_slots: id  0 | task 9509 | prompt processing progress, n_tokens = 7849, batch.n_tokens = 11, progress = 0.991912
slot update_slots: id  0 | task 9509 | n_tokens = 7849, memory_seq_rm [7849, end)
slot update_slots: id  0 | task 9509 | prompt processing progress, n_tokens = 7913, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9509 | prompt done, n_tokens = 7913, batch.n_tokens = 64
slot init_sampler: id  0 | task 9509 | init sampler, took 2.71 ms, tokens: text = 7913, total = 7913
slot update_slots: id  0 | task 9509 | erasing old context checkpoint (pos_min = 3518, pos_max = 4285, size = 18.009 MiB)
slot update_slots: id  0 | task 9509 | created context checkpoint 8 of 8 (pos_min = 7097, pos_max = 7848, size = 17.634 MiB)
slot print_timing: id  0 | task 9509 | 
prompt eval time =     262.36 ms /    75 tokens (    3.50 ms per token,   285.87 tokens per second)
       eval time =    1211.69 ms /    45 tokens (   26.93 ms per token,    37.14 tokens per second)
      total time =    1474.05 ms /   120 tokens
slot      release: id  0 | task 9509 | stop processing: n_tokens = 7957, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9556 | processing task, is_child = 0
slot update_slots: id  0 | task 9556 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8232
slot update_slots: id  0 | task 9556 | n_tokens = 7942, memory_seq_rm [7942, end)
slot update_slots: id  0 | task 9556 | prompt processing progress, n_tokens = 8168, batch.n_tokens = 226, progress = 0.992225
slot update_slots: id  0 | task 9556 | n_tokens = 8168, memory_seq_rm [8168, end)
slot update_slots: id  0 | task 9556 | prompt processing progress, n_tokens = 8232, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9556 | prompt done, n_tokens = 8232, batch.n_tokens = 64
slot init_sampler: id  0 | task 9556 | init sampler, took 3.59 ms, tokens: text = 8232, total = 8232
slot update_slots: id  0 | task 9556 | erasing old context checkpoint (pos_min = 3618, pos_max = 4385, size = 18.009 MiB)
slot update_slots: id  0 | task 9556 | created context checkpoint 8 of 8 (pos_min = 7400, pos_max = 8167, size = 18.009 MiB)
slot print_timing: id  0 | task 9556 | 
prompt eval time =     543.78 ms /   290 tokens (    1.88 ms per token,   533.30 tokens per second)
       eval time =    4874.77 ms /   174 tokens (   28.02 ms per token,    35.69 tokens per second)
      total time =    5418.56 ms /   464 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 9556 | stop processing: n_tokens = 8405, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9732 | processing task, is_child = 0
slot update_slots: id  0 | task 9732 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8559
slot update_slots: id  0 | task 9732 | n_tokens = 8378, memory_seq_rm [8378, end)
slot update_slots: id  0 | task 9732 | prompt processing progress, n_tokens = 8495, batch.n_tokens = 117, progress = 0.992522
slot update_slots: id  0 | task 9732 | n_tokens = 8495, memory_seq_rm [8495, end)
slot update_slots: id  0 | task 9732 | prompt processing progress, n_tokens = 8559, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9732 | prompt done, n_tokens = 8559, batch.n_tokens = 64
slot init_sampler: id  0 | task 9732 | init sampler, took 1.61 ms, tokens: text = 8559, total = 8559
slot update_slots: id  0 | task 9732 | erasing old context checkpoint (pos_min = 4268, pos_max = 5035, size = 18.009 MiB)
slot update_slots: id  0 | task 9732 | created context checkpoint 8 of 8 (pos_min = 7727, pos_max = 8494, size = 18.009 MiB)
slot print_timing: id  0 | task 9732 | 
prompt eval time =     491.43 ms /   181 tokens (    2.72 ms per token,   368.31 tokens per second)
       eval time =    1540.47 ms /    58 tokens (   26.56 ms per token,    37.65 tokens per second)
      total time =    2031.90 ms /   239 tokens
slot      release: id  0 | task 9732 | stop processing: n_tokens = 8616, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9792 | processing task, is_child = 0
slot update_slots: id  0 | task 9792 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8787
slot update_slots: id  0 | task 9792 | n_tokens = 8591, memory_seq_rm [8591, end)
slot update_slots: id  0 | task 9792 | prompt processing progress, n_tokens = 8723, batch.n_tokens = 132, progress = 0.992716
slot update_slots: id  0 | task 9792 | n_tokens = 8723, memory_seq_rm [8723, end)
slot update_slots: id  0 | task 9792 | prompt processing progress, n_tokens = 8787, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9792 | prompt done, n_tokens = 8787, batch.n_tokens = 64
slot init_sampler: id  0 | task 9792 | init sampler, took 3.91 ms, tokens: text = 8787, total = 8787
slot update_slots: id  0 | task 9792 | erasing old context checkpoint (pos_min = 6097, pos_max = 6864, size = 18.009 MiB)
slot update_slots: id  0 | task 9792 | created context checkpoint 8 of 8 (pos_min = 7955, pos_max = 8722, size = 18.009 MiB)
slot print_timing: id  0 | task 9792 | 
prompt eval time =     445.06 ms /   196 tokens (    2.27 ms per token,   440.39 tokens per second)
       eval time =    1162.34 ms /    42 tokens (   27.67 ms per token,    36.13 tokens per second)
      total time =    1607.39 ms /   238 tokens
slot      release: id  0 | task 9792 | stop processing: n_tokens = 8828, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.917 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9836 | processing task, is_child = 0
slot update_slots: id  0 | task 9836 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 9595
slot update_slots: id  0 | task 9836 | n_tokens = 8801, memory_seq_rm [8801, end)
slot update_slots: id  0 | task 9836 | prompt processing progress, n_tokens = 9531, batch.n_tokens = 730, progress = 0.993330
slot update_slots: id  0 | task 9836 | n_tokens = 9531, memory_seq_rm [9531, end)
slot update_slots: id  0 | task 9836 | prompt processing progress, n_tokens = 9595, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9836 | prompt done, n_tokens = 9595, batch.n_tokens = 64
slot init_sampler: id  0 | task 9836 | init sampler, took 1.88 ms, tokens: text = 9595, total = 9595
slot update_slots: id  0 | task 9836 | erasing old context checkpoint (pos_min = 6745, pos_max = 7512, size = 18.009 MiB)
slot update_slots: id  0 | task 9836 | created context checkpoint 8 of 8 (pos_min = 8763, pos_max = 9530, size = 18.009 MiB)
slot print_timing: id  0 | task 9836 | 
prompt eval time =    1127.89 ms /   794 tokens (    1.42 ms per token,   703.97 tokens per second)
       eval time =    1089.53 ms /    41 tokens (   26.57 ms per token,    37.63 tokens per second)
      total time =    2217.42 ms /   835 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 9836 | stop processing: n_tokens = 9635, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9879 | processing task, is_child = 0
slot update_slots: id  0 | task 9879 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 10303
slot update_slots: id  0 | task 9879 | n_tokens = 9608, memory_seq_rm [9608, end)
slot update_slots: id  0 | task 9879 | prompt processing progress, n_tokens = 10239, batch.n_tokens = 631, progress = 0.993788
slot update_slots: id  0 | task 9879 | n_tokens = 10239, memory_seq_rm [10239, end)
slot update_slots: id  0 | task 9879 | prompt processing progress, n_tokens = 10303, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9879 | prompt done, n_tokens = 10303, batch.n_tokens = 64
slot init_sampler: id  0 | task 9879 | init sampler, took 1.98 ms, tokens: text = 10303, total = 10303
slot update_slots: id  0 | task 9879 | erasing old context checkpoint (pos_min = 6847, pos_max = 7594, size = 17.540 MiB)
slot update_slots: id  0 | task 9879 | created context checkpoint 8 of 8 (pos_min = 9471, pos_max = 10238, size = 18.009 MiB)
slot print_timing: id  0 | task 9879 | 
prompt eval time =    1120.85 ms /   695 tokens (    1.61 ms per token,   620.06 tokens per second)
       eval time =   18484.38 ms /   656 tokens (   28.18 ms per token,    35.49 tokens per second)
      total time =   19605.23 ms /  1351 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 9879 | stop processing: n_tokens = 10958, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 1406763825
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 10958, total state size = 274.963 MiB
srv          load:  - looking for better prompt, base f_keep = 0.005, sim = 0.007
srv        update:  - cache state: 3 prompts, 1218.554 MiB (limits: 8192.000 MiB, 40192 tokens, 246609 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv  get_availabl: prompt cache update took 1109.25 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 10537 | processing task, is_child = 0
slot update_slots: id  0 | task 10537 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8501
slot update_slots: id  0 | task 10537 | n_past = 59, slot.prompt.tokens.size() = 10958, seq_id = 0, pos_min = 10190, n_swa = 128
slot update_slots: id  0 | task 10537 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 6939, pos_max = 7688, n_swa = 128, size = 17.587 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 7016, pos_max = 7768, n_swa = 128, size = 17.657 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 7097, pos_max = 7848, n_swa = 128, size = 17.634 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 7400, pos_max = 8167, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 7727, pos_max = 8494, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 7955, pos_max = 8722, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 8763, pos_max = 9530, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 10537 | erased invalidated context checkpoint (pos_min = 9471, pos_max = 10238, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 10537 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.240913
slot update_slots: id  0 | task 10537 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.481826
slot update_slots: id  0 | task 10537 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.722739
slot update_slots: id  0 | task 10537 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.963651
slot update_slots: id  0 | task 10537 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 8437, batch.n_tokens = 245, progress = 0.992471
slot update_slots: id  0 | task 10537 | n_tokens = 8437, memory_seq_rm [8437, end)
slot update_slots: id  0 | task 10537 | prompt processing progress, n_tokens = 8501, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 10537 | prompt done, n_tokens = 8501, batch.n_tokens = 64
slot init_sampler: id  0 | task 10537 | init sampler, took 1.66 ms, tokens: text = 8501, total = 8501
slot update_slots: id  0 | task 10537 | created context checkpoint 1 of 8 (pos_min = 7669, pos_max = 8436, size = 18.009 MiB)
slot print_timing: id  0 | task 10537 | 
prompt eval time =    9812.42 ms /  8501 tokens (    1.15 ms per token,   866.35 tokens per second)
       eval time =   62380.58 ms /  1980 tokens (   31.51 ms per token,    31.74 tokens per second)
      total time =   72193.00 ms / 10481 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 10537 | stop processing: n_tokens = 10480, truncated = 0
srv  update_slots: all slots are idle
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 1481477090
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 10480, total state size = 263.754 MiB
srv          load:  - looking for better prompt, base f_keep = 0.006, sim = 0.081
srv        update:  - cache state: 4 prompts, 1500.318 MiB (limits: 8192.000 MiB, 40192 tokens, 257518 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv  get_availabl: prompt cache update took 403.87 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 12523 | processing task, is_child = 0
slot update_slots: id  0 | task 12523 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 729
slot update_slots: id  0 | task 12523 | n_past = 59, slot.prompt.tokens.size() = 10480, seq_id = 0, pos_min = 9712, n_swa = 128
slot update_slots: id  0 | task 12523 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 12523 | erased invalidated context checkpoint (pos_min = 7669, pos_max = 8436, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 12523 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 12523 | prompt processing progress, n_tokens = 665, batch.n_tokens = 665, progress = 0.912208
slot update_slots: id  0 | task 12523 | n_tokens = 665, memory_seq_rm [665, end)
slot update_slots: id  0 | task 12523 | prompt processing progress, n_tokens = 729, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 12523 | prompt done, n_tokens = 729, batch.n_tokens = 64
slot init_sampler: id  0 | task 12523 | init sampler, took 0.16 ms, tokens: text = 729, total = 729
slot update_slots: id  0 | task 12523 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 664, size = 15.594 MiB)
slot print_timing: id  0 | task 12523 | 
prompt eval time =     870.21 ms /   729 tokens (    1.19 ms per token,   837.73 tokens per second)
       eval time =    1130.69 ms /    49 tokens (   23.08 ms per token,    43.34 tokens per second)
      total time =    2000.89 ms /   778 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 12523 | stop processing: n_tokens = 777, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.592 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 12574 | processing task, is_child = 0
slot update_slots: id  0 | task 12574 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1281
slot update_slots: id  0 | task 12574 | n_tokens = 758, memory_seq_rm [758, end)
slot update_slots: id  0 | task 12574 | prompt processing progress, n_tokens = 1217, batch.n_tokens = 459, progress = 0.950039
slot update_slots: id  0 | task 12574 | n_tokens = 1217, memory_seq_rm [1217, end)
slot update_slots: id  0 | task 12574 | prompt processing progress, n_tokens = 1281, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 12574 | prompt done, n_tokens = 1281, batch.n_tokens = 64
slot init_sampler: id  0 | task 12574 | init sampler, took 0.26 ms, tokens: text = 1281, total = 1281
slot update_slots: id  0 | task 12574 | created context checkpoint 2 of 8 (pos_min = 459, pos_max = 1216, size = 17.775 MiB)
slot print_timing: id  0 | task 12574 | 
prompt eval time =     579.20 ms /   523 tokens (    1.11 ms per token,   902.97 tokens per second)
       eval time =    1302.48 ms /    47 tokens (   27.71 ms per token,    36.08 tokens per second)
      total time =    1881.68 ms /   570 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 12574 | stop processing: n_tokens = 1327, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.938 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 12623 | processing task, is_child = 0
slot update_slots: id  0 | task 12623 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1383
slot update_slots: id  0 | task 12623 | n_tokens = 1297, memory_seq_rm [1297, end)
slot update_slots: id  0 | task 12623 | prompt processing progress, n_tokens = 1319, batch.n_tokens = 22, progress = 0.953724
slot update_slots: id  0 | task 12623 | n_tokens = 1319, memory_seq_rm [1319, end)
slot update_slots: id  0 | task 12623 | prompt processing progress, n_tokens = 1383, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 12623 | prompt done, n_tokens = 1383, batch.n_tokens = 64
slot init_sampler: id  0 | task 12623 | init sampler, took 0.29 ms, tokens: text = 1383, total = 1383
slot update_slots: id  0 | task 12623 | created context checkpoint 3 of 8 (pos_min = 569, pos_max = 1318, size = 17.587 MiB)
slot print_timing: id  0 | task 12623 | 
prompt eval time =     356.50 ms /    86 tokens (    4.15 ms per token,   241.24 tokens per second)
       eval time =    1478.27 ms /    52 tokens (   28.43 ms per token,    35.18 tokens per second)
      total time =    1834.77 ms /   138 tokens
slot      release: id  0 | task 12623 | stop processing: n_tokens = 1434, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 12677 | processing task, is_child = 0
slot update_slots: id  0 | task 12677 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1490
slot update_slots: id  0 | task 12677 | n_tokens = 1406, memory_seq_rm [1406, end)
slot update_slots: id  0 | task 12677 | prompt processing progress, n_tokens = 1426, batch.n_tokens = 20, progress = 0.957047
slot update_slots: id  0 | task 12677 | n_tokens = 1426, memory_seq_rm [1426, end)
slot update_slots: id  0 | task 12677 | prompt processing progress, n_tokens = 1490, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 12677 | prompt done, n_tokens = 1490, batch.n_tokens = 64
slot init_sampler: id  0 | task 12677 | init sampler, took 0.28 ms, tokens: text = 1490, total = 1490
slot update_slots: id  0 | task 12677 | created context checkpoint 4 of 8 (pos_min = 676, pos_max = 1425, size = 17.587 MiB)
slot print_timing: id  0 | task 12677 | 
prompt eval time =     351.12 ms /    84 tokens (    4.18 ms per token,   239.23 tokens per second)
       eval time =    1731.99 ms /    71 tokens (   24.39 ms per token,    40.99 tokens per second)
      total time =    2083.12 ms /   155 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 12677 | stop processing: n_tokens = 1560, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.494 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 12750 | processing task, is_child = 0
slot update_slots: id  0 | task 12750 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3099
slot update_slots: id  0 | task 12750 | n_tokens = 1530, memory_seq_rm [1530, end)
slot update_slots: id  0 | task 12750 | prompt processing progress, n_tokens = 3035, batch.n_tokens = 1505, progress = 0.979348
slot update_slots: id  0 | task 12750 | n_tokens = 3035, memory_seq_rm [3035, end)
slot update_slots: id  0 | task 12750 | prompt processing progress, n_tokens = 3099, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 12750 | prompt done, n_tokens = 3099, batch.n_tokens = 64
slot init_sampler: id  0 | task 12750 | init sampler, took 0.59 ms, tokens: text = 3099, total = 3099
slot update_slots: id  0 | task 12750 | created context checkpoint 5 of 8 (pos_min = 2267, pos_max = 3034, size = 18.009 MiB)
slot print_timing: id  0 | task 12750 | 
prompt eval time =    1596.81 ms /  1569 tokens (    1.02 ms per token,   982.58 tokens per second)
       eval time =   75602.33 ms /  2732 tokens (   27.67 ms per token,    36.14 tokens per second)
      total time =   77199.15 ms /  4301 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 12750 | stop processing: n_tokens = 5830, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.137 (> 0.100 thold), f_keep = 0.125
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5830, total state size = 154.717 MiB
srv          load:  - looking for better prompt, base f_keep = 0.125, sim = 0.137
srv        update:  - cache state: 5 prompts, 1741.586 MiB (limits: 8192.000 MiB, 40192 tokens, 249266 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv  get_availabl: prompt cache update took 394.04 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 15484 | processing task, is_child = 0
slot update_slots: id  0 | task 15484 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5303
slot update_slots: id  0 | task 15484 | n_past = 729, slot.prompt.tokens.size() = 5830, seq_id = 0, pos_min = 5062, n_swa = 128
slot update_slots: id  0 | task 15484 | restored context checkpoint (pos_min = 569, pos_max = 1318, size = 17.587 MiB)
slot update_slots: id  0 | task 15484 | erased invalidated context checkpoint (pos_min = 676, pos_max = 1425, n_swa = 128, size = 17.587 MiB)
slot update_slots: id  0 | task 15484 | erased invalidated context checkpoint (pos_min = 2267, pos_max = 3034, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 15484 | n_tokens = 729, memory_seq_rm [729, end)
slot update_slots: id  0 | task 15484 | prompt processing progress, n_tokens = 2777, batch.n_tokens = 2048, progress = 0.523666
slot update_slots: id  0 | task 15484 | n_tokens = 2777, memory_seq_rm [2777, end)
slot update_slots: id  0 | task 15484 | prompt processing progress, n_tokens = 4825, batch.n_tokens = 2048, progress = 0.909862
slot update_slots: id  0 | task 15484 | n_tokens = 4825, memory_seq_rm [4825, end)
slot update_slots: id  0 | task 15484 | prompt processing progress, n_tokens = 5239, batch.n_tokens = 414, progress = 0.987931
slot update_slots: id  0 | task 15484 | n_tokens = 5239, memory_seq_rm [5239, end)
slot update_slots: id  0 | task 15484 | prompt processing progress, n_tokens = 5303, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 15484 | prompt done, n_tokens = 5303, batch.n_tokens = 64
slot init_sampler: id  0 | task 15484 | init sampler, took 1.02 ms, tokens: text = 5303, total = 5303
slot update_slots: id  0 | task 15484 | created context checkpoint 4 of 8 (pos_min = 4471, pos_max = 5238, size = 18.009 MiB)
slot print_timing: id  0 | task 15484 | 
prompt eval time =    4362.65 ms /  4574 tokens (    0.95 ms per token,  1048.44 tokens per second)
       eval time =    3600.28 ms /   144 tokens (   25.00 ms per token,    40.00 tokens per second)
      total time =    7962.93 ms /  4718 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 15484 | stop processing: n_tokens = 5446, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.775 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 15632 | processing task, is_child = 0
slot update_slots: id  0 | task 15632 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6985
slot update_slots: id  0 | task 15632 | n_tokens = 5416, memory_seq_rm [5416, end)
slot update_slots: id  0 | task 15632 | prompt processing progress, n_tokens = 6921, batch.n_tokens = 1505, progress = 0.990838
slot update_slots: id  0 | task 15632 | n_tokens = 6921, memory_seq_rm [6921, end)
slot update_slots: id  0 | task 15632 | prompt processing progress, n_tokens = 6985, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 15632 | prompt done, n_tokens = 6985, batch.n_tokens = 64
slot init_sampler: id  0 | task 15632 | init sampler, took 1.49 ms, tokens: text = 6985, total = 6985
slot update_slots: id  0 | task 15632 | created context checkpoint 5 of 8 (pos_min = 6153, pos_max = 6920, size = 18.009 MiB)
slot print_timing: id  0 | task 15632 | 
prompt eval time =    1683.84 ms /  1569 tokens (    1.07 ms per token,   931.80 tokens per second)
       eval time =   72952.80 ms /  2806 tokens (   26.00 ms per token,    38.46 tokens per second)
      total time =   74636.64 ms /  4375 tokens
slot      release: id  0 | task 15632 | stop processing: n_tokens = 9790, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 2117561613
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9790, total state size = 247.575 MiB
srv          load:  - looking for better prompt, base f_keep = 0.006, sim = 0.012
srv        update:  - cache state: 6 prompts, 2076.134 MiB (limits: 8192.000 MiB, 40192 tokens, 247728 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv  get_availabl: prompt cache update took 902.36 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18440 | processing task, is_child = 0
slot update_slots: id  0 | task 18440 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4904
slot update_slots: id  0 | task 18440 | n_past = 59, slot.prompt.tokens.size() = 9790, seq_id = 0, pos_min = 9022, n_swa = 128
slot update_slots: id  0 | task 18440 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 18440 | erased invalidated context checkpoint (pos_min = 459, pos_max = 1216, n_swa = 128, size = 17.775 MiB)
slot update_slots: id  0 | task 18440 | erased invalidated context checkpoint (pos_min = 569, pos_max = 1318, n_swa = 128, size = 17.587 MiB)
slot update_slots: id  0 | task 18440 | erased invalidated context checkpoint (pos_min = 4471, pos_max = 5238, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 18440 | erased invalidated context checkpoint (pos_min = 6153, pos_max = 6920, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 18440 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 18440 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.417618
slot update_slots: id  0 | task 18440 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 18440 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.835237
slot update_slots: id  0 | task 18440 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 18440 | prompt processing progress, n_tokens = 4840, batch.n_tokens = 744, progress = 0.986949
slot update_slots: id  0 | task 18440 | n_tokens = 4840, memory_seq_rm [4840, end)
slot update_slots: id  0 | task 18440 | prompt processing progress, n_tokens = 4904, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18440 | prompt done, n_tokens = 4904, batch.n_tokens = 64
slot init_sampler: id  0 | task 18440 | init sampler, took 1.50 ms, tokens: text = 4904, total = 4904
slot update_slots: id  0 | task 18440 | created context checkpoint 2 of 8 (pos_min = 4072, pos_max = 4839, size = 18.009 MiB)
slot print_timing: id  0 | task 18440 | 
prompt eval time =    5180.21 ms /  4904 tokens (    1.06 ms per token,   946.68 tokens per second)
       eval time =   31853.14 ms /  1186 tokens (   26.86 ms per token,    37.23 tokens per second)
      total time =   37033.35 ms /  6090 tokens
slot      release: id  0 | task 18440 | stop processing: n_tokens = 6089, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 2156122084
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6089, total state size = 160.790 MiB
srv          load:  - looking for better prompt, base f_keep = 0.010, sim = 0.012
srv        update:  - cache state: 7 prompts, 2270.527 MiB (limits: 8192.000 MiB, 40192 tokens, 248488 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv  get_availabl: prompt cache update took 295.82 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 19630 | processing task, is_child = 0
slot update_slots: id  0 | task 19630 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4945
slot update_slots: id  0 | task 19630 | n_past = 59, slot.prompt.tokens.size() = 6089, seq_id = 0, pos_min = 5321, n_swa = 128
slot update_slots: id  0 | task 19630 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 19630 | erased invalidated context checkpoint (pos_min = 4072, pos_max = 4839, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 19630 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 19630 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.414156
slot update_slots: id  0 | task 19630 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 19630 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.828311
slot update_slots: id  0 | task 19630 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 19630 | prompt processing progress, n_tokens = 4881, batch.n_tokens = 785, progress = 0.987058
slot update_slots: id  0 | task 19630 | n_tokens = 4881, memory_seq_rm [4881, end)
slot update_slots: id  0 | task 19630 | prompt processing progress, n_tokens = 4945, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 19630 | prompt done, n_tokens = 4945, batch.n_tokens = 64
slot init_sampler: id  0 | task 19630 | init sampler, took 0.95 ms, tokens: text = 4945, total = 4945
slot update_slots: id  0 | task 19630 | created context checkpoint 2 of 8 (pos_min = 4113, pos_max = 4880, size = 18.009 MiB)
slot print_timing: id  0 | task 19630 | 
prompt eval time =    5238.90 ms /  4945 tokens (    1.06 ms per token,   943.90 tokens per second)
       eval time =   19377.70 ms /   698 tokens (   27.76 ms per token,    36.02 tokens per second)
      total time =   24616.60 ms /  5643 tokens
slot      release: id  0 | task 19630 | stop processing: n_tokens = 5642, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.878 (> 0.100 thold), f_keep = 0.884
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20332 | processing task, is_child = 0
slot update_slots: id  0 | task 20332 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5681
slot update_slots: id  0 | task 20332 | n_past = 4986, slot.prompt.tokens.size() = 5642, seq_id = 0, pos_min = 4874, n_swa = 128
slot update_slots: id  0 | task 20332 | restored context checkpoint (pos_min = 4113, pos_max = 4880, size = 18.009 MiB)
slot update_slots: id  0 | task 20332 | n_tokens = 4880, memory_seq_rm [4880, end)
slot update_slots: id  0 | task 20332 | prompt processing progress, n_tokens = 5617, batch.n_tokens = 737, progress = 0.988734
slot update_slots: id  0 | task 20332 | n_tokens = 5617, memory_seq_rm [5617, end)
slot update_slots: id  0 | task 20332 | prompt processing progress, n_tokens = 5681, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20332 | prompt done, n_tokens = 5681, batch.n_tokens = 64
slot init_sampler: id  0 | task 20332 | init sampler, took 1.15 ms, tokens: text = 5681, total = 5681
slot update_slots: id  0 | task 20332 | created context checkpoint 3 of 8 (pos_min = 4849, pos_max = 5616, size = 18.009 MiB)
slot print_timing: id  0 | task 20332 | 
prompt eval time =    1215.23 ms /   801 tokens (    1.52 ms per token,   659.14 tokens per second)
       eval time =    6147.81 ms /   217 tokens (   28.33 ms per token,    35.30 tokens per second)
      total time =    7363.04 ms /  1018 tokens
slot      release: id  0 | task 20332 | stop processing: n_tokens = 5897, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20551 | processing task, is_child = 0
slot update_slots: id  0 | task 20551 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5955
slot update_slots: id  0 | task 20551 | n_tokens = 5874, memory_seq_rm [5874, end)
slot update_slots: id  0 | task 20551 | prompt processing progress, n_tokens = 5891, batch.n_tokens = 17, progress = 0.989253
slot update_slots: id  0 | task 20551 | n_tokens = 5891, memory_seq_rm [5891, end)
slot update_slots: id  0 | task 20551 | prompt processing progress, n_tokens = 5955, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20551 | prompt done, n_tokens = 5955, batch.n_tokens = 64
slot init_sampler: id  0 | task 20551 | init sampler, took 2.54 ms, tokens: text = 5955, total = 5955
slot update_slots: id  0 | task 20551 | created context checkpoint 4 of 8 (pos_min = 5129, pos_max = 5890, size = 17.868 MiB)
slot print_timing: id  0 | task 20551 | 
prompt eval time =     293.31 ms /    81 tokens (    3.62 ms per token,   276.15 tokens per second)
       eval time =    1775.62 ms /    61 tokens (   29.11 ms per token,    34.35 tokens per second)
      total time =    2068.94 ms /   142 tokens
slot      release: id  0 | task 20551 | stop processing: n_tokens = 6015, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20614 | processing task, is_child = 0
slot update_slots: id  0 | task 20614 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6058
slot update_slots: id  0 | task 20614 | n_tokens = 5992, memory_seq_rm [5992, end)
slot update_slots: id  0 | task 20614 | prompt processing progress, n_tokens = 5994, batch.n_tokens = 2, progress = 0.989435
slot update_slots: id  0 | task 20614 | n_tokens = 5994, memory_seq_rm [5994, end)
slot update_slots: id  0 | task 20614 | prompt processing progress, n_tokens = 6058, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20614 | prompt done, n_tokens = 6058, batch.n_tokens = 64
slot init_sampler: id  0 | task 20614 | init sampler, took 3.90 ms, tokens: text = 6058, total = 6058
slot update_slots: id  0 | task 20614 | created context checkpoint 5 of 8 (pos_min = 5247, pos_max = 5993, size = 17.517 MiB)
slot print_timing: id  0 | task 20614 | 
prompt eval time =     261.33 ms /    66 tokens (    3.96 ms per token,   252.55 tokens per second)
       eval time =   16560.42 ms /   575 tokens (   28.80 ms per token,    34.72 tokens per second)
      total time =   16821.75 ms /   641 tokens
slot      release: id  0 | task 20614 | stop processing: n_tokens = 6632, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.836 (> 0.100 thold), f_keep = 0.746
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 21191 | processing task, is_child = 0
slot update_slots: id  0 | task 21191 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5912
slot update_slots: id  0 | task 21191 | n_past = 4945, slot.prompt.tokens.size() = 6632, seq_id = 0, pos_min = 5864, n_swa = 128
slot update_slots: id  0 | task 21191 | restored context checkpoint (pos_min = 4113, pos_max = 4880, size = 18.009 MiB)
slot update_slots: id  0 | task 21191 | erased invalidated context checkpoint (pos_min = 4849, pos_max = 5616, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 21191 | erased invalidated context checkpoint (pos_min = 5129, pos_max = 5890, n_swa = 128, size = 17.868 MiB)
slot update_slots: id  0 | task 21191 | erased invalidated context checkpoint (pos_min = 5247, pos_max = 5993, n_swa = 128, size = 17.517 MiB)
slot update_slots: id  0 | task 21191 | n_tokens = 4880, memory_seq_rm [4880, end)
slot update_slots: id  0 | task 21191 | prompt processing progress, n_tokens = 5848, batch.n_tokens = 968, progress = 0.989175
slot update_slots: id  0 | task 21191 | n_tokens = 5848, memory_seq_rm [5848, end)
slot update_slots: id  0 | task 21191 | prompt processing progress, n_tokens = 5912, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 21191 | prompt done, n_tokens = 5912, batch.n_tokens = 64
slot init_sampler: id  0 | task 21191 | init sampler, took 1.13 ms, tokens: text = 5912, total = 5912
slot update_slots: id  0 | task 21191 | created context checkpoint 3 of 8 (pos_min = 5080, pos_max = 5847, size = 18.009 MiB)
slot print_timing: id  0 | task 21191 | 
prompt eval time =    1259.29 ms /  1032 tokens (    1.22 ms per token,   819.51 tokens per second)
       eval time =   12181.05 ms /   475 tokens (   25.64 ms per token,    39.00 tokens per second)
      total time =   13440.34 ms /  1507 tokens
slot      release: id  0 | task 21191 | stop processing: n_tokens = 6386, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 2495991069
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6386, total state size = 167.754 MiB
srv          load:  - looking for better prompt, base f_keep = 0.009, sim = 0.011
srv        update:  - cache state: 8 prompts, 2489.893 MiB (limits: 8192.000 MiB, 40192 tokens, 247606 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv  get_availabl: prompt cache update took 269.31 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 21668 | processing task, is_child = 0
slot update_slots: id  0 | task 21668 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5490
slot update_slots: id  0 | task 21668 | n_past = 59, slot.prompt.tokens.size() = 6386, seq_id = 0, pos_min = 5618, n_swa = 128
slot update_slots: id  0 | task 21668 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 21668 | erased invalidated context checkpoint (pos_min = 4113, pos_max = 4880, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 21668 | erased invalidated context checkpoint (pos_min = 5080, pos_max = 5847, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 21668 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 21668 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.373042
slot update_slots: id  0 | task 21668 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 21668 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.746084
slot update_slots: id  0 | task 21668 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 21668 | prompt processing progress, n_tokens = 5426, batch.n_tokens = 1330, progress = 0.988342
slot update_slots: id  0 | task 21668 | n_tokens = 5426, memory_seq_rm [5426, end)
slot update_slots: id  0 | task 21668 | prompt processing progress, n_tokens = 5490, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 21668 | prompt done, n_tokens = 5490, batch.n_tokens = 64
slot init_sampler: id  0 | task 21668 | init sampler, took 2.13 ms, tokens: text = 5490, total = 5490
slot update_slots: id  0 | task 21668 | created context checkpoint 2 of 8 (pos_min = 4658, pos_max = 5425, size = 18.009 MiB)
slot print_timing: id  0 | task 21668 | 
prompt eval time =    5372.86 ms /  5490 tokens (    0.98 ms per token,  1021.80 tokens per second)
       eval time =   13171.99 ms /   537 tokens (   24.53 ms per token,    40.77 tokens per second)
      total time =   18544.84 ms /  6027 tokens
slot      release: id  0 | task 21668 | stop processing: n_tokens = 6026, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 2522875346
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6026, total state size = 159.313 MiB
srv          load:  - looking for better prompt, base f_keep = 0.010, sim = 0.065
srv        update:  - cache state: 9 prompts, 2682.809 MiB (limits: 8192.000 MiB, 40192 tokens, 248202 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv        update:    - prompt 0x59729e4b9300:    6026 tokens, checkpoints:  2,   192.916 MiB
srv  get_availabl: prompt cache update took 536.56 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22209 | processing task, is_child = 0
slot update_slots: id  0 | task 22209 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 906
slot update_slots: id  0 | task 22209 | n_past = 59, slot.prompt.tokens.size() = 6026, seq_id = 0, pos_min = 5258, n_swa = 128
slot update_slots: id  0 | task 22209 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 22209 | erased invalidated context checkpoint (pos_min = 4658, pos_max = 5425, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 22209 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 22209 | prompt processing progress, n_tokens = 842, batch.n_tokens = 842, progress = 0.929360
slot update_slots: id  0 | task 22209 | n_tokens = 842, memory_seq_rm [842, end)
slot update_slots: id  0 | task 22209 | prompt processing progress, n_tokens = 906, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22209 | prompt done, n_tokens = 906, batch.n_tokens = 64
slot init_sampler: id  0 | task 22209 | init sampler, took 0.17 ms, tokens: text = 906, total = 906
slot update_slots: id  0 | task 22209 | created context checkpoint 2 of 8 (pos_min = 74, pos_max = 841, size = 18.009 MiB)
slot print_timing: id  0 | task 22209 | 
prompt eval time =     993.22 ms /   906 tokens (    1.10 ms per token,   912.19 tokens per second)
       eval time =    2305.92 ms /    85 tokens (   27.13 ms per token,    36.86 tokens per second)
      total time =    3299.14 ms /   991 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 22209 | stop processing: n_tokens = 990, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.851 (> 0.100 thold), f_keep = 0.967
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22296 | processing task, is_child = 0
slot update_slots: id  0 | task 22296 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1124
slot update_slots: id  0 | task 22296 | n_tokens = 957, memory_seq_rm [957, end)
slot update_slots: id  0 | task 22296 | prompt processing progress, n_tokens = 1060, batch.n_tokens = 103, progress = 0.943061
slot update_slots: id  0 | task 22296 | n_tokens = 1060, memory_seq_rm [1060, end)
slot update_slots: id  0 | task 22296 | prompt processing progress, n_tokens = 1124, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22296 | prompt done, n_tokens = 1124, batch.n_tokens = 64
slot init_sampler: id  0 | task 22296 | init sampler, took 0.24 ms, tokens: text = 1124, total = 1124
slot update_slots: id  0 | task 22296 | created context checkpoint 3 of 8 (pos_min = 292, pos_max = 1059, size = 18.009 MiB)
slot print_timing: id  0 | task 22296 | 
prompt eval time =     513.20 ms /   167 tokens (    3.07 ms per token,   325.41 tokens per second)
       eval time =    2129.42 ms /    70 tokens (   30.42 ms per token,    32.87 tokens per second)
      total time =    2642.62 ms /   237 tokens
slot      release: id  0 | task 22296 | stop processing: n_tokens = 1193, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22368 | processing task, is_child = 0
slot update_slots: id  0 | task 22368 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1224
slot update_slots: id  0 | task 22368 | n_tokens = 1139, memory_seq_rm [1139, end)
slot update_slots: id  0 | task 22368 | prompt processing progress, n_tokens = 1160, batch.n_tokens = 21, progress = 0.947712
slot update_slots: id  0 | task 22368 | n_tokens = 1160, memory_seq_rm [1160, end)
slot update_slots: id  0 | task 22368 | prompt processing progress, n_tokens = 1224, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22368 | prompt done, n_tokens = 1224, batch.n_tokens = 64
slot init_sampler: id  0 | task 22368 | init sampler, took 0.26 ms, tokens: text = 1224, total = 1224
slot update_slots: id  0 | task 22368 | created context checkpoint 4 of 8 (pos_min = 425, pos_max = 1159, size = 17.235 MiB)
slot print_timing: id  0 | task 22368 | 
prompt eval time =     357.71 ms /    85 tokens (    4.21 ms per token,   237.62 tokens per second)
       eval time =    3870.24 ms /   143 tokens (   27.06 ms per token,    36.95 tokens per second)
      total time =    4227.95 ms /   228 tokens
slot      release: id  0 | task 22368 | stop processing: n_tokens = 1366, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.960
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22513 | processing task, is_child = 0
slot update_slots: id  0 | task 22513 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1397
slot update_slots: id  0 | task 22513 | n_tokens = 1312, memory_seq_rm [1312, end)
slot update_slots: id  0 | task 22513 | prompt processing progress, n_tokens = 1333, batch.n_tokens = 21, progress = 0.954188
slot update_slots: id  0 | task 22513 | n_tokens = 1333, memory_seq_rm [1333, end)
slot update_slots: id  0 | task 22513 | prompt processing progress, n_tokens = 1397, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22513 | prompt done, n_tokens = 1397, batch.n_tokens = 64
slot init_sampler: id  0 | task 22513 | init sampler, took 0.26 ms, tokens: text = 1397, total = 1397
slot update_slots: id  0 | task 22513 | created context checkpoint 5 of 8 (pos_min = 598, pos_max = 1332, size = 17.235 MiB)
slot print_timing: id  0 | task 22513 | 
prompt eval time =     355.54 ms /    85 tokens (    4.18 ms per token,   239.07 tokens per second)
       eval time =    8715.29 ms /   336 tokens (   25.94 ms per token,    38.55 tokens per second)
      total time =    9070.83 ms /   421 tokens
slot      release: id  0 | task 22513 | stop processing: n_tokens = 1732, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.906 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22851 | processing task, is_child = 0
slot update_slots: id  0 | task 22851 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1871
slot update_slots: id  0 | task 22851 | n_tokens = 1696, memory_seq_rm [1696, end)
slot update_slots: id  0 | task 22851 | prompt processing progress, n_tokens = 1807, batch.n_tokens = 111, progress = 0.965794
slot update_slots: id  0 | task 22851 | n_tokens = 1807, memory_seq_rm [1807, end)
slot update_slots: id  0 | task 22851 | prompt processing progress, n_tokens = 1871, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22851 | prompt done, n_tokens = 1871, batch.n_tokens = 64
slot init_sampler: id  0 | task 22851 | init sampler, took 0.35 ms, tokens: text = 1871, total = 1871
slot update_slots: id  0 | task 22851 | created context checkpoint 6 of 8 (pos_min = 1039, pos_max = 1806, size = 18.009 MiB)
slot print_timing: id  0 | task 22851 | 
prompt eval time =     542.71 ms /   175 tokens (    3.10 ms per token,   322.46 tokens per second)
       eval time =    2613.76 ms /   103 tokens (   25.38 ms per token,    39.41 tokens per second)
      total time =    3156.46 ms /   278 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 22851 | stop processing: n_tokens = 1973, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.558 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22956 | processing task, is_child = 0
slot update_slots: id  0 | task 22956 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3443
slot update_slots: id  0 | task 22956 | n_tokens = 1922, memory_seq_rm [1922, end)
slot update_slots: id  0 | task 22956 | prompt processing progress, n_tokens = 3379, batch.n_tokens = 1457, progress = 0.981412
slot update_slots: id  0 | task 22956 | n_tokens = 3379, memory_seq_rm [3379, end)
slot update_slots: id  0 | task 22956 | prompt processing progress, n_tokens = 3443, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22956 | prompt done, n_tokens = 3443, batch.n_tokens = 64
slot init_sampler: id  0 | task 22956 | init sampler, took 2.62 ms, tokens: text = 3443, total = 3443
slot update_slots: id  0 | task 22956 | created context checkpoint 7 of 8 (pos_min = 2611, pos_max = 3378, size = 18.009 MiB)
slot print_timing: id  0 | task 22956 | 
prompt eval time =    1716.39 ms /  1521 tokens (    1.13 ms per token,   886.16 tokens per second)
       eval time =   13678.97 ms /   530 tokens (   25.81 ms per token,    38.75 tokens per second)
      total time =   15395.36 ms /  2051 tokens
slot      release: id  0 | task 22956 | stop processing: n_tokens = 3972, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.698 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 23488 | processing task, is_child = 0
slot update_slots: id  0 | task 23488 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5425
slot update_slots: id  0 | task 23488 | n_tokens = 3788, memory_seq_rm [3788, end)
slot update_slots: id  0 | task 23488 | prompt processing progress, n_tokens = 5361, batch.n_tokens = 1573, progress = 0.988203
slot update_slots: id  0 | task 23488 | n_tokens = 5361, memory_seq_rm [5361, end)
slot update_slots: id  0 | task 23488 | prompt processing progress, n_tokens = 5425, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 23488 | prompt done, n_tokens = 5425, batch.n_tokens = 64
slot init_sampler: id  0 | task 23488 | init sampler, took 1.10 ms, tokens: text = 5425, total = 5425
slot update_slots: id  0 | task 23488 | created context checkpoint 8 of 8 (pos_min = 4593, pos_max = 5360, size = 18.009 MiB)
slot print_timing: id  0 | task 23488 | 
prompt eval time =    1919.56 ms /  1637 tokens (    1.17 ms per token,   852.80 tokens per second)
       eval time =   10067.40 ms /   379 tokens (   26.56 ms per token,    37.65 tokens per second)
      total time =   11986.96 ms /  2016 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 23488 | stop processing: n_tokens = 5803, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 23869 | processing task, is_child = 0
slot update_slots: id  0 | task 23869 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5989
slot update_slots: id  0 | task 23869 | n_tokens = 5752, memory_seq_rm [5752, end)
slot update_slots: id  0 | task 23869 | prompt processing progress, n_tokens = 5925, batch.n_tokens = 173, progress = 0.989314
slot update_slots: id  0 | task 23869 | n_tokens = 5925, memory_seq_rm [5925, end)
slot update_slots: id  0 | task 23869 | prompt processing progress, n_tokens = 5989, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 23869 | prompt done, n_tokens = 5989, batch.n_tokens = 64
slot init_sampler: id  0 | task 23869 | init sampler, took 1.98 ms, tokens: text = 5989, total = 5989
slot update_slots: id  0 | task 23869 | erasing old context checkpoint (pos_min = 0, pos_max = 664, size = 15.594 MiB)
slot update_slots: id  0 | task 23869 | created context checkpoint 8 of 8 (pos_min = 5157, pos_max = 5924, size = 18.009 MiB)
slot print_timing: id  0 | task 23869 | 
prompt eval time =     531.33 ms /   237 tokens (    2.24 ms per token,   446.05 tokens per second)
       eval time =    5497.41 ms /   209 tokens (   26.30 ms per token,    38.02 tokens per second)
      total time =    6028.74 ms /   446 tokens
slot      release: id  0 | task 23869 | stop processing: n_tokens = 6197, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 24080 | processing task, is_child = 0
slot update_slots: id  0 | task 24080 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6299
slot update_slots: id  0 | task 24080 | n_tokens = 6143, memory_seq_rm [6143, end)
slot update_slots: id  0 | task 24080 | prompt processing progress, n_tokens = 6235, batch.n_tokens = 92, progress = 0.989840
slot update_slots: id  0 | task 24080 | n_tokens = 6235, memory_seq_rm [6235, end)
slot update_slots: id  0 | task 24080 | prompt processing progress, n_tokens = 6299, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 24080 | prompt done, n_tokens = 6299, batch.n_tokens = 64
slot init_sampler: id  0 | task 24080 | init sampler, took 3.23 ms, tokens: text = 6299, total = 6299
slot update_slots: id  0 | task 24080 | erasing old context checkpoint (pos_min = 74, pos_max = 841, size = 18.009 MiB)
slot update_slots: id  0 | task 24080 | created context checkpoint 8 of 8 (pos_min = 5467, pos_max = 6234, size = 18.009 MiB)
slot print_timing: id  0 | task 24080 | 
prompt eval time =     475.63 ms /   156 tokens (    3.05 ms per token,   327.98 tokens per second)
       eval time =    9457.60 ms /   353 tokens (   26.79 ms per token,    37.32 tokens per second)
      total time =    9933.24 ms /   509 tokens
slot      release: id  0 | task 24080 | stop processing: n_tokens = 6651, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.884 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 24435 | processing task, is_child = 0
slot update_slots: id  0 | task 24435 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7471
slot update_slots: id  0 | task 24435 | n_tokens = 6602, memory_seq_rm [6602, end)
slot update_slots: id  0 | task 24435 | prompt processing progress, n_tokens = 7407, batch.n_tokens = 805, progress = 0.991434
slot update_slots: id  0 | task 24435 | n_tokens = 7407, memory_seq_rm [7407, end)
slot update_slots: id  0 | task 24435 | prompt processing progress, n_tokens = 7471, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 24435 | prompt done, n_tokens = 7471, batch.n_tokens = 64
slot init_sampler: id  0 | task 24435 | init sampler, took 1.41 ms, tokens: text = 7471, total = 7471
slot update_slots: id  0 | task 24435 | erasing old context checkpoint (pos_min = 292, pos_max = 1059, size = 18.009 MiB)
slot update_slots: id  0 | task 24435 | created context checkpoint 8 of 8 (pos_min = 6639, pos_max = 7406, size = 18.009 MiB)
slot print_timing: id  0 | task 24435 | 
prompt eval time =    1184.53 ms /   869 tokens (    1.36 ms per token,   733.63 tokens per second)
       eval time =    2058.41 ms /    71 tokens (   28.99 ms per token,    34.49 tokens per second)
      total time =    3242.94 ms /   940 tokens
slot      release: id  0 | task 24435 | stop processing: n_tokens = 7541, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.902 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 24508 | processing task, is_child = 0
slot update_slots: id  0 | task 24508 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8322
slot update_slots: id  0 | task 24508 | n_tokens = 7509, memory_seq_rm [7509, end)
slot update_slots: id  0 | task 24508 | prompt processing progress, n_tokens = 8258, batch.n_tokens = 749, progress = 0.992310
slot update_slots: id  0 | task 24508 | n_tokens = 8258, memory_seq_rm [8258, end)
slot update_slots: id  0 | task 24508 | prompt processing progress, n_tokens = 8322, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 24508 | prompt done, n_tokens = 8322, batch.n_tokens = 64
slot init_sampler: id  0 | task 24508 | init sampler, took 1.58 ms, tokens: text = 8322, total = 8322
slot update_slots: id  0 | task 24508 | erasing old context checkpoint (pos_min = 425, pos_max = 1159, size = 17.235 MiB)
slot update_slots: id  0 | task 24508 | created context checkpoint 8 of 8 (pos_min = 7490, pos_max = 8257, size = 18.009 MiB)
slot print_timing: id  0 | task 24508 | 
prompt eval time =    1123.45 ms /   813 tokens (    1.38 ms per token,   723.66 tokens per second)
       eval time =    1223.61 ms /    47 tokens (   26.03 ms per token,    38.41 tokens per second)
      total time =    2347.06 ms /   860 tokens
slot      release: id  0 | task 24508 | stop processing: n_tokens = 8368, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.124 (> 0.100 thold), f_keep = 0.108
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8368, total state size = 214.230 MiB
srv          load:  - looking for better prompt, base f_keep = 0.108, sim = 0.124
srv        update:  - cache state: 10 prompts, 3040.338 MiB (limits: 8192.000 MiB, 40192 tokens, 241561 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv        update:    - prompt 0x59729e4b9300:    6026 tokens, checkpoints:  2,   192.916 MiB
srv        update:    - prompt 0x59729721bfc0:    8368 tokens, checkpoints:  8,   357.529 MiB
srv  get_availabl: prompt cache update took 429.74 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 24557 | processing task, is_child = 0
slot update_slots: id  0 | task 24557 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7336
slot update_slots: id  0 | task 24557 | n_past = 906, slot.prompt.tokens.size() = 8368, seq_id = 0, pos_min = 7600, n_swa = 128
slot update_slots: id  0 | task 24557 | restored context checkpoint (pos_min = 598, pos_max = 1332, size = 17.235 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 1039, pos_max = 1806, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 2611, pos_max = 3378, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 4593, pos_max = 5360, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 5157, pos_max = 5924, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 5467, pos_max = 6234, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 6639, pos_max = 7406, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | erased invalidated context checkpoint (pos_min = 7490, pos_max = 8257, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 24557 | n_tokens = 906, memory_seq_rm [906, end)
slot update_slots: id  0 | task 24557 | prompt processing progress, n_tokens = 2954, batch.n_tokens = 2048, progress = 0.402672
slot update_slots: id  0 | task 24557 | n_tokens = 2954, memory_seq_rm [2954, end)
slot update_slots: id  0 | task 24557 | prompt processing progress, n_tokens = 5002, batch.n_tokens = 2048, progress = 0.681843
slot update_slots: id  0 | task 24557 | n_tokens = 5002, memory_seq_rm [5002, end)
slot update_slots: id  0 | task 24557 | prompt processing progress, n_tokens = 7050, batch.n_tokens = 2048, progress = 0.961014
slot update_slots: id  0 | task 24557 | n_tokens = 7050, memory_seq_rm [7050, end)
slot update_slots: id  0 | task 24557 | prompt processing progress, n_tokens = 7272, batch.n_tokens = 222, progress = 0.991276
slot update_slots: id  0 | task 24557 | n_tokens = 7272, memory_seq_rm [7272, end)
slot update_slots: id  0 | task 24557 | prompt processing progress, n_tokens = 7336, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 24557 | prompt done, n_tokens = 7336, batch.n_tokens = 64
slot init_sampler: id  0 | task 24557 | init sampler, took 3.11 ms, tokens: text = 7336, total = 7336
slot update_slots: id  0 | task 24557 | created context checkpoint 2 of 8 (pos_min = 6504, pos_max = 7271, size = 18.009 MiB)
slot print_timing: id  0 | task 24557 | 
prompt eval time =    6438.17 ms /  6430 tokens (    1.00 ms per token,   998.73 tokens per second)
       eval time =    3724.89 ms /   138 tokens (   26.99 ms per token,    37.05 tokens per second)
      total time =   10163.06 ms /  6568 tokens
slot      release: id  0 | task 24557 | stop processing: n_tokens = 7473, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.827 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 24700 | processing task, is_child = 0
slot update_slots: id  0 | task 24700 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8874
slot update_slots: id  0 | task 24700 | n_tokens = 7336, memory_seq_rm [7336, end)
slot update_slots: id  0 | task 24700 | prompt processing progress, n_tokens = 8810, batch.n_tokens = 1474, progress = 0.992788
slot update_slots: id  0 | task 24700 | n_tokens = 8810, memory_seq_rm [8810, end)
slot update_slots: id  0 | task 24700 | prompt processing progress, n_tokens = 8874, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 24700 | prompt done, n_tokens = 8874, batch.n_tokens = 64
slot init_sampler: id  0 | task 24700 | init sampler, took 1.82 ms, tokens: text = 8874, total = 8874
slot update_slots: id  0 | task 24700 | created context checkpoint 3 of 8 (pos_min = 8042, pos_max = 8809, size = 18.009 MiB)
slot print_timing: id  0 | task 24700 | 
prompt eval time =    1834.58 ms /  1538 tokens (    1.19 ms per token,   838.34 tokens per second)
       eval time =   10360.82 ms /   390 tokens (   26.57 ms per token,    37.64 tokens per second)
      total time =   12195.40 ms /  1928 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 24700 | stop processing: n_tokens = 9263, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.854 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 25092 | processing task, is_child = 0
slot update_slots: id  0 | task 25092 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 10395
slot update_slots: id  0 | task 25092 | n_tokens = 8874, memory_seq_rm [8874, end)
slot update_slots: id  0 | task 25092 | prompt processing progress, n_tokens = 10331, batch.n_tokens = 1457, progress = 0.993843
slot update_slots: id  0 | task 25092 | n_tokens = 10331, memory_seq_rm [10331, end)
slot update_slots: id  0 | task 25092 | prompt processing progress, n_tokens = 10395, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 25092 | prompt done, n_tokens = 10395, batch.n_tokens = 64
slot init_sampler: id  0 | task 25092 | init sampler, took 2.02 ms, tokens: text = 10395, total = 10395
slot update_slots: id  0 | task 25092 | created context checkpoint 4 of 8 (pos_min = 9563, pos_max = 10330, size = 18.009 MiB)
slot print_timing: id  0 | task 25092 | 
prompt eval time =    1834.96 ms /  1521 tokens (    1.21 ms per token,   828.90 tokens per second)
       eval time =    6486.75 ms /   245 tokens (   26.48 ms per token,    37.77 tokens per second)
      total time =    8321.71 ms /  1766 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 25092 | stop processing: n_tokens = 10639, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.873 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 25339 | processing task, is_child = 0
slot update_slots: id  0 | task 25339 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 11910
slot update_slots: id  0 | task 25339 | n_tokens = 10395, memory_seq_rm [10395, end)
slot update_slots: id  0 | task 25339 | prompt processing progress, n_tokens = 11846, batch.n_tokens = 1451, progress = 0.994626
slot update_slots: id  0 | task 25339 | n_tokens = 11846, memory_seq_rm [11846, end)
slot update_slots: id  0 | task 25339 | prompt processing progress, n_tokens = 11910, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 25339 | prompt done, n_tokens = 11910, batch.n_tokens = 64
slot init_sampler: id  0 | task 25339 | init sampler, took 3.78 ms, tokens: text = 11910, total = 11910
slot update_slots: id  0 | task 25339 | created context checkpoint 5 of 8 (pos_min = 11078, pos_max = 11845, size = 18.009 MiB)
slot print_timing: id  0 | task 25339 | 
prompt eval time =    1882.75 ms /  1515 tokens (    1.24 ms per token,   804.67 tokens per second)
       eval time =    3829.53 ms /   147 tokens (   26.05 ms per token,    38.39 tokens per second)
      total time =    5712.28 ms /  1662 tokens
slot      release: id  0 | task 25339 | stop processing: n_tokens = 12056, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.884 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 25488 | processing task, is_child = 0
slot update_slots: id  0 | task 25488 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 13466
slot update_slots: id  0 | task 25488 | n_tokens = 11910, memory_seq_rm [11910, end)
slot update_slots: id  0 | task 25488 | prompt processing progress, n_tokens = 13402, batch.n_tokens = 1492, progress = 0.995247
slot update_slots: id  0 | task 25488 | n_tokens = 13402, memory_seq_rm [13402, end)
slot update_slots: id  0 | task 25488 | prompt processing progress, n_tokens = 13466, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 25488 | prompt done, n_tokens = 13466, batch.n_tokens = 64
slot init_sampler: id  0 | task 25488 | init sampler, took 2.60 ms, tokens: text = 13466, total = 13466
slot update_slots: id  0 | task 25488 | created context checkpoint 6 of 8 (pos_min = 12634, pos_max = 13401, size = 18.009 MiB)
slot print_timing: id  0 | task 25488 | 
prompt eval time =    1991.63 ms /  1556 tokens (    1.28 ms per token,   781.27 tokens per second)
       eval time =   13441.47 ms /   487 tokens (   27.60 ms per token,    36.23 tokens per second)
      total time =   15433.10 ms /  2043 tokens
slot      release: id  0 | task 25488 | stop processing: n_tokens = 13952, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.899 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 25977 | processing task, is_child = 0
slot update_slots: id  0 | task 25977 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 14984
slot update_slots: id  0 | task 25977 | n_tokens = 13466, memory_seq_rm [13466, end)
slot update_slots: id  0 | task 25977 | prompt processing progress, n_tokens = 14920, batch.n_tokens = 1454, progress = 0.995729
slot update_slots: id  0 | task 25977 | n_tokens = 14920, memory_seq_rm [14920, end)
slot update_slots: id  0 | task 25977 | prompt processing progress, n_tokens = 14984, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 25977 | prompt done, n_tokens = 14984, batch.n_tokens = 64
slot init_sampler: id  0 | task 25977 | init sampler, took 7.36 ms, tokens: text = 14984, total = 14984
slot update_slots: id  0 | task 25977 | created context checkpoint 7 of 8 (pos_min = 14152, pos_max = 14919, size = 18.009 MiB)
slot print_timing: id  0 | task 25977 | 
prompt eval time =    1947.40 ms /  1518 tokens (    1.28 ms per token,   779.50 tokens per second)
       eval time =    7573.50 ms /   279 tokens (   27.15 ms per token,    36.84 tokens per second)
      total time =    9520.90 ms /  1797 tokens
slot      release: id  0 | task 25977 | stop processing: n_tokens = 15262, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.907 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 26258 | processing task, is_child = 0
slot update_slots: id  0 | task 26258 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 16526
slot update_slots: id  0 | task 26258 | n_tokens = 14984, memory_seq_rm [14984, end)
slot update_slots: id  0 | task 26258 | prompt processing progress, n_tokens = 16462, batch.n_tokens = 1478, progress = 0.996127
slot update_slots: id  0 | task 26258 | n_tokens = 16462, memory_seq_rm [16462, end)
slot update_slots: id  0 | task 26258 | prompt processing progress, n_tokens = 16526, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 26258 | prompt done, n_tokens = 16526, batch.n_tokens = 64
slot init_sampler: id  0 | task 26258 | init sampler, took 5.88 ms, tokens: text = 16526, total = 16526
slot update_slots: id  0 | task 26258 | created context checkpoint 8 of 8 (pos_min = 15694, pos_max = 16461, size = 18.009 MiB)
slot print_timing: id  0 | task 26258 | 
prompt eval time =    2032.80 ms /  1542 tokens (    1.32 ms per token,   758.56 tokens per second)
       eval time =    6399.37 ms /   230 tokens (   27.82 ms per token,    35.94 tokens per second)
      total time =    8432.17 ms /  1772 tokens
slot      release: id  0 | task 26258 | stop processing: n_tokens = 16755, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.913 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 26490 | processing task, is_child = 0
slot update_slots: id  0 | task 26490 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 18098
slot update_slots: id  0 | task 26490 | n_tokens = 16526, memory_seq_rm [16526, end)
slot update_slots: id  0 | task 26490 | prompt processing progress, n_tokens = 18034, batch.n_tokens = 1508, progress = 0.996464
slot update_slots: id  0 | task 26490 | n_tokens = 18034, memory_seq_rm [18034, end)
slot update_slots: id  0 | task 26490 | prompt processing progress, n_tokens = 18098, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 26490 | prompt done, n_tokens = 18098, batch.n_tokens = 64
slot init_sampler: id  0 | task 26490 | init sampler, took 7.85 ms, tokens: text = 18098, total = 18098
slot update_slots: id  0 | task 26490 | erasing old context checkpoint (pos_min = 598, pos_max = 1332, size = 17.235 MiB)
slot update_slots: id  0 | task 26490 | created context checkpoint 8 of 8 (pos_min = 17266, pos_max = 18033, size = 18.009 MiB)
slot print_timing: id  0 | task 26490 | 
prompt eval time =    2102.10 ms /  1572 tokens (    1.34 ms per token,   747.82 tokens per second)
       eval time =   12207.10 ms /   438 tokens (   27.87 ms per token,    35.88 tokens per second)
      total time =   14309.21 ms /  2010 tokens
slot      release: id  0 | task 26490 | stop processing: n_tokens = 18535, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 26930 | processing task, is_child = 0
slot update_slots: id  0 | task 26930 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 18899
slot update_slots: id  0 | task 26930 | n_tokens = 18098, memory_seq_rm [18098, end)
slot update_slots: id  0 | task 26930 | prompt processing progress, n_tokens = 18835, batch.n_tokens = 737, progress = 0.996614
slot update_slots: id  0 | task 26930 | n_tokens = 18835, memory_seq_rm [18835, end)
slot update_slots: id  0 | task 26930 | prompt processing progress, n_tokens = 18899, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 26930 | prompt done, n_tokens = 18899, batch.n_tokens = 64
slot init_sampler: id  0 | task 26930 | init sampler, took 3.74 ms, tokens: text = 18899, total = 18899
slot update_slots: id  0 | task 26930 | erasing old context checkpoint (pos_min = 6504, pos_max = 7271, size = 18.009 MiB)
slot update_slots: id  0 | task 26930 | created context checkpoint 8 of 8 (pos_min = 18067, pos_max = 18834, size = 18.009 MiB)
slot print_timing: id  0 | task 26930 | 
prompt eval time =    1319.03 ms /   801 tokens (    1.65 ms per token,   607.26 tokens per second)
       eval time =   12243.29 ms /   430 tokens (   28.47 ms per token,    35.12 tokens per second)
      total time =   13562.33 ms /  1231 tokens
slot      release: id  0 | task 26930 | stop processing: n_tokens = 19328, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 27362 | processing task, is_child = 0
slot update_slots: id  0 | task 27362 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 19136
slot update_slots: id  0 | task 27362 | n_tokens = 18899, memory_seq_rm [18899, end)
slot update_slots: id  0 | task 27362 | prompt processing progress, n_tokens = 19072, batch.n_tokens = 173, progress = 0.996656
slot update_slots: id  0 | task 27362 | n_tokens = 19072, memory_seq_rm [19072, end)
slot update_slots: id  0 | task 27362 | prompt processing progress, n_tokens = 19136, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 27362 | prompt done, n_tokens = 19136, batch.n_tokens = 64
slot init_sampler: id  0 | task 27362 | init sampler, took 5.31 ms, tokens: text = 19136, total = 19136
slot update_slots: id  0 | task 27362 | erasing old context checkpoint (pos_min = 8042, pos_max = 8809, size = 18.009 MiB)
slot update_slots: id  0 | task 27362 | created context checkpoint 8 of 8 (pos_min = 18560, pos_max = 19071, size = 12.006 MiB)
slot print_timing: id  0 | task 27362 | 
prompt eval time =     617.10 ms /   237 tokens (    2.60 ms per token,   384.05 tokens per second)
       eval time =   19949.84 ms /   695 tokens (   28.70 ms per token,    34.84 tokens per second)
      total time =   20566.94 ms /   932 tokens
slot      release: id  0 | task 27362 | stop processing: n_tokens = 19830, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.926 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28059 | processing task, is_child = 0
slot update_slots: id  0 | task 28059 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 20665
slot update_slots: id  0 | task 28059 | n_past = 19136, slot.prompt.tokens.size() = 19830, seq_id = 0, pos_min = 19062, n_swa = 128
slot update_slots: id  0 | task 28059 | restored context checkpoint (pos_min = 18560, pos_max = 19071, size = 12.006 MiB)
slot update_slots: id  0 | task 28059 | n_tokens = 19071, memory_seq_rm [19071, end)
slot update_slots: id  0 | task 28059 | prompt processing progress, n_tokens = 20601, batch.n_tokens = 1530, progress = 0.996903
slot update_slots: id  0 | task 28059 | n_tokens = 20601, memory_seq_rm [20601, end)
slot update_slots: id  0 | task 28059 | prompt processing progress, n_tokens = 20665, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28059 | prompt done, n_tokens = 20665, batch.n_tokens = 64
slot init_sampler: id  0 | task 28059 | init sampler, took 3.91 ms, tokens: text = 20665, total = 20665
slot update_slots: id  0 | task 28059 | erasing old context checkpoint (pos_min = 9563, pos_max = 10330, size = 18.009 MiB)
slot update_slots: id  0 | task 28059 | created context checkpoint 8 of 8 (pos_min = 19833, pos_max = 20600, size = 18.009 MiB)
slot print_timing: id  0 | task 28059 | 
prompt eval time =    2350.34 ms /  1594 tokens (    1.47 ms per token,   678.20 tokens per second)
       eval time =    8304.87 ms /   277 tokens (   29.98 ms per token,    33.35 tokens per second)
      total time =   10655.22 ms /  1871 tokens
slot      release: id  0 | task 28059 | stop processing: n_tokens = 20941, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28338 | processing task, is_child = 0
slot update_slots: id  0 | task 28338 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 22176
slot update_slots: id  0 | task 28338 | n_tokens = 20665, memory_seq_rm [20665, end)
slot update_slots: id  0 | task 28338 | prompt processing progress, n_tokens = 22112, batch.n_tokens = 1447, progress = 0.997114
slot update_slots: id  0 | task 28338 | n_tokens = 22112, memory_seq_rm [22112, end)
slot update_slots: id  0 | task 28338 | prompt processing progress, n_tokens = 22176, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28338 | prompt done, n_tokens = 22176, batch.n_tokens = 64
slot init_sampler: id  0 | task 28338 | init sampler, took 5.26 ms, tokens: text = 22176, total = 22176
slot update_slots: id  0 | task 28338 | erasing old context checkpoint (pos_min = 11078, pos_max = 11845, size = 18.009 MiB)
slot update_slots: id  0 | task 28338 | created context checkpoint 8 of 8 (pos_min = 21344, pos_max = 22111, size = 18.009 MiB)
slot print_timing: id  0 | task 28338 | 
prompt eval time =    2330.87 ms /  1511 tokens (    1.54 ms per token,   648.25 tokens per second)
       eval time =   15010.25 ms /   485 tokens (   30.95 ms per token,    32.31 tokens per second)
      total time =   17341.12 ms /  1996 tokens
slot      release: id  0 | task 28338 | stop processing: n_tokens = 22660, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28825 | processing task, is_child = 0
slot update_slots: id  0 | task 28825 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 23004
slot update_slots: id  0 | task 28825 | n_tokens = 22177, memory_seq_rm [22177, end)
slot update_slots: id  0 | task 28825 | prompt processing progress, n_tokens = 22940, batch.n_tokens = 763, progress = 0.997218
slot update_slots: id  0 | task 28825 | n_tokens = 22940, memory_seq_rm [22940, end)
slot update_slots: id  0 | task 28825 | prompt processing progress, n_tokens = 23004, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28825 | prompt done, n_tokens = 23004, batch.n_tokens = 64
slot init_sampler: id  0 | task 28825 | init sampler, took 13.61 ms, tokens: text = 23004, total = 23004
slot update_slots: id  0 | task 28825 | erasing old context checkpoint (pos_min = 12634, pos_max = 13401, size = 18.009 MiB)
slot update_slots: id  0 | task 28825 | created context checkpoint 8 of 8 (pos_min = 22299, pos_max = 22939, size = 15.031 MiB)
slot print_timing: id  0 | task 28825 | 
prompt eval time =    1573.30 ms /   827 tokens (    1.90 ms per token,   525.65 tokens per second)
       eval time =    1844.65 ms /    55 tokens (   33.54 ms per token,    29.82 tokens per second)
      total time =    3417.95 ms /   882 tokens
slot      release: id  0 | task 28825 | stop processing: n_tokens = 23058, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28882 | processing task, is_child = 0
slot update_slots: id  0 | task 28882 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 23285
slot update_slots: id  0 | task 28882 | n_tokens = 23004, memory_seq_rm [23004, end)
slot update_slots: id  0 | task 28882 | prompt processing progress, n_tokens = 23221, batch.n_tokens = 217, progress = 0.997251
slot update_slots: id  0 | task 28882 | n_tokens = 23221, memory_seq_rm [23221, end)
slot update_slots: id  0 | task 28882 | prompt processing progress, n_tokens = 23285, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28882 | prompt done, n_tokens = 23285, batch.n_tokens = 64
slot init_sampler: id  0 | task 28882 | init sampler, took 8.86 ms, tokens: text = 23285, total = 23285
slot update_slots: id  0 | task 28882 | erasing old context checkpoint (pos_min = 14152, pos_max = 14919, size = 18.009 MiB)
slot update_slots: id  0 | task 28882 | created context checkpoint 8 of 8 (pos_min = 22453, pos_max = 23220, size = 18.009 MiB)
slot print_timing: id  0 | task 28882 | 
prompt eval time =     824.43 ms /   281 tokens (    2.93 ms per token,   340.84 tokens per second)
       eval time =    1806.79 ms /    56 tokens (   32.26 ms per token,    30.99 tokens per second)
      total time =    2631.22 ms /   337 tokens
slot      release: id  0 | task 28882 | stop processing: n_tokens = 23340, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28940 | processing task, is_child = 0
slot update_slots: id  0 | task 28940 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 23603
slot update_slots: id  0 | task 28940 | n_tokens = 23285, memory_seq_rm [23285, end)
slot update_slots: id  0 | task 28940 | prompt processing progress, n_tokens = 23539, batch.n_tokens = 254, progress = 0.997288
slot update_slots: id  0 | task 28940 | n_tokens = 23539, memory_seq_rm [23539, end)
slot update_slots: id  0 | task 28940 | prompt processing progress, n_tokens = 23603, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28940 | prompt done, n_tokens = 23603, batch.n_tokens = 64
slot init_sampler: id  0 | task 28940 | init sampler, took 13.88 ms, tokens: text = 23603, total = 23603
slot update_slots: id  0 | task 28940 | erasing old context checkpoint (pos_min = 15694, pos_max = 16461, size = 18.009 MiB)
slot update_slots: id  0 | task 28940 | created context checkpoint 8 of 8 (pos_min = 22771, pos_max = 23538, size = 18.009 MiB)
slot print_timing: id  0 | task 28940 | 
prompt eval time =     761.51 ms /   318 tokens (    2.39 ms per token,   417.59 tokens per second)
       eval time =    1130.82 ms /    38 tokens (   29.76 ms per token,    33.60 tokens per second)
      total time =    1892.33 ms /   356 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 28940 | stop processing: n_tokens = 23640, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28980 | processing task, is_child = 0
slot update_slots: id  0 | task 28980 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 24026
slot update_slots: id  0 | task 28980 | n_tokens = 23603, memory_seq_rm [23603, end)
slot update_slots: id  0 | task 28980 | prompt processing progress, n_tokens = 23962, batch.n_tokens = 359, progress = 0.997336
slot update_slots: id  0 | task 28980 | n_tokens = 23962, memory_seq_rm [23962, end)
slot update_slots: id  0 | task 28980 | prompt processing progress, n_tokens = 24026, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28980 | prompt done, n_tokens = 24026, batch.n_tokens = 64
slot init_sampler: id  0 | task 28980 | init sampler, took 4.72 ms, tokens: text = 24026, total = 24026
slot update_slots: id  0 | task 28980 | erasing old context checkpoint (pos_min = 17266, pos_max = 18033, size = 18.009 MiB)
slot update_slots: id  0 | task 28980 | created context checkpoint 8 of 8 (pos_min = 23194, pos_max = 23961, size = 18.009 MiB)
slot print_timing: id  0 | task 28980 | 
prompt eval time =     832.13 ms /   423 tokens (    1.97 ms per token,   508.33 tokens per second)
       eval time =    1937.76 ms /    68 tokens (   28.50 ms per token,    35.09 tokens per second)
      total time =    2769.89 ms /   491 tokens
slot      release: id  0 | task 28980 | stop processing: n_tokens = 24093, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29050 | processing task, is_child = 0
slot update_slots: id  0 | task 29050 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 24299
slot update_slots: id  0 | task 29050 | n_tokens = 24026, memory_seq_rm [24026, end)
slot update_slots: id  0 | task 29050 | prompt processing progress, n_tokens = 24235, batch.n_tokens = 209, progress = 0.997366
slot update_slots: id  0 | task 29050 | n_tokens = 24235, memory_seq_rm [24235, end)
slot update_slots: id  0 | task 29050 | prompt processing progress, n_tokens = 24299, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29050 | prompt done, n_tokens = 24299, batch.n_tokens = 64
slot init_sampler: id  0 | task 29050 | init sampler, took 7.08 ms, tokens: text = 24299, total = 24299
slot update_slots: id  0 | task 29050 | erasing old context checkpoint (pos_min = 18067, pos_max = 18834, size = 18.009 MiB)
slot update_slots: id  0 | task 29050 | created context checkpoint 8 of 8 (pos_min = 23467, pos_max = 24234, size = 18.009 MiB)
slot print_timing: id  0 | task 29050 | 
prompt eval time =     687.54 ms /   273 tokens (    2.52 ms per token,   397.07 tokens per second)
       eval time =    1074.87 ms /    38 tokens (   28.29 ms per token,    35.35 tokens per second)
      total time =    1762.41 ms /   311 tokens
slot      release: id  0 | task 29050 | stop processing: n_tokens = 24336, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29090 | processing task, is_child = 0
slot update_slots: id  0 | task 29090 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 24604
slot update_slots: id  0 | task 29090 | n_tokens = 24299, memory_seq_rm [24299, end)
slot update_slots: id  0 | task 29090 | prompt processing progress, n_tokens = 24540, batch.n_tokens = 241, progress = 0.997399
slot update_slots: id  0 | task 29090 | n_tokens = 24540, memory_seq_rm [24540, end)
slot update_slots: id  0 | task 29090 | prompt processing progress, n_tokens = 24604, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29090 | prompt done, n_tokens = 24604, batch.n_tokens = 64
slot init_sampler: id  0 | task 29090 | init sampler, took 9.34 ms, tokens: text = 24604, total = 24604
slot update_slots: id  0 | task 29090 | erasing old context checkpoint (pos_min = 18560, pos_max = 19071, size = 12.006 MiB)
slot update_slots: id  0 | task 29090 | created context checkpoint 8 of 8 (pos_min = 23772, pos_max = 24539, size = 18.009 MiB)
slot print_timing: id  0 | task 29090 | 
prompt eval time =     773.11 ms /   305 tokens (    2.53 ms per token,   394.51 tokens per second)
       eval time =    1781.45 ms /    62 tokens (   28.73 ms per token,    34.80 tokens per second)
      total time =    2554.57 ms /   367 tokens
slot      release: id  0 | task 29090 | stop processing: n_tokens = 24665, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29154 | processing task, is_child = 0
slot update_slots: id  0 | task 29154 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 25063
slot update_slots: id  0 | task 29154 | n_tokens = 24605, memory_seq_rm [24605, end)
slot update_slots: id  0 | task 29154 | prompt processing progress, n_tokens = 24999, batch.n_tokens = 394, progress = 0.997446
slot update_slots: id  0 | task 29154 | n_tokens = 24999, memory_seq_rm [24999, end)
slot update_slots: id  0 | task 29154 | prompt processing progress, n_tokens = 25063, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29154 | prompt done, n_tokens = 25063, batch.n_tokens = 64
slot init_sampler: id  0 | task 29154 | init sampler, took 10.31 ms, tokens: text = 25063, total = 25063
slot update_slots: id  0 | task 29154 | erasing old context checkpoint (pos_min = 19833, pos_max = 20600, size = 18.009 MiB)
slot update_slots: id  0 | task 29154 | created context checkpoint 8 of 8 (pos_min = 24231, pos_max = 24998, size = 18.009 MiB)
slot print_timing: id  0 | task 29154 | 
prompt eval time =     946.38 ms /   458 tokens (    2.07 ms per token,   483.95 tokens per second)
       eval time =    5503.78 ms /   196 tokens (   28.08 ms per token,    35.61 tokens per second)
      total time =    6450.16 ms /   654 tokens
slot      release: id  0 | task 29154 | stop processing: n_tokens = 25258, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29352 | processing task, is_child = 0
slot update_slots: id  0 | task 29352 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 25159
slot update_slots: id  0 | task 29352 | n_tokens = 25063, memory_seq_rm [25063, end)
slot update_slots: id  0 | task 29352 | prompt processing progress, n_tokens = 25095, batch.n_tokens = 32, progress = 0.997456
slot update_slots: id  0 | task 29352 | n_tokens = 25095, memory_seq_rm [25095, end)
slot update_slots: id  0 | task 29352 | prompt processing progress, n_tokens = 25159, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29352 | prompt done, n_tokens = 25159, batch.n_tokens = 64
slot init_sampler: id  0 | task 29352 | init sampler, took 5.28 ms, tokens: text = 25159, total = 25159
slot update_slots: id  0 | task 29352 | erasing old context checkpoint (pos_min = 21344, pos_max = 22111, size = 18.009 MiB)
slot update_slots: id  0 | task 29352 | created context checkpoint 8 of 8 (pos_min = 24490, pos_max = 25094, size = 14.187 MiB)
slot print_timing: id  0 | task 29352 | 
prompt eval time =     450.09 ms /    96 tokens (    4.69 ms per token,   213.29 tokens per second)
       eval time =    4919.71 ms /   176 tokens (   27.95 ms per token,    35.77 tokens per second)
      total time =    5369.81 ms /   272 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 29352 | stop processing: n_tokens = 25334, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29530 | processing task, is_child = 0
slot update_slots: id  0 | task 29530 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 25885
slot update_slots: id  0 | task 29530 | n_tokens = 25159, memory_seq_rm [25159, end)
slot update_slots: id  0 | task 29530 | prompt processing progress, n_tokens = 25821, batch.n_tokens = 662, progress = 0.997528
slot update_slots: id  0 | task 29530 | n_tokens = 25821, memory_seq_rm [25821, end)
slot update_slots: id  0 | task 29530 | prompt processing progress, n_tokens = 25885, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29530 | prompt done, n_tokens = 25885, batch.n_tokens = 64
slot init_sampler: id  0 | task 29530 | init sampler, took 4.91 ms, tokens: text = 25885, total = 25885
slot update_slots: id  0 | task 29530 | erasing old context checkpoint (pos_min = 22299, pos_max = 22939, size = 15.031 MiB)
slot update_slots: id  0 | task 29530 | created context checkpoint 8 of 8 (pos_min = 25063, pos_max = 25820, size = 17.775 MiB)
slot print_timing: id  0 | task 29530 | 
prompt eval time =    1270.88 ms /   726 tokens (    1.75 ms per token,   571.26 tokens per second)
       eval time =    2707.04 ms /    96 tokens (   28.20 ms per token,    35.46 tokens per second)
      total time =    3977.92 ms /   822 tokens
slot      release: id  0 | task 29530 | stop processing: n_tokens = 25980, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29628 | processing task, is_child = 0
slot update_slots: id  0 | task 29628 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 26556
slot update_slots: id  0 | task 29628 | n_tokens = 25885, memory_seq_rm [25885, end)
slot update_slots: id  0 | task 29628 | prompt processing progress, n_tokens = 26492, batch.n_tokens = 607, progress = 0.997590
slot update_slots: id  0 | task 29628 | n_tokens = 26492, memory_seq_rm [26492, end)
slot update_slots: id  0 | task 29628 | prompt processing progress, n_tokens = 26556, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29628 | prompt done, n_tokens = 26556, batch.n_tokens = 64
slot init_sampler: id  0 | task 29628 | init sampler, took 8.70 ms, tokens: text = 26556, total = 26556
slot update_slots: id  0 | task 29628 | erasing old context checkpoint (pos_min = 22453, pos_max = 23220, size = 18.009 MiB)
slot update_slots: id  0 | task 29628 | created context checkpoint 8 of 8 (pos_min = 25724, pos_max = 26491, size = 18.009 MiB)
slot print_timing: id  0 | task 29628 | 
prompt eval time =    1261.15 ms /   671 tokens (    1.88 ms per token,   532.05 tokens per second)
       eval time =    5106.47 ms /   180 tokens (   28.37 ms per token,    35.25 tokens per second)
      total time =    6367.62 ms /   851 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 29628 | stop processing: n_tokens = 26735, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29810 | processing task, is_child = 0
slot update_slots: id  0 | task 29810 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 26685
slot update_slots: id  0 | task 29810 | n_tokens = 26556, memory_seq_rm [26556, end)
slot update_slots: id  0 | task 29810 | prompt processing progress, n_tokens = 26621, batch.n_tokens = 65, progress = 0.997602
slot update_slots: id  0 | task 29810 | n_tokens = 26621, memory_seq_rm [26621, end)
slot update_slots: id  0 | task 29810 | prompt processing progress, n_tokens = 26685, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29810 | prompt done, n_tokens = 26685, batch.n_tokens = 64
slot init_sampler: id  0 | task 29810 | init sampler, took 5.39 ms, tokens: text = 26685, total = 26685
slot update_slots: id  0 | task 29810 | erasing old context checkpoint (pos_min = 22771, pos_max = 23538, size = 18.009 MiB)
slot update_slots: id  0 | task 29810 | created context checkpoint 8 of 8 (pos_min = 25967, pos_max = 26620, size = 15.336 MiB)
slot print_timing: id  0 | task 29810 | 
prompt eval time =     573.83 ms /   129 tokens (    4.45 ms per token,   224.80 tokens per second)
       eval time =   11248.65 ms /   394 tokens (   28.55 ms per token,    35.03 tokens per second)
      total time =   11822.48 ms /   523 tokens
slot      release: id  0 | task 29810 | stop processing: n_tokens = 27078, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 30206 | processing task, is_child = 0
slot update_slots: id  0 | task 30206 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 28257
slot update_slots: id  0 | task 30206 | n_tokens = 26685, memory_seq_rm [26685, end)
slot update_slots: id  0 | task 30206 | prompt processing progress, n_tokens = 28193, batch.n_tokens = 1508, progress = 0.997735
slot update_slots: id  0 | task 30206 | n_tokens = 28193, memory_seq_rm [28193, end)
slot update_slots: id  0 | task 30206 | prompt processing progress, n_tokens = 28257, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 30206 | prompt done, n_tokens = 28257, batch.n_tokens = 64
slot init_sampler: id  0 | task 30206 | init sampler, took 5.71 ms, tokens: text = 28257, total = 28257
slot update_slots: id  0 | task 30206 | erasing old context checkpoint (pos_min = 23194, pos_max = 23961, size = 18.009 MiB)
slot update_slots: id  0 | task 30206 | created context checkpoint 8 of 8 (pos_min = 27425, pos_max = 28192, size = 18.009 MiB)
slot print_timing: id  0 | task 30206 | 
prompt eval time =    2428.24 ms /  1572 tokens (    1.54 ms per token,   647.38 tokens per second)
       eval time =   19348.22 ms /   665 tokens (   29.10 ms per token,    34.37 tokens per second)
      total time =   21776.46 ms /  2237 tokens
slot      release: id  0 | task 30206 | stop processing: n_tokens = 28921, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 30873 | processing task, is_child = 0
slot update_slots: id  0 | task 30873 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 29790
slot update_slots: id  0 | task 30873 | n_past = 28257, slot.prompt.tokens.size() = 28921, seq_id = 0, pos_min = 28153, n_swa = 128
slot update_slots: id  0 | task 30873 | restored context checkpoint (pos_min = 27425, pos_max = 28192, size = 18.009 MiB)
slot update_slots: id  0 | task 30873 | n_tokens = 28192, memory_seq_rm [28192, end)
slot update_slots: id  0 | task 30873 | prompt processing progress, n_tokens = 29726, batch.n_tokens = 1534, progress = 0.997852
slot update_slots: id  0 | task 30873 | n_tokens = 29726, memory_seq_rm [29726, end)
slot update_slots: id  0 | task 30873 | prompt processing progress, n_tokens = 29790, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 30873 | prompt done, n_tokens = 29790, batch.n_tokens = 64
slot init_sampler: id  0 | task 30873 | init sampler, took 12.85 ms, tokens: text = 29790, total = 29790
slot update_slots: id  0 | task 30873 | erasing old context checkpoint (pos_min = 23467, pos_max = 24234, size = 18.009 MiB)
slot update_slots: id  0 | task 30873 | created context checkpoint 8 of 8 (pos_min = 28958, pos_max = 29725, size = 18.009 MiB)
slot print_timing: id  0 | task 30873 | 
prompt eval time =    2478.72 ms /  1598 tokens (    1.55 ms per token,   644.69 tokens per second)
       eval time =   28258.47 ms /   937 tokens (   30.16 ms per token,    33.16 tokens per second)
      total time =   30737.19 ms /  2535 tokens
slot      release: id  0 | task 30873 | stop processing: n_tokens = 30726, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31812 | processing task, is_child = 0
slot update_slots: id  0 | task 31812 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 29920
slot update_slots: id  0 | task 31812 | n_past = 29790, slot.prompt.tokens.size() = 30726, seq_id = 0, pos_min = 29958, n_swa = 128
slot update_slots: id  0 | task 31812 | restored context checkpoint (pos_min = 28958, pos_max = 29725, size = 18.009 MiB)
slot update_slots: id  0 | task 31812 | n_tokens = 29725, memory_seq_rm [29725, end)
slot update_slots: id  0 | task 31812 | prompt processing progress, n_tokens = 29856, batch.n_tokens = 131, progress = 0.997861
slot update_slots: id  0 | task 31812 | n_tokens = 29856, memory_seq_rm [29856, end)
slot update_slots: id  0 | task 31812 | prompt processing progress, n_tokens = 29920, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31812 | prompt done, n_tokens = 29920, batch.n_tokens = 64
slot init_sampler: id  0 | task 31812 | init sampler, took 5.83 ms, tokens: text = 29920, total = 29920
slot update_slots: id  0 | task 31812 | erasing old context checkpoint (pos_min = 23772, pos_max = 24539, size = 18.009 MiB)
slot update_slots: id  0 | task 31812 | created context checkpoint 8 of 8 (pos_min = 29088, pos_max = 29855, size = 18.009 MiB)
slot print_timing: id  0 | task 31812 | 
prompt eval time =     637.14 ms /   195 tokens (    3.27 ms per token,   306.05 tokens per second)
       eval time =    9814.21 ms /   339 tokens (   28.95 ms per token,    34.54 tokens per second)
      total time =   10451.35 ms /   534 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 31812 | stop processing: n_tokens = 30258, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 32153 | processing task, is_child = 0
slot update_slots: id  0 | task 32153 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 30646
slot update_slots: id  0 | task 32153 | n_tokens = 29920, memory_seq_rm [29920, end)
slot update_slots: id  0 | task 32153 | prompt processing progress, n_tokens = 30582, batch.n_tokens = 662, progress = 0.997912
slot update_slots: id  0 | task 32153 | n_tokens = 30582, memory_seq_rm [30582, end)
slot update_slots: id  0 | task 32153 | prompt processing progress, n_tokens = 30646, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 32153 | prompt done, n_tokens = 30646, batch.n_tokens = 64
slot init_sampler: id  0 | task 32153 | init sampler, took 12.09 ms, tokens: text = 30646, total = 30646
slot update_slots: id  0 | task 32153 | erasing old context checkpoint (pos_min = 24231, pos_max = 24998, size = 18.009 MiB)
slot update_slots: id  0 | task 32153 | created context checkpoint 8 of 8 (pos_min = 29814, pos_max = 30581, size = 18.009 MiB)
slot print_timing: id  0 | task 32153 | 
prompt eval time =    1418.04 ms /   726 tokens (    1.95 ms per token,   511.97 tokens per second)
       eval time =    4848.01 ms /   153 tokens (   31.69 ms per token,    31.56 tokens per second)
      total time =    6266.05 ms /   879 tokens
slot      release: id  0 | task 32153 | stop processing: n_tokens = 30798, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 32308 | processing task, is_child = 0
slot update_slots: id  0 | task 32308 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 30763
slot update_slots: id  0 | task 32308 | n_tokens = 30646, memory_seq_rm [30646, end)
slot update_slots: id  0 | task 32308 | prompt processing progress, n_tokens = 30699, batch.n_tokens = 53, progress = 0.997920
slot update_slots: id  0 | task 32308 | n_tokens = 30699, memory_seq_rm [30699, end)
slot update_slots: id  0 | task 32308 | prompt processing progress, n_tokens = 30763, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 32308 | prompt done, n_tokens = 30763, batch.n_tokens = 64
slot init_sampler: id  0 | task 32308 | init sampler, took 6.46 ms, tokens: text = 30763, total = 30763
slot update_slots: id  0 | task 32308 | erasing old context checkpoint (pos_min = 24490, pos_max = 25094, size = 14.187 MiB)
slot update_slots: id  0 | task 32308 | created context checkpoint 8 of 8 (pos_min = 30030, pos_max = 30698, size = 15.688 MiB)
slot print_timing: id  0 | task 32308 | 
prompt eval time =     501.33 ms /   117 tokens (    4.28 ms per token,   233.38 tokens per second)
       eval time =   20297.42 ms /   669 tokens (   30.34 ms per token,    32.96 tokens per second)
      total time =   20798.76 ms /   786 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 32308 | stop processing: n_tokens = 31431, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 32979 | processing task, is_child = 0
slot update_slots: id  0 | task 32979 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 32278
slot update_slots: id  0 | task 32979 | n_past = 30763, slot.prompt.tokens.size() = 31431, seq_id = 0, pos_min = 30663, n_swa = 128
slot update_slots: id  0 | task 32979 | restored context checkpoint (pos_min = 30030, pos_max = 30698, size = 15.688 MiB)
slot update_slots: id  0 | task 32979 | n_tokens = 30698, memory_seq_rm [30698, end)
slot update_slots: id  0 | task 32979 | prompt processing progress, n_tokens = 32214, batch.n_tokens = 1516, progress = 0.998017
slot update_slots: id  0 | task 32979 | n_tokens = 32214, memory_seq_rm [32214, end)
slot update_slots: id  0 | task 32979 | prompt processing progress, n_tokens = 32278, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 32979 | prompt done, n_tokens = 32278, batch.n_tokens = 64
slot init_sampler: id  0 | task 32979 | init sampler, took 6.56 ms, tokens: text = 32278, total = 32278
slot update_slots: id  0 | task 32979 | erasing old context checkpoint (pos_min = 25063, pos_max = 25820, size = 17.775 MiB)
slot update_slots: id  0 | task 32979 | created context checkpoint 8 of 8 (pos_min = 31446, pos_max = 32213, size = 18.009 MiB)
slot print_timing: id  0 | task 32979 | 
prompt eval time =    2603.29 ms /  1580 tokens (    1.65 ms per token,   606.92 tokens per second)
       eval time =    6378.67 ms /   216 tokens (   29.53 ms per token,    33.86 tokens per second)
      total time =    8981.97 ms /  1796 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 32979 | stop processing: n_tokens = 32493, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.956 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 33197 | processing task, is_child = 0
slot update_slots: id  0 | task 33197 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 33771
slot update_slots: id  0 | task 33197 | n_tokens = 32278, memory_seq_rm [32278, end)
slot update_slots: id  0 | task 33197 | prompt processing progress, n_tokens = 33707, batch.n_tokens = 1429, progress = 0.998105
slot update_slots: id  0 | task 33197 | n_tokens = 33707, memory_seq_rm [33707, end)
slot update_slots: id  0 | task 33197 | prompt processing progress, n_tokens = 33771, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 33197 | prompt done, n_tokens = 33771, batch.n_tokens = 64
slot init_sampler: id  0 | task 33197 | init sampler, took 6.53 ms, tokens: text = 33771, total = 33771
slot update_slots: id  0 | task 33197 | erasing old context checkpoint (pos_min = 25724, pos_max = 26491, size = 18.009 MiB)
slot update_slots: id  0 | task 33197 | created context checkpoint 8 of 8 (pos_min = 32939, pos_max = 33706, size = 18.009 MiB)
slot print_timing: id  0 | task 33197 | 
prompt eval time =    2471.36 ms /  1493 tokens (    1.66 ms per token,   604.12 tokens per second)
       eval time =   15078.74 ms /   501 tokens (   30.10 ms per token,    33.23 tokens per second)
      total time =   17550.10 ms /  1994 tokens
slot      release: id  0 | task 33197 | stop processing: n_tokens = 34271, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 33700 | processing task, is_child = 0
slot update_slots: id  0 | task 33700 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 33872
slot update_slots: id  0 | task 33700 | n_tokens = 33771, memory_seq_rm [33771, end)
slot update_slots: id  0 | task 33700 | prompt processing progress, n_tokens = 33808, batch.n_tokens = 37, progress = 0.998111
slot update_slots: id  0 | task 33700 | n_tokens = 33808, memory_seq_rm [33808, end)
slot update_slots: id  0 | task 33700 | prompt processing progress, n_tokens = 33872, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 33700 | prompt done, n_tokens = 33872, batch.n_tokens = 64
slot init_sampler: id  0 | task 33700 | init sampler, took 12.53 ms, tokens: text = 33872, total = 33872
slot update_slots: id  0 | task 33700 | erasing old context checkpoint (pos_min = 25967, pos_max = 26620, size = 15.336 MiB)
slot update_slots: id  0 | task 33700 | created context checkpoint 8 of 8 (pos_min = 33505, pos_max = 33807, size = 7.105 MiB)
slot print_timing: id  0 | task 33700 | 
prompt eval time =     463.74 ms /   101 tokens (    4.59 ms per token,   217.79 tokens per second)
       eval time =    5479.61 ms /   175 tokens (   31.31 ms per token,    31.94 tokens per second)
      total time =    5943.35 ms /   276 tokens
slot      release: id  0 | task 33700 | stop processing: n_tokens = 34046, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 33877 | processing task, is_child = 0
slot update_slots: id  0 | task 33877 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 34307
slot update_slots: id  0 | task 33877 | n_tokens = 33872, memory_seq_rm [33872, end)
slot update_slots: id  0 | task 33877 | prompt processing progress, n_tokens = 34243, batch.n_tokens = 371, progress = 0.998134
slot update_slots: id  0 | task 33877 | n_tokens = 34243, memory_seq_rm [34243, end)
slot update_slots: id  0 | task 33877 | prompt processing progress, n_tokens = 34307, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 33877 | prompt done, n_tokens = 34307, batch.n_tokens = 64
slot init_sampler: id  0 | task 33877 | init sampler, took 11.53 ms, tokens: text = 34307, total = 34307
slot update_slots: id  0 | task 33877 | erasing old context checkpoint (pos_min = 27425, pos_max = 28192, size = 18.009 MiB)
slot update_slots: id  0 | task 33877 | created context checkpoint 8 of 8 (pos_min = 33745, pos_max = 34242, size = 11.678 MiB)
slot print_timing: id  0 | task 33877 | 
prompt eval time =     970.55 ms /   435 tokens (    2.23 ms per token,   448.20 tokens per second)
       eval time =    6500.87 ms /   207 tokens (   31.41 ms per token,    31.84 tokens per second)
      total time =    7471.43 ms /   642 tokens
slot      release: id  0 | task 33877 | stop processing: n_tokens = 34513, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34086 | processing task, is_child = 0
slot update_slots: id  0 | task 34086 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 35033
slot update_slots: id  0 | task 34086 | n_tokens = 34307, memory_seq_rm [34307, end)
slot update_slots: id  0 | task 34086 | prompt processing progress, n_tokens = 34969, batch.n_tokens = 662, progress = 0.998173
slot update_slots: id  0 | task 34086 | n_tokens = 34969, memory_seq_rm [34969, end)
slot update_slots: id  0 | task 34086 | prompt processing progress, n_tokens = 35033, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34086 | prompt done, n_tokens = 35033, batch.n_tokens = 64
slot init_sampler: id  0 | task 34086 | init sampler, took 9.20 ms, tokens: text = 35033, total = 35033
slot update_slots: id  0 | task 34086 | erasing old context checkpoint (pos_min = 28958, pos_max = 29725, size = 18.009 MiB)
slot update_slots: id  0 | task 34086 | created context checkpoint 8 of 8 (pos_min = 34201, pos_max = 34968, size = 18.009 MiB)
slot print_timing: id  0 | task 34086 | 
prompt eval time =    1449.06 ms /   726 tokens (    2.00 ms per token,   501.01 tokens per second)
       eval time =   11855.72 ms /   390 tokens (   30.40 ms per token,    32.90 tokens per second)
      total time =   13304.78 ms /  1116 tokens
slot      release: id  0 | task 34086 | stop processing: n_tokens = 35422, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34478 | processing task, is_child = 0
slot update_slots: id  0 | task 34478 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 35759
slot update_slots: id  0 | task 34478 | n_tokens = 35033, memory_seq_rm [35033, end)
slot update_slots: id  0 | task 34478 | prompt processing progress, n_tokens = 35695, batch.n_tokens = 662, progress = 0.998210
slot update_slots: id  0 | task 34478 | n_tokens = 35695, memory_seq_rm [35695, end)
slot update_slots: id  0 | task 34478 | prompt processing progress, n_tokens = 35759, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34478 | prompt done, n_tokens = 35759, batch.n_tokens = 64
slot init_sampler: id  0 | task 34478 | init sampler, took 7.49 ms, tokens: text = 35759, total = 35759
slot update_slots: id  0 | task 34478 | erasing old context checkpoint (pos_min = 29088, pos_max = 29855, size = 18.009 MiB)
slot update_slots: id  0 | task 34478 | created context checkpoint 8 of 8 (pos_min = 35054, pos_max = 35694, size = 15.031 MiB)
slot print_timing: id  0 | task 34478 | 
prompt eval time =    1468.01 ms /   726 tokens (    2.02 ms per token,   494.55 tokens per second)
       eval time =    4845.02 ms /   150 tokens (   32.30 ms per token,    30.96 tokens per second)
      total time =    6313.03 ms /   876 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 34478 | stop processing: n_tokens = 35908, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 34630 | processing task, is_child = 0
slot update_slots: id  0 | task 34630 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 36467
slot update_slots: id  0 | task 34630 | n_tokens = 35759, memory_seq_rm [35759, end)
slot update_slots: id  0 | task 34630 | prompt processing progress, n_tokens = 36403, batch.n_tokens = 644, progress = 0.998245
slot update_slots: id  0 | task 34630 | n_tokens = 36403, memory_seq_rm [36403, end)
slot update_slots: id  0 | task 34630 | prompt processing progress, n_tokens = 36467, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 34630 | prompt done, n_tokens = 36467, batch.n_tokens = 64
slot init_sampler: id  0 | task 34630 | init sampler, took 10.74 ms, tokens: text = 36467, total = 36467
slot update_slots: id  0 | task 34630 | erasing old context checkpoint (pos_min = 29814, pos_max = 30581, size = 18.009 MiB)
slot update_slots: id  0 | task 34630 | created context checkpoint 8 of 8 (pos_min = 35635, pos_max = 36402, size = 18.009 MiB)
slot print_timing: id  0 | task 34630 | 
prompt eval time =    1530.75 ms /   708 tokens (    2.16 ms per token,   462.52 tokens per second)
       eval time =   15303.88 ms /   504 tokens (   30.36 ms per token,    32.93 tokens per second)
      total time =   16834.62 ms /  1212 tokens
slot      release: id  0 | task 34630 | stop processing: n_tokens = 36970, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35136 | processing task, is_child = 0
slot update_slots: id  0 | task 35136 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 37193
slot update_slots: id  0 | task 35136 | n_tokens = 36467, memory_seq_rm [36467, end)
slot update_slots: id  0 | task 35136 | prompt processing progress, n_tokens = 37129, batch.n_tokens = 662, progress = 0.998279
slot update_slots: id  0 | task 35136 | n_tokens = 37129, memory_seq_rm [37129, end)
slot update_slots: id  0 | task 35136 | prompt processing progress, n_tokens = 37193, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35136 | prompt done, n_tokens = 37193, batch.n_tokens = 64
slot init_sampler: id  0 | task 35136 | init sampler, took 13.83 ms, tokens: text = 37193, total = 37193
slot update_slots: id  0 | task 35136 | erasing old context checkpoint (pos_min = 30030, pos_max = 30698, size = 15.688 MiB)
slot update_slots: id  0 | task 35136 | created context checkpoint 8 of 8 (pos_min = 36361, pos_max = 37128, size = 18.009 MiB)
slot print_timing: id  0 | task 35136 | 
prompt eval time =    1473.51 ms /   726 tokens (    2.03 ms per token,   492.70 tokens per second)
       eval time =   34450.77 ms /  1117 tokens (   30.84 ms per token,    32.42 tokens per second)
      total time =   35924.28 ms /  1843 tokens
slot      release: id  0 | task 35136 | stop processing: n_tokens = 38309, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36255 | processing task, is_child = 0
slot update_slots: id  0 | task 36255 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 37434
slot update_slots: id  0 | task 36255 | n_past = 37193, slot.prompt.tokens.size() = 38309, seq_id = 0, pos_min = 37541, n_swa = 128
slot update_slots: id  0 | task 36255 | restored context checkpoint (pos_min = 36361, pos_max = 37128, size = 18.009 MiB)
slot update_slots: id  0 | task 36255 | n_tokens = 37128, memory_seq_rm [37128, end)
slot update_slots: id  0 | task 36255 | prompt processing progress, n_tokens = 37370, batch.n_tokens = 242, progress = 0.998290
slot update_slots: id  0 | task 36255 | n_tokens = 37370, memory_seq_rm [37370, end)
slot update_slots: id  0 | task 36255 | prompt processing progress, n_tokens = 37434, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36255 | prompt done, n_tokens = 37434, batch.n_tokens = 64
slot init_sampler: id  0 | task 36255 | init sampler, took 7.08 ms, tokens: text = 37434, total = 37434
slot update_slots: id  0 | task 36255 | erasing old context checkpoint (pos_min = 31446, pos_max = 32213, size = 18.009 MiB)
slot update_slots: id  0 | task 36255 | created context checkpoint 8 of 8 (pos_min = 36602, pos_max = 37369, size = 18.009 MiB)
slot print_timing: id  0 | task 36255 | 
prompt eval time =     847.35 ms /   306 tokens (    2.77 ms per token,   361.12 tokens per second)
       eval time =   14927.81 ms /   481 tokens (   31.03 ms per token,    32.22 tokens per second)
      total time =   15775.16 ms /   787 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 36255 | stop processing: n_tokens = 37914, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36738 | processing task, is_child = 0
slot update_slots: id  0 | task 36738 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 38272
slot update_slots: id  0 | task 36738 | n_tokens = 37434, memory_seq_rm [37434, end)
slot update_slots: id  0 | task 36738 | prompt processing progress, n_tokens = 38208, batch.n_tokens = 774, progress = 0.998328
slot update_slots: id  0 | task 36738 | n_tokens = 38208, memory_seq_rm [38208, end)
slot update_slots: id  0 | task 36738 | prompt processing progress, n_tokens = 38272, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36738 | prompt done, n_tokens = 38272, batch.n_tokens = 64
slot init_sampler: id  0 | task 36738 | init sampler, took 7.27 ms, tokens: text = 38272, total = 38272
slot update_slots: id  0 | task 36738 | erasing old context checkpoint (pos_min = 32939, pos_max = 33706, size = 18.009 MiB)
slot update_slots: id  0 | task 36738 | created context checkpoint 8 of 8 (pos_min = 37440, pos_max = 38207, size = 18.009 MiB)
slot print_timing: id  0 | task 36738 | 
prompt eval time =    1711.47 ms /   838 tokens (    2.04 ms per token,   489.64 tokens per second)
       eval time =   58184.99 ms /  1791 tokens (   32.49 ms per token,    30.78 tokens per second)
      total time =   59896.46 ms /  2629 tokens
slot      release: id  0 | task 36738 | stop processing: n_tokens = 40062, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.568 (> 0.100 thold), f_keep = 0.554
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 38531 | processing task, is_child = 0
slot update_slots: id  0 | task 38531 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 39032
slot update_slots: id  0 | task 38531 | n_past = 22176, slot.prompt.tokens.size() = 40062, seq_id = 0, pos_min = 39294, n_swa = 128
slot update_slots: id  0 | task 38531 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 38531 | erased invalidated context checkpoint (pos_min = 33505, pos_max = 33807, n_swa = 128, size = 7.105 MiB)
slot update_slots: id  0 | task 38531 | erased invalidated context checkpoint (pos_min = 33745, pos_max = 34242, n_swa = 128, size = 11.678 MiB)
slot update_slots: id  0 | task 38531 | erased invalidated context checkpoint (pos_min = 34201, pos_max = 34968, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 38531 | erased invalidated context checkpoint (pos_min = 35054, pos_max = 35694, n_swa = 128, size = 15.031 MiB)
slot update_slots: id  0 | task 38531 | erased invalidated context checkpoint (pos_min = 35635, pos_max = 36402, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 38531 | erased invalidated context checkpoint (pos_min = 36361, pos_max = 37128, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 38531 | erased invalidated context checkpoint (pos_min = 36602, pos_max = 37369, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 38531 | erased invalidated context checkpoint (pos_min = 37440, pos_max = 38207, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 38531 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.052470
slot update_slots: id  0 | task 38531 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.104940
slot update_slots: id  0 | task 38531 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.157409
slot update_slots: id  0 | task 38531 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.209879
slot update_slots: id  0 | task 38531 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.262349
slot update_slots: id  0 | task 38531 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.314819
slot update_slots: id  0 | task 38531 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.367288
slot update_slots: id  0 | task 38531 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.419758
slot update_slots: id  0 | task 38531 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.472228
slot update_slots: id  0 | task 38531 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 2048, progress = 0.524698
slot update_slots: id  0 | task 38531 | n_tokens = 20480, memory_seq_rm [20480, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 22528, batch.n_tokens = 2048, progress = 0.577167
slot update_slots: id  0 | task 38531 | n_tokens = 22528, memory_seq_rm [22528, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 24576, batch.n_tokens = 2048, progress = 0.629637
slot update_slots: id  0 | task 38531 | n_tokens = 24576, memory_seq_rm [24576, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 26624, batch.n_tokens = 2048, progress = 0.682107
slot update_slots: id  0 | task 38531 | n_tokens = 26624, memory_seq_rm [26624, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 28672, batch.n_tokens = 2048, progress = 0.734577
slot update_slots: id  0 | task 38531 | n_tokens = 28672, memory_seq_rm [28672, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 30720, batch.n_tokens = 2048, progress = 0.787047
slot update_slots: id  0 | task 38531 | n_tokens = 30720, memory_seq_rm [30720, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 32768, batch.n_tokens = 2048, progress = 0.839516
slot update_slots: id  0 | task 38531 | n_tokens = 32768, memory_seq_rm [32768, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 34816, batch.n_tokens = 2048, progress = 0.891986
slot update_slots: id  0 | task 38531 | n_tokens = 34816, memory_seq_rm [34816, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 36864, batch.n_tokens = 2048, progress = 0.944456
slot update_slots: id  0 | task 38531 | n_tokens = 36864, memory_seq_rm [36864, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 38912, batch.n_tokens = 2048, progress = 0.996926
slot update_slots: id  0 | task 38531 | n_tokens = 38912, memory_seq_rm [38912, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 38968, batch.n_tokens = 56, progress = 0.998360
slot update_slots: id  0 | task 38531 | n_tokens = 38968, memory_seq_rm [38968, end)
slot update_slots: id  0 | task 38531 | prompt processing progress, n_tokens = 39032, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 38531 | prompt done, n_tokens = 39032, batch.n_tokens = 64
slot init_sampler: id  0 | task 38531 | init sampler, took 7.59 ms, tokens: text = 39032, total = 39032
slot update_slots: id  0 | task 38531 | created context checkpoint 1 of 8 (pos_min = 38200, pos_max = 38967, size = 18.009 MiB)
slot print_timing: id  0 | task 38531 | 
prompt eval time =   48210.43 ms / 39032 tokens (    1.24 ms per token,   809.62 tokens per second)
       eval time =   36045.69 ms /  1160 tokens (   31.07 ms per token,    32.18 tokens per second)
      total time =   84256.12 ms / 40192 tokens
slot      release: id  0 | task 38531 | stop processing: n_tokens = 40191, truncated = 1
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 38531
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 39713 | processing task, is_child = 0
slot update_slots: id  0 | task 39713 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 39047
slot update_slots: id  0 | task 39713 | n_past = 39031, slot.prompt.tokens.size() = 40191, seq_id = 0, pos_min = 39423, n_swa = 128
slot update_slots: id  0 | task 39713 | restored context checkpoint (pos_min = 38200, pos_max = 38967, size = 18.009 MiB)
slot update_slots: id  0 | task 39713 | n_tokens = 38967, memory_seq_rm [38967, end)
slot update_slots: id  0 | task 39713 | prompt processing progress, n_tokens = 38983, batch.n_tokens = 16, progress = 0.998361
slot update_slots: id  0 | task 39713 | n_tokens = 38983, memory_seq_rm [38983, end)
slot update_slots: id  0 | task 39713 | prompt processing progress, n_tokens = 39047, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 39713 | prompt done, n_tokens = 39047, batch.n_tokens = 64
slot init_sampler: id  0 | task 39713 | init sampler, took 14.53 ms, tokens: text = 39047, total = 39047
slot print_timing: id  0 | task 39713 | 
prompt eval time =     483.25 ms /    80 tokens (    6.04 ms per token,   165.55 tokens per second)
       eval time =   36818.07 ms /  1145 tokens (   32.16 ms per token,    31.10 tokens per second)
      total time =   37301.32 ms /  1225 tokens
slot      release: id  0 | task 39713 | stop processing: n_tokens = 40191, truncated = 1
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 39713
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 1.000 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 40861 | processing task, is_child = 0
slot update_slots: id  0 | task 40861 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 39054
slot update_slots: id  0 | task 40861 | n_past = 39046, slot.prompt.tokens.size() = 40191, seq_id = 0, pos_min = 39423, n_swa = 128
slot update_slots: id  0 | task 40861 | restored context checkpoint (pos_min = 38200, pos_max = 38967, size = 18.009 MiB)
slot update_slots: id  0 | task 40861 | n_tokens = 38967, memory_seq_rm [38967, end)
slot update_slots: id  0 | task 40861 | prompt processing progress, n_tokens = 38990, batch.n_tokens = 23, progress = 0.998361
slot update_slots: id  0 | task 40861 | n_tokens = 38990, memory_seq_rm [38990, end)
slot update_slots: id  0 | task 40861 | prompt processing progress, n_tokens = 39054, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 40861 | prompt done, n_tokens = 39054, batch.n_tokens = 64
slot init_sampler: id  0 | task 40861 | init sampler, took 15.08 ms, tokens: text = 39054, total = 39054
slot print_timing: id  0 | task 40861 | 
prompt eval time =     737.26 ms /    87 tokens (    8.47 ms per token,   118.00 tokens per second)
       eval time =   16005.75 ms /   409 tokens (   39.13 ms per token,    25.55 tokens per second)
      total time =   16743.01 ms /   496 tokens
slot      release: id  0 | task 40861 | stop processing: n_tokens = 39462, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 4852149045
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 39462, total state size = 943.352 MiB
srv          load:  - looking for better prompt, base f_keep = 0.001, sim = 0.002
srv        update:  - cache state: 11 prompts, 4001.699 MiB (limits: 8192.000 MiB, 40192 tokens, 264313 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv        update:    - prompt 0x59729e4b9300:    6026 tokens, checkpoints:  2,   192.916 MiB
srv        update:    - prompt 0x59729721bfc0:    8368 tokens, checkpoints:  8,   357.529 MiB
srv        update:    - prompt 0x5972a2bc32a0:   39462 tokens, checkpoints:  1,   961.361 MiB
srv  get_availabl: prompt cache update took 1684.52 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41272 | processing task, is_child = 0
slot update_slots: id  0 | task 41272 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 38675
slot update_slots: id  0 | task 41272 | n_past = 59, slot.prompt.tokens.size() = 39462, seq_id = 0, pos_min = 38694, n_swa = 128
slot update_slots: id  0 | task 41272 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 41272 | erased invalidated context checkpoint (pos_min = 38200, pos_max = 38967, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 41272 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 41272 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.052954
srv          stop: cancel task, id_task = 41272
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 41272 | stop processing: n_tokens = 2048, truncated = 0
srv  update_slots: all slots are idle
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 4858038611
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2048, total state size = 66.033 MiB
srv          load:  - looking for better prompt, base f_keep = 0.029, sim = 0.055
srv        update:  - cache state: 12 prompts, 4067.731 MiB (limits: 8192.000 MiB, 40192 tokens, 264147 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv        update:    - prompt 0x59729e4b9300:    6026 tokens, checkpoints:  2,   192.916 MiB
srv        update:    - prompt 0x59729721bfc0:    8368 tokens, checkpoints:  8,   357.529 MiB
srv        update:    - prompt 0x5972a2bc32a0:   39462 tokens, checkpoints:  1,   961.361 MiB
srv        update:    - prompt 0x597299270410:    2048 tokens, checkpoints:  0,    66.033 MiB
srv  get_availabl: prompt cache update took 59.57 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41275 | processing task, is_child = 0
slot update_slots: id  0 | task 41275 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1069
slot update_slots: id  0 | task 41275 | n_past = 59, slot.prompt.tokens.size() = 2048, seq_id = 0, pos_min = 1280, n_swa = 128
slot update_slots: id  0 | task 41275 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 41275 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 41275 | prompt processing progress, n_tokens = 1005, batch.n_tokens = 1005, progress = 0.940131
slot update_slots: id  0 | task 41275 | n_tokens = 1005, memory_seq_rm [1005, end)
slot update_slots: id  0 | task 41275 | prompt processing progress, n_tokens = 1069, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41275 | prompt done, n_tokens = 1069, batch.n_tokens = 64
slot init_sampler: id  0 | task 41275 | init sampler, took 0.20 ms, tokens: text = 1069, total = 1069
slot update_slots: id  0 | task 41275 | created context checkpoint 1 of 8 (pos_min = 237, pos_max = 1004, size = 18.009 MiB)
slot print_timing: id  0 | task 41275 | 
prompt eval time =    1459.05 ms /  1069 tokens (    1.36 ms per token,   732.67 tokens per second)
       eval time =    8423.81 ms /   251 tokens (   33.56 ms per token,    29.80 tokens per second)
      total time =    9882.85 ms /  1320 tokens
slot      release: id  0 | task 41275 | stop processing: n_tokens = 1319, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.694 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41528 | processing task, is_child = 0
slot update_slots: id  0 | task 41528 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1873
slot update_slots: id  0 | task 41528 | n_tokens = 1299, memory_seq_rm [1299, end)
slot update_slots: id  0 | task 41528 | prompt processing progress, n_tokens = 1809, batch.n_tokens = 510, progress = 0.965830
slot update_slots: id  0 | task 41528 | n_tokens = 1809, memory_seq_rm [1809, end)
slot update_slots: id  0 | task 41528 | prompt processing progress, n_tokens = 1873, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41528 | prompt done, n_tokens = 1873, batch.n_tokens = 64
slot init_sampler: id  0 | task 41528 | init sampler, took 0.27 ms, tokens: text = 1873, total = 1873
slot update_slots: id  0 | task 41528 | created context checkpoint 2 of 8 (pos_min = 1041, pos_max = 1808, size = 18.009 MiB)
slot print_timing: id  0 | task 41528 | 
prompt eval time =     866.07 ms /   574 tokens (    1.51 ms per token,   662.77 tokens per second)
       eval time =    1398.92 ms /    40 tokens (   34.97 ms per token,    28.59 tokens per second)
      total time =    2264.99 ms /   614 tokens
slot      release: id  0 | task 41528 | stop processing: n_tokens = 1912, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.814 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41570 | processing task, is_child = 0
slot update_slots: id  0 | task 41570 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 2316
slot update_slots: id  0 | task 41570 | n_tokens = 1885, memory_seq_rm [1885, end)
slot update_slots: id  0 | task 41570 | prompt processing progress, n_tokens = 2252, batch.n_tokens = 367, progress = 0.972366
slot update_slots: id  0 | task 41570 | n_tokens = 2252, memory_seq_rm [2252, end)
slot update_slots: id  0 | task 41570 | prompt processing progress, n_tokens = 2316, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41570 | prompt done, n_tokens = 2316, batch.n_tokens = 64
slot init_sampler: id  0 | task 41570 | init sampler, took 0.42 ms, tokens: text = 2316, total = 2316
slot update_slots: id  0 | task 41570 | created context checkpoint 3 of 8 (pos_min = 1484, pos_max = 2251, size = 18.009 MiB)
slot print_timing: id  0 | task 41570 | 
prompt eval time =     837.12 ms /   431 tokens (    1.94 ms per token,   514.86 tokens per second)
       eval time =    6301.76 ms /   177 tokens (   35.60 ms per token,    28.09 tokens per second)
      total time =    7138.89 ms /   608 tokens
slot      release: id  0 | task 41570 | stop processing: n_tokens = 2492, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.866 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41749 | processing task, is_child = 0
slot update_slots: id  0 | task 41749 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 2841
slot update_slots: id  0 | task 41749 | n_tokens = 2459, memory_seq_rm [2459, end)
slot update_slots: id  0 | task 41749 | prompt processing progress, n_tokens = 2777, batch.n_tokens = 318, progress = 0.977473
slot update_slots: id  0 | task 41749 | n_tokens = 2777, memory_seq_rm [2777, end)
slot update_slots: id  0 | task 41749 | prompt processing progress, n_tokens = 2841, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41749 | prompt done, n_tokens = 2841, batch.n_tokens = 64
slot init_sampler: id  0 | task 41749 | init sampler, took 0.41 ms, tokens: text = 2841, total = 2841
slot update_slots: id  0 | task 41749 | created context checkpoint 4 of 8 (pos_min = 2009, pos_max = 2776, size = 18.009 MiB)
slot print_timing: id  0 | task 41749 | 
prompt eval time =     710.16 ms /   382 tokens (    1.86 ms per token,   537.91 tokens per second)
       eval time =    1609.87 ms /    52 tokens (   30.96 ms per token,    32.30 tokens per second)
      total time =    2320.02 ms /   434 tokens
slot      release: id  0 | task 41749 | stop processing: n_tokens = 2892, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.911 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41803 | processing task, is_child = 0
slot update_slots: id  0 | task 41803 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3140
slot update_slots: id  0 | task 41803 | n_tokens = 2859, memory_seq_rm [2859, end)
slot update_slots: id  0 | task 41803 | prompt processing progress, n_tokens = 3076, batch.n_tokens = 217, progress = 0.979618
slot update_slots: id  0 | task 41803 | n_tokens = 3076, memory_seq_rm [3076, end)
slot update_slots: id  0 | task 41803 | prompt processing progress, n_tokens = 3140, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41803 | prompt done, n_tokens = 3140, batch.n_tokens = 64
slot init_sampler: id  0 | task 41803 | init sampler, took 0.44 ms, tokens: text = 3140, total = 3140
slot update_slots: id  0 | task 41803 | created context checkpoint 5 of 8 (pos_min = 2308, pos_max = 3075, size = 18.009 MiB)
slot print_timing: id  0 | task 41803 | 
prompt eval time =     625.62 ms /   281 tokens (    2.23 ms per token,   449.15 tokens per second)
       eval time =    1549.77 ms /    53 tokens (   29.24 ms per token,    34.20 tokens per second)
      total time =    2175.39 ms /   334 tokens
slot      release: id  0 | task 41803 | stop processing: n_tokens = 3192, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.908 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41858 | processing task, is_child = 0
slot update_slots: id  0 | task 41858 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3473
slot update_slots: id  0 | task 41858 | n_tokens = 3155, memory_seq_rm [3155, end)
slot update_slots: id  0 | task 41858 | prompt processing progress, n_tokens = 3409, batch.n_tokens = 254, progress = 0.981572
slot update_slots: id  0 | task 41858 | n_tokens = 3409, memory_seq_rm [3409, end)
slot update_slots: id  0 | task 41858 | prompt processing progress, n_tokens = 3473, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41858 | prompt done, n_tokens = 3473, batch.n_tokens = 64
slot init_sampler: id  0 | task 41858 | init sampler, took 0.49 ms, tokens: text = 3473, total = 3473
slot update_slots: id  0 | task 41858 | created context checkpoint 6 of 8 (pos_min = 2641, pos_max = 3408, size = 18.009 MiB)
slot print_timing: id  0 | task 41858 | 
prompt eval time =     657.94 ms /   318 tokens (    2.07 ms per token,   483.33 tokens per second)
       eval time =    1498.73 ms /    53 tokens (   28.28 ms per token,    35.36 tokens per second)
      total time =    2156.67 ms /   371 tokens
slot      release: id  0 | task 41858 | stop processing: n_tokens = 3525, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.892 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41913 | processing task, is_child = 0
slot update_slots: id  0 | task 41913 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3911
slot update_slots: id  0 | task 41913 | n_tokens = 3488, memory_seq_rm [3488, end)
slot update_slots: id  0 | task 41913 | prompt processing progress, n_tokens = 3847, batch.n_tokens = 359, progress = 0.983636
slot update_slots: id  0 | task 41913 | n_tokens = 3847, memory_seq_rm [3847, end)
slot update_slots: id  0 | task 41913 | prompt processing progress, n_tokens = 3911, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41913 | prompt done, n_tokens = 3911, batch.n_tokens = 64
slot init_sampler: id  0 | task 41913 | init sampler, took 0.57 ms, tokens: text = 3911, total = 3911
slot update_slots: id  0 | task 41913 | created context checkpoint 7 of 8 (pos_min = 3079, pos_max = 3846, size = 18.009 MiB)
slot print_timing: id  0 | task 41913 | 
prompt eval time =     708.40 ms /   423 tokens (    1.67 ms per token,   597.12 tokens per second)
       eval time =    1516.76 ms /    53 tokens (   28.62 ms per token,    34.94 tokens per second)
      total time =    2225.16 ms /   476 tokens
slot      release: id  0 | task 41913 | stop processing: n_tokens = 3963, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.935 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41968 | processing task, is_child = 0
slot update_slots: id  0 | task 41968 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4199
slot update_slots: id  0 | task 41968 | n_tokens = 3926, memory_seq_rm [3926, end)
slot update_slots: id  0 | task 41968 | prompt processing progress, n_tokens = 4135, batch.n_tokens = 209, progress = 0.984758
slot update_slots: id  0 | task 41968 | n_tokens = 4135, memory_seq_rm [4135, end)
slot update_slots: id  0 | task 41968 | prompt processing progress, n_tokens = 4199, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41968 | prompt done, n_tokens = 4199, batch.n_tokens = 64
slot init_sampler: id  0 | task 41968 | init sampler, took 0.60 ms, tokens: text = 4199, total = 4199
slot update_slots: id  0 | task 41968 | created context checkpoint 8 of 8 (pos_min = 3367, pos_max = 4134, size = 18.009 MiB)
slot print_timing: id  0 | task 41968 | 
prompt eval time =     630.22 ms /   273 tokens (    2.31 ms per token,   433.18 tokens per second)
       eval time =    1572.33 ms /    53 tokens (   29.67 ms per token,    33.71 tokens per second)
      total time =    2202.55 ms /   326 tokens
slot      release: id  0 | task 41968 | stop processing: n_tokens = 4251, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 42023 | processing task, is_child = 0
slot update_slots: id  0 | task 42023 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4519
slot update_slots: id  0 | task 42023 | n_tokens = 4221, memory_seq_rm [4221, end)
slot update_slots: id  0 | task 42023 | prompt processing progress, n_tokens = 4455, batch.n_tokens = 234, progress = 0.985838
slot update_slots: id  0 | task 42023 | n_tokens = 4455, memory_seq_rm [4455, end)
slot update_slots: id  0 | task 42023 | prompt processing progress, n_tokens = 4519, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 42023 | prompt done, n_tokens = 4519, batch.n_tokens = 64
slot init_sampler: id  0 | task 42023 | init sampler, took 0.91 ms, tokens: text = 4519, total = 4519
slot update_slots: id  0 | task 42023 | erasing old context checkpoint (pos_min = 237, pos_max = 1004, size = 18.009 MiB)
slot update_slots: id  0 | task 42023 | created context checkpoint 8 of 8 (pos_min = 3687, pos_max = 4454, size = 18.009 MiB)
slot print_timing: id  0 | task 42023 | 
prompt eval time =     677.64 ms /   298 tokens (    2.27 ms per token,   439.76 tokens per second)
       eval time =    1591.91 ms /    53 tokens (   30.04 ms per token,    33.29 tokens per second)
      total time =    2269.55 ms /   351 tokens
slot      release: id  0 | task 42023 | stop processing: n_tokens = 4571, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.912 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 42078 | processing task, is_child = 0
slot update_slots: id  0 | task 42078 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4969
slot update_slots: id  0 | task 42078 | n_tokens = 4534, memory_seq_rm [4534, end)
slot update_slots: id  0 | task 42078 | prompt processing progress, n_tokens = 4905, batch.n_tokens = 371, progress = 0.987120
slot update_slots: id  0 | task 42078 | n_tokens = 4905, memory_seq_rm [4905, end)
slot update_slots: id  0 | task 42078 | prompt processing progress, n_tokens = 4969, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 42078 | prompt done, n_tokens = 4969, batch.n_tokens = 64
slot init_sampler: id  0 | task 42078 | init sampler, took 0.69 ms, tokens: text = 4969, total = 4969
slot update_slots: id  0 | task 42078 | erasing old context checkpoint (pos_min = 1041, pos_max = 1808, size = 18.009 MiB)
slot update_slots: id  0 | task 42078 | created context checkpoint 8 of 8 (pos_min = 4137, pos_max = 4904, size = 18.009 MiB)
slot print_timing: id  0 | task 42078 | 
prompt eval time =     800.13 ms /   435 tokens (    1.84 ms per token,   543.66 tokens per second)
       eval time =   13356.69 ms /   405 tokens (   32.98 ms per token,    30.32 tokens per second)
      total time =   14156.82 ms /   840 tokens
slot      release: id  0 | task 42078 | stop processing: n_tokens = 5373, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.925 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 42485 | processing task, is_child = 0
slot update_slots: id  0 | task 42485 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5777
slot update_slots: id  0 | task 42485 | n_tokens = 5346, memory_seq_rm [5346, end)
slot update_slots: id  0 | task 42485 | prompt processing progress, n_tokens = 5713, batch.n_tokens = 367, progress = 0.988922
slot update_slots: id  0 | task 42485 | n_tokens = 5713, memory_seq_rm [5713, end)
slot update_slots: id  0 | task 42485 | prompt processing progress, n_tokens = 5777, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 42485 | prompt done, n_tokens = 5777, batch.n_tokens = 64
slot init_sampler: id  0 | task 42485 | init sampler, took 0.80 ms, tokens: text = 5777, total = 5777
slot update_slots: id  0 | task 42485 | erasing old context checkpoint (pos_min = 1484, pos_max = 2251, size = 18.009 MiB)
slot update_slots: id  0 | task 42485 | created context checkpoint 8 of 8 (pos_min = 4945, pos_max = 5712, size = 18.009 MiB)
slot print_timing: id  0 | task 42485 | 
prompt eval time =     846.38 ms /   431 tokens (    1.96 ms per token,   509.23 tokens per second)
       eval time =   57298.89 ms /  1789 tokens (   32.03 ms per token,    31.22 tokens per second)
      total time =   58145.28 ms /  2220 tokens
slot      release: id  0 | task 42485 | stop processing: n_tokens = 7565, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.785 (> 0.100 thold), f_keep = 0.824
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44276 | processing task, is_child = 0
slot update_slots: id  0 | task 44276 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7936
slot update_slots: id  0 | task 44276 | n_past = 6233, slot.prompt.tokens.size() = 7565, seq_id = 0, pos_min = 6797, n_swa = 128
slot update_slots: id  0 | task 44276 | restored context checkpoint (pos_min = 4945, pos_max = 5712, size = 18.009 MiB)
slot update_slots: id  0 | task 44276 | n_tokens = 5712, memory_seq_rm [5712, end)
slot update_slots: id  0 | task 44276 | prompt processing progress, n_tokens = 7760, batch.n_tokens = 2048, progress = 0.977823
slot update_slots: id  0 | task 44276 | n_tokens = 7760, memory_seq_rm [7760, end)
slot update_slots: id  0 | task 44276 | prompt processing progress, n_tokens = 7872, batch.n_tokens = 112, progress = 0.991935
slot update_slots: id  0 | task 44276 | n_tokens = 7872, memory_seq_rm [7872, end)
slot update_slots: id  0 | task 44276 | prompt processing progress, n_tokens = 7936, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44276 | prompt done, n_tokens = 7936, batch.n_tokens = 64
slot init_sampler: id  0 | task 44276 | init sampler, took 1.63 ms, tokens: text = 7936, total = 7936
slot update_slots: id  0 | task 44276 | erasing old context checkpoint (pos_min = 2009, pos_max = 2776, size = 18.009 MiB)
slot update_slots: id  0 | task 44276 | created context checkpoint 8 of 8 (pos_min = 7104, pos_max = 7871, size = 18.009 MiB)
slot print_timing: id  0 | task 44276 | 
prompt eval time =    3110.59 ms /  2224 tokens (    1.40 ms per token,   714.98 tokens per second)
       eval time =    1583.64 ms /    52 tokens (   30.45 ms per token,    32.84 tokens per second)
      total time =    4694.23 ms /  2276 tokens
slot      release: id  0 | task 44276 | stop processing: n_tokens = 7987, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 5361570653
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7987, total state size = 205.296 MiB
srv          load:  - looking for better prompt, base f_keep = 0.007, sim = 0.010
srv        update:  - cache state: 13 prompts, 4417.100 MiB (limits: 8192.000 MiB, 40192 tokens, 258067 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv        update:    - prompt 0x59729e4b9300:    6026 tokens, checkpoints:  2,   192.916 MiB
srv        update:    - prompt 0x59729721bfc0:    8368 tokens, checkpoints:  8,   357.529 MiB
srv        update:    - prompt 0x5972a2bc32a0:   39462 tokens, checkpoints:  1,   961.361 MiB
srv        update:    - prompt 0x597299270410:    2048 tokens, checkpoints:  0,    66.033 MiB
srv        update:    - prompt 0x59729817f9a0:    7987 tokens, checkpoints:  8,   349.369 MiB
srv  get_availabl: prompt cache update took 331.31 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44331 | processing task, is_child = 0
slot update_slots: id  0 | task 44331 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6060
slot update_slots: id  0 | task 44331 | n_past = 59, slot.prompt.tokens.size() = 7987, seq_id = 0, pos_min = 7219, n_swa = 128
slot update_slots: id  0 | task 44331 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 44331 | erased invalidated context checkpoint (pos_min = 2308, pos_max = 3075, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 44331 | erased invalidated context checkpoint (pos_min = 2641, pos_max = 3408, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 44331 | erased invalidated context checkpoint (pos_min = 3079, pos_max = 3846, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 44331 | erased invalidated context checkpoint (pos_min = 3367, pos_max = 4134, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 44331 | erased invalidated context checkpoint (pos_min = 3687, pos_max = 4454, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 44331 | erased invalidated context checkpoint (pos_min = 4137, pos_max = 4904, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 44331 | erased invalidated context checkpoint (pos_min = 4945, pos_max = 5712, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 44331 | erased invalidated context checkpoint (pos_min = 7104, pos_max = 7871, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 44331 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 44331 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.337954
slot update_slots: id  0 | task 44331 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 44331 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.675908
slot update_slots: id  0 | task 44331 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 44331 | prompt processing progress, n_tokens = 5996, batch.n_tokens = 1900, progress = 0.989439
slot update_slots: id  0 | task 44331 | n_tokens = 5996, memory_seq_rm [5996, end)
slot update_slots: id  0 | task 44331 | prompt processing progress, n_tokens = 6060, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44331 | prompt done, n_tokens = 6060, batch.n_tokens = 64
slot init_sampler: id  0 | task 44331 | init sampler, took 0.84 ms, tokens: text = 6060, total = 6060
slot update_slots: id  0 | task 44331 | created context checkpoint 1 of 8 (pos_min = 5228, pos_max = 5995, size = 18.009 MiB)
slot print_timing: id  0 | task 44331 | 
prompt eval time =    7330.37 ms /  6060 tokens (    1.21 ms per token,   826.70 tokens per second)
       eval time =   24813.12 ms /   762 tokens (   32.56 ms per token,    30.71 tokens per second)
      total time =   32143.49 ms /  6822 tokens
slot      release: id  0 | task 44331 | stop processing: n_tokens = 6821, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 5394379467
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6821, total state size = 177.955 MiB
srv          load:  - looking for better prompt, base f_keep = 0.009, sim = 0.055
srv        update:  - cache state: 14 prompts, 4613.064 MiB (limits: 8192.000 MiB, 40192 tokens, 259217 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv        update:    - prompt 0x59729e4b9300:    6026 tokens, checkpoints:  2,   192.916 MiB
srv        update:    - prompt 0x59729721bfc0:    8368 tokens, checkpoints:  8,   357.529 MiB
srv        update:    - prompt 0x5972a2bc32a0:   39462 tokens, checkpoints:  1,   961.361 MiB
srv        update:    - prompt 0x597299270410:    2048 tokens, checkpoints:  0,    66.033 MiB
srv        update:    - prompt 0x59729817f9a0:    7987 tokens, checkpoints:  8,   349.369 MiB
srv        update:    - prompt 0x5972a2bd71a0:    6821 tokens, checkpoints:  1,   195.964 MiB
srv  get_availabl: prompt cache update took 184.71 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45097 | processing task, is_child = 0
slot update_slots: id  0 | task 45097 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1076
slot update_slots: id  0 | task 45097 | n_past = 59, slot.prompt.tokens.size() = 6821, seq_id = 0, pos_min = 6053, n_swa = 128
slot update_slots: id  0 | task 45097 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 45097 | erased invalidated context checkpoint (pos_min = 5228, pos_max = 5995, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 45097 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 45097 | prompt processing progress, n_tokens = 1012, batch.n_tokens = 1012, progress = 0.940520
slot update_slots: id  0 | task 45097 | n_tokens = 1012, memory_seq_rm [1012, end)
slot update_slots: id  0 | task 45097 | prompt processing progress, n_tokens = 1076, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45097 | prompt done, n_tokens = 1076, batch.n_tokens = 64
slot init_sampler: id  0 | task 45097 | init sampler, took 0.16 ms, tokens: text = 1076, total = 1076
slot update_slots: id  0 | task 45097 | created context checkpoint 1 of 8 (pos_min = 244, pos_max = 1011, size = 18.009 MiB)
slot print_timing: id  0 | task 45097 | 
prompt eval time =    1455.74 ms /  1076 tokens (    1.35 ms per token,   739.14 tokens per second)
       eval time =    7326.17 ms /   216 tokens (   33.92 ms per token,    29.48 tokens per second)
      total time =    8781.91 ms /  1292 tokens
slot      release: id  0 | task 45097 | stop processing: n_tokens = 1291, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.689 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45315 | processing task, is_child = 0
slot update_slots: id  0 | task 45315 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1845
slot update_slots: id  0 | task 45315 | n_tokens = 1272, memory_seq_rm [1272, end)
slot update_slots: id  0 | task 45315 | prompt processing progress, n_tokens = 1781, batch.n_tokens = 509, progress = 0.965312
slot update_slots: id  0 | task 45315 | n_tokens = 1781, memory_seq_rm [1781, end)
slot update_slots: id  0 | task 45315 | prompt processing progress, n_tokens = 1845, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45315 | prompt done, n_tokens = 1845, batch.n_tokens = 64
slot init_sampler: id  0 | task 45315 | init sampler, took 0.27 ms, tokens: text = 1845, total = 1845
slot update_slots: id  0 | task 45315 | created context checkpoint 2 of 8 (pos_min = 1013, pos_max = 1780, size = 18.009 MiB)
slot print_timing: id  0 | task 45315 | 
prompt eval time =     873.79 ms /   573 tokens (    1.52 ms per token,   655.76 tokens per second)
       eval time =    1373.84 ms /    36 tokens (   38.16 ms per token,    26.20 tokens per second)
      total time =    2247.64 ms /   609 tokens
slot      release: id  0 | task 45315 | stop processing: n_tokens = 1880, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45353 | processing task, is_child = 0
slot update_slots: id  0 | task 45353 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1939
slot update_slots: id  0 | task 45353 | n_tokens = 1857, memory_seq_rm [1857, end)
slot update_slots: id  0 | task 45353 | prompt processing progress, n_tokens = 1875, batch.n_tokens = 18, progress = 0.966993
slot update_slots: id  0 | task 45353 | n_tokens = 1875, memory_seq_rm [1875, end)
slot update_slots: id  0 | task 45353 | prompt processing progress, n_tokens = 1939, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45353 | prompt done, n_tokens = 1939, batch.n_tokens = 64
slot init_sampler: id  0 | task 45353 | init sampler, took 0.28 ms, tokens: text = 1939, total = 1939
slot update_slots: id  0 | task 45353 | created context checkpoint 3 of 8 (pos_min = 1112, pos_max = 1874, size = 17.892 MiB)
slot print_timing: id  0 | task 45353 | 
prompt eval time =     404.93 ms /    82 tokens (    4.94 ms per token,   202.50 tokens per second)
       eval time =    1780.06 ms /    52 tokens (   34.23 ms per token,    29.21 tokens per second)
      total time =    2184.99 ms /   134 tokens
slot      release: id  0 | task 45353 | stop processing: n_tokens = 1990, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.823 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45407 | processing task, is_child = 0
slot update_slots: id  0 | task 45407 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 2394
slot update_slots: id  0 | task 45407 | n_tokens = 1971, memory_seq_rm [1971, end)
slot update_slots: id  0 | task 45407 | prompt processing progress, n_tokens = 2330, batch.n_tokens = 359, progress = 0.973266
slot update_slots: id  0 | task 45407 | n_tokens = 2330, memory_seq_rm [2330, end)
slot update_slots: id  0 | task 45407 | prompt processing progress, n_tokens = 2394, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45407 | prompt done, n_tokens = 2394, batch.n_tokens = 64
slot init_sampler: id  0 | task 45407 | init sampler, took 0.34 ms, tokens: text = 2394, total = 2394
slot update_slots: id  0 | task 45407 | created context checkpoint 4 of 8 (pos_min = 1562, pos_max = 2329, size = 18.009 MiB)
slot print_timing: id  0 | task 45407 | 
prompt eval time =     811.68 ms /   423 tokens (    1.92 ms per token,   521.14 tokens per second)
       eval time =    3398.40 ms /    99 tokens (   34.33 ms per token,    29.13 tokens per second)
      total time =    4210.07 ms /   522 tokens
slot      release: id  0 | task 45407 | stop processing: n_tokens = 2492, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.741 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45508 | processing task, is_child = 0
slot update_slots: id  0 | task 45508 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3318
slot update_slots: id  0 | task 45508 | n_tokens = 2460, memory_seq_rm [2460, end)
slot update_slots: id  0 | task 45508 | prompt processing progress, n_tokens = 3254, batch.n_tokens = 794, progress = 0.980711
slot update_slots: id  0 | task 45508 | n_tokens = 3254, memory_seq_rm [3254, end)
slot update_slots: id  0 | task 45508 | prompt processing progress, n_tokens = 3318, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45508 | prompt done, n_tokens = 3318, batch.n_tokens = 64
slot init_sampler: id  0 | task 45508 | init sampler, took 0.68 ms, tokens: text = 3318, total = 3318
slot update_slots: id  0 | task 45508 | created context checkpoint 5 of 8 (pos_min = 2486, pos_max = 3253, size = 18.009 MiB)
slot print_timing: id  0 | task 45508 | 
prompt eval time =    1255.33 ms /   858 tokens (    1.46 ms per token,   683.49 tokens per second)
       eval time =    3643.85 ms /   120 tokens (   30.37 ms per token,    32.93 tokens per second)
      total time =    4899.18 ms /   978 tokens
slot      release: id  0 | task 45508 | stop processing: n_tokens = 3437, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.807 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45630 | processing task, is_child = 0
slot update_slots: id  0 | task 45630 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4218
slot update_slots: id  0 | task 45630 | n_tokens = 3405, memory_seq_rm [3405, end)
slot update_slots: id  0 | task 45630 | prompt processing progress, n_tokens = 4154, batch.n_tokens = 749, progress = 0.984827
slot update_slots: id  0 | task 45630 | n_tokens = 4154, memory_seq_rm [4154, end)
slot update_slots: id  0 | task 45630 | prompt processing progress, n_tokens = 4218, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45630 | prompt done, n_tokens = 4218, batch.n_tokens = 64
slot init_sampler: id  0 | task 45630 | init sampler, took 0.59 ms, tokens: text = 4218, total = 4218
slot update_slots: id  0 | task 45630 | created context checkpoint 6 of 8 (pos_min = 3386, pos_max = 4153, size = 18.009 MiB)
slot print_timing: id  0 | task 45630 | 
prompt eval time =    1201.66 ms /   813 tokens (    1.48 ms per token,   676.57 tokens per second)
       eval time =    1424.06 ms /    47 tokens (   30.30 ms per token,    33.00 tokens per second)
      total time =    2625.72 ms /   860 tokens
slot      release: id  0 | task 45630 | stop processing: n_tokens = 4264, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.837 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45679 | processing task, is_child = 0
slot update_slots: id  0 | task 45679 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5058
slot update_slots: id  0 | task 45679 | n_tokens = 4236, memory_seq_rm [4236, end)
slot update_slots: id  0 | task 45679 | prompt processing progress, n_tokens = 4994, batch.n_tokens = 758, progress = 0.987347
slot update_slots: id  0 | task 45679 | n_tokens = 4994, memory_seq_rm [4994, end)
slot update_slots: id  0 | task 45679 | prompt processing progress, n_tokens = 5058, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45679 | prompt done, n_tokens = 5058, batch.n_tokens = 64
slot init_sampler: id  0 | task 45679 | init sampler, took 1.01 ms, tokens: text = 5058, total = 5058
slot update_slots: id  0 | task 45679 | created context checkpoint 7 of 8 (pos_min = 4226, pos_max = 4993, size = 18.009 MiB)
slot print_timing: id  0 | task 45679 | 
prompt eval time =    1205.45 ms /   822 tokens (    1.47 ms per token,   681.90 tokens per second)
       eval time =    1373.57 ms /    47 tokens (   29.22 ms per token,    34.22 tokens per second)
      total time =    2579.02 ms /   869 tokens
slot      release: id  0 | task 45679 | stop processing: n_tokens = 5104, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.842 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45728 | processing task, is_child = 0
slot update_slots: id  0 | task 45728 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6022
slot update_slots: id  0 | task 45728 | n_tokens = 5072, memory_seq_rm [5072, end)
slot update_slots: id  0 | task 45728 | prompt processing progress, n_tokens = 5958, batch.n_tokens = 886, progress = 0.989372
slot update_slots: id  0 | task 45728 | n_tokens = 5958, memory_seq_rm [5958, end)
slot update_slots: id  0 | task 45728 | prompt processing progress, n_tokens = 6022, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45728 | prompt done, n_tokens = 6022, batch.n_tokens = 64
slot init_sampler: id  0 | task 45728 | init sampler, took 0.90 ms, tokens: text = 6022, total = 6022
slot update_slots: id  0 | task 45728 | created context checkpoint 8 of 8 (pos_min = 5190, pos_max = 5957, size = 18.009 MiB)
slot print_timing: id  0 | task 45728 | 
prompt eval time =    1326.17 ms /   950 tokens (    1.40 ms per token,   716.35 tokens per second)
       eval time =    1265.05 ms /    43 tokens (   29.42 ms per token,    33.99 tokens per second)
      total time =    2591.21 ms /   993 tokens
slot      release: id  0 | task 45728 | stop processing: n_tokens = 6064, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.883 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45773 | processing task, is_child = 0
slot update_slots: id  0 | task 45773 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6837
slot update_slots: id  0 | task 45773 | n_tokens = 6036, memory_seq_rm [6036, end)
slot update_slots: id  0 | task 45773 | prompt processing progress, n_tokens = 6773, batch.n_tokens = 737, progress = 0.990639
slot update_slots: id  0 | task 45773 | n_tokens = 6773, memory_seq_rm [6773, end)
slot update_slots: id  0 | task 45773 | prompt processing progress, n_tokens = 6837, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45773 | prompt done, n_tokens = 6837, batch.n_tokens = 64
slot init_sampler: id  0 | task 45773 | init sampler, took 1.31 ms, tokens: text = 6837, total = 6837
slot update_slots: id  0 | task 45773 | erasing old context checkpoint (pos_min = 244, pos_max = 1011, size = 18.009 MiB)
slot update_slots: id  0 | task 45773 | created context checkpoint 8 of 8 (pos_min = 6005, pos_max = 6772, size = 18.009 MiB)
slot print_timing: id  0 | task 45773 | 
prompt eval time =    1248.04 ms /   801 tokens (    1.56 ms per token,   641.81 tokens per second)
       eval time =    1397.05 ms /    47 tokens (   29.72 ms per token,    33.64 tokens per second)
      total time =    2645.09 ms /   848 tokens
slot      release: id  0 | task 45773 | stop processing: n_tokens = 6883, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.894 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45822 | processing task, is_child = 0
slot update_slots: id  0 | task 45822 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 7663
slot update_slots: id  0 | task 45822 | n_tokens = 6851, memory_seq_rm [6851, end)
slot update_slots: id  0 | task 45822 | prompt processing progress, n_tokens = 7599, batch.n_tokens = 748, progress = 0.991648
slot update_slots: id  0 | task 45822 | n_tokens = 7599, memory_seq_rm [7599, end)
slot update_slots: id  0 | task 45822 | prompt processing progress, n_tokens = 7663, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45822 | prompt done, n_tokens = 7663, batch.n_tokens = 64
slot init_sampler: id  0 | task 45822 | init sampler, took 1.07 ms, tokens: text = 7663, total = 7663
slot update_slots: id  0 | task 45822 | erasing old context checkpoint (pos_min = 1013, pos_max = 1780, size = 18.009 MiB)
slot update_slots: id  0 | task 45822 | created context checkpoint 8 of 8 (pos_min = 6831, pos_max = 7598, size = 18.009 MiB)
slot print_timing: id  0 | task 45822 | 
prompt eval time =    1262.89 ms /   812 tokens (    1.56 ms per token,   642.97 tokens per second)
       eval time =    1414.14 ms /    47 tokens (   30.09 ms per token,    33.24 tokens per second)
      total time =    2677.03 ms /   859 tokens
slot      release: id  0 | task 45822 | stop processing: n_tokens = 7709, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.894 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45871 | processing task, is_child = 0
slot update_slots: id  0 | task 45871 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 8587
slot update_slots: id  0 | task 45871 | n_tokens = 7677, memory_seq_rm [7677, end)
slot update_slots: id  0 | task 45871 | prompt processing progress, n_tokens = 8523, batch.n_tokens = 846, progress = 0.992547
slot update_slots: id  0 | task 45871 | n_tokens = 8523, memory_seq_rm [8523, end)
slot update_slots: id  0 | task 45871 | prompt processing progress, n_tokens = 8587, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45871 | prompt done, n_tokens = 8587, batch.n_tokens = 64
slot init_sampler: id  0 | task 45871 | init sampler, took 1.91 ms, tokens: text = 8587, total = 8587
slot update_slots: id  0 | task 45871 | erasing old context checkpoint (pos_min = 1112, pos_max = 1874, size = 17.892 MiB)
slot update_slots: id  0 | task 45871 | created context checkpoint 8 of 8 (pos_min = 7755, pos_max = 8522, size = 18.009 MiB)
slot print_timing: id  0 | task 45871 | 
prompt eval time =    1413.35 ms /   910 tokens (    1.55 ms per token,   643.86 tokens per second)
       eval time =  131787.78 ms /  4096 tokens (   32.17 ms per token,    31.08 tokens per second)
      total time =  133201.13 ms /  5006 tokens
slot      release: id  0 | task 45871 | stop processing: n_tokens = 12682, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 45871
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.702 (> 0.100 thold), f_keep = 0.060
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 12682, total state size = 315.389 MiB
srv          load:  - looking for better prompt, base f_keep = 0.060, sim = 0.702
srv        update:  - cache state: 15 prompts, 5072.525 MiB (limits: 8192.000 MiB, 40192 tokens, 256218 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv        update:    - prompt 0x59729e4b9300:    6026 tokens, checkpoints:  2,   192.916 MiB
srv        update:    - prompt 0x59729721bfc0:    8368 tokens, checkpoints:  8,   357.529 MiB
srv        update:    - prompt 0x5972a2bc32a0:   39462 tokens, checkpoints:  1,   961.361 MiB
srv        update:    - prompt 0x597299270410:    2048 tokens, checkpoints:  0,    66.033 MiB
srv        update:    - prompt 0x59729817f9a0:    7987 tokens, checkpoints:  8,   349.369 MiB
srv        update:    - prompt 0x5972a2bd71a0:    6821 tokens, checkpoints:  1,   195.964 MiB
srv        update:    - prompt 0x5972911f8e70:   12682 tokens, checkpoints:  8,   459.462 MiB
srv  get_availabl: prompt cache update took 356.42 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 49970 | processing task, is_child = 0
slot update_slots: id  0 | task 49970 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 1076
slot update_slots: id  0 | task 49970 | n_past = 755, slot.prompt.tokens.size() = 12682, seq_id = 0, pos_min = 11914, n_swa = 128
slot update_slots: id  0 | task 49970 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 49970 | erased invalidated context checkpoint (pos_min = 1562, pos_max = 2329, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 49970 | erased invalidated context checkpoint (pos_min = 2486, pos_max = 3253, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 49970 | erased invalidated context checkpoint (pos_min = 3386, pos_max = 4153, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 49970 | erased invalidated context checkpoint (pos_min = 4226, pos_max = 4993, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 49970 | erased invalidated context checkpoint (pos_min = 5190, pos_max = 5957, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 49970 | erased invalidated context checkpoint (pos_min = 6005, pos_max = 6772, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 49970 | erased invalidated context checkpoint (pos_min = 6831, pos_max = 7598, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 49970 | erased invalidated context checkpoint (pos_min = 7755, pos_max = 8522, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 49970 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 49970 | prompt processing progress, n_tokens = 1012, batch.n_tokens = 1012, progress = 0.940520
slot update_slots: id  0 | task 49970 | n_tokens = 1012, memory_seq_rm [1012, end)
slot update_slots: id  0 | task 49970 | prompt processing progress, n_tokens = 1076, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 49970 | prompt done, n_tokens = 1076, batch.n_tokens = 64
slot init_sampler: id  0 | task 49970 | init sampler, took 0.17 ms, tokens: text = 1076, total = 1076
slot update_slots: id  0 | task 49970 | created context checkpoint 1 of 8 (pos_min = 244, pos_max = 1011, size = 18.009 MiB)
slot print_timing: id  0 | task 49970 | 
prompt eval time =    1460.08 ms /  1076 tokens (    1.36 ms per token,   736.94 tokens per second)
       eval time =   21111.97 ms /   597 tokens (   35.36 ms per token,    28.28 tokens per second)
      total time =   22572.06 ms /  1673 tokens
slot      release: id  0 | task 49970 | stop processing: n_tokens = 1672, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.655 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 50569 | processing task, is_child = 0
slot update_slots: id  0 | task 50569 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 2498
slot update_slots: id  0 | task 50569 | n_tokens = 1636, memory_seq_rm [1636, end)
slot update_slots: id  0 | task 50569 | prompt processing progress, n_tokens = 2434, batch.n_tokens = 798, progress = 0.974379
slot update_slots: id  0 | task 50569 | n_tokens = 2434, memory_seq_rm [2434, end)
slot update_slots: id  0 | task 50569 | prompt processing progress, n_tokens = 2498, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 50569 | prompt done, n_tokens = 2498, batch.n_tokens = 64
slot init_sampler: id  0 | task 50569 | init sampler, took 0.37 ms, tokens: text = 2498, total = 2498
slot update_slots: id  0 | task 50569 | created context checkpoint 2 of 8 (pos_min = 1666, pos_max = 2433, size = 18.009 MiB)
slot print_timing: id  0 | task 50569 | 
prompt eval time =    1246.95 ms /   862 tokens (    1.45 ms per token,   691.28 tokens per second)
       eval time =    3343.46 ms /   111 tokens (   30.12 ms per token,    33.20 tokens per second)
      total time =    4590.42 ms /   973 tokens
slot      release: id  0 | task 50569 | stop processing: n_tokens = 2608, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.871 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 50682 | processing task, is_child = 0
slot update_slots: id  0 | task 50682 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 2957
slot update_slots: id  0 | task 50682 | n_tokens = 2575, memory_seq_rm [2575, end)
slot update_slots: id  0 | task 50682 | prompt processing progress, n_tokens = 2893, batch.n_tokens = 318, progress = 0.978356
slot update_slots: id  0 | task 50682 | n_tokens = 2893, memory_seq_rm [2893, end)
slot update_slots: id  0 | task 50682 | prompt processing progress, n_tokens = 2957, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 50682 | prompt done, n_tokens = 2957, batch.n_tokens = 64
slot init_sampler: id  0 | task 50682 | init sampler, took 0.41 ms, tokens: text = 2957, total = 2957
slot update_slots: id  0 | task 50682 | created context checkpoint 3 of 8 (pos_min = 2125, pos_max = 2892, size = 18.009 MiB)
slot print_timing: id  0 | task 50682 | 
prompt eval time =     681.24 ms /   382 tokens (    1.78 ms per token,   560.74 tokens per second)
       eval time =    1765.80 ms /    62 tokens (   28.48 ms per token,    35.11 tokens per second)
      total time =    2447.04 ms /   444 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 50682 | stop processing: n_tokens = 3018, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.914 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 50746 | processing task, is_child = 0
slot update_slots: id  0 | task 50746 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3266
slot update_slots: id  0 | task 50746 | n_tokens = 2985, memory_seq_rm [2985, end)
slot update_slots: id  0 | task 50746 | prompt processing progress, n_tokens = 3202, batch.n_tokens = 217, progress = 0.980404
slot update_slots: id  0 | task 50746 | n_tokens = 3202, memory_seq_rm [3202, end)
slot update_slots: id  0 | task 50746 | prompt processing progress, n_tokens = 3266, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 50746 | prompt done, n_tokens = 3266, batch.n_tokens = 64
slot init_sampler: id  0 | task 50746 | init sampler, took 0.45 ms, tokens: text = 3266, total = 3266
slot update_slots: id  0 | task 50746 | created context checkpoint 4 of 8 (pos_min = 2434, pos_max = 3201, size = 18.009 MiB)
slot print_timing: id  0 | task 50746 | 
prompt eval time =     600.89 ms /   281 tokens (    2.14 ms per token,   467.64 tokens per second)
       eval time =    1376.56 ms /    50 tokens (   27.53 ms per token,    36.32 tokens per second)
      total time =    1977.45 ms /   331 tokens
slot      release: id  0 | task 50746 | stop processing: n_tokens = 3315, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.912 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 50798 | processing task, is_child = 0
slot update_slots: id  0 | task 50798 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3600
slot update_slots: id  0 | task 50798 | n_tokens = 3282, memory_seq_rm [3282, end)
slot update_slots: id  0 | task 50798 | prompt processing progress, n_tokens = 3536, batch.n_tokens = 254, progress = 0.982222
slot update_slots: id  0 | task 50798 | n_tokens = 3536, memory_seq_rm [3536, end)
slot update_slots: id  0 | task 50798 | prompt processing progress, n_tokens = 3600, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 50798 | prompt done, n_tokens = 3600, batch.n_tokens = 64
slot init_sampler: id  0 | task 50798 | init sampler, took 0.50 ms, tokens: text = 3600, total = 3600
slot update_slots: id  0 | task 50798 | created context checkpoint 5 of 8 (pos_min = 2768, pos_max = 3535, size = 18.009 MiB)
slot print_timing: id  0 | task 50798 | 
prompt eval time =     667.11 ms /   318 tokens (    2.10 ms per token,   476.68 tokens per second)
       eval time =    1451.87 ms /    52 tokens (   27.92 ms per token,    35.82 tokens per second)
      total time =    2118.98 ms /   370 tokens
slot      release: id  0 | task 50798 | stop processing: n_tokens = 3651, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.895 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 50852 | processing task, is_child = 0
slot update_slots: id  0 | task 50852 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4037
slot update_slots: id  0 | task 50852 | n_tokens = 3614, memory_seq_rm [3614, end)
slot update_slots: id  0 | task 50852 | prompt processing progress, n_tokens = 3973, batch.n_tokens = 359, progress = 0.984147
slot update_slots: id  0 | task 50852 | n_tokens = 3973, memory_seq_rm [3973, end)
slot update_slots: id  0 | task 50852 | prompt processing progress, n_tokens = 4037, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 50852 | prompt done, n_tokens = 4037, batch.n_tokens = 64
slot init_sampler: id  0 | task 50852 | init sampler, took 0.77 ms, tokens: text = 4037, total = 4037
slot update_slots: id  0 | task 50852 | created context checkpoint 6 of 8 (pos_min = 3205, pos_max = 3972, size = 18.009 MiB)
slot print_timing: id  0 | task 50852 | 
prompt eval time =     716.25 ms /   423 tokens (    1.69 ms per token,   590.57 tokens per second)
       eval time =    1474.72 ms /    52 tokens (   28.36 ms per token,    35.26 tokens per second)
      total time =    2190.98 ms /   475 tokens
slot      release: id  0 | task 50852 | stop processing: n_tokens = 4088, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.937 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 50906 | processing task, is_child = 0
slot update_slots: id  0 | task 50906 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4324
slot update_slots: id  0 | task 50906 | n_tokens = 4051, memory_seq_rm [4051, end)
slot update_slots: id  0 | task 50906 | prompt processing progress, n_tokens = 4260, batch.n_tokens = 209, progress = 0.985199
slot update_slots: id  0 | task 50906 | n_tokens = 4260, memory_seq_rm [4260, end)
slot update_slots: id  0 | task 50906 | prompt processing progress, n_tokens = 4324, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 50906 | prompt done, n_tokens = 4324, batch.n_tokens = 64
slot init_sampler: id  0 | task 50906 | init sampler, took 0.60 ms, tokens: text = 4324, total = 4324
slot update_slots: id  0 | task 50906 | created context checkpoint 7 of 8 (pos_min = 3492, pos_max = 4259, size = 18.009 MiB)
slot print_timing: id  0 | task 50906 | 
prompt eval time =     616.16 ms /   273 tokens (    2.26 ms per token,   443.07 tokens per second)
       eval time =    1414.14 ms /    48 tokens (   29.46 ms per token,    33.94 tokens per second)
      total time =    2030.30 ms /   321 tokens
slot      release: id  0 | task 50906 | stop processing: n_tokens = 4371, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 50956 | processing task, is_child = 0
slot update_slots: id  0 | task 50956 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 4643
slot update_slots: id  0 | task 50956 | n_tokens = 4338, memory_seq_rm [4338, end)
slot update_slots: id  0 | task 50956 | prompt processing progress, n_tokens = 4579, batch.n_tokens = 241, progress = 0.986216
slot update_slots: id  0 | task 50956 | n_tokens = 4579, memory_seq_rm [4579, end)
slot update_slots: id  0 | task 50956 | prompt processing progress, n_tokens = 4643, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 50956 | prompt done, n_tokens = 4643, batch.n_tokens = 64
slot init_sampler: id  0 | task 50956 | init sampler, took 0.85 ms, tokens: text = 4643, total = 4643
slot update_slots: id  0 | task 50956 | created context checkpoint 8 of 8 (pos_min = 3811, pos_max = 4578, size = 18.009 MiB)
slot print_timing: id  0 | task 50956 | 
prompt eval time =     670.18 ms /   305 tokens (    2.20 ms per token,   455.10 tokens per second)
       eval time =    1558.84 ms /    52 tokens (   29.98 ms per token,    33.36 tokens per second)
      total time =    2229.02 ms /   357 tokens
slot      release: id  0 | task 50956 | stop processing: n_tokens = 4694, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.915 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 51010 | processing task, is_child = 0
slot update_slots: id  0 | task 51010 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 5092
slot update_slots: id  0 | task 51010 | n_tokens = 4657, memory_seq_rm [4657, end)
slot update_slots: id  0 | task 51010 | prompt processing progress, n_tokens = 5028, batch.n_tokens = 371, progress = 0.987431
slot update_slots: id  0 | task 51010 | n_tokens = 5028, memory_seq_rm [5028, end)
slot update_slots: id  0 | task 51010 | prompt processing progress, n_tokens = 5092, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 51010 | prompt done, n_tokens = 5092, batch.n_tokens = 64
slot init_sampler: id  0 | task 51010 | init sampler, took 0.90 ms, tokens: text = 5092, total = 5092
slot update_slots: id  0 | task 51010 | erasing old context checkpoint (pos_min = 244, pos_max = 1011, size = 18.009 MiB)
slot update_slots: id  0 | task 51010 | created context checkpoint 8 of 8 (pos_min = 4260, pos_max = 5027, size = 18.009 MiB)
slot print_timing: id  0 | task 51010 | 
prompt eval time =     780.00 ms /   435 tokens (    1.79 ms per token,   557.69 tokens per second)
       eval time =    2666.25 ms /    88 tokens (   30.30 ms per token,    33.01 tokens per second)
      total time =    3446.25 ms /   523 tokens
slot      release: id  0 | task 51010 | stop processing: n_tokens = 5179, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.800 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 51100 | processing task, is_child = 0
slot update_slots: id  0 | task 51100 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 6448
slot update_slots: id  0 | task 51100 | n_tokens = 5157, memory_seq_rm [5157, end)
slot update_slots: id  0 | task 51100 | prompt processing progress, n_tokens = 6384, batch.n_tokens = 1227, progress = 0.990074
slot update_slots: id  0 | task 51100 | n_tokens = 6384, memory_seq_rm [6384, end)
slot update_slots: id  0 | task 51100 | prompt processing progress, n_tokens = 6448, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 51100 | prompt done, n_tokens = 6448, batch.n_tokens = 64
slot init_sampler: id  0 | task 51100 | init sampler, took 1.12 ms, tokens: text = 6448, total = 6448
slot update_slots: id  0 | task 51100 | erasing old context checkpoint (pos_min = 1666, pos_max = 2433, size = 18.009 MiB)
slot update_slots: id  0 | task 51100 | created context checkpoint 8 of 8 (pos_min = 5616, pos_max = 6383, size = 18.009 MiB)
slot print_timing: id  0 | task 51100 | 
prompt eval time =    1953.15 ms /  1291 tokens (    1.51 ms per token,   660.98 tokens per second)
       eval time =   77558.62 ms /  2419 tokens (   32.06 ms per token,    31.19 tokens per second)
      total time =   79511.77 ms /  3710 tokens
slot      release: id  0 | task 51100 | stop processing: n_tokens = 8866, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = 6860309882
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8866, total state size = 225.908 MiB
srv          load:  - looking for better prompt, base f_keep = 0.007, sim = 0.015
srv        update:  - cache state: 16 prompts, 5442.506 MiB (limits: 8192.000 MiB, 40192 tokens, 252146 est)
srv        update:    - prompt 0x59729844bc90:   13521 tokens, checkpoints:  8,   478.479 MiB
srv        update:    - prompt 0x59729716e1c0:   12204 tokens, checkpoints:  1,   322.189 MiB
srv        update:    - prompt 0x59729844baa0:   10958 tokens, checkpoints:  8,   417.887 MiB
srv        update:    - prompt 0x597299082d10:   10480 tokens, checkpoints:  1,   281.763 MiB
srv        update:    - prompt 0x59729b4ac4d0:    5830 tokens, checkpoints:  5,   241.268 MiB
srv        update:    - prompt 0x597298341840:    9790 tokens, checkpoints:  5,   334.548 MiB
srv        update:    - prompt 0x5972971d9590:    6089 tokens, checkpoints:  2,   194.393 MiB
srv        update:    - prompt 0x59729921c810:    6386 tokens, checkpoints:  3,   219.366 MiB
srv        update:    - prompt 0x59729e4b9300:    6026 tokens, checkpoints:  2,   192.916 MiB
srv        update:    - prompt 0x59729721bfc0:    8368 tokens, checkpoints:  8,   357.529 MiB
srv        update:    - prompt 0x5972a2bc32a0:   39462 tokens, checkpoints:  1,   961.361 MiB
srv        update:    - prompt 0x597299270410:    2048 tokens, checkpoints:  0,    66.033 MiB
srv        update:    - prompt 0x59729817f9a0:    7987 tokens, checkpoints:  8,   349.369 MiB
srv        update:    - prompt 0x5972a2bd71a0:    6821 tokens, checkpoints:  1,   195.964 MiB
srv        update:    - prompt 0x5972911f8e70:   12682 tokens, checkpoints:  8,   459.462 MiB
srv        update:    - prompt 0x59729d140db0:    8866 tokens, checkpoints:  8,   369.980 MiB
srv  get_availabl: prompt cache update took 293.35 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 53521 | processing task, is_child = 0
slot update_slots: id  0 | task 53521 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 3957
slot update_slots: id  0 | task 53521 | n_past = 59, slot.prompt.tokens.size() = 8866, seq_id = 0, pos_min = 8098, n_swa = 128
slot update_slots: id  0 | task 53521 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 53521 | erased invalidated context checkpoint (pos_min = 2125, pos_max = 2892, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 53521 | erased invalidated context checkpoint (pos_min = 2434, pos_max = 3201, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 53521 | erased invalidated context checkpoint (pos_min = 2768, pos_max = 3535, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 53521 | erased invalidated context checkpoint (pos_min = 3205, pos_max = 3972, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 53521 | erased invalidated context checkpoint (pos_min = 3492, pos_max = 4259, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 53521 | erased invalidated context checkpoint (pos_min = 3811, pos_max = 4578, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 53521 | erased invalidated context checkpoint (pos_min = 4260, pos_max = 5027, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 53521 | erased invalidated context checkpoint (pos_min = 5616, pos_max = 6383, n_swa = 128, size = 18.009 MiB)
slot update_slots: id  0 | task 53521 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 53521 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.517564
slot update_slots: id  0 | task 53521 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 53521 | prompt processing progress, n_tokens = 3893, batch.n_tokens = 1845, progress = 0.983826
slot update_slots: id  0 | task 53521 | n_tokens = 3893, memory_seq_rm [3893, end)
slot update_slots: id  0 | task 53521 | prompt processing progress, n_tokens = 3957, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 53521 | prompt done, n_tokens = 3957, batch.n_tokens = 64
slot init_sampler: id  0 | task 53521 | init sampler, took 0.57 ms, tokens: text = 3957, total = 3957
slot update_slots: id  0 | task 53521 | created context checkpoint 1 of 8 (pos_min = 3125, pos_max = 3892, size = 18.009 MiB)
slot print_timing: id  0 | task 53521 | 
prompt eval time =    4853.26 ms /  3957 tokens (    1.23 ms per token,   815.33 tokens per second)
       eval time =   24713.70 ms /   755 tokens (   32.73 ms per token,    30.55 tokens per second)
      total time =   29566.96 ms /  4712 tokens
slot      release: id  0 | task 53521 | stop processing: n_tokens = 4711, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
