ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14807 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1763 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 56064 -> need 1767 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 2.40 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14807 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.....srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 56064
llama_context: n_ctx_seq     = 56064
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (56064) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 56064 cells
llama_kv_cache:      CUDA0 KV buffer size =  1314.00 MiB
llama_kv_cache: size = 1314.00 MiB ( 56064 cells,  12 layers,  4/1 seqs), K (f16):  657.00 MiB, V (f16):  657.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   117.15 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 56.24 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  1 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  2 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  3 | task -1 | new slot, n_ctx = 56064
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-18

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 682
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 618, batch.n_tokens = 618, progress = 0.906158
slot update_slots: id  3 | task 0 | n_tokens = 618, memory_seq_rm [618, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 682, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 682, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.11 ms, tokens: text = 682, total = 682
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 617, size = 14.492 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =    1160.64 ms /   682 tokens (    1.70 ms per token,   587.61 tokens per second)
       eval time =    1030.32 ms /    45 tokens (   22.90 ms per token,    43.68 tokens per second)
      total time =    2190.96 ms /   727 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 726, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.916 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 47 | processing task, is_child = 0
slot update_slots: id  3 | task 47 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 778
slot update_slots: id  3 | task 47 | n_tokens = 713, memory_seq_rm [713, end)
slot update_slots: id  3 | task 47 | prompt processing progress, n_tokens = 714, batch.n_tokens = 1, progress = 0.917738
slot update_slots: id  3 | task 47 | n_tokens = 714, memory_seq_rm [714, end)
slot update_slots: id  3 | task 47 | prompt processing progress, n_tokens = 778, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 47 | prompt done, n_tokens = 778, batch.n_tokens = 64
slot init_sampler: id  3 | task 47 | init sampler, took 0.12 ms, tokens: text = 778, total = 778
slot update_slots: id  3 | task 47 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 713, size = 16.743 MiB)
slot print_timing: id  3 | task 47 | 
prompt eval time =     290.29 ms /    65 tokens (    4.47 ms per token,   223.91 tokens per second)
       eval time =     591.87 ms /    27 tokens (   21.92 ms per token,    45.62 tokens per second)
      total time =     882.16 ms /    92 tokens
slot      release: id  3 | task 47 | stop processing: n_tokens = 804, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.863 (> 0.100 thold), f_keep = 0.848
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 76 | processing task, is_child = 0
slot update_slots: id  3 | task 76 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 790
slot update_slots: id  3 | task 76 | n_tokens = 682, memory_seq_rm [682, end)
slot update_slots: id  3 | task 76 | prompt processing progress, n_tokens = 726, batch.n_tokens = 44, progress = 0.918987
slot update_slots: id  3 | task 76 | n_tokens = 726, memory_seq_rm [726, end)
slot update_slots: id  3 | task 76 | prompt processing progress, n_tokens = 790, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 76 | prompt done, n_tokens = 790, batch.n_tokens = 64
slot init_sampler: id  3 | task 76 | init sampler, took 0.14 ms, tokens: text = 790, total = 790
slot print_timing: id  3 | task 76 | 
prompt eval time =     404.15 ms /   108 tokens (    3.74 ms per token,   267.22 tokens per second)
       eval time =     938.69 ms /    42 tokens (   22.35 ms per token,    44.74 tokens per second)
      total time =    1342.84 ms /   150 tokens
slot      release: id  3 | task 76 | stop processing: n_tokens = 831, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.831 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 120 | processing task, is_child = 0
slot update_slots: id  3 | task 120 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 978
slot update_slots: id  3 | task 120 | n_tokens = 813, memory_seq_rm [813, end)
slot update_slots: id  3 | task 120 | prompt processing progress, n_tokens = 914, batch.n_tokens = 101, progress = 0.934560
slot update_slots: id  3 | task 120 | n_tokens = 914, memory_seq_rm [914, end)
slot update_slots: id  3 | task 120 | prompt processing progress, n_tokens = 978, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 120 | prompt done, n_tokens = 978, batch.n_tokens = 64
slot init_sampler: id  3 | task 120 | init sampler, took 0.15 ms, tokens: text = 978, total = 978
slot update_slots: id  3 | task 120 | created context checkpoint 3 of 8 (pos_min = 0, pos_max = 913, size = 21.433 MiB)
slot print_timing: id  3 | task 120 | 
prompt eval time =     376.92 ms /   165 tokens (    2.28 ms per token,   437.76 tokens per second)
       eval time =    2273.24 ms /    99 tokens (   22.96 ms per token,    43.55 tokens per second)
      total time =    2650.16 ms /   264 tokens
slot      release: id  3 | task 120 | stop processing: n_tokens = 1076, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.948 (> 0.100 thold), f_keep = 0.629
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 221 | processing task, is_child = 0
slot update_slots: id  3 | task 221 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 714
slot update_slots: id  3 | task 221 | n_tokens = 677, memory_seq_rm [677, end)
slot update_slots: id  3 | task 221 | prompt processing progress, n_tokens = 714, batch.n_tokens = 37, progress = 1.000000
slot update_slots: id  3 | task 221 | prompt done, n_tokens = 714, batch.n_tokens = 37
slot init_sampler: id  3 | task 221 | init sampler, took 0.10 ms, tokens: text = 714, total = 714
slot print_timing: id  3 | task 221 | 
prompt eval time =     212.57 ms /    37 tokens (    5.75 ms per token,   174.06 tokens per second)
       eval time =    1573.85 ms /    65 tokens (   24.21 ms per token,    41.30 tokens per second)
      total time =    1786.42 ms /   102 tokens
slot      release: id  3 | task 221 | stop processing: n_tokens = 778, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.516 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 287 | processing task, is_child = 0
slot update_slots: id  3 | task 287 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1462
slot update_slots: id  3 | task 287 | n_tokens = 754, memory_seq_rm [754, end)
slot update_slots: id  3 | task 287 | prompt processing progress, n_tokens = 1398, batch.n_tokens = 644, progress = 0.956224
slot update_slots: id  3 | task 287 | n_tokens = 1398, memory_seq_rm [1398, end)
slot update_slots: id  3 | task 287 | prompt processing progress, n_tokens = 1462, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 287 | prompt done, n_tokens = 1462, batch.n_tokens = 64
slot init_sampler: id  3 | task 287 | init sampler, took 0.22 ms, tokens: text = 1462, total = 1462
slot update_slots: id  3 | task 287 | created context checkpoint 4 of 8 (pos_min = 677, pos_max = 1397, size = 16.907 MiB)
slot print_timing: id  3 | task 287 | 
prompt eval time =     863.57 ms /   708 tokens (    1.22 ms per token,   819.86 tokens per second)
       eval time =    1088.13 ms /    44 tokens (   24.73 ms per token,    40.44 tokens per second)
      total time =    1951.69 ms /   752 tokens
slot      release: id  3 | task 287 | stop processing: n_tokens = 1505, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.457 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 333 | processing task, is_child = 0
slot update_slots: id  3 | task 333 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3228
slot update_slots: id  3 | task 333 | n_tokens = 1475, memory_seq_rm [1475, end)
slot update_slots: id  3 | task 333 | prompt processing progress, n_tokens = 3164, batch.n_tokens = 1689, progress = 0.980173
slot update_slots: id  3 | task 333 | n_tokens = 3164, memory_seq_rm [3164, end)
slot update_slots: id  3 | task 333 | prompt processing progress, n_tokens = 3228, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 333 | prompt done, n_tokens = 3228, batch.n_tokens = 64
slot init_sampler: id  3 | task 333 | init sampler, took 0.49 ms, tokens: text = 3228, total = 3228
slot update_slots: id  3 | task 333 | created context checkpoint 5 of 8 (pos_min = 2140, pos_max = 3163, size = 24.012 MiB)
slot print_timing: id  3 | task 333 | 
prompt eval time =    1968.33 ms /  1753 tokens (    1.12 ms per token,   890.60 tokens per second)
       eval time =    1005.93 ms /    39 tokens (   25.79 ms per token,    38.77 tokens per second)
      total time =    2974.26 ms /  1792 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 333 | stop processing: n_tokens = 3266, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.563 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 374 | processing task, is_child = 0
slot update_slots: id  3 | task 374 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5752
slot update_slots: id  3 | task 374 | n_tokens = 3236, memory_seq_rm [3236, end)
slot update_slots: id  3 | task 374 | prompt processing progress, n_tokens = 5284, batch.n_tokens = 2048, progress = 0.918637
slot update_slots: id  3 | task 374 | n_tokens = 5284, memory_seq_rm [5284, end)
slot update_slots: id  3 | task 374 | prompt processing progress, n_tokens = 5688, batch.n_tokens = 404, progress = 0.988873
slot update_slots: id  3 | task 374 | n_tokens = 5688, memory_seq_rm [5688, end)
slot update_slots: id  3 | task 374 | prompt processing progress, n_tokens = 5752, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 374 | prompt done, n_tokens = 5752, batch.n_tokens = 64
slot init_sampler: id  3 | task 374 | init sampler, took 0.82 ms, tokens: text = 5752, total = 5752
slot update_slots: id  3 | task 374 | created context checkpoint 6 of 8 (pos_min = 4664, pos_max = 5687, size = 24.012 MiB)
slot print_timing: id  3 | task 374 | 
prompt eval time =    2795.81 ms /  2516 tokens (    1.11 ms per token,   899.92 tokens per second)
       eval time =    1015.26 ms /    39 tokens (   26.03 ms per token,    38.41 tokens per second)
      total time =    3811.07 ms /  2555 tokens
slot      release: id  3 | task 374 | stop processing: n_tokens = 5790, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.771 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 416 | processing task, is_child = 0
slot update_slots: id  3 | task 416 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7466
slot update_slots: id  3 | task 416 | n_tokens = 5760, memory_seq_rm [5760, end)
slot update_slots: id  3 | task 416 | prompt processing progress, n_tokens = 7402, batch.n_tokens = 1642, progress = 0.991428
slot update_slots: id  3 | task 416 | n_tokens = 7402, memory_seq_rm [7402, end)
slot update_slots: id  3 | task 416 | prompt processing progress, n_tokens = 7466, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 416 | prompt done, n_tokens = 7466, batch.n_tokens = 64
slot init_sampler: id  3 | task 416 | init sampler, took 1.42 ms, tokens: text = 7466, total = 7466
slot update_slots: id  3 | task 416 | created context checkpoint 7 of 8 (pos_min = 6378, pos_max = 7401, size = 24.012 MiB)
slot print_timing: id  3 | task 416 | 
prompt eval time =    2171.56 ms /  1706 tokens (    1.27 ms per token,   785.61 tokens per second)
       eval time =   66424.41 ms /  2458 tokens (   27.02 ms per token,    37.00 tokens per second)
      total time =   68595.97 ms /  4164 tokens
slot      release: id  3 | task 416 | stop processing: n_tokens = 9923, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 2876 | processing task, is_child = 0
slot update_slots: id  2 | task 2876 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9502
slot update_slots: id  2 | task 2876 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.215534
slot update_slots: id  2 | task 2876 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.431067
slot update_slots: id  2 | task 2876 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.646601
slot update_slots: id  2 | task 2876 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.862134
slot update_slots: id  2 | task 2876 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 9438, batch.n_tokens = 1246, progress = 0.993265
slot update_slots: id  2 | task 2876 | n_tokens = 9438, memory_seq_rm [9438, end)
slot update_slots: id  2 | task 2876 | prompt processing progress, n_tokens = 9502, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 2876 | prompt done, n_tokens = 9502, batch.n_tokens = 64
slot init_sampler: id  2 | task 2876 | init sampler, took 1.40 ms, tokens: text = 9502, total = 9502
slot update_slots: id  2 | task 2876 | created context checkpoint 1 of 8 (pos_min = 8541, pos_max = 9437, size = 21.034 MiB)
slot print_timing: id  2 | task 2876 | 
prompt eval time =   11765.15 ms /  9502 tokens (    1.24 ms per token,   807.64 tokens per second)
       eval time =    5365.91 ms /   189 tokens (   28.39 ms per token,    35.22 tokens per second)
      total time =   17131.06 ms /  9691 tokens
slot      release: id  2 | task 2876 | stop processing: n_tokens = 9690, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3071 | processing task, is_child = 0
slot update_slots: id  2 | task 3071 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9789
slot update_slots: id  2 | task 3071 | n_tokens = 9660, memory_seq_rm [9660, end)
slot update_slots: id  2 | task 3071 | prompt processing progress, n_tokens = 9725, batch.n_tokens = 65, progress = 0.993462
slot update_slots: id  2 | task 3071 | n_tokens = 9725, memory_seq_rm [9725, end)
slot update_slots: id  2 | task 3071 | prompt processing progress, n_tokens = 9789, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3071 | prompt done, n_tokens = 9789, batch.n_tokens = 64
slot init_sampler: id  2 | task 3071 | init sampler, took 1.42 ms, tokens: text = 9789, total = 9789
slot update_slots: id  2 | task 3071 | created context checkpoint 2 of 8 (pos_min = 8828, pos_max = 9724, size = 21.034 MiB)
slot print_timing: id  2 | task 3071 | 
prompt eval time =     457.56 ms /   129 tokens (    3.55 ms per token,   281.93 tokens per second)
       eval time =    7526.95 ms /   253 tokens (   29.75 ms per token,    33.61 tokens per second)
      total time =    7984.51 ms /   382 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 3071 | stop processing: n_tokens = 10041, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3326 | processing task, is_child = 0
slot update_slots: id  2 | task 3326 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10083
slot update_slots: id  2 | task 3326 | n_tokens = 9872, memory_seq_rm [9872, end)
slot update_slots: id  2 | task 3326 | prompt processing progress, n_tokens = 10019, batch.n_tokens = 147, progress = 0.993653
slot update_slots: id  2 | task 3326 | n_tokens = 10019, memory_seq_rm [10019, end)
slot update_slots: id  2 | task 3326 | prompt processing progress, n_tokens = 10083, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3326 | prompt done, n_tokens = 10083, batch.n_tokens = 64
slot init_sampler: id  2 | task 3326 | init sampler, took 2.02 ms, tokens: text = 10083, total = 10083
slot update_slots: id  2 | task 3326 | created context checkpoint 3 of 8 (pos_min = 9144, pos_max = 10018, size = 20.518 MiB)
slot print_timing: id  2 | task 3326 | 
prompt eval time =     551.65 ms /   211 tokens (    2.61 ms per token,   382.49 tokens per second)
       eval time =    3696.32 ms /   127 tokens (   29.10 ms per token,    34.36 tokens per second)
      total time =    4247.96 ms /   338 tokens
slot      release: id  2 | task 3326 | stop processing: n_tokens = 10209, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.931
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3455 | processing task, is_child = 0
slot update_slots: id  2 | task 3455 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9988
slot update_slots: id  2 | task 3455 | n_tokens = 9502, memory_seq_rm [9502, end)
slot update_slots: id  2 | task 3455 | prompt processing progress, n_tokens = 9924, batch.n_tokens = 422, progress = 0.993592
slot update_slots: id  2 | task 3455 | n_tokens = 9924, memory_seq_rm [9924, end)
slot update_slots: id  2 | task 3455 | prompt processing progress, n_tokens = 9988, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3455 | prompt done, n_tokens = 9988, batch.n_tokens = 64
slot init_sampler: id  2 | task 3455 | init sampler, took 2.19 ms, tokens: text = 9988, total = 9988
slot print_timing: id  2 | task 3455 | 
prompt eval time =     871.06 ms /   486 tokens (    1.79 ms per token,   557.94 tokens per second)
       eval time =    4755.41 ms /   177 tokens (   26.87 ms per token,    37.22 tokens per second)
      total time =    5626.47 ms /   663 tokens
slot      release: id  2 | task 3455 | stop processing: n_tokens = 10164, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3634 | processing task, is_child = 0
slot update_slots: id  2 | task 3634 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10113
slot update_slots: id  2 | task 3634 | n_tokens = 9989, memory_seq_rm [9989, end)
slot update_slots: id  2 | task 3634 | prompt processing progress, n_tokens = 10049, batch.n_tokens = 60, progress = 0.993672
slot update_slots: id  2 | task 3634 | n_tokens = 10049, memory_seq_rm [10049, end)
slot update_slots: id  2 | task 3634 | prompt processing progress, n_tokens = 10113, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3634 | prompt done, n_tokens = 10113, batch.n_tokens = 64
slot init_sampler: id  2 | task 3634 | init sampler, took 3.51 ms, tokens: text = 10113, total = 10113
slot print_timing: id  2 | task 3634 | 
prompt eval time =     467.94 ms /   124 tokens (    3.77 ms per token,   264.99 tokens per second)
       eval time =    2451.58 ms /    90 tokens (   27.24 ms per token,    36.71 tokens per second)
      total time =    2919.51 ms /   214 tokens
slot      release: id  2 | task 3634 | stop processing: n_tokens = 10202, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3726 | processing task, is_child = 0
slot update_slots: id  2 | task 3726 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10259
slot update_slots: id  2 | task 3726 | n_tokens = 10143, memory_seq_rm [10143, end)
slot update_slots: id  2 | task 3726 | prompt processing progress, n_tokens = 10195, batch.n_tokens = 52, progress = 0.993762
slot update_slots: id  2 | task 3726 | n_tokens = 10195, memory_seq_rm [10195, end)
slot update_slots: id  2 | task 3726 | prompt processing progress, n_tokens = 10259, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3726 | prompt done, n_tokens = 10259, batch.n_tokens = 64
slot init_sampler: id  2 | task 3726 | init sampler, took 1.46 ms, tokens: text = 10259, total = 10259
slot update_slots: id  2 | task 3726 | created context checkpoint 4 of 8 (pos_min = 9375, pos_max = 10194, size = 19.228 MiB)
slot print_timing: id  2 | task 3726 | 
prompt eval time =     388.11 ms /   116 tokens (    3.35 ms per token,   298.89 tokens per second)
       eval time =    1885.63 ms /    69 tokens (   27.33 ms per token,    36.59 tokens per second)
      total time =    2273.74 ms /   185 tokens
slot      release: id  2 | task 3726 | stop processing: n_tokens = 10327, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3797 | processing task, is_child = 0
slot update_slots: id  2 | task 3797 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10309
slot update_slots: id  2 | task 3797 | n_tokens = 10113, memory_seq_rm [10113, end)
slot update_slots: id  2 | task 3797 | prompt processing progress, n_tokens = 10245, batch.n_tokens = 132, progress = 0.993792
slot update_slots: id  2 | task 3797 | n_tokens = 10245, memory_seq_rm [10245, end)
slot update_slots: id  2 | task 3797 | prompt processing progress, n_tokens = 10309, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3797 | prompt done, n_tokens = 10309, batch.n_tokens = 64
slot init_sampler: id  2 | task 3797 | init sampler, took 1.45 ms, tokens: text = 10309, total = 10309
slot print_timing: id  2 | task 3797 | 
prompt eval time =     603.26 ms /   196 tokens (    3.08 ms per token,   324.90 tokens per second)
       eval time =    2849.66 ms /   103 tokens (   27.67 ms per token,    36.14 tokens per second)
      total time =    3452.92 ms /   299 tokens
slot      release: id  2 | task 3797 | stop processing: n_tokens = 10411, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3902 | processing task, is_child = 0
slot update_slots: id  2 | task 3902 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10449
slot update_slots: id  2 | task 3902 | n_tokens = 10338, memory_seq_rm [10338, end)
slot update_slots: id  2 | task 3902 | prompt processing progress, n_tokens = 10385, batch.n_tokens = 47, progress = 0.993875
slot update_slots: id  2 | task 3902 | n_tokens = 10385, memory_seq_rm [10385, end)
slot update_slots: id  2 | task 3902 | prompt processing progress, n_tokens = 10449, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3902 | prompt done, n_tokens = 10449, batch.n_tokens = 64
slot init_sampler: id  2 | task 3902 | init sampler, took 1.47 ms, tokens: text = 10449, total = 10449
slot update_slots: id  2 | task 3902 | created context checkpoint 5 of 8 (pos_min = 9800, pos_max = 10384, size = 13.718 MiB)
slot print_timing: id  2 | task 3902 | 
prompt eval time =     373.87 ms /   111 tokens (    3.37 ms per token,   296.90 tokens per second)
       eval time =    1041.64 ms /    38 tokens (   27.41 ms per token,    36.48 tokens per second)
      total time =    1415.51 ms /   149 tokens
slot      release: id  2 | task 3902 | stop processing: n_tokens = 10486, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 3942 | processing task, is_child = 0
slot update_slots: id  2 | task 3942 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10474
slot update_slots: id  2 | task 3942 | n_tokens = 10309, memory_seq_rm [10309, end)
slot update_slots: id  2 | task 3942 | prompt processing progress, n_tokens = 10410, batch.n_tokens = 101, progress = 0.993890
slot update_slots: id  2 | task 3942 | n_tokens = 10410, memory_seq_rm [10410, end)
slot update_slots: id  2 | task 3942 | prompt processing progress, n_tokens = 10474, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 3942 | prompt done, n_tokens = 10474, batch.n_tokens = 64
slot init_sampler: id  2 | task 3942 | init sampler, took 1.51 ms, tokens: text = 10474, total = 10474
slot print_timing: id  2 | task 3942 | 
prompt eval time =     595.58 ms /   165 tokens (    3.61 ms per token,   277.04 tokens per second)
       eval time =    2556.71 ms /    93 tokens (   27.49 ms per token,    36.37 tokens per second)
      total time =    3152.29 ms /   258 tokens
slot      release: id  2 | task 3942 | stop processing: n_tokens = 10566, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4037 | processing task, is_child = 0
slot update_slots: id  2 | task 4037 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10605
slot update_slots: id  2 | task 4037 | n_tokens = 10497, memory_seq_rm [10497, end)
slot update_slots: id  2 | task 4037 | prompt processing progress, n_tokens = 10541, batch.n_tokens = 44, progress = 0.993965
slot update_slots: id  2 | task 4037 | n_tokens = 10541, memory_seq_rm [10541, end)
slot update_slots: id  2 | task 4037 | prompt processing progress, n_tokens = 10605, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4037 | prompt done, n_tokens = 10605, batch.n_tokens = 64
slot init_sampler: id  2 | task 4037 | init sampler, took 1.50 ms, tokens: text = 10605, total = 10605
slot update_slots: id  2 | task 4037 | created context checkpoint 6 of 8 (pos_min = 9907, pos_max = 10540, size = 14.867 MiB)
slot print_timing: id  2 | task 4037 | 
prompt eval time =     377.60 ms /   108 tokens (    3.50 ms per token,   286.02 tokens per second)
       eval time =    1326.69 ms /    49 tokens (   27.08 ms per token,    36.93 tokens per second)
      total time =    1704.30 ms /   157 tokens
slot      release: id  2 | task 4037 | stop processing: n_tokens = 10653, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4088 | processing task, is_child = 0
slot update_slots: id  2 | task 4088 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10642
slot update_slots: id  2 | task 4088 | n_tokens = 10474, memory_seq_rm [10474, end)
slot update_slots: id  2 | task 4088 | prompt processing progress, n_tokens = 10578, batch.n_tokens = 104, progress = 0.993986
slot update_slots: id  2 | task 4088 | n_tokens = 10578, memory_seq_rm [10578, end)
slot update_slots: id  2 | task 4088 | prompt processing progress, n_tokens = 10642, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4088 | prompt done, n_tokens = 10642, batch.n_tokens = 64
slot init_sampler: id  2 | task 4088 | init sampler, took 1.52 ms, tokens: text = 10642, total = 10642
slot print_timing: id  2 | task 4088 | 
prompt eval time =     603.21 ms /   168 tokens (    3.59 ms per token,   278.51 tokens per second)
       eval time =    2511.39 ms /    91 tokens (   27.60 ms per token,    36.23 tokens per second)
      total time =    3114.60 ms /   259 tokens
slot      release: id  2 | task 4088 | stop processing: n_tokens = 10732, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4181 | processing task, is_child = 0
slot update_slots: id  2 | task 4181 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10769
slot update_slots: id  2 | task 4181 | n_tokens = 10659, memory_seq_rm [10659, end)
slot update_slots: id  2 | task 4181 | prompt processing progress, n_tokens = 10705, batch.n_tokens = 46, progress = 0.994057
slot update_slots: id  2 | task 4181 | n_tokens = 10705, memory_seq_rm [10705, end)
slot update_slots: id  2 | task 4181 | prompt processing progress, n_tokens = 10769, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4181 | prompt done, n_tokens = 10769, batch.n_tokens = 64
slot init_sampler: id  2 | task 4181 | init sampler, took 1.62 ms, tokens: text = 10769, total = 10769
slot update_slots: id  2 | task 4181 | created context checkpoint 7 of 8 (pos_min = 9994, pos_max = 10704, size = 16.672 MiB)
slot print_timing: id  2 | task 4181 | 
prompt eval time =     382.21 ms /   110 tokens (    3.47 ms per token,   287.80 tokens per second)
       eval time =    1245.15 ms /    45 tokens (   27.67 ms per token,    36.14 tokens per second)
      total time =    1627.37 ms /   155 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 4181 | stop processing: n_tokens = 10813, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4228 | processing task, is_child = 0
slot update_slots: id  2 | task 4228 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10810
slot update_slots: id  2 | task 4228 | n_tokens = 10642, memory_seq_rm [10642, end)
slot update_slots: id  2 | task 4228 | prompt processing progress, n_tokens = 10746, batch.n_tokens = 104, progress = 0.994080
slot update_slots: id  2 | task 4228 | n_tokens = 10746, memory_seq_rm [10746, end)
slot update_slots: id  2 | task 4228 | prompt processing progress, n_tokens = 10810, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4228 | prompt done, n_tokens = 10810, batch.n_tokens = 64
slot init_sampler: id  2 | task 4228 | init sampler, took 1.53 ms, tokens: text = 10810, total = 10810
slot print_timing: id  2 | task 4228 | 
prompt eval time =     594.11 ms /   168 tokens (    3.54 ms per token,   282.77 tokens per second)
       eval time =    5740.17 ms /   206 tokens (   27.86 ms per token,    35.89 tokens per second)
      total time =    6334.28 ms /   374 tokens
slot      release: id  2 | task 4228 | stop processing: n_tokens = 11015, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4436 | processing task, is_child = 0
slot update_slots: id  2 | task 4436 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10869
slot update_slots: id  2 | task 4436 | n_tokens = 10811, memory_seq_rm [10811, end)
slot update_slots: id  2 | task 4436 | prompt processing progress, n_tokens = 10869, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  2 | task 4436 | prompt done, n_tokens = 10869, batch.n_tokens = 58
slot init_sampler: id  2 | task 4436 | init sampler, took 2.19 ms, tokens: text = 10869, total = 10869
slot update_slots: id  2 | task 4436 | created context checkpoint 8 of 8 (pos_min = 10118, pos_max = 10810, size = 16.250 MiB)
slot print_timing: id  2 | task 4436 | 
prompt eval time =     312.95 ms /    58 tokens (    5.40 ms per token,   185.34 tokens per second)
       eval time =    2595.34 ms /    94 tokens (   27.61 ms per token,    36.22 tokens per second)
      total time =    2908.29 ms /   152 tokens
slot      release: id  2 | task 4436 | stop processing: n_tokens = 10962, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4531 | processing task, is_child = 0
slot update_slots: id  2 | task 4531 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10999
slot update_slots: id  2 | task 4531 | n_tokens = 10889, memory_seq_rm [10889, end)
slot update_slots: id  2 | task 4531 | prompt processing progress, n_tokens = 10935, batch.n_tokens = 46, progress = 0.994181
slot update_slots: id  2 | task 4531 | n_tokens = 10935, memory_seq_rm [10935, end)
slot update_slots: id  2 | task 4531 | prompt processing progress, n_tokens = 10999, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4531 | prompt done, n_tokens = 10999, batch.n_tokens = 64
slot init_sampler: id  2 | task 4531 | init sampler, took 1.58 ms, tokens: text = 10999, total = 10999
slot update_slots: id  2 | task 4531 | erasing old context checkpoint (pos_min = 8541, pos_max = 9437, size = 21.034 MiB)
slot update_slots: id  2 | task 4531 | created context checkpoint 8 of 8 (pos_min = 10264, pos_max = 10934, size = 15.735 MiB)
slot print_timing: id  2 | task 4531 | 
prompt eval time =     374.88 ms /   110 tokens (    3.41 ms per token,   293.43 tokens per second)
       eval time =    1216.40 ms /    45 tokens (   27.03 ms per token,    36.99 tokens per second)
      total time =    1591.27 ms /   155 tokens
slot      release: id  2 | task 4531 | stop processing: n_tokens = 11043, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.061
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 11043, total state size = 275.315 MiB
srv          load:  - looking for better prompt, base f_keep = 0.061, sim = 0.962
srv        update:  - cache state: 1 prompts, 413.337 MiB (limits: 8192.000 MiB, 56064 tokens, 218862 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv  get_availabl: prompt cache update took 300.13 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4578 | processing task, is_child = 0
slot update_slots: id  2 | task 4578 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 704
slot update_slots: id  2 | task 4578 | n_past = 677, slot.prompt.tokens.size() = 11043, seq_id = 2, pos_min = 10345, n_swa = 128
slot update_slots: id  2 | task 4578 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 8828, pos_max = 9724, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 9144, pos_max = 10018, n_swa = 128, size = 20.518 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 9375, pos_max = 10194, n_swa = 128, size = 19.228 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 9800, pos_max = 10384, n_swa = 128, size = 13.718 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 9907, pos_max = 10540, n_swa = 128, size = 14.867 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 9994, pos_max = 10704, n_swa = 128, size = 16.672 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 10118, pos_max = 10810, n_swa = 128, size = 16.250 MiB)
slot update_slots: id  2 | task 4578 | erased invalidated context checkpoint (pos_min = 10264, pos_max = 10934, n_swa = 128, size = 15.735 MiB)
slot update_slots: id  2 | task 4578 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 4578 | prompt processing progress, n_tokens = 640, batch.n_tokens = 640, progress = 0.909091
slot update_slots: id  2 | task 4578 | n_tokens = 640, memory_seq_rm [640, end)
slot update_slots: id  2 | task 4578 | prompt processing progress, n_tokens = 704, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4578 | prompt done, n_tokens = 704, batch.n_tokens = 64
slot init_sampler: id  2 | task 4578 | init sampler, took 0.11 ms, tokens: text = 704, total = 704
slot update_slots: id  2 | task 4578 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot print_timing: id  2 | task 4578 | 
prompt eval time =    1161.70 ms /   704 tokens (    1.65 ms per token,   606.01 tokens per second)
       eval time =    1720.09 ms /    66 tokens (   26.06 ms per token,    38.37 tokens per second)
      total time =    2881.78 ms /   770 tokens
slot      release: id  2 | task 4578 | stop processing: n_tokens = 769, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.513 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4646 | processing task, is_child = 0
slot update_slots: id  2 | task 4646 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1453
slot update_slots: id  2 | task 4646 | n_tokens = 745, memory_seq_rm [745, end)
slot update_slots: id  2 | task 4646 | prompt processing progress, n_tokens = 1389, batch.n_tokens = 644, progress = 0.955953
slot update_slots: id  2 | task 4646 | n_tokens = 1389, memory_seq_rm [1389, end)
slot update_slots: id  2 | task 4646 | prompt processing progress, n_tokens = 1453, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4646 | prompt done, n_tokens = 1453, batch.n_tokens = 64
slot init_sampler: id  2 | task 4646 | init sampler, took 0.21 ms, tokens: text = 1453, total = 1453
slot update_slots: id  2 | task 4646 | created context checkpoint 2 of 8 (pos_min = 492, pos_max = 1388, size = 21.034 MiB)
slot print_timing: id  2 | task 4646 | 
prompt eval time =    1042.84 ms /   708 tokens (    1.47 ms per token,   678.91 tokens per second)
       eval time =    1285.68 ms /    48 tokens (   26.78 ms per token,    37.33 tokens per second)
      total time =    2328.52 ms /   756 tokens
slot      release: id  2 | task 4646 | stop processing: n_tokens = 1500, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.457 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4696 | processing task, is_child = 0
slot update_slots: id  2 | task 4696 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3208
slot update_slots: id  2 | task 4696 | n_tokens = 1466, memory_seq_rm [1466, end)
slot update_slots: id  2 | task 4696 | prompt processing progress, n_tokens = 3144, batch.n_tokens = 1678, progress = 0.980050
slot update_slots: id  2 | task 4696 | n_tokens = 3144, memory_seq_rm [3144, end)
slot update_slots: id  2 | task 4696 | prompt processing progress, n_tokens = 3208, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4696 | prompt done, n_tokens = 3208, batch.n_tokens = 64
slot init_sampler: id  2 | task 4696 | init sampler, took 0.59 ms, tokens: text = 3208, total = 3208
slot update_slots: id  2 | task 4696 | created context checkpoint 3 of 8 (pos_min = 2247, pos_max = 3143, size = 21.034 MiB)
slot print_timing: id  2 | task 4696 | 
prompt eval time =    2344.20 ms /  1742 tokens (    1.35 ms per token,   743.11 tokens per second)
       eval time =    1210.53 ms /    43 tokens (   28.15 ms per token,    35.52 tokens per second)
      total time =    3554.73 ms /  1785 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 4696 | stop processing: n_tokens = 3250, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.564 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4741 | processing task, is_child = 0
slot update_slots: id  2 | task 4741 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5699
slot update_slots: id  2 | task 4741 | n_tokens = 3216, memory_seq_rm [3216, end)
slot update_slots: id  2 | task 4741 | prompt processing progress, n_tokens = 5264, batch.n_tokens = 2048, progress = 0.923671
slot update_slots: id  2 | task 4741 | n_tokens = 5264, memory_seq_rm [5264, end)
slot update_slots: id  2 | task 4741 | prompt processing progress, n_tokens = 5635, batch.n_tokens = 371, progress = 0.988770
slot update_slots: id  2 | task 4741 | n_tokens = 5635, memory_seq_rm [5635, end)
slot update_slots: id  2 | task 4741 | prompt processing progress, n_tokens = 5699, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4741 | prompt done, n_tokens = 5699, batch.n_tokens = 64
slot init_sampler: id  2 | task 4741 | init sampler, took 0.81 ms, tokens: text = 5699, total = 5699
slot update_slots: id  2 | task 4741 | created context checkpoint 4 of 8 (pos_min = 4738, pos_max = 5634, size = 21.034 MiB)
slot print_timing: id  2 | task 4741 | 
prompt eval time =    3361.15 ms /  2483 tokens (    1.35 ms per token,   738.74 tokens per second)
       eval time =    1117.34 ms /    39 tokens (   28.65 ms per token,    34.90 tokens per second)
      total time =    4478.49 ms /  2522 tokens
slot      release: id  2 | task 4741 | stop processing: n_tokens = 5737, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.771 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 4783 | processing task, is_child = 0
slot update_slots: id  2 | task 4783 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7399
slot update_slots: id  2 | task 4783 | n_tokens = 5707, memory_seq_rm [5707, end)
slot update_slots: id  2 | task 4783 | prompt processing progress, n_tokens = 7335, batch.n_tokens = 1628, progress = 0.991350
slot update_slots: id  2 | task 4783 | n_tokens = 7335, memory_seq_rm [7335, end)
slot update_slots: id  2 | task 4783 | prompt processing progress, n_tokens = 7399, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 4783 | prompt done, n_tokens = 7399, batch.n_tokens = 64
slot init_sampler: id  2 | task 4783 | init sampler, took 1.21 ms, tokens: text = 7399, total = 7399
slot update_slots: id  2 | task 4783 | created context checkpoint 5 of 8 (pos_min = 6438, pos_max = 7334, size = 21.034 MiB)
slot print_timing: id  2 | task 4783 | 
prompt eval time =    2537.78 ms /  1692 tokens (    1.50 ms per token,   666.72 tokens per second)
       eval time =   10007.33 ms /   338 tokens (   29.61 ms per token,    33.78 tokens per second)
      total time =   12545.12 ms /  2030 tokens
slot      release: id  2 | task 4783 | stop processing: n_tokens = 7736, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 5123 | processing task, is_child = 0
slot update_slots: id  2 | task 5123 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7776
slot update_slots: id  2 | task 5123 | n_tokens = 7586, memory_seq_rm [7586, end)
slot update_slots: id  2 | task 5123 | prompt processing progress, n_tokens = 7712, batch.n_tokens = 126, progress = 0.991770
slot update_slots: id  2 | task 5123 | n_tokens = 7712, memory_seq_rm [7712, end)
slot update_slots: id  2 | task 5123 | prompt processing progress, n_tokens = 7776, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 5123 | prompt done, n_tokens = 7776, batch.n_tokens = 64
slot init_sampler: id  2 | task 5123 | init sampler, took 1.70 ms, tokens: text = 7776, total = 7776
slot update_slots: id  2 | task 5123 | created context checkpoint 6 of 8 (pos_min = 6839, pos_max = 7711, size = 20.471 MiB)
slot print_timing: id  2 | task 5123 | 
prompt eval time =     635.88 ms /   190 tokens (    3.35 ms per token,   298.80 tokens per second)
       eval time =    6565.73 ms /   236 tokens (   27.82 ms per token,    35.94 tokens per second)
      total time =    7201.61 ms /   426 tokens
slot      release: id  2 | task 5123 | stop processing: n_tokens = 8011, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5361 | processing task, is_child = 0
slot update_slots: id  1 | task 5361 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7719
slot update_slots: id  1 | task 5361 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 5361 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.265319
slot update_slots: id  1 | task 5361 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 5361 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.530639
slot update_slots: id  1 | task 5361 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 5361 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.795958
slot update_slots: id  1 | task 5361 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 5361 | prompt processing progress, n_tokens = 7655, batch.n_tokens = 1511, progress = 0.991709
slot update_slots: id  1 | task 5361 | n_tokens = 7655, memory_seq_rm [7655, end)
slot update_slots: id  1 | task 5361 | prompt processing progress, n_tokens = 7719, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5361 | prompt done, n_tokens = 7719, batch.n_tokens = 64
slot init_sampler: id  1 | task 5361 | init sampler, took 1.10 ms, tokens: text = 7719, total = 7719
slot update_slots: id  1 | task 5361 | created context checkpoint 1 of 8 (pos_min = 6885, pos_max = 7654, size = 18.056 MiB)
slot print_timing: id  1 | task 5361 | 
prompt eval time =   10161.75 ms /  7719 tokens (    1.32 ms per token,   759.61 tokens per second)
       eval time =    8704.34 ms /   309 tokens (   28.17 ms per token,    35.50 tokens per second)
      total time =   18866.09 ms /  8028 tokens
slot      release: id  1 | task 5361 | stop processing: n_tokens = 8027, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5675 | processing task, is_child = 0
slot update_slots: id  1 | task 5675 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8067
slot update_slots: id  1 | task 5675 | n_tokens = 7935, memory_seq_rm [7935, end)
slot update_slots: id  1 | task 5675 | prompt processing progress, n_tokens = 8003, batch.n_tokens = 68, progress = 0.992066
slot update_slots: id  1 | task 5675 | n_tokens = 8003, memory_seq_rm [8003, end)
slot update_slots: id  1 | task 5675 | prompt processing progress, n_tokens = 8067, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5675 | prompt done, n_tokens = 8067, batch.n_tokens = 64
slot init_sampler: id  1 | task 5675 | init sampler, took 1.52 ms, tokens: text = 8067, total = 8067
slot update_slots: id  1 | task 5675 | created context checkpoint 2 of 8 (pos_min = 7257, pos_max = 8002, size = 17.493 MiB)
slot print_timing: id  1 | task 5675 | 
prompt eval time =     511.47 ms /   132 tokens (    3.87 ms per token,   258.08 tokens per second)
       eval time =    3206.26 ms /   110 tokens (   29.15 ms per token,    34.31 tokens per second)
      total time =    3717.73 ms /   242 tokens
slot      release: id  1 | task 5675 | stop processing: n_tokens = 8176, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.084
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8176, total state size = 209.775 MiB
srv          load:  - looking for better prompt, base f_keep = 0.084, sim = 0.973
srv        update:  - cache state: 2 prompts, 658.661 MiB (limits: 8192.000 MiB, 56064 tokens, 239033 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv  get_availabl: prompt cache update took 188.16 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5787 | processing task, is_child = 0
slot update_slots: id  1 | task 5787 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 706
slot update_slots: id  1 | task 5787 | n_past = 687, slot.prompt.tokens.size() = 8176, seq_id = 1, pos_min = 7406, n_swa = 128
slot update_slots: id  1 | task 5787 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 5787 | erased invalidated context checkpoint (pos_min = 6885, pos_max = 7654, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 5787 | erased invalidated context checkpoint (pos_min = 7257, pos_max = 8002, n_swa = 128, size = 17.493 MiB)
slot update_slots: id  1 | task 5787 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 5787 | prompt processing progress, n_tokens = 642, batch.n_tokens = 642, progress = 0.909348
slot update_slots: id  1 | task 5787 | n_tokens = 642, memory_seq_rm [642, end)
slot update_slots: id  1 | task 5787 | prompt processing progress, n_tokens = 706, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5787 | prompt done, n_tokens = 706, batch.n_tokens = 64
slot init_sampler: id  1 | task 5787 | init sampler, took 0.11 ms, tokens: text = 706, total = 706
slot update_slots: id  1 | task 5787 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 641, size = 15.055 MiB)
slot print_timing: id  1 | task 5787 | 
prompt eval time =    1202.42 ms /   706 tokens (    1.70 ms per token,   587.15 tokens per second)
       eval time =    1455.90 ms /    54 tokens (   26.96 ms per token,    37.09 tokens per second)
      total time =    2658.32 ms /   760 tokens
slot      release: id  1 | task 5787 | stop processing: n_tokens = 759, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.897 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5843 | processing task, is_child = 0
slot update_slots: id  1 | task 5843 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 814
slot update_slots: id  1 | task 5843 | n_tokens = 730, memory_seq_rm [730, end)
slot update_slots: id  1 | task 5843 | prompt processing progress, n_tokens = 750, batch.n_tokens = 20, progress = 0.921376
slot update_slots: id  1 | task 5843 | n_tokens = 750, memory_seq_rm [750, end)
slot update_slots: id  1 | task 5843 | prompt processing progress, n_tokens = 814, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5843 | prompt done, n_tokens = 814, batch.n_tokens = 64
slot init_sampler: id  1 | task 5843 | init sampler, took 0.12 ms, tokens: text = 814, total = 814
slot update_slots: id  1 | task 5843 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 749, size = 17.587 MiB)
slot print_timing: id  1 | task 5843 | 
prompt eval time =     384.52 ms /    84 tokens (    4.58 ms per token,   218.45 tokens per second)
       eval time =    1348.96 ms /    50 tokens (   26.98 ms per token,    37.07 tokens per second)
      total time =    1733.48 ms /   134 tokens
slot      release: id  1 | task 5843 | stop processing: n_tokens = 863, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.127 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 5895 | processing task, is_child = 0
slot update_slots: id  1 | task 5895 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6657
slot update_slots: id  1 | task 5895 | n_tokens = 845, memory_seq_rm [845, end)
slot update_slots: id  1 | task 5895 | prompt processing progress, n_tokens = 2893, batch.n_tokens = 2048, progress = 0.434580
slot update_slots: id  1 | task 5895 | n_tokens = 2893, memory_seq_rm [2893, end)
slot update_slots: id  1 | task 5895 | prompt processing progress, n_tokens = 4941, batch.n_tokens = 2048, progress = 0.742226
slot update_slots: id  1 | task 5895 | n_tokens = 4941, memory_seq_rm [4941, end)
slot update_slots: id  1 | task 5895 | prompt processing progress, n_tokens = 6593, batch.n_tokens = 1652, progress = 0.990386
slot update_slots: id  1 | task 5895 | n_tokens = 6593, memory_seq_rm [6593, end)
slot update_slots: id  1 | task 5895 | prompt processing progress, n_tokens = 6657, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 5895 | prompt done, n_tokens = 6657, batch.n_tokens = 64
slot init_sampler: id  1 | task 5895 | init sampler, took 0.95 ms, tokens: text = 6657, total = 6657
slot update_slots: id  1 | task 5895 | created context checkpoint 3 of 8 (pos_min = 5823, pos_max = 6592, size = 18.056 MiB)
slot print_timing: id  1 | task 5895 | 
prompt eval time =    8325.30 ms /  5812 tokens (    1.43 ms per token,   698.11 tokens per second)
       eval time =   52465.08 ms /  1805 tokens (   29.07 ms per token,    34.40 tokens per second)
      total time =   60790.38 ms /  7617 tokens
slot      release: id  1 | task 5895 | stop processing: n_tokens = 8461, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 7704 | processing task, is_child = 0
slot update_slots: id  0 | task 7704 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8379
slot update_slots: id  0 | task 7704 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.244421
slot update_slots: id  0 | task 7704 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.488841
slot update_slots: id  0 | task 7704 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.733262
slot update_slots: id  0 | task 7704 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.977682
slot update_slots: id  0 | task 7704 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 8315, batch.n_tokens = 123, progress = 0.992362
slot update_slots: id  0 | task 7704 | n_tokens = 8315, memory_seq_rm [8315, end)
slot update_slots: id  0 | task 7704 | prompt processing progress, n_tokens = 8379, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 7704 | prompt done, n_tokens = 8379, batch.n_tokens = 64
slot init_sampler: id  0 | task 7704 | init sampler, took 1.59 ms, tokens: text = 8379, total = 8379
slot update_slots: id  0 | task 7704 | created context checkpoint 1 of 8 (pos_min = 7672, pos_max = 8314, size = 15.078 MiB)
slot print_timing: id  0 | task 7704 | 
prompt eval time =   12563.23 ms /  8379 tokens (    1.50 ms per token,   666.95 tokens per second)
       eval time =   32340.88 ms /  1065 tokens (   30.37 ms per token,    32.93 tokens per second)
      total time =   44904.11 ms /  9444 tokens
slot      release: id  0 | task 7704 | stop processing: n_tokens = 9443, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8775 | processing task, is_child = 0
slot update_slots: id  0 | task 8775 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9470
slot update_slots: id  0 | task 8775 | n_tokens = 9377, memory_seq_rm [9377, end)
slot update_slots: id  0 | task 8775 | prompt processing progress, n_tokens = 9406, batch.n_tokens = 29, progress = 0.993242
slot update_slots: id  0 | task 8775 | n_tokens = 9406, memory_seq_rm [9406, end)
slot update_slots: id  0 | task 8775 | prompt processing progress, n_tokens = 9470, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8775 | prompt done, n_tokens = 9470, batch.n_tokens = 64
slot init_sampler: id  0 | task 8775 | init sampler, took 1.78 ms, tokens: text = 9470, total = 9470
slot update_slots: id  0 | task 8775 | created context checkpoint 2 of 8 (pos_min = 8800, pos_max = 9405, size = 14.210 MiB)
slot print_timing: id  0 | task 8775 | 
prompt eval time =     400.21 ms /    93 tokens (    4.30 ms per token,   232.38 tokens per second)
       eval time =    2300.63 ms /    77 tokens (   29.88 ms per token,    33.47 tokens per second)
      total time =    2700.84 ms /   170 tokens
slot      release: id  0 | task 8775 | stop processing: n_tokens = 9546, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 8854 | processing task, is_child = 0
slot update_slots: id  0 | task 8854 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9574
slot update_slots: id  0 | task 8854 | n_tokens = 9480, memory_seq_rm [9480, end)
slot update_slots: id  0 | task 8854 | prompt processing progress, n_tokens = 9510, batch.n_tokens = 30, progress = 0.993315
slot update_slots: id  0 | task 8854 | n_tokens = 9510, memory_seq_rm [9510, end)
slot update_slots: id  0 | task 8854 | prompt processing progress, n_tokens = 9574, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 8854 | prompt done, n_tokens = 9574, batch.n_tokens = 64
slot init_sampler: id  0 | task 8854 | init sampler, took 1.37 ms, tokens: text = 9574, total = 9574
slot update_slots: id  0 | task 8854 | created context checkpoint 3 of 8 (pos_min = 8903, pos_max = 9509, size = 14.234 MiB)
slot print_timing: id  0 | task 8854 | 
prompt eval time =     401.60 ms /    94 tokens (    4.27 ms per token,   234.07 tokens per second)
       eval time =    6322.26 ms /   214 tokens (   29.54 ms per token,    33.85 tokens per second)
      total time =    6723.85 ms /   308 tokens
slot      release: id  0 | task 8854 | stop processing: n_tokens = 9787, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9070 | processing task, is_child = 0
slot update_slots: id  0 | task 9070 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9815
slot update_slots: id  0 | task 9070 | n_tokens = 9585, memory_seq_rm [9585, end)
slot update_slots: id  0 | task 9070 | prompt processing progress, n_tokens = 9751, batch.n_tokens = 166, progress = 0.993479
slot update_slots: id  0 | task 9070 | n_tokens = 9751, memory_seq_rm [9751, end)
slot update_slots: id  0 | task 9070 | prompt processing progress, n_tokens = 9815, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9070 | prompt done, n_tokens = 9815, batch.n_tokens = 64
slot init_sampler: id  0 | task 9070 | init sampler, took 1.41 ms, tokens: text = 9815, total = 9815
slot update_slots: id  0 | task 9070 | created context checkpoint 4 of 8 (pos_min = 9177, pos_max = 9750, size = 13.460 MiB)
slot print_timing: id  0 | task 9070 | 
prompt eval time =     637.59 ms /   230 tokens (    2.77 ms per token,   360.73 tokens per second)
       eval time =    8652.06 ms /   298 tokens (   29.03 ms per token,    34.44 tokens per second)
      total time =    9289.66 ms /   528 tokens
slot      release: id  0 | task 9070 | stop processing: n_tokens = 10112, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9370 | processing task, is_child = 0
slot update_slots: id  0 | task 9370 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10139
slot update_slots: id  0 | task 9370 | n_tokens = 9829, memory_seq_rm [9829, end)
slot update_slots: id  0 | task 9370 | prompt processing progress, n_tokens = 10075, batch.n_tokens = 246, progress = 0.993688
slot update_slots: id  0 | task 9370 | n_tokens = 10075, memory_seq_rm [10075, end)
slot update_slots: id  0 | task 9370 | prompt processing progress, n_tokens = 10139, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9370 | prompt done, n_tokens = 10139, batch.n_tokens = 64
slot init_sampler: id  0 | task 9370 | init sampler, took 1.88 ms, tokens: text = 10139, total = 10139
slot update_slots: id  0 | task 9370 | created context checkpoint 5 of 8 (pos_min = 9538, pos_max = 10074, size = 12.592 MiB)
slot print_timing: id  0 | task 9370 | 
prompt eval time =     756.89 ms /   310 tokens (    2.44 ms per token,   409.57 tokens per second)
       eval time =   14984.57 ms /   515 tokens (   29.10 ms per token,    34.37 tokens per second)
      total time =   15741.47 ms /   825 tokens
slot      release: id  0 | task 9370 | stop processing: n_tokens = 10653, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 9887 | processing task, is_child = 0
slot update_slots: id  0 | task 9887 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10685
slot update_slots: id  0 | task 9887 | n_tokens = 10148, memory_seq_rm [10148, end)
slot update_slots: id  0 | task 9887 | prompt processing progress, n_tokens = 10621, batch.n_tokens = 473, progress = 0.994010
slot update_slots: id  0 | task 9887 | n_tokens = 10621, memory_seq_rm [10621, end)
slot update_slots: id  0 | task 9887 | prompt processing progress, n_tokens = 10685, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 9887 | prompt done, n_tokens = 10685, batch.n_tokens = 64
slot init_sampler: id  0 | task 9887 | init sampler, took 1.51 ms, tokens: text = 10685, total = 10685
slot update_slots: id  0 | task 9887 | created context checkpoint 6 of 8 (pos_min = 10021, pos_max = 10620, size = 14.070 MiB)
slot print_timing: id  0 | task 9887 | 
prompt eval time =    1025.54 ms /   537 tokens (    1.91 ms per token,   523.63 tokens per second)
       eval time =    5661.80 ms /   194 tokens (   29.18 ms per token,    34.26 tokens per second)
      total time =    6687.34 ms /   731 tokens
slot      release: id  0 | task 9887 | stop processing: n_tokens = 10878, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.866 (> 0.100 thold), f_keep = 0.770
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 10083 | processing task, is_child = 0
slot update_slots: id  0 | task 10083 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9673
slot update_slots: id  0 | task 10083 | n_past = 8379, slot.prompt.tokens.size() = 10878, seq_id = 0, pos_min = 10362, n_swa = 128
slot update_slots: id  0 | task 10083 | restored context checkpoint (pos_min = 7672, pos_max = 8314, size = 15.078 MiB)
slot update_slots: id  0 | task 10083 | erased invalidated context checkpoint (pos_min = 8800, pos_max = 9405, n_swa = 128, size = 14.210 MiB)
slot update_slots: id  0 | task 10083 | erased invalidated context checkpoint (pos_min = 8903, pos_max = 9509, n_swa = 128, size = 14.234 MiB)
slot update_slots: id  0 | task 10083 | erased invalidated context checkpoint (pos_min = 9177, pos_max = 9750, n_swa = 128, size = 13.460 MiB)
slot update_slots: id  0 | task 10083 | erased invalidated context checkpoint (pos_min = 9538, pos_max = 10074, n_swa = 128, size = 12.592 MiB)
slot update_slots: id  0 | task 10083 | erased invalidated context checkpoint (pos_min = 10021, pos_max = 10620, n_swa = 128, size = 14.070 MiB)
slot update_slots: id  0 | task 10083 | n_tokens = 8314, memory_seq_rm [8314, end)
slot update_slots: id  0 | task 10083 | prompt processing progress, n_tokens = 9609, batch.n_tokens = 1295, progress = 0.993384
slot update_slots: id  0 | task 10083 | n_tokens = 9609, memory_seq_rm [9609, end)
slot update_slots: id  0 | task 10083 | prompt processing progress, n_tokens = 9673, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 10083 | prompt done, n_tokens = 9673, batch.n_tokens = 64
slot init_sampler: id  0 | task 10083 | init sampler, took 1.88 ms, tokens: text = 9673, total = 9673
slot update_slots: id  0 | task 10083 | created context checkpoint 2 of 8 (pos_min = 8966, pos_max = 9608, size = 15.078 MiB)
slot print_timing: id  0 | task 10083 | 
prompt eval time =    2476.77 ms /  1359 tokens (    1.82 ms per token,   548.70 tokens per second)
       eval time =   34754.25 ms /  1148 tokens (   30.27 ms per token,    33.03 tokens per second)
      total time =   37231.02 ms /  2507 tokens
slot      release: id  0 | task 10083 | stop processing: n_tokens = 10820, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 11233 | processing task, is_child = 0
slot update_slots: id  0 | task 11233 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10848
slot update_slots: id  0 | task 11233 | n_tokens = 10750, memory_seq_rm [10750, end)
slot update_slots: id  0 | task 11233 | prompt processing progress, n_tokens = 10784, batch.n_tokens = 34, progress = 0.994100
slot update_slots: id  0 | task 11233 | n_tokens = 10784, memory_seq_rm [10784, end)
slot update_slots: id  0 | task 11233 | prompt processing progress, n_tokens = 10848, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 11233 | prompt done, n_tokens = 10848, batch.n_tokens = 64
slot init_sampler: id  0 | task 11233 | init sampler, took 1.54 ms, tokens: text = 10848, total = 10848
slot update_slots: id  0 | task 11233 | created context checkpoint 3 of 8 (pos_min = 10177, pos_max = 10783, size = 14.234 MiB)
slot print_timing: id  0 | task 11233 | 
prompt eval time =     425.72 ms /    98 tokens (    4.34 ms per token,   230.20 tokens per second)
       eval time =   19120.95 ms /   635 tokens (   30.11 ms per token,    33.21 tokens per second)
      total time =   19546.68 ms /   733 tokens
slot      release: id  0 | task 11233 | stop processing: n_tokens = 11482, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.956
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 11870 | processing task, is_child = 0
slot update_slots: id  0 | task 11870 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11505
slot update_slots: id  0 | task 11870 | n_tokens = 10980, memory_seq_rm [10980, end)
slot update_slots: id  0 | task 11870 | prompt processing progress, n_tokens = 11441, batch.n_tokens = 461, progress = 0.994437
slot update_slots: id  0 | task 11870 | n_tokens = 11441, memory_seq_rm [11441, end)
slot update_slots: id  0 | task 11870 | prompt processing progress, n_tokens = 11505, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 11870 | prompt done, n_tokens = 11505, batch.n_tokens = 64
slot init_sampler: id  0 | task 11870 | init sampler, took 1.62 ms, tokens: text = 11505, total = 11505
slot update_slots: id  0 | task 11870 | created context checkpoint 4 of 8 (pos_min = 10839, pos_max = 11440, size = 14.117 MiB)
slot print_timing: id  0 | task 11870 | 
prompt eval time =    1026.57 ms /   525 tokens (    1.96 ms per token,   511.41 tokens per second)
       eval time =  123332.81 ms /  4096 tokens (   30.11 ms per token,    33.21 tokens per second)
      total time =  124359.38 ms /  4621 tokens
slot      release: id  0 | task 11870 | stop processing: n_tokens = 15600, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 11870
srv  update_slots: all slots are idle
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.318 (> 0.100 thold), f_keep = 0.043
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 15600, total state size = 380.882 MiB
srv          load:  - looking for better prompt, base f_keep = 0.043, sim = 0.318
srv        update:  - cache state: 3 prompts, 1098.050 MiB (limits: 8192.000 MiB, 56064 tokens, 259767 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv  get_availabl: prompt cache update took 336.12 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 15969 | processing task, is_child = 0
slot update_slots: id  0 | task 15969 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2131
slot update_slots: id  0 | task 15969 | n_past = 677, slot.prompt.tokens.size() = 15600, seq_id = 0, pos_min = 14957, n_swa = 128
slot update_slots: id  0 | task 15969 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 15969 | erased invalidated context checkpoint (pos_min = 7672, pos_max = 8314, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 15969 | erased invalidated context checkpoint (pos_min = 8966, pos_max = 9608, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 15969 | erased invalidated context checkpoint (pos_min = 10177, pos_max = 10783, n_swa = 128, size = 14.234 MiB)
slot update_slots: id  0 | task 15969 | erased invalidated context checkpoint (pos_min = 10839, pos_max = 11440, n_swa = 128, size = 14.117 MiB)
slot update_slots: id  0 | task 15969 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 15969 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.961051
slot update_slots: id  0 | task 15969 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 15969 | prompt processing progress, n_tokens = 2067, batch.n_tokens = 19, progress = 0.969967
slot update_slots: id  0 | task 15969 | n_tokens = 2067, memory_seq_rm [2067, end)
slot update_slots: id  0 | task 15969 | prompt processing progress, n_tokens = 2131, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 15969 | prompt done, n_tokens = 2131, batch.n_tokens = 64
slot init_sampler: id  0 | task 15969 | init sampler, took 0.31 ms, tokens: text = 2131, total = 2131
slot update_slots: id  0 | task 15969 | created context checkpoint 1 of 8 (pos_min = 1424, pos_max = 2066, size = 15.078 MiB)
slot print_timing: id  0 | task 15969 | 
prompt eval time =    3244.73 ms /  2131 tokens (    1.52 ms per token,   656.76 tokens per second)
       eval time =     901.93 ms /    33 tokens (   27.33 ms per token,    36.59 tokens per second)
      total time =    4146.66 ms /  2164 tokens
slot      release: id  0 | task 15969 | stop processing: n_tokens = 2163, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.739 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16005 | processing task, is_child = 0
slot update_slots: id  0 | task 16005 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2901
slot update_slots: id  0 | task 16005 | n_tokens = 2143, memory_seq_rm [2143, end)
slot update_slots: id  0 | task 16005 | prompt processing progress, n_tokens = 2837, batch.n_tokens = 694, progress = 0.977939
slot update_slots: id  0 | task 16005 | n_tokens = 2837, memory_seq_rm [2837, end)
slot update_slots: id  0 | task 16005 | prompt processing progress, n_tokens = 2901, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16005 | prompt done, n_tokens = 2901, batch.n_tokens = 64
slot init_sampler: id  0 | task 16005 | init sampler, took 0.42 ms, tokens: text = 2901, total = 2901
slot update_slots: id  0 | task 16005 | created context checkpoint 2 of 8 (pos_min = 2194, pos_max = 2836, size = 15.078 MiB)
slot print_timing: id  0 | task 16005 | 
prompt eval time =    1323.40 ms /   758 tokens (    1.75 ms per token,   572.77 tokens per second)
       eval time =    1565.17 ms /    55 tokens (   28.46 ms per token,    35.14 tokens per second)
      total time =    2888.57 ms /   813 tokens
slot      release: id  0 | task 16005 | stop processing: n_tokens = 2955, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.865 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16062 | processing task, is_child = 0
slot update_slots: id  0 | task 16062 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3384
slot update_slots: id  0 | task 16062 | n_tokens = 2926, memory_seq_rm [2926, end)
slot update_slots: id  0 | task 16062 | prompt processing progress, n_tokens = 3320, batch.n_tokens = 394, progress = 0.981087
slot update_slots: id  0 | task 16062 | n_tokens = 3320, memory_seq_rm [3320, end)
slot update_slots: id  0 | task 16062 | prompt processing progress, n_tokens = 3384, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16062 | prompt done, n_tokens = 3384, batch.n_tokens = 64
slot init_sampler: id  0 | task 16062 | init sampler, took 0.48 ms, tokens: text = 3384, total = 3384
slot update_slots: id  0 | task 16062 | created context checkpoint 3 of 8 (pos_min = 2677, pos_max = 3319, size = 15.078 MiB)
slot print_timing: id  0 | task 16062 | 
prompt eval time =     856.47 ms /   458 tokens (    1.87 ms per token,   534.75 tokens per second)
       eval time =    1146.69 ms /    40 tokens (   28.67 ms per token,    34.88 tokens per second)
      total time =    2003.16 ms /   498 tokens
slot      release: id  0 | task 16062 | stop processing: n_tokens = 3423, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16104 | processing task, is_child = 0
slot update_slots: id  0 | task 16104 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3451
slot update_slots: id  0 | task 16104 | n_tokens = 3393, memory_seq_rm [3393, end)
slot update_slots: id  0 | task 16104 | prompt processing progress, n_tokens = 3451, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 16104 | prompt done, n_tokens = 3451, batch.n_tokens = 58
slot init_sampler: id  0 | task 16104 | init sampler, took 0.53 ms, tokens: text = 3451, total = 3451
slot update_slots: id  0 | task 16104 | created context checkpoint 4 of 8 (pos_min = 2780, pos_max = 3392, size = 14.374 MiB)
slot print_timing: id  0 | task 16104 | 
prompt eval time =     211.39 ms /    58 tokens (    3.64 ms per token,   274.37 tokens per second)
       eval time =    1529.51 ms /    52 tokens (   29.41 ms per token,    34.00 tokens per second)
      total time =    1740.90 ms /   110 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 16104 | stop processing: n_tokens = 3502, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.882 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16157 | processing task, is_child = 0
slot update_slots: id  0 | task 16157 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3941
slot update_slots: id  0 | task 16157 | n_tokens = 3476, memory_seq_rm [3476, end)
slot update_slots: id  0 | task 16157 | prompt processing progress, n_tokens = 3877, batch.n_tokens = 401, progress = 0.983760
slot update_slots: id  0 | task 16157 | n_tokens = 3877, memory_seq_rm [3877, end)
slot update_slots: id  0 | task 16157 | prompt processing progress, n_tokens = 3941, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16157 | prompt done, n_tokens = 3941, batch.n_tokens = 64
slot init_sampler: id  0 | task 16157 | init sampler, took 0.75 ms, tokens: text = 3941, total = 3941
slot update_slots: id  0 | task 16157 | created context checkpoint 5 of 8 (pos_min = 3234, pos_max = 3876, size = 15.078 MiB)
slot print_timing: id  0 | task 16157 | 
prompt eval time =     887.94 ms /   465 tokens (    1.91 ms per token,   523.69 tokens per second)
       eval time =    1677.65 ms /    57 tokens (   29.43 ms per token,    33.98 tokens per second)
      total time =    2565.59 ms /   522 tokens
slot      release: id  0 | task 16157 | stop processing: n_tokens = 3997, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16216 | processing task, is_child = 0
slot update_slots: id  0 | task 16216 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4167
slot update_slots: id  0 | task 16216 | n_tokens = 3971, memory_seq_rm [3971, end)
slot update_slots: id  0 | task 16216 | prompt processing progress, n_tokens = 4103, batch.n_tokens = 132, progress = 0.984641
slot update_slots: id  0 | task 16216 | n_tokens = 4103, memory_seq_rm [4103, end)
slot update_slots: id  0 | task 16216 | prompt processing progress, n_tokens = 4167, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16216 | prompt done, n_tokens = 4167, batch.n_tokens = 64
slot init_sampler: id  0 | task 16216 | init sampler, took 0.65 ms, tokens: text = 4167, total = 4167
slot update_slots: id  0 | task 16216 | created context checkpoint 6 of 8 (pos_min = 3460, pos_max = 4102, size = 15.078 MiB)
slot print_timing: id  0 | task 16216 | 
prompt eval time =     632.72 ms /   196 tokens (    3.23 ms per token,   309.78 tokens per second)
       eval time =    2235.85 ms /    74 tokens (   30.21 ms per token,    33.10 tokens per second)
      total time =    2868.57 ms /   270 tokens
slot      release: id  0 | task 16216 | stop processing: n_tokens = 4240, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.940 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16292 | processing task, is_child = 0
slot update_slots: id  0 | task 16292 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4478
slot update_slots: id  0 | task 16292 | n_tokens = 4210, memory_seq_rm [4210, end)
slot update_slots: id  0 | task 16292 | prompt processing progress, n_tokens = 4414, batch.n_tokens = 204, progress = 0.985708
slot update_slots: id  0 | task 16292 | n_tokens = 4414, memory_seq_rm [4414, end)
slot update_slots: id  0 | task 16292 | prompt processing progress, n_tokens = 4478, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16292 | prompt done, n_tokens = 4478, batch.n_tokens = 64
slot init_sampler: id  0 | task 16292 | init sampler, took 0.65 ms, tokens: text = 4478, total = 4478
slot update_slots: id  0 | task 16292 | created context checkpoint 7 of 8 (pos_min = 3771, pos_max = 4413, size = 15.078 MiB)
slot print_timing: id  0 | task 16292 | 
prompt eval time =     699.75 ms /   268 tokens (    2.61 ms per token,   382.99 tokens per second)
       eval time =    1655.95 ms /    55 tokens (   30.11 ms per token,    33.21 tokens per second)
      total time =    2355.70 ms /   323 tokens
slot      release: id  0 | task 16292 | stop processing: n_tokens = 4532, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.722 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16349 | processing task, is_child = 0
slot update_slots: id  0 | task 16349 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6238
slot update_slots: id  0 | task 16349 | n_tokens = 4506, memory_seq_rm [4506, end)
slot update_slots: id  0 | task 16349 | prompt processing progress, n_tokens = 6174, batch.n_tokens = 1668, progress = 0.989740
slot update_slots: id  0 | task 16349 | n_tokens = 6174, memory_seq_rm [6174, end)
slot update_slots: id  0 | task 16349 | prompt processing progress, n_tokens = 6238, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16349 | prompt done, n_tokens = 6238, batch.n_tokens = 64
slot init_sampler: id  0 | task 16349 | init sampler, took 1.09 ms, tokens: text = 6238, total = 6238
slot update_slots: id  0 | task 16349 | created context checkpoint 8 of 8 (pos_min = 5531, pos_max = 6173, size = 15.078 MiB)
slot print_timing: id  0 | task 16349 | 
prompt eval time =    3154.54 ms /  1732 tokens (    1.82 ms per token,   549.05 tokens per second)
       eval time =    3068.57 ms /   100 tokens (   30.69 ms per token,    32.59 tokens per second)
      total time =    6223.10 ms /  1832 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 16349 | stop processing: n_tokens = 6337, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16451 | processing task, is_child = 0
slot update_slots: id  0 | task 16451 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6369
slot update_slots: id  0 | task 16451 | n_tokens = 6311, memory_seq_rm [6311, end)
slot update_slots: id  0 | task 16451 | prompt processing progress, n_tokens = 6369, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 16451 | prompt done, n_tokens = 6369, batch.n_tokens = 58
slot init_sampler: id  0 | task 16451 | init sampler, took 0.92 ms, tokens: text = 6369, total = 6369
slot update_slots: id  0 | task 16451 | erasing old context checkpoint (pos_min = 1424, pos_max = 2066, size = 15.078 MiB)
slot update_slots: id  0 | task 16451 | created context checkpoint 8 of 8 (pos_min = 5694, pos_max = 6310, size = 14.468 MiB)
slot print_timing: id  0 | task 16451 | 
prompt eval time =     217.67 ms /    58 tokens (    3.75 ms per token,   266.46 tokens per second)
       eval time =    2316.81 ms /    76 tokens (   30.48 ms per token,    32.80 tokens per second)
      total time =    2534.47 ms /   134 tokens
slot      release: id  0 | task 16451 | stop processing: n_tokens = 6444, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16528 | processing task, is_child = 0
slot update_slots: id  0 | task 16528 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6876
slot update_slots: id  0 | task 16528 | n_tokens = 6418, memory_seq_rm [6418, end)
slot update_slots: id  0 | task 16528 | prompt processing progress, n_tokens = 6812, batch.n_tokens = 394, progress = 0.990692
slot update_slots: id  0 | task 16528 | n_tokens = 6812, memory_seq_rm [6812, end)
slot update_slots: id  0 | task 16528 | prompt processing progress, n_tokens = 6876, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16528 | prompt done, n_tokens = 6876, batch.n_tokens = 64
slot init_sampler: id  0 | task 16528 | init sampler, took 0.98 ms, tokens: text = 6876, total = 6876
slot update_slots: id  0 | task 16528 | erasing old context checkpoint (pos_min = 2194, pos_max = 2836, size = 15.078 MiB)
slot update_slots: id  0 | task 16528 | created context checkpoint 8 of 8 (pos_min = 6181, pos_max = 6811, size = 14.797 MiB)
slot print_timing: id  0 | task 16528 | 
prompt eval time =     941.96 ms /   458 tokens (    2.06 ms per token,   486.22 tokens per second)
       eval time =    5141.94 ms /   170 tokens (   30.25 ms per token,    33.06 tokens per second)
      total time =    6083.90 ms /   628 tokens
slot      release: id  0 | task 16528 | stop processing: n_tokens = 7045, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.849 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16700 | processing task, is_child = 0
slot update_slots: id  0 | task 16700 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8270
slot update_slots: id  0 | task 16700 | n_tokens = 7020, memory_seq_rm [7020, end)
slot update_slots: id  0 | task 16700 | prompt processing progress, n_tokens = 8206, batch.n_tokens = 1186, progress = 0.992261
slot update_slots: id  0 | task 16700 | n_tokens = 8206, memory_seq_rm [8206, end)
slot update_slots: id  0 | task 16700 | prompt processing progress, n_tokens = 8270, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16700 | prompt done, n_tokens = 8270, batch.n_tokens = 64
slot init_sampler: id  0 | task 16700 | init sampler, took 1.54 ms, tokens: text = 8270, total = 8270
slot update_slots: id  0 | task 16700 | erasing old context checkpoint (pos_min = 2677, pos_max = 3319, size = 15.078 MiB)
slot update_slots: id  0 | task 16700 | created context checkpoint 8 of 8 (pos_min = 7563, pos_max = 8205, size = 15.078 MiB)
slot print_timing: id  0 | task 16700 | 
prompt eval time =    2312.64 ms /  1250 tokens (    1.85 ms per token,   540.51 tokens per second)
       eval time =    6206.96 ms /   208 tokens (   29.84 ms per token,    33.51 tokens per second)
      total time =    8519.60 ms /  1458 tokens
slot      release: id  0 | task 16700 | stop processing: n_tokens = 8477, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.356 (> 0.100 thold), f_keep = 0.087
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8477, total state size = 213.855 MiB
srv          load:  - looking for better prompt, base f_keep = 0.087, sim = 0.356
srv        update:  - cache state: 4 prompts, 1430.934 MiB (limits: 8192.000 MiB, 56064 tokens, 247866 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv  get_availabl: prompt cache update took 249.15 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16910 | processing task, is_child = 0
slot update_slots: id  0 | task 16910 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2065
slot update_slots: id  0 | task 16910 | n_past = 736, slot.prompt.tokens.size() = 8477, seq_id = 0, pos_min = 7834, n_swa = 128
slot update_slots: id  0 | task 16910 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 2780, pos_max = 3392, n_swa = 128, size = 14.374 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 3234, pos_max = 3876, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 3460, pos_max = 4102, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 3771, pos_max = 4413, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 5531, pos_max = 6173, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 5694, pos_max = 6310, n_swa = 128, size = 14.468 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 6181, pos_max = 6811, n_swa = 128, size = 14.797 MiB)
slot update_slots: id  0 | task 16910 | erased invalidated context checkpoint (pos_min = 7563, pos_max = 8205, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 16910 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 16910 | prompt processing progress, n_tokens = 2001, batch.n_tokens = 2001, progress = 0.969007
slot update_slots: id  0 | task 16910 | n_tokens = 2001, memory_seq_rm [2001, end)
slot update_slots: id  0 | task 16910 | prompt processing progress, n_tokens = 2065, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16910 | prompt done, n_tokens = 2065, batch.n_tokens = 64
slot init_sampler: id  0 | task 16910 | init sampler, took 0.31 ms, tokens: text = 2065, total = 2065
slot update_slots: id  0 | task 16910 | created context checkpoint 1 of 8 (pos_min = 1358, pos_max = 2000, size = 15.078 MiB)
slot print_timing: id  0 | task 16910 | 
prompt eval time =    3078.35 ms /  2065 tokens (    1.49 ms per token,   670.81 tokens per second)
       eval time =    1198.15 ms /    44 tokens (   27.23 ms per token,    36.72 tokens per second)
      total time =    4276.50 ms /  2109 tokens
slot      release: id  0 | task 16910 | stop processing: n_tokens = 2108, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.734 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 16956 | processing task, is_child = 0
slot update_slots: id  0 | task 16956 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2846
slot update_slots: id  0 | task 16956 | n_tokens = 2089, memory_seq_rm [2089, end)
slot update_slots: id  0 | task 16956 | prompt processing progress, n_tokens = 2782, batch.n_tokens = 693, progress = 0.977512
slot update_slots: id  0 | task 16956 | n_tokens = 2782, memory_seq_rm [2782, end)
slot update_slots: id  0 | task 16956 | prompt processing progress, n_tokens = 2846, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 16956 | prompt done, n_tokens = 2846, batch.n_tokens = 64
slot init_sampler: id  0 | task 16956 | init sampler, took 0.46 ms, tokens: text = 2846, total = 2846
slot update_slots: id  0 | task 16956 | created context checkpoint 2 of 8 (pos_min = 2139, pos_max = 2781, size = 15.078 MiB)
slot print_timing: id  0 | task 16956 | 
prompt eval time =    1302.17 ms /   757 tokens (    1.72 ms per token,   581.34 tokens per second)
       eval time =    2002.73 ms /    71 tokens (   28.21 ms per token,    35.45 tokens per second)
      total time =    3304.90 ms /   828 tokens
slot      release: id  0 | task 16956 | stop processing: n_tokens = 2916, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.625 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17029 | processing task, is_child = 0
slot update_slots: id  0 | task 17029 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4622
slot update_slots: id  0 | task 17029 | n_tokens = 2890, memory_seq_rm [2890, end)
slot update_slots: id  0 | task 17029 | prompt processing progress, n_tokens = 4558, batch.n_tokens = 1668, progress = 0.986153
slot update_slots: id  0 | task 17029 | n_tokens = 4558, memory_seq_rm [4558, end)
slot update_slots: id  0 | task 17029 | prompt processing progress, n_tokens = 4622, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17029 | prompt done, n_tokens = 4622, batch.n_tokens = 64
slot init_sampler: id  0 | task 17029 | init sampler, took 0.89 ms, tokens: text = 4622, total = 4622
slot update_slots: id  0 | task 17029 | created context checkpoint 3 of 8 (pos_min = 3915, pos_max = 4557, size = 15.078 MiB)
slot print_timing: id  0 | task 17029 | 
prompt eval time =    2854.08 ms /  1732 tokens (    1.65 ms per token,   606.85 tokens per second)
       eval time =    1113.78 ms /    38 tokens (   29.31 ms per token,    34.12 tokens per second)
      total time =    3967.85 ms /  1770 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 17029 | stop processing: n_tokens = 4659, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.648 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17069 | processing task, is_child = 0
slot update_slots: id  0 | task 17069 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7145
slot update_slots: id  0 | task 17069 | n_tokens = 4633, memory_seq_rm [4633, end)
slot update_slots: id  0 | task 17069 | prompt processing progress, n_tokens = 6681, batch.n_tokens = 2048, progress = 0.935059
slot update_slots: id  0 | task 17069 | n_tokens = 6681, memory_seq_rm [6681, end)
slot update_slots: id  0 | task 17069 | prompt processing progress, n_tokens = 7081, batch.n_tokens = 400, progress = 0.991043
slot update_slots: id  0 | task 17069 | n_tokens = 7081, memory_seq_rm [7081, end)
slot update_slots: id  0 | task 17069 | prompt processing progress, n_tokens = 7145, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17069 | prompt done, n_tokens = 7145, batch.n_tokens = 64
slot init_sampler: id  0 | task 17069 | init sampler, took 1.01 ms, tokens: text = 7145, total = 7145
slot update_slots: id  0 | task 17069 | created context checkpoint 4 of 8 (pos_min = 6438, pos_max = 7080, size = 15.078 MiB)
slot print_timing: id  0 | task 17069 | 
prompt eval time =    4049.09 ms /  2512 tokens (    1.61 ms per token,   620.39 tokens per second)
       eval time =    1044.24 ms /    35 tokens (   29.84 ms per token,    33.52 tokens per second)
      total time =    5093.34 ms /  2547 tokens
slot      release: id  0 | task 17069 | stop processing: n_tokens = 7179, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.807 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17107 | processing task, is_child = 0
slot update_slots: id  0 | task 17107 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8864
slot update_slots: id  0 | task 17107 | n_tokens = 7153, memory_seq_rm [7153, end)
slot update_slots: id  0 | task 17107 | prompt processing progress, n_tokens = 8800, batch.n_tokens = 1647, progress = 0.992780
slot update_slots: id  0 | task 17107 | n_tokens = 8800, memory_seq_rm [8800, end)
slot update_slots: id  0 | task 17107 | prompt processing progress, n_tokens = 8864, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17107 | prompt done, n_tokens = 8864, batch.n_tokens = 64
slot init_sampler: id  0 | task 17107 | init sampler, took 1.25 ms, tokens: text = 8864, total = 8864
slot update_slots: id  0 | task 17107 | created context checkpoint 5 of 8 (pos_min = 8157, pos_max = 8799, size = 15.078 MiB)
slot print_timing: id  0 | task 17107 | 
prompt eval time =    3047.08 ms /  1711 tokens (    1.78 ms per token,   561.52 tokens per second)
       eval time =    2293.14 ms /    76 tokens (   30.17 ms per token,    33.14 tokens per second)
      total time =    5340.21 ms /  1787 tokens
slot      release: id  0 | task 17107 | stop processing: n_tokens = 8939, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17185 | processing task, is_child = 0
slot update_slots: id  0 | task 17185 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9368
slot update_slots: id  0 | task 17185 | n_tokens = 8910, memory_seq_rm [8910, end)
slot update_slots: id  0 | task 17185 | prompt processing progress, n_tokens = 9304, batch.n_tokens = 394, progress = 0.993168
slot update_slots: id  0 | task 17185 | n_tokens = 9304, memory_seq_rm [9304, end)
slot update_slots: id  0 | task 17185 | prompt processing progress, n_tokens = 9368, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17185 | prompt done, n_tokens = 9368, batch.n_tokens = 64
slot init_sampler: id  0 | task 17185 | init sampler, took 1.34 ms, tokens: text = 9368, total = 9368
slot update_slots: id  0 | task 17185 | created context checkpoint 6 of 8 (pos_min = 8661, pos_max = 9303, size = 15.078 MiB)
slot print_timing: id  0 | task 17185 | 
prompt eval time =     964.33 ms /   458 tokens (    2.11 ms per token,   474.94 tokens per second)
       eval time =    1209.14 ms /    39 tokens (   31.00 ms per token,    32.25 tokens per second)
      total time =    2173.47 ms /   497 tokens
slot      release: id  0 | task 17185 | stop processing: n_tokens = 9406, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17226 | processing task, is_child = 0
slot update_slots: id  0 | task 17226 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9435
slot update_slots: id  0 | task 17226 | n_tokens = 9377, memory_seq_rm [9377, end)
slot update_slots: id  0 | task 17226 | prompt processing progress, n_tokens = 9435, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 17226 | prompt done, n_tokens = 9435, batch.n_tokens = 58
slot init_sampler: id  0 | task 17226 | init sampler, took 1.85 ms, tokens: text = 9435, total = 9435
slot update_slots: id  0 | task 17226 | created context checkpoint 7 of 8 (pos_min = 8763, pos_max = 9376, size = 14.398 MiB)
slot print_timing: id  0 | task 17226 | 
prompt eval time =     223.73 ms /    58 tokens (    3.86 ms per token,   259.24 tokens per second)
       eval time =    1526.32 ms /    49 tokens (   31.15 ms per token,    32.10 tokens per second)
      total time =    1750.05 ms /   107 tokens
slot      release: id  0 | task 17226 | stop processing: n_tokens = 9483, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17276 | processing task, is_child = 0
slot update_slots: id  0 | task 17276 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9915
slot update_slots: id  0 | task 17276 | n_tokens = 9457, memory_seq_rm [9457, end)
slot update_slots: id  0 | task 17276 | prompt processing progress, n_tokens = 9851, batch.n_tokens = 394, progress = 0.993545
slot update_slots: id  0 | task 17276 | n_tokens = 9851, memory_seq_rm [9851, end)
slot update_slots: id  0 | task 17276 | prompt processing progress, n_tokens = 9915, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17276 | prompt done, n_tokens = 9915, batch.n_tokens = 64
slot init_sampler: id  0 | task 17276 | init sampler, took 1.41 ms, tokens: text = 9915, total = 9915
slot update_slots: id  0 | task 17276 | created context checkpoint 8 of 8 (pos_min = 9208, pos_max = 9850, size = 15.078 MiB)
slot print_timing: id  0 | task 17276 | 
prompt eval time =     973.70 ms /   458 tokens (    2.13 ms per token,   470.37 tokens per second)
       eval time =    1547.79 ms /    50 tokens (   30.96 ms per token,    32.30 tokens per second)
      total time =    2521.49 ms /   508 tokens
slot      release: id  0 | task 17276 | stop processing: n_tokens = 9964, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17328 | processing task, is_child = 0
slot update_slots: id  0 | task 17328 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9996
slot update_slots: id  0 | task 17328 | n_tokens = 9938, memory_seq_rm [9938, end)
slot update_slots: id  0 | task 17328 | prompt processing progress, n_tokens = 9996, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 17328 | prompt done, n_tokens = 9996, batch.n_tokens = 58
slot init_sampler: id  0 | task 17328 | init sampler, took 1.71 ms, tokens: text = 9996, total = 9996
slot update_slots: id  0 | task 17328 | erasing old context checkpoint (pos_min = 1358, pos_max = 2000, size = 15.078 MiB)
slot update_slots: id  0 | task 17328 | created context checkpoint 8 of 8 (pos_min = 9321, pos_max = 9937, size = 14.468 MiB)
slot print_timing: id  0 | task 17328 | 
prompt eval time =     223.49 ms /    58 tokens (    3.85 ms per token,   259.52 tokens per second)
       eval time =    1632.82 ms /    53 tokens (   30.81 ms per token,    32.46 tokens per second)
      total time =    1856.30 ms /   111 tokens
slot      release: id  0 | task 17328 | stop processing: n_tokens = 10048, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17382 | processing task, is_child = 0
slot update_slots: id  0 | task 17382 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10218
slot update_slots: id  0 | task 17382 | n_tokens = 10022, memory_seq_rm [10022, end)
slot update_slots: id  0 | task 17382 | prompt processing progress, n_tokens = 10154, batch.n_tokens = 132, progress = 0.993737
slot update_slots: id  0 | task 17382 | n_tokens = 10154, memory_seq_rm [10154, end)
slot update_slots: id  0 | task 17382 | prompt processing progress, n_tokens = 10218, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17382 | prompt done, n_tokens = 10218, batch.n_tokens = 64
slot init_sampler: id  0 | task 17382 | init sampler, took 1.45 ms, tokens: text = 10218, total = 10218
slot update_slots: id  0 | task 17382 | erasing old context checkpoint (pos_min = 2139, pos_max = 2781, size = 15.078 MiB)
slot update_slots: id  0 | task 17382 | created context checkpoint 8 of 8 (pos_min = 9511, pos_max = 10153, size = 15.078 MiB)
slot print_timing: id  0 | task 17382 | 
prompt eval time =     689.79 ms /   196 tokens (    3.52 ms per token,   284.14 tokens per second)
       eval time =    5684.45 ms /   186 tokens (   30.56 ms per token,    32.72 tokens per second)
      total time =    6374.24 ms /   382 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 17382 | stop processing: n_tokens = 10403, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17570 | processing task, is_child = 0
slot update_slots: id  0 | task 17570 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10842
slot update_slots: id  0 | task 17570 | n_tokens = 10377, memory_seq_rm [10377, end)
slot update_slots: id  0 | task 17570 | prompt processing progress, n_tokens = 10778, batch.n_tokens = 401, progress = 0.994097
slot update_slots: id  0 | task 17570 | n_tokens = 10778, memory_seq_rm [10778, end)
slot update_slots: id  0 | task 17570 | prompt processing progress, n_tokens = 10842, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17570 | prompt done, n_tokens = 10842, batch.n_tokens = 64
slot init_sampler: id  0 | task 17570 | init sampler, took 2.07 ms, tokens: text = 10842, total = 10842
slot update_slots: id  0 | task 17570 | erasing old context checkpoint (pos_min = 3915, pos_max = 4557, size = 15.078 MiB)
slot update_slots: id  0 | task 17570 | created context checkpoint 8 of 8 (pos_min = 10135, pos_max = 10777, size = 15.078 MiB)
slot print_timing: id  0 | task 17570 | 
prompt eval time =     961.58 ms /   465 tokens (    2.07 ms per token,   483.58 tokens per second)
       eval time =   12093.50 ms /   401 tokens (   30.16 ms per token,    33.16 tokens per second)
      total time =   13055.08 ms /   866 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 17570 | stop processing: n_tokens = 11242, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 17973 | processing task, is_child = 0
slot update_slots: id  0 | task 17973 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11291
slot update_slots: id  0 | task 17973 | n_tokens = 11216, memory_seq_rm [11216, end)
slot update_slots: id  0 | task 17973 | prompt processing progress, n_tokens = 11227, batch.n_tokens = 11, progress = 0.994332
slot update_slots: id  0 | task 17973 | n_tokens = 11227, memory_seq_rm [11227, end)
slot update_slots: id  0 | task 17973 | prompt processing progress, n_tokens = 11291, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 17973 | prompt done, n_tokens = 11291, batch.n_tokens = 64
slot init_sampler: id  0 | task 17973 | init sampler, took 1.62 ms, tokens: text = 11291, total = 11291
slot update_slots: id  0 | task 17973 | erasing old context checkpoint (pos_min = 6438, pos_max = 7080, size = 15.078 MiB)
slot update_slots: id  0 | task 17973 | created context checkpoint 8 of 8 (pos_min = 10599, pos_max = 11226, size = 14.726 MiB)
slot print_timing: id  0 | task 17973 | 
prompt eval time =     354.82 ms /    75 tokens (    4.73 ms per token,   211.37 tokens per second)
       eval time =    1960.70 ms /    67 tokens (   29.26 ms per token,    34.17 tokens per second)
      total time =    2315.53 ms /   142 tokens
slot      release: id  0 | task 17973 | stop processing: n_tokens = 11357, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18042 | processing task, is_child = 0
slot update_slots: id  0 | task 18042 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11598
slot update_slots: id  0 | task 18042 | n_tokens = 11330, memory_seq_rm [11330, end)
slot update_slots: id  0 | task 18042 | prompt processing progress, n_tokens = 11534, batch.n_tokens = 204, progress = 0.994482
slot update_slots: id  0 | task 18042 | n_tokens = 11534, memory_seq_rm [11534, end)
slot update_slots: id  0 | task 18042 | prompt processing progress, n_tokens = 11598, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18042 | prompt done, n_tokens = 11598, batch.n_tokens = 64
slot init_sampler: id  0 | task 18042 | init sampler, took 1.64 ms, tokens: text = 11598, total = 11598
slot update_slots: id  0 | task 18042 | erasing old context checkpoint (pos_min = 8157, pos_max = 8799, size = 15.078 MiB)
slot update_slots: id  0 | task 18042 | created context checkpoint 8 of 8 (pos_min = 10891, pos_max = 11533, size = 15.078 MiB)
slot print_timing: id  0 | task 18042 | 
prompt eval time =     709.70 ms /   268 tokens (    2.65 ms per token,   377.62 tokens per second)
       eval time =    4995.09 ms /   171 tokens (   29.21 ms per token,    34.23 tokens per second)
      total time =    5704.79 ms /   439 tokens
slot      release: id  0 | task 18042 | stop processing: n_tokens = 11768, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.904 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18215 | processing task, is_child = 0
slot update_slots: id  0 | task 18215 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12993
slot update_slots: id  0 | task 18215 | n_tokens = 11743, memory_seq_rm [11743, end)
slot update_slots: id  0 | task 18215 | prompt processing progress, n_tokens = 12929, batch.n_tokens = 1186, progress = 0.995074
slot update_slots: id  0 | task 18215 | n_tokens = 12929, memory_seq_rm [12929, end)
slot update_slots: id  0 | task 18215 | prompt processing progress, n_tokens = 12993, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18215 | prompt done, n_tokens = 12993, batch.n_tokens = 64
slot init_sampler: id  0 | task 18215 | init sampler, took 2.67 ms, tokens: text = 12993, total = 12993
slot update_slots: id  0 | task 18215 | erasing old context checkpoint (pos_min = 8661, pos_max = 9303, size = 15.078 MiB)
slot update_slots: id  0 | task 18215 | created context checkpoint 8 of 8 (pos_min = 12286, pos_max = 12928, size = 15.078 MiB)
slot print_timing: id  0 | task 18215 | 
prompt eval time =    2314.46 ms /  1250 tokens (    1.85 ms per token,   540.08 tokens per second)
       eval time =   10312.92 ms /   350 tokens (   29.47 ms per token,    33.94 tokens per second)
      total time =   12627.38 ms /  1600 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 18215 | stop processing: n_tokens = 13342, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18567 | processing task, is_child = 0
slot update_slots: id  0 | task 18567 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13536
slot update_slots: id  0 | task 18567 | n_tokens = 13316, memory_seq_rm [13316, end)
slot update_slots: id  0 | task 18567 | prompt processing progress, n_tokens = 13472, batch.n_tokens = 156, progress = 0.995272
slot update_slots: id  0 | task 18567 | n_tokens = 13472, memory_seq_rm [13472, end)
slot update_slots: id  0 | task 18567 | prompt processing progress, n_tokens = 13536, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18567 | prompt done, n_tokens = 13536, batch.n_tokens = 64
slot init_sampler: id  0 | task 18567 | init sampler, took 1.92 ms, tokens: text = 13536, total = 13536
slot update_slots: id  0 | task 18567 | erasing old context checkpoint (pos_min = 8763, pos_max = 9376, size = 14.398 MiB)
slot update_slots: id  0 | task 18567 | created context checkpoint 8 of 8 (pos_min = 12829, pos_max = 13471, size = 15.078 MiB)
slot print_timing: id  0 | task 18567 | 
prompt eval time =     603.72 ms /   220 tokens (    2.74 ms per token,   364.41 tokens per second)
       eval time =    3760.05 ms /   125 tokens (   30.08 ms per token,    33.24 tokens per second)
      total time =    4363.76 ms /   345 tokens
slot      release: id  0 | task 18567 | stop processing: n_tokens = 13660, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18694 | processing task, is_child = 0
slot update_slots: id  0 | task 18694 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13923
slot update_slots: id  0 | task 18694 | n_tokens = 13636, memory_seq_rm [13636, end)
slot update_slots: id  0 | task 18694 | prompt processing progress, n_tokens = 13859, batch.n_tokens = 223, progress = 0.995403
slot update_slots: id  0 | task 18694 | n_tokens = 13859, memory_seq_rm [13859, end)
slot update_slots: id  0 | task 18694 | prompt processing progress, n_tokens = 13923, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18694 | prompt done, n_tokens = 13923, batch.n_tokens = 64
slot init_sampler: id  0 | task 18694 | init sampler, took 1.97 ms, tokens: text = 13923, total = 13923
slot update_slots: id  0 | task 18694 | erasing old context checkpoint (pos_min = 9208, pos_max = 9850, size = 15.078 MiB)
slot update_slots: id  0 | task 18694 | created context checkpoint 8 of 8 (pos_min = 13216, pos_max = 13858, size = 15.078 MiB)
slot print_timing: id  0 | task 18694 | 
prompt eval time =     709.68 ms /   287 tokens (    2.47 ms per token,   404.40 tokens per second)
       eval time =    4518.94 ms /   152 tokens (   29.73 ms per token,    33.64 tokens per second)
      total time =    5228.63 ms /   439 tokens
slot      release: id  0 | task 18694 | stop processing: n_tokens = 14074, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 18848 | processing task, is_child = 0
slot update_slots: id  0 | task 18848 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14120
slot update_slots: id  0 | task 18848 | n_tokens = 14047, memory_seq_rm [14047, end)
slot update_slots: id  0 | task 18848 | prompt processing progress, n_tokens = 14056, batch.n_tokens = 9, progress = 0.995467
slot update_slots: id  0 | task 18848 | n_tokens = 14056, memory_seq_rm [14056, end)
slot update_slots: id  0 | task 18848 | prompt processing progress, n_tokens = 14120, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 18848 | prompt done, n_tokens = 14120, batch.n_tokens = 64
slot init_sampler: id  0 | task 18848 | init sampler, took 2.06 ms, tokens: text = 14120, total = 14120
slot update_slots: id  0 | task 18848 | erasing old context checkpoint (pos_min = 9321, pos_max = 9937, size = 14.468 MiB)
slot update_slots: id  0 | task 18848 | created context checkpoint 8 of 8 (pos_min = 13431, pos_max = 14055, size = 14.656 MiB)
slot print_timing: id  0 | task 18848 | 
prompt eval time =     349.73 ms /    73 tokens (    4.79 ms per token,   208.73 tokens per second)
       eval time =   18739.35 ms /   616 tokens (   30.42 ms per token,    32.87 tokens per second)
      total time =   19089.09 ms /   689 tokens
slot      release: id  0 | task 18848 | stop processing: n_tokens = 14735, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 19466 | processing task, is_child = 0
slot update_slots: id  0 | task 19466 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14779
slot update_slots: id  0 | task 19466 | n_tokens = 14711, memory_seq_rm [14711, end)
slot update_slots: id  0 | task 19466 | prompt processing progress, n_tokens = 14715, batch.n_tokens = 4, progress = 0.995670
slot update_slots: id  0 | task 19466 | n_tokens = 14715, memory_seq_rm [14715, end)
slot update_slots: id  0 | task 19466 | prompt processing progress, n_tokens = 14779, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 19466 | prompt done, n_tokens = 14779, batch.n_tokens = 64
slot init_sampler: id  0 | task 19466 | init sampler, took 2.28 ms, tokens: text = 14779, total = 14779
slot update_slots: id  0 | task 19466 | erasing old context checkpoint (pos_min = 9511, pos_max = 10153, size = 15.078 MiB)
slot update_slots: id  0 | task 19466 | created context checkpoint 8 of 8 (pos_min = 14092, pos_max = 14714, size = 14.609 MiB)
slot print_timing: id  0 | task 19466 | 
prompt eval time =     318.53 ms /    68 tokens (    4.68 ms per token,   213.48 tokens per second)
       eval time =    6917.40 ms /   228 tokens (   30.34 ms per token,    32.96 tokens per second)
      total time =    7235.93 ms /   296 tokens
slot      release: id  0 | task 19466 | stop processing: n_tokens = 15006, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 19696 | processing task, is_child = 0
slot update_slots: id  0 | task 19696 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15131
slot update_slots: id  0 | task 19696 | n_tokens = 14982, memory_seq_rm [14982, end)
slot update_slots: id  0 | task 19696 | prompt processing progress, n_tokens = 15067, batch.n_tokens = 85, progress = 0.995770
slot update_slots: id  0 | task 19696 | n_tokens = 15067, memory_seq_rm [15067, end)
slot update_slots: id  0 | task 19696 | prompt processing progress, n_tokens = 15131, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 19696 | prompt done, n_tokens = 15131, batch.n_tokens = 64
slot init_sampler: id  0 | task 19696 | init sampler, took 2.14 ms, tokens: text = 15131, total = 15131
slot update_slots: id  0 | task 19696 | erasing old context checkpoint (pos_min = 10135, pos_max = 10777, size = 15.078 MiB)
slot update_slots: id  0 | task 19696 | created context checkpoint 8 of 8 (pos_min = 14424, pos_max = 15066, size = 15.078 MiB)
slot print_timing: id  0 | task 19696 | 
prompt eval time =     636.31 ms /   149 tokens (    4.27 ms per token,   234.16 tokens per second)
       eval time =    1630.28 ms /    54 tokens (   30.19 ms per token,    33.12 tokens per second)
      total time =    2266.59 ms /   203 tokens
slot      release: id  0 | task 19696 | stop processing: n_tokens = 15184, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.945 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 19752 | processing task, is_child = 0
slot update_slots: id  0 | task 19752 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16035
slot update_slots: id  0 | task 19752 | n_tokens = 15157, memory_seq_rm [15157, end)
slot update_slots: id  0 | task 19752 | prompt processing progress, n_tokens = 15971, batch.n_tokens = 814, progress = 0.996009
slot update_slots: id  0 | task 19752 | n_tokens = 15971, memory_seq_rm [15971, end)
slot update_slots: id  0 | task 19752 | prompt processing progress, n_tokens = 16035, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 19752 | prompt done, n_tokens = 16035, batch.n_tokens = 64
slot init_sampler: id  0 | task 19752 | init sampler, took 3.03 ms, tokens: text = 16035, total = 16035
slot update_slots: id  0 | task 19752 | erasing old context checkpoint (pos_min = 10599, pos_max = 11226, size = 14.726 MiB)
slot update_slots: id  0 | task 19752 | created context checkpoint 8 of 8 (pos_min = 15328, pos_max = 15970, size = 15.078 MiB)
slot print_timing: id  0 | task 19752 | 
prompt eval time =    1779.50 ms /   878 tokens (    2.03 ms per token,   493.40 tokens per second)
       eval time =   15019.74 ms /   494 tokens (   30.40 ms per token,    32.89 tokens per second)
      total time =   16799.23 ms /  1372 tokens
slot      release: id  0 | task 19752 | stop processing: n_tokens = 16528, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20248 | processing task, is_child = 0
slot update_slots: id  0 | task 20248 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16574
slot update_slots: id  0 | task 20248 | n_tokens = 16505, memory_seq_rm [16505, end)
slot update_slots: id  0 | task 20248 | prompt processing progress, n_tokens = 16510, batch.n_tokens = 5, progress = 0.996139
slot update_slots: id  0 | task 20248 | n_tokens = 16510, memory_seq_rm [16510, end)
slot update_slots: id  0 | task 20248 | prompt processing progress, n_tokens = 16574, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20248 | prompt done, n_tokens = 16574, batch.n_tokens = 64
slot init_sampler: id  0 | task 20248 | init sampler, took 2.37 ms, tokens: text = 16574, total = 16574
slot update_slots: id  0 | task 20248 | erasing old context checkpoint (pos_min = 10891, pos_max = 11533, size = 15.078 MiB)
slot update_slots: id  0 | task 20248 | created context checkpoint 8 of 8 (pos_min = 15885, pos_max = 16509, size = 14.656 MiB)
slot print_timing: id  0 | task 20248 | 
prompt eval time =     323.13 ms /    69 tokens (    4.68 ms per token,   213.53 tokens per second)
       eval time =   16473.38 ms /   543 tokens (   30.34 ms per token,    32.96 tokens per second)
      total time =   16796.51 ms /   612 tokens
slot      release: id  0 | task 20248 | stop processing: n_tokens = 17116, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20793 | processing task, is_child = 0
slot update_slots: id  0 | task 20793 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17156
slot update_slots: id  0 | task 20793 | n_tokens = 16651, memory_seq_rm [16651, end)
slot update_slots: id  0 | task 20793 | prompt processing progress, n_tokens = 17092, batch.n_tokens = 441, progress = 0.996270
slot update_slots: id  0 | task 20793 | n_tokens = 17092, memory_seq_rm [17092, end)
slot update_slots: id  0 | task 20793 | prompt processing progress, n_tokens = 17156, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20793 | prompt done, n_tokens = 17156, batch.n_tokens = 64
slot init_sampler: id  0 | task 20793 | init sampler, took 2.65 ms, tokens: text = 17156, total = 17156
slot update_slots: id  0 | task 20793 | erasing old context checkpoint (pos_min = 12286, pos_max = 12928, size = 15.078 MiB)
slot update_slots: id  0 | task 20793 | created context checkpoint 8 of 8 (pos_min = 16516, pos_max = 17091, size = 13.507 MiB)
slot print_timing: id  0 | task 20793 | 
prompt eval time =    1048.67 ms /   505 tokens (    2.08 ms per token,   481.56 tokens per second)
       eval time =    5551.76 ms /   183 tokens (   30.34 ms per token,    32.96 tokens per second)
      total time =    6600.43 ms /   688 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 20793 | stop processing: n_tokens = 17338, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 20978 | processing task, is_child = 0
slot update_slots: id  0 | task 20978 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17368
slot update_slots: id  0 | task 20978 | n_tokens = 17167, memory_seq_rm [17167, end)
slot update_slots: id  0 | task 20978 | prompt processing progress, n_tokens = 17304, batch.n_tokens = 137, progress = 0.996315
slot update_slots: id  0 | task 20978 | n_tokens = 17304, memory_seq_rm [17304, end)
slot update_slots: id  0 | task 20978 | prompt processing progress, n_tokens = 17368, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 20978 | prompt done, n_tokens = 17368, batch.n_tokens = 64
slot init_sampler: id  0 | task 20978 | init sampler, took 2.46 ms, tokens: text = 17368, total = 17368
slot update_slots: id  0 | task 20978 | erasing old context checkpoint (pos_min = 12829, pos_max = 13471, size = 15.078 MiB)
slot update_slots: id  0 | task 20978 | created context checkpoint 8 of 8 (pos_min = 16788, pos_max = 17303, size = 12.100 MiB)
slot print_timing: id  0 | task 20978 | 
prompt eval time =     631.90 ms /   201 tokens (    3.14 ms per token,   318.09 tokens per second)
       eval time =   12537.47 ms /   413 tokens (   30.36 ms per token,    32.94 tokens per second)
      total time =   13169.37 ms /   614 tokens
slot      release: id  0 | task 20978 | stop processing: n_tokens = 17780, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 21393 | processing task, is_child = 0
slot update_slots: id  0 | task 21393 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17810
slot update_slots: id  0 | task 21393 | n_tokens = 17379, memory_seq_rm [17379, end)
slot update_slots: id  0 | task 21393 | prompt processing progress, n_tokens = 17746, batch.n_tokens = 367, progress = 0.996406
slot update_slots: id  0 | task 21393 | n_tokens = 17746, memory_seq_rm [17746, end)
slot update_slots: id  0 | task 21393 | prompt processing progress, n_tokens = 17810, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 21393 | prompt done, n_tokens = 17810, batch.n_tokens = 64
slot init_sampler: id  0 | task 21393 | init sampler, took 2.53 ms, tokens: text = 17810, total = 17810
slot update_slots: id  0 | task 21393 | erasing old context checkpoint (pos_min = 13216, pos_max = 13858, size = 15.078 MiB)
slot update_slots: id  0 | task 21393 | created context checkpoint 8 of 8 (pos_min = 17167, pos_max = 17745, size = 13.577 MiB)
slot print_timing: id  0 | task 21393 | 
prompt eval time =     995.64 ms /   431 tokens (    2.31 ms per token,   432.89 tokens per second)
       eval time =    4277.96 ms /   141 tokens (   30.34 ms per token,    32.96 tokens per second)
      total time =    5273.60 ms /   572 tokens
slot      release: id  0 | task 21393 | stop processing: n_tokens = 17950, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 21536 | processing task, is_child = 0
slot update_slots: id  0 | task 21536 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17980
slot update_slots: id  0 | task 21536 | n_tokens = 17821, memory_seq_rm [17821, end)
slot update_slots: id  0 | task 21536 | prompt processing progress, n_tokens = 17916, batch.n_tokens = 95, progress = 0.996440
slot update_slots: id  0 | task 21536 | n_tokens = 17916, memory_seq_rm [17916, end)
slot update_slots: id  0 | task 21536 | prompt processing progress, n_tokens = 17980, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 21536 | prompt done, n_tokens = 17980, batch.n_tokens = 64
slot init_sampler: id  0 | task 21536 | init sampler, took 2.58 ms, tokens: text = 17980, total = 17980
slot update_slots: id  0 | task 21536 | erasing old context checkpoint (pos_min = 13431, pos_max = 14055, size = 14.656 MiB)
slot update_slots: id  0 | task 21536 | created context checkpoint 8 of 8 (pos_min = 17307, pos_max = 17915, size = 14.281 MiB)
slot print_timing: id  0 | task 21536 | 
prompt eval time =     576.89 ms /   159 tokens (    3.63 ms per token,   275.62 tokens per second)
       eval time =   26585.86 ms /   874 tokens (   30.42 ms per token,    32.87 tokens per second)
      total time =   27162.75 ms /  1033 tokens
slot      release: id  0 | task 21536 | stop processing: n_tokens = 18853, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22412 | processing task, is_child = 0
slot update_slots: id  0 | task 22412 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18884
slot update_slots: id  0 | task 22412 | n_past = 18146, slot.prompt.tokens.size() = 18853, seq_id = 0, pos_min = 18210, n_swa = 128
slot update_slots: id  0 | task 22412 | restored context checkpoint (pos_min = 17307, pos_max = 17915, size = 14.281 MiB)
slot update_slots: id  0 | task 22412 | n_tokens = 17915, memory_seq_rm [17915, end)
slot update_slots: id  0 | task 22412 | prompt processing progress, n_tokens = 18820, batch.n_tokens = 905, progress = 0.996611
slot update_slots: id  0 | task 22412 | n_tokens = 18820, memory_seq_rm [18820, end)
slot update_slots: id  0 | task 22412 | prompt processing progress, n_tokens = 18884, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22412 | prompt done, n_tokens = 18884, batch.n_tokens = 64
slot init_sampler: id  0 | task 22412 | init sampler, took 2.65 ms, tokens: text = 18884, total = 18884
slot update_slots: id  0 | task 22412 | erasing old context checkpoint (pos_min = 14092, pos_max = 14714, size = 14.609 MiB)
slot update_slots: id  0 | task 22412 | created context checkpoint 8 of 8 (pos_min = 18199, pos_max = 18819, size = 14.562 MiB)
slot print_timing: id  0 | task 22412 | 
prompt eval time =    1950.30 ms /   969 tokens (    2.01 ms per token,   496.85 tokens per second)
       eval time =    6849.10 ms /   224 tokens (   30.58 ms per token,    32.71 tokens per second)
      total time =    8799.40 ms /  1193 tokens
slot      release: id  0 | task 22412 | stop processing: n_tokens = 19107, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 22638 | processing task, is_child = 0
slot update_slots: id  0 | task 22638 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19155
slot update_slots: id  0 | task 22638 | n_tokens = 19087, memory_seq_rm [19087, end)
slot update_slots: id  0 | task 22638 | prompt processing progress, n_tokens = 19091, batch.n_tokens = 4, progress = 0.996659
slot update_slots: id  0 | task 22638 | n_tokens = 19091, memory_seq_rm [19091, end)
slot update_slots: id  0 | task 22638 | prompt processing progress, n_tokens = 19155, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 22638 | prompt done, n_tokens = 19155, batch.n_tokens = 64
slot init_sampler: id  0 | task 22638 | init sampler, took 2.78 ms, tokens: text = 19155, total = 19155
slot update_slots: id  0 | task 22638 | erasing old context checkpoint (pos_min = 14424, pos_max = 15066, size = 15.078 MiB)
slot update_slots: id  0 | task 22638 | created context checkpoint 8 of 8 (pos_min = 18464, pos_max = 19090, size = 14.703 MiB)
slot print_timing: id  0 | task 22638 | 
prompt eval time =     387.50 ms /    68 tokens (    5.70 ms per token,   175.48 tokens per second)
       eval time =   27636.32 ms /   908 tokens (   30.44 ms per token,    32.86 tokens per second)
      total time =   28023.82 ms /   976 tokens
slot      release: id  0 | task 22638 | stop processing: n_tokens = 20062, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 23548 | processing task, is_child = 0
slot update_slots: id  0 | task 23548 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 20094
slot update_slots: id  0 | task 23548 | n_tokens = 19717, memory_seq_rm [19717, end)
slot update_slots: id  0 | task 23548 | prompt processing progress, n_tokens = 20030, batch.n_tokens = 313, progress = 0.996815
slot update_slots: id  0 | task 23548 | n_tokens = 20030, memory_seq_rm [20030, end)
slot update_slots: id  0 | task 23548 | prompt processing progress, n_tokens = 20094, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 23548 | prompt done, n_tokens = 20094, batch.n_tokens = 64
slot init_sampler: id  0 | task 23548 | init sampler, took 3.81 ms, tokens: text = 20094, total = 20094
slot update_slots: id  0 | task 23548 | erasing old context checkpoint (pos_min = 15328, pos_max = 15970, size = 15.078 MiB)
slot update_slots: id  0 | task 23548 | created context checkpoint 8 of 8 (pos_min = 19419, pos_max = 20029, size = 14.328 MiB)
slot print_timing: id  0 | task 23548 | 
prompt eval time =     928.45 ms /   377 tokens (    2.46 ms per token,   406.05 tokens per second)
       eval time =    5692.38 ms /   187 tokens (   30.44 ms per token,    32.85 tokens per second)
      total time =    6620.83 ms /   564 tokens
slot      release: id  0 | task 23548 | stop processing: n_tokens = 20280, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.126 (> 0.100 thold), f_keep = 0.102
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 20280, total state size = 490.623 MiB
srv          load:  - looking for better prompt, base f_keep = 0.102, sim = 0.126
srv        update:  - cache state: 5 prompts, 2033.270 MiB (limits: 8192.000 MiB, 56064 tokens, 256146 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv  get_availabl: prompt cache update took 612.88 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 23737 | processing task, is_child = 0
slot update_slots: id  0 | task 23737 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16404
slot update_slots: id  0 | task 23737 | n_past = 2065, slot.prompt.tokens.size() = 20280, seq_id = 0, pos_min = 19637, n_swa = 128
slot update_slots: id  0 | task 23737 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 15885, pos_max = 16509, n_swa = 128, size = 14.656 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 16516, pos_max = 17091, n_swa = 128, size = 13.507 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 16788, pos_max = 17303, n_swa = 128, size = 12.100 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 17167, pos_max = 17745, n_swa = 128, size = 13.577 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 17307, pos_max = 17915, n_swa = 128, size = 14.281 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 18199, pos_max = 18819, n_swa = 128, size = 14.562 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 18464, pos_max = 19090, n_swa = 128, size = 14.703 MiB)
slot update_slots: id  0 | task 23737 | erased invalidated context checkpoint (pos_min = 19419, pos_max = 20029, n_swa = 128, size = 14.328 MiB)
slot update_slots: id  0 | task 23737 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.124848
slot update_slots: id  0 | task 23737 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.249695
slot update_slots: id  0 | task 23737 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.374543
slot update_slots: id  0 | task 23737 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.499390
slot update_slots: id  0 | task 23737 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.624238
slot update_slots: id  0 | task 23737 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.749086
slot update_slots: id  0 | task 23737 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.873933
slot update_slots: id  0 | task 23737 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 16340, batch.n_tokens = 2004, progress = 0.996099
slot update_slots: id  0 | task 23737 | n_tokens = 16340, memory_seq_rm [16340, end)
slot update_slots: id  0 | task 23737 | prompt processing progress, n_tokens = 16404, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 23737 | prompt done, n_tokens = 16404, batch.n_tokens = 64
slot init_sampler: id  0 | task 23737 | init sampler, took 4.28 ms, tokens: text = 16404, total = 16404
slot update_slots: id  0 | task 23737 | created context checkpoint 1 of 8 (pos_min = 15697, pos_max = 16339, size = 15.078 MiB)
slot print_timing: id  0 | task 23737 | 
prompt eval time =   26032.02 ms / 16404 tokens (    1.59 ms per token,   630.15 tokens per second)
       eval time =   60755.76 ms /  1980 tokens (   30.68 ms per token,    32.59 tokens per second)
      total time =   86787.79 ms / 18384 tokens
slot      release: id  0 | task 23737 | stop processing: n_tokens = 18383, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.892
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 25726 | processing task, is_child = 0
slot update_slots: id  0 | task 25726 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17262
slot update_slots: id  0 | task 25726 | n_past = 16405, slot.prompt.tokens.size() = 18383, seq_id = 0, pos_min = 17740, n_swa = 128
slot update_slots: id  0 | task 25726 | restored context checkpoint (pos_min = 15697, pos_max = 16339, size = 15.078 MiB)
slot update_slots: id  0 | task 25726 | n_tokens = 16339, memory_seq_rm [16339, end)
slot update_slots: id  0 | task 25726 | prompt processing progress, n_tokens = 17198, batch.n_tokens = 859, progress = 0.996292
slot update_slots: id  0 | task 25726 | n_tokens = 17198, memory_seq_rm [17198, end)
slot update_slots: id  0 | task 25726 | prompt processing progress, n_tokens = 17262, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 25726 | prompt done, n_tokens = 17262, batch.n_tokens = 64
slot init_sampler: id  0 | task 25726 | init sampler, took 2.44 ms, tokens: text = 17262, total = 17262
slot update_slots: id  0 | task 25726 | created context checkpoint 2 of 8 (pos_min = 16555, pos_max = 17197, size = 15.078 MiB)
slot print_timing: id  0 | task 25726 | 
prompt eval time =    1940.36 ms /   923 tokens (    2.10 ms per token,   475.69 tokens per second)
       eval time =  126339.09 ms /  4096 tokens (   30.84 ms per token,    32.42 tokens per second)
      total time =  128279.45 ms /  5019 tokens
slot      release: id  0 | task 25726 | stop processing: n_tokens = 21357, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 25726
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.032
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 21357, total state size = 515.877 MiB
srv          load:  - looking for better prompt, base f_keep = 0.032, sim = 0.990
srv        update:  - cache state: 6 prompts, 2579.303 MiB (limits: 8192.000 MiB, 56064 tokens, 269751 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv  get_availabl: prompt cache update took 397.56 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 29825 | processing task, is_child = 0
slot update_slots: id  0 | task 29825 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 684
slot update_slots: id  0 | task 29825 | n_past = 677, slot.prompt.tokens.size() = 21357, seq_id = 0, pos_min = 20714, n_swa = 128
slot update_slots: id  0 | task 29825 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 29825 | erased invalidated context checkpoint (pos_min = 15697, pos_max = 16339, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 29825 | erased invalidated context checkpoint (pos_min = 16555, pos_max = 17197, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  0 | task 29825 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 29825 | prompt processing progress, n_tokens = 620, batch.n_tokens = 620, progress = 0.906433
slot update_slots: id  0 | task 29825 | n_tokens = 620, memory_seq_rm [620, end)
slot update_slots: id  0 | task 29825 | prompt processing progress, n_tokens = 684, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 29825 | prompt done, n_tokens = 684, batch.n_tokens = 64
slot init_sampler: id  0 | task 29825 | init sampler, took 0.11 ms, tokens: text = 684, total = 684
slot update_slots: id  0 | task 29825 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 619, size = 14.539 MiB)
slot print_timing: id  0 | task 29825 | 
prompt eval time =    1310.14 ms /   684 tokens (    1.92 ms per token,   522.08 tokens per second)
       eval time =    5188.76 ms /   182 tokens (   28.51 ms per token,    35.08 tokens per second)
      total time =    6498.90 ms /   866 tokens
slot      release: id  0 | task 29825 | stop processing: n_tokens = 865, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 30009 | processing task, is_child = 0
slot update_slots: id  0 | task 30009 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 910
slot update_slots: id  0 | task 30009 | n_tokens = 848, memory_seq_rm [848, end)
slot update_slots: id  0 | task 30009 | prompt processing progress, n_tokens = 910, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  0 | task 30009 | prompt done, n_tokens = 910, batch.n_tokens = 62
slot init_sampler: id  0 | task 30009 | init sampler, took 0.14 ms, tokens: text = 910, total = 910
slot update_slots: id  0 | task 30009 | created context checkpoint 2 of 8 (pos_min = 222, pos_max = 847, size = 14.679 MiB)
slot print_timing: id  0 | task 30009 | 
prompt eval time =     223.84 ms /    62 tokens (    3.61 ms per token,   276.99 tokens per second)
       eval time =     524.52 ms /    19 tokens (   27.61 ms per token,    36.22 tokens per second)
      total time =     748.36 ms /    81 tokens
slot      release: id  0 | task 30009 | stop processing: n_tokens = 928, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.081
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8461, total state size = 201.380 MiB
srv          load:  - looking for better prompt, base f_keep = 0.081, sim = 0.976
srv        update:  - cache state: 7 prompts, 2831.380 MiB (limits: 8192.000 MiB, 56064 tokens, 270215 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv  get_availabl: prompt cache update took 181.57 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30029 | processing task, is_child = 0
slot update_slots: id  1 | task 30029 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 704
slot update_slots: id  1 | task 30029 | n_past = 687, slot.prompt.tokens.size() = 8461, seq_id = 1, pos_min = 8334, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  1 | task 30029 | failed to restore context checkpoint (pos_min = 0, pos_max = 749, size = 17.587 MiB)
slot update_slots: id  1 | task 30029 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 30029 | erased invalidated context checkpoint (pos_min = 5823, pos_max = 6592, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 30029 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 30029 | prompt processing progress, n_tokens = 640, batch.n_tokens = 640, progress = 0.909091
slot update_slots: id  1 | task 30029 | n_tokens = 640, memory_seq_rm [640, end)
slot update_slots: id  1 | task 30029 | prompt processing progress, n_tokens = 704, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30029 | prompt done, n_tokens = 704, batch.n_tokens = 64
slot init_sampler: id  1 | task 30029 | init sampler, took 0.12 ms, tokens: text = 704, total = 704
slot print_timing: id  1 | task 30029 | 
prompt eval time =    1409.63 ms /   704 tokens (    2.00 ms per token,   499.42 tokens per second)
       eval time =    2676.08 ms /    95 tokens (   28.17 ms per token,    35.50 tokens per second)
      total time =    4085.71 ms /   799 tokens
slot      release: id  1 | task 30029 | stop processing: n_tokens = 798, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.307 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30126 | processing task, is_child = 0
slot update_slots: id  1 | task 30126 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2500
slot update_slots: id  1 | task 30126 | n_tokens = 768, memory_seq_rm [768, end)
slot update_slots: id  1 | task 30126 | prompt processing progress, n_tokens = 2436, batch.n_tokens = 1668, progress = 0.974400
slot update_slots: id  1 | task 30126 | n_tokens = 2436, memory_seq_rm [2436, end)
slot update_slots: id  1 | task 30126 | prompt processing progress, n_tokens = 2500, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30126 | prompt done, n_tokens = 2500, batch.n_tokens = 64
slot init_sampler: id  1 | task 30126 | init sampler, took 0.56 ms, tokens: text = 2500, total = 2500
slot update_slots: id  1 | task 30126 | created context checkpoint 3 of 8 (pos_min = 1793, pos_max = 2435, size = 15.078 MiB)
slot print_timing: id  1 | task 30126 | 
prompt eval time =    2835.30 ms /  1732 tokens (    1.64 ms per token,   610.87 tokens per second)
       eval time =    1178.95 ms /    39 tokens (   30.23 ms per token,    33.08 tokens per second)
      total time =    4014.25 ms /  1771 tokens
slot      release: id  1 | task 30126 | stop processing: n_tokens = 2538, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.500 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30167 | processing task, is_child = 0
slot update_slots: id  1 | task 30167 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5020
slot update_slots: id  1 | task 30167 | n_tokens = 2508, memory_seq_rm [2508, end)
slot update_slots: id  1 | task 30167 | prompt processing progress, n_tokens = 4556, batch.n_tokens = 2048, progress = 0.907570
slot update_slots: id  1 | task 30167 | n_tokens = 4556, memory_seq_rm [4556, end)
slot update_slots: id  1 | task 30167 | prompt processing progress, n_tokens = 4956, batch.n_tokens = 400, progress = 0.987251
slot update_slots: id  1 | task 30167 | n_tokens = 4956, memory_seq_rm [4956, end)
slot update_slots: id  1 | task 30167 | prompt processing progress, n_tokens = 5020, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30167 | prompt done, n_tokens = 5020, batch.n_tokens = 64
slot init_sampler: id  1 | task 30167 | init sampler, took 0.77 ms, tokens: text = 5020, total = 5020
slot update_slots: id  1 | task 30167 | created context checkpoint 4 of 8 (pos_min = 4313, pos_max = 4955, size = 15.078 MiB)
slot print_timing: id  1 | task 30167 | 
prompt eval time =    3991.56 ms /  2512 tokens (    1.59 ms per token,   629.33 tokens per second)
       eval time =    1171.18 ms /    39 tokens (   30.03 ms per token,    33.30 tokens per second)
      total time =    5162.74 ms /  2551 tokens
slot      release: id  1 | task 30167 | stop processing: n_tokens = 5058, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.746 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30209 | processing task, is_child = 0
slot update_slots: id  1 | task 30209 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6739
slot update_slots: id  1 | task 30209 | n_tokens = 5028, memory_seq_rm [5028, end)
slot update_slots: id  1 | task 30209 | prompt processing progress, n_tokens = 6675, batch.n_tokens = 1647, progress = 0.990503
slot update_slots: id  1 | task 30209 | n_tokens = 6675, memory_seq_rm [6675, end)
slot update_slots: id  1 | task 30209 | prompt processing progress, n_tokens = 6739, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30209 | prompt done, n_tokens = 6739, batch.n_tokens = 64
slot init_sampler: id  1 | task 30209 | init sampler, took 1.15 ms, tokens: text = 6739, total = 6739
slot update_slots: id  1 | task 30209 | created context checkpoint 5 of 8 (pos_min = 6032, pos_max = 6674, size = 15.078 MiB)
slot print_timing: id  1 | task 30209 | 
prompt eval time =    2960.03 ms /  1711 tokens (    1.73 ms per token,   578.04 tokens per second)
       eval time =    2996.60 ms /   100 tokens (   29.97 ms per token,    33.37 tokens per second)
      total time =    5956.62 ms /  1811 tokens
slot      release: id  1 | task 30209 | stop processing: n_tokens = 6838, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.105 (> 0.100 thold), f_keep = 0.103
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6838, total state size = 175.422 MiB
srv          load:  - looking for better prompt, base f_keep = 0.103, sim = 0.105
srv        update:  - cache state: 8 prompts, 3084.678 MiB (limits: 8192.000 MiB, 56064 tokens, 266186 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv  get_availabl: prompt cache update took 187.39 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30311 | processing task, is_child = 0
slot update_slots: id  1 | task 30311 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6732
slot update_slots: id  1 | task 30311 | n_past = 704, slot.prompt.tokens.size() = 6838, seq_id = 1, pos_min = 6195, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  1 | task 30311 | failed to restore context checkpoint (pos_min = 0, pos_max = 749, size = 17.587 MiB)
slot update_slots: id  1 | task 30311 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 30311 | erased invalidated context checkpoint (pos_min = 1793, pos_max = 2435, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 30311 | erased invalidated context checkpoint (pos_min = 4313, pos_max = 4955, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 30311 | erased invalidated context checkpoint (pos_min = 6032, pos_max = 6674, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 30311 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 30311 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.304219
slot update_slots: id  1 | task 30311 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 30311 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.608437
slot update_slots: id  1 | task 30311 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 30311 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.912656
slot update_slots: id  1 | task 30311 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 30311 | prompt processing progress, n_tokens = 6668, batch.n_tokens = 524, progress = 0.990493
slot update_slots: id  1 | task 30311 | n_tokens = 6668, memory_seq_rm [6668, end)
slot update_slots: id  1 | task 30311 | prompt processing progress, n_tokens = 6732, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30311 | prompt done, n_tokens = 6732, batch.n_tokens = 64
slot init_sampler: id  1 | task 30311 | init sampler, took 0.98 ms, tokens: text = 6732, total = 6732
slot update_slots: id  1 | task 30311 | created context checkpoint 3 of 8 (pos_min = 6025, pos_max = 6667, size = 15.078 MiB)
slot print_timing: id  1 | task 30311 | 
prompt eval time =    9519.14 ms /  6732 tokens (    1.41 ms per token,   707.21 tokens per second)
       eval time =    1095.66 ms /    40 tokens (   27.39 ms per token,    36.51 tokens per second)
      total time =   10614.80 ms /  6772 tokens
slot      release: id  1 | task 30311 | stop processing: n_tokens = 6771, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30356 | processing task, is_child = 0
slot update_slots: id  1 | task 30356 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6867
slot update_slots: id  1 | task 30356 | n_tokens = 6750, memory_seq_rm [6750, end)
slot update_slots: id  1 | task 30356 | prompt processing progress, n_tokens = 6803, batch.n_tokens = 53, progress = 0.990680
slot update_slots: id  1 | task 30356 | n_tokens = 6803, memory_seq_rm [6803, end)
slot update_slots: id  1 | task 30356 | prompt processing progress, n_tokens = 6867, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30356 | prompt done, n_tokens = 6867, batch.n_tokens = 64
slot init_sampler: id  1 | task 30356 | init sampler, took 0.98 ms, tokens: text = 6867, total = 6867
slot update_slots: id  1 | task 30356 | created context checkpoint 4 of 8 (pos_min = 6160, pos_max = 6802, size = 15.078 MiB)
slot print_timing: id  1 | task 30356 | 
prompt eval time =     387.69 ms /   117 tokens (    3.31 ms per token,   301.79 tokens per second)
       eval time =    1182.81 ms /    42 tokens (   28.16 ms per token,    35.51 tokens per second)
      total time =    1570.50 ms /   159 tokens
slot      release: id  1 | task 30356 | stop processing: n_tokens = 6908, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30400 | processing task, is_child = 0
slot update_slots: id  1 | task 30400 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6950
slot update_slots: id  1 | task 30400 | n_tokens = 6877, memory_seq_rm [6877, end)
slot update_slots: id  1 | task 30400 | prompt processing progress, n_tokens = 6886, batch.n_tokens = 9, progress = 0.990791
slot update_slots: id  1 | task 30400 | n_tokens = 6886, memory_seq_rm [6886, end)
slot update_slots: id  1 | task 30400 | prompt processing progress, n_tokens = 6950, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30400 | prompt done, n_tokens = 6950, batch.n_tokens = 64
slot init_sampler: id  1 | task 30400 | init sampler, took 1.01 ms, tokens: text = 6950, total = 6950
slot update_slots: id  1 | task 30400 | created context checkpoint 5 of 8 (pos_min = 6265, pos_max = 6885, size = 14.562 MiB)
slot print_timing: id  1 | task 30400 | 
prompt eval time =     312.56 ms /    73 tokens (    4.28 ms per token,   233.55 tokens per second)
       eval time =    1156.85 ms /    41 tokens (   28.22 ms per token,    35.44 tokens per second)
      total time =    1469.41 ms /   114 tokens
slot      release: id  1 | task 30400 | stop processing: n_tokens = 6990, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.938 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30443 | processing task, is_child = 0
slot update_slots: id  1 | task 30443 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7418
slot update_slots: id  1 | task 30443 | n_tokens = 6960, memory_seq_rm [6960, end)
slot update_slots: id  1 | task 30443 | prompt processing progress, n_tokens = 7354, batch.n_tokens = 394, progress = 0.991372
slot update_slots: id  1 | task 30443 | n_tokens = 7354, memory_seq_rm [7354, end)
slot update_slots: id  1 | task 30443 | prompt processing progress, n_tokens = 7418, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30443 | prompt done, n_tokens = 7418, batch.n_tokens = 64
slot init_sampler: id  1 | task 30443 | init sampler, took 1.40 ms, tokens: text = 7418, total = 7418
slot update_slots: id  1 | task 30443 | created context checkpoint 6 of 8 (pos_min = 6711, pos_max = 7353, size = 15.078 MiB)
slot print_timing: id  1 | task 30443 | 
prompt eval time =     852.28 ms /   458 tokens (    1.86 ms per token,   537.38 tokens per second)
       eval time =    1171.21 ms /    40 tokens (   29.28 ms per token,    34.15 tokens per second)
      total time =    2023.50 ms /   498 tokens
slot      release: id  1 | task 30443 | stop processing: n_tokens = 7457, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30485 | processing task, is_child = 0
slot update_slots: id  1 | task 30485 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7485
slot update_slots: id  1 | task 30485 | n_tokens = 7427, memory_seq_rm [7427, end)
slot update_slots: id  1 | task 30485 | prompt processing progress, n_tokens = 7485, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  1 | task 30485 | prompt done, n_tokens = 7485, batch.n_tokens = 58
slot init_sampler: id  1 | task 30485 | init sampler, took 1.76 ms, tokens: text = 7485, total = 7485
slot update_slots: id  1 | task 30485 | created context checkpoint 7 of 8 (pos_min = 6814, pos_max = 7426, size = 14.374 MiB)
slot print_timing: id  1 | task 30485 | 
prompt eval time =     210.64 ms /    58 tokens (    3.63 ms per token,   275.36 tokens per second)
       eval time =    1631.91 ms /    57 tokens (   28.63 ms per token,    34.93 tokens per second)
      total time =    1842.55 ms /   115 tokens
slot      release: id  1 | task 30485 | stop processing: n_tokens = 7541, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.943 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30543 | processing task, is_child = 0
slot update_slots: id  1 | task 30543 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7973
slot update_slots: id  1 | task 30543 | n_tokens = 7515, memory_seq_rm [7515, end)
slot update_slots: id  1 | task 30543 | prompt processing progress, n_tokens = 7909, batch.n_tokens = 394, progress = 0.991973
slot update_slots: id  1 | task 30543 | n_tokens = 7909, memory_seq_rm [7909, end)
slot update_slots: id  1 | task 30543 | prompt processing progress, n_tokens = 7973, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30543 | prompt done, n_tokens = 7973, batch.n_tokens = 64
slot init_sampler: id  1 | task 30543 | init sampler, took 1.21 ms, tokens: text = 7973, total = 7973
slot update_slots: id  1 | task 30543 | created context checkpoint 8 of 8 (pos_min = 7266, pos_max = 7908, size = 15.078 MiB)
slot print_timing: id  1 | task 30543 | 
prompt eval time =     843.07 ms /   458 tokens (    1.84 ms per token,   543.25 tokens per second)
       eval time =    1306.42 ms /    45 tokens (   29.03 ms per token,    34.45 tokens per second)
      total time =    2149.49 ms /   503 tokens
slot      release: id  1 | task 30543 | stop processing: n_tokens = 8017, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30590 | processing task, is_child = 0
slot update_slots: id  1 | task 30590 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8057
slot update_slots: id  1 | task 30590 | n_tokens = 7997, memory_seq_rm [7997, end)
slot update_slots: id  1 | task 30590 | prompt processing progress, n_tokens = 8057, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  1 | task 30590 | prompt done, n_tokens = 8057, batch.n_tokens = 60
slot init_sampler: id  1 | task 30590 | init sampler, took 1.14 ms, tokens: text = 8057, total = 8057
slot update_slots: id  1 | task 30590 | erasing old context checkpoint (pos_min = 0, pos_max = 641, size = 15.055 MiB)
slot update_slots: id  1 | task 30590 | created context checkpoint 8 of 8 (pos_min = 7374, pos_max = 7996, size = 14.609 MiB)
slot print_timing: id  1 | task 30590 | 
prompt eval time =     211.39 ms /    60 tokens (    3.52 ms per token,   283.83 tokens per second)
       eval time =    1838.03 ms /    63 tokens (   29.18 ms per token,    34.28 tokens per second)
      total time =    2049.42 ms /   123 tokens
slot      release: id  1 | task 30590 | stop processing: n_tokens = 8119, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30654 | processing task, is_child = 0
slot update_slots: id  1 | task 30654 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8263
slot update_slots: id  1 | task 30654 | n_tokens = 8093, memory_seq_rm [8093, end)
slot update_slots: id  1 | task 30654 | prompt processing progress, n_tokens = 8199, batch.n_tokens = 106, progress = 0.992255
slot update_slots: id  1 | task 30654 | n_tokens = 8199, memory_seq_rm [8199, end)
slot update_slots: id  1 | task 30654 | prompt processing progress, n_tokens = 8263, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30654 | prompt done, n_tokens = 8263, batch.n_tokens = 64
slot init_sampler: id  1 | task 30654 | init sampler, took 1.22 ms, tokens: text = 8263, total = 8263
slot update_slots: id  1 | task 30654 | erasing old context checkpoint (pos_min = 0, pos_max = 749, size = 17.587 MiB)
slot update_slots: id  1 | task 30654 | created context checkpoint 8 of 8 (pos_min = 7556, pos_max = 8198, size = 15.078 MiB)
slot print_timing: id  1 | task 30654 | 
prompt eval time =     591.43 ms /   170 tokens (    3.48 ms per token,   287.44 tokens per second)
       eval time =    1062.95 ms /    36 tokens (   29.53 ms per token,    33.87 tokens per second)
      total time =    1654.38 ms /   206 tokens
slot      release: id  1 | task 30654 | stop processing: n_tokens = 8298, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30692 | processing task, is_child = 0
slot update_slots: id  1 | task 30692 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8406
slot update_slots: id  1 | task 30692 | n_tokens = 8272, memory_seq_rm [8272, end)
slot update_slots: id  1 | task 30692 | prompt processing progress, n_tokens = 8342, batch.n_tokens = 70, progress = 0.992386
slot update_slots: id  1 | task 30692 | n_tokens = 8342, memory_seq_rm [8342, end)
slot update_slots: id  1 | task 30692 | prompt processing progress, n_tokens = 8406, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30692 | prompt done, n_tokens = 8406, batch.n_tokens = 64
slot init_sampler: id  1 | task 30692 | init sampler, took 1.20 ms, tokens: text = 8406, total = 8406
slot update_slots: id  1 | task 30692 | erasing old context checkpoint (pos_min = 6025, pos_max = 6667, size = 15.078 MiB)
slot update_slots: id  1 | task 30692 | created context checkpoint 8 of 8 (pos_min = 7699, pos_max = 8341, size = 15.078 MiB)
slot print_timing: id  1 | task 30692 | 
prompt eval time =     543.89 ms /   134 tokens (    4.06 ms per token,   246.37 tokens per second)
       eval time =    1117.76 ms /    38 tokens (   29.41 ms per token,    34.00 tokens per second)
      total time =    1661.65 ms /   172 tokens
slot      release: id  1 | task 30692 | stop processing: n_tokens = 8443, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30732 | processing task, is_child = 0
slot update_slots: id  1 | task 30732 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8737
slot update_slots: id  1 | task 30732 | n_tokens = 8416, memory_seq_rm [8416, end)
slot update_slots: id  1 | task 30732 | prompt processing progress, n_tokens = 8673, batch.n_tokens = 257, progress = 0.992675
slot update_slots: id  1 | task 30732 | n_tokens = 8673, memory_seq_rm [8673, end)
slot update_slots: id  1 | task 30732 | prompt processing progress, n_tokens = 8737, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30732 | prompt done, n_tokens = 8737, batch.n_tokens = 64
slot init_sampler: id  1 | task 30732 | init sampler, took 1.69 ms, tokens: text = 8737, total = 8737
slot update_slots: id  1 | task 30732 | erasing old context checkpoint (pos_min = 6160, pos_max = 6802, size = 15.078 MiB)
slot update_slots: id  1 | task 30732 | created context checkpoint 8 of 8 (pos_min = 8030, pos_max = 8672, size = 15.078 MiB)
slot print_timing: id  1 | task 30732 | 
prompt eval time =     750.82 ms /   321 tokens (    2.34 ms per token,   427.53 tokens per second)
       eval time =    1120.65 ms /    37 tokens (   30.29 ms per token,    33.02 tokens per second)
      total time =    1871.46 ms /   358 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 30732 | stop processing: n_tokens = 8773, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.959 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30771 | processing task, is_child = 0
slot update_slots: id  1 | task 30771 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9125
slot update_slots: id  1 | task 30771 | n_tokens = 8747, memory_seq_rm [8747, end)
slot update_slots: id  1 | task 30771 | prompt processing progress, n_tokens = 9061, batch.n_tokens = 314, progress = 0.992986
slot update_slots: id  1 | task 30771 | n_tokens = 9061, memory_seq_rm [9061, end)
slot update_slots: id  1 | task 30771 | prompt processing progress, n_tokens = 9125, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30771 | prompt done, n_tokens = 9125, batch.n_tokens = 64
slot init_sampler: id  1 | task 30771 | init sampler, took 1.69 ms, tokens: text = 9125, total = 9125
slot update_slots: id  1 | task 30771 | erasing old context checkpoint (pos_min = 6265, pos_max = 6885, size = 14.562 MiB)
slot update_slots: id  1 | task 30771 | created context checkpoint 8 of 8 (pos_min = 8418, pos_max = 9060, size = 15.078 MiB)
slot print_timing: id  1 | task 30771 | 
prompt eval time =     822.13 ms /   378 tokens (    2.17 ms per token,   459.78 tokens per second)
       eval time =    1119.42 ms /    37 tokens (   30.25 ms per token,    33.05 tokens per second)
      total time =    1941.55 ms /   415 tokens
slot      release: id  1 | task 30771 | stop processing: n_tokens = 9161, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30810 | processing task, is_child = 0
slot update_slots: id  1 | task 30810 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9577
slot update_slots: id  1 | task 30810 | n_tokens = 9135, memory_seq_rm [9135, end)
slot update_slots: id  1 | task 30810 | prompt processing progress, n_tokens = 9513, batch.n_tokens = 378, progress = 0.993317
slot update_slots: id  1 | task 30810 | n_tokens = 9513, memory_seq_rm [9513, end)
slot update_slots: id  1 | task 30810 | prompt processing progress, n_tokens = 9577, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30810 | prompt done, n_tokens = 9577, batch.n_tokens = 64
slot init_sampler: id  1 | task 30810 | init sampler, took 1.35 ms, tokens: text = 9577, total = 9577
slot update_slots: id  1 | task 30810 | erasing old context checkpoint (pos_min = 6711, pos_max = 7353, size = 15.078 MiB)
slot update_slots: id  1 | task 30810 | created context checkpoint 8 of 8 (pos_min = 8870, pos_max = 9512, size = 15.078 MiB)
slot print_timing: id  1 | task 30810 | 
prompt eval time =     909.88 ms /   442 tokens (    2.06 ms per token,   485.78 tokens per second)
       eval time =    1108.21 ms /    37 tokens (   29.95 ms per token,    33.39 tokens per second)
      total time =    2018.10 ms /   479 tokens
slot      release: id  1 | task 30810 | stop processing: n_tokens = 9613, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30849 | processing task, is_child = 0
slot update_slots: id  1 | task 30849 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10052
slot update_slots: id  1 | task 30849 | n_tokens = 9587, memory_seq_rm [9587, end)
slot update_slots: id  1 | task 30849 | prompt processing progress, n_tokens = 9988, batch.n_tokens = 401, progress = 0.993633
slot update_slots: id  1 | task 30849 | n_tokens = 9988, memory_seq_rm [9988, end)
slot update_slots: id  1 | task 30849 | prompt processing progress, n_tokens = 10052, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30849 | prompt done, n_tokens = 10052, batch.n_tokens = 64
slot init_sampler: id  1 | task 30849 | init sampler, took 1.99 ms, tokens: text = 10052, total = 10052
slot update_slots: id  1 | task 30849 | erasing old context checkpoint (pos_min = 6814, pos_max = 7426, size = 14.374 MiB)
slot update_slots: id  1 | task 30849 | created context checkpoint 8 of 8 (pos_min = 9345, pos_max = 9987, size = 15.078 MiB)
slot print_timing: id  1 | task 30849 | 
prompt eval time =     906.16 ms /   465 tokens (    1.95 ms per token,   513.15 tokens per second)
       eval time =    1078.67 ms /    36 tokens (   29.96 ms per token,    33.37 tokens per second)
      total time =    1984.84 ms /   501 tokens
slot      release: id  1 | task 30849 | stop processing: n_tokens = 10087, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30887 | processing task, is_child = 0
slot update_slots: id  1 | task 30887 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10257
slot update_slots: id  1 | task 30887 | n_tokens = 10061, memory_seq_rm [10061, end)
slot update_slots: id  1 | task 30887 | prompt processing progress, n_tokens = 10193, batch.n_tokens = 132, progress = 0.993760
slot update_slots: id  1 | task 30887 | n_tokens = 10193, memory_seq_rm [10193, end)
slot update_slots: id  1 | task 30887 | prompt processing progress, n_tokens = 10257, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30887 | prompt done, n_tokens = 10257, batch.n_tokens = 64
slot init_sampler: id  1 | task 30887 | init sampler, took 1.45 ms, tokens: text = 10257, total = 10257
slot update_slots: id  1 | task 30887 | erasing old context checkpoint (pos_min = 7266, pos_max = 7908, size = 15.078 MiB)
slot update_slots: id  1 | task 30887 | created context checkpoint 8 of 8 (pos_min = 9550, pos_max = 10192, size = 15.078 MiB)
slot print_timing: id  1 | task 30887 | 
prompt eval time =     635.01 ms /   196 tokens (    3.24 ms per token,   308.66 tokens per second)
       eval time =    1145.54 ms /    38 tokens (   30.15 ms per token,    33.17 tokens per second)
      total time =    1780.55 ms /   234 tokens
slot      release: id  1 | task 30887 | stop processing: n_tokens = 10294, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30927 | processing task, is_child = 0
slot update_slots: id  1 | task 30927 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10930
slot update_slots: id  1 | task 30927 | n_tokens = 10268, memory_seq_rm [10268, end)
slot update_slots: id  1 | task 30927 | prompt processing progress, n_tokens = 10866, batch.n_tokens = 598, progress = 0.994145
slot update_slots: id  1 | task 30927 | n_tokens = 10866, memory_seq_rm [10866, end)
slot update_slots: id  1 | task 30927 | prompt processing progress, n_tokens = 10930, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30927 | prompt done, n_tokens = 10930, batch.n_tokens = 64
slot init_sampler: id  1 | task 30927 | init sampler, took 1.56 ms, tokens: text = 10930, total = 10930
slot update_slots: id  1 | task 30927 | erasing old context checkpoint (pos_min = 7374, pos_max = 7996, size = 14.609 MiB)
slot update_slots: id  1 | task 30927 | created context checkpoint 8 of 8 (pos_min = 10350, pos_max = 10865, size = 12.100 MiB)
slot print_timing: id  1 | task 30927 | 
prompt eval time =    1433.20 ms /   662 tokens (    2.16 ms per token,   461.90 tokens per second)
       eval time =    1123.81 ms /    37 tokens (   30.37 ms per token,    32.92 tokens per second)
      total time =    2557.02 ms /   699 tokens
slot      release: id  1 | task 30927 | stop processing: n_tokens = 10966, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 30966 | processing task, is_child = 0
slot update_slots: id  1 | task 30966 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11207
slot update_slots: id  1 | task 30966 | n_tokens = 10939, memory_seq_rm [10939, end)
slot update_slots: id  1 | task 30966 | prompt processing progress, n_tokens = 11143, batch.n_tokens = 204, progress = 0.994289
slot update_slots: id  1 | task 30966 | n_tokens = 11143, memory_seq_rm [11143, end)
slot update_slots: id  1 | task 30966 | prompt processing progress, n_tokens = 11207, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 30966 | prompt done, n_tokens = 11207, batch.n_tokens = 64
slot init_sampler: id  1 | task 30966 | init sampler, took 2.15 ms, tokens: text = 11207, total = 11207
slot update_slots: id  1 | task 30966 | erasing old context checkpoint (pos_min = 7556, pos_max = 8198, size = 15.078 MiB)
slot update_slots: id  1 | task 30966 | created context checkpoint 8 of 8 (pos_min = 10627, pos_max = 11142, size = 12.100 MiB)
slot print_timing: id  1 | task 30966 | 
prompt eval time =     695.13 ms /   268 tokens (    2.59 ms per token,   385.54 tokens per second)
       eval time =    1149.31 ms /    38 tokens (   30.25 ms per token,    33.06 tokens per second)
      total time =    1844.44 ms /   306 tokens
slot      release: id  1 | task 30966 | stop processing: n_tokens = 11244, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31006 | processing task, is_child = 0
slot update_slots: id  1 | task 31006 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11338
slot update_slots: id  1 | task 31006 | n_tokens = 11218, memory_seq_rm [11218, end)
slot update_slots: id  1 | task 31006 | prompt processing progress, n_tokens = 11274, batch.n_tokens = 56, progress = 0.994355
slot update_slots: id  1 | task 31006 | n_tokens = 11274, memory_seq_rm [11274, end)
slot update_slots: id  1 | task 31006 | prompt processing progress, n_tokens = 11338, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31006 | prompt done, n_tokens = 11338, batch.n_tokens = 64
slot init_sampler: id  1 | task 31006 | init sampler, took 1.62 ms, tokens: text = 11338, total = 11338
slot update_slots: id  1 | task 31006 | erasing old context checkpoint (pos_min = 7699, pos_max = 8341, size = 15.078 MiB)
slot update_slots: id  1 | task 31006 | created context checkpoint 8 of 8 (pos_min = 10758, pos_max = 11273, size = 12.100 MiB)
slot print_timing: id  1 | task 31006 | 
prompt eval time =     479.79 ms /   120 tokens (    4.00 ms per token,   250.11 tokens per second)
       eval time =    1064.85 ms /    36 tokens (   29.58 ms per token,    33.81 tokens per second)
      total time =    1544.63 ms /   156 tokens
slot      release: id  1 | task 31006 | stop processing: n_tokens = 11373, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.881 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31044 | processing task, is_child = 0
slot update_slots: id  1 | task 31044 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12881
slot update_slots: id  1 | task 31044 | n_tokens = 11348, memory_seq_rm [11348, end)
slot update_slots: id  1 | task 31044 | prompt processing progress, n_tokens = 12817, batch.n_tokens = 1469, progress = 0.995031
slot update_slots: id  1 | task 31044 | n_tokens = 12817, memory_seq_rm [12817, end)
slot update_slots: id  1 | task 31044 | prompt processing progress, n_tokens = 12881, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31044 | prompt done, n_tokens = 12881, batch.n_tokens = 64
slot init_sampler: id  1 | task 31044 | init sampler, took 1.81 ms, tokens: text = 12881, total = 12881
slot update_slots: id  1 | task 31044 | erasing old context checkpoint (pos_min = 8030, pos_max = 8672, size = 15.078 MiB)
slot update_slots: id  1 | task 31044 | created context checkpoint 8 of 8 (pos_min = 12174, pos_max = 12816, size = 15.078 MiB)
slot print_timing: id  1 | task 31044 | 
prompt eval time =    2559.44 ms /  1533 tokens (    1.67 ms per token,   598.96 tokens per second)
       eval time =    1075.42 ms /    37 tokens (   29.07 ms per token,    34.41 tokens per second)
      total time =    3634.85 ms /  1570 tokens
slot      release: id  1 | task 31044 | stop processing: n_tokens = 12917, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31083 | processing task, is_child = 0
slot update_slots: id  1 | task 31083 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12949
slot update_slots: id  1 | task 31083 | n_tokens = 12892, memory_seq_rm [12892, end)
slot update_slots: id  1 | task 31083 | prompt processing progress, n_tokens = 12949, batch.n_tokens = 57, progress = 1.000000
slot update_slots: id  1 | task 31083 | prompt done, n_tokens = 12949, batch.n_tokens = 57
slot init_sampler: id  1 | task 31083 | init sampler, took 1.92 ms, tokens: text = 12949, total = 12949
slot update_slots: id  1 | task 31083 | erasing old context checkpoint (pos_min = 8418, pos_max = 9060, size = 15.078 MiB)
slot update_slots: id  1 | task 31083 | created context checkpoint 8 of 8 (pos_min = 12274, pos_max = 12891, size = 14.492 MiB)
slot print_timing: id  1 | task 31083 | 
prompt eval time =     202.20 ms /    57 tokens (    3.55 ms per token,   281.90 tokens per second)
       eval time =    1013.70 ms /    36 tokens (   28.16 ms per token,    35.51 tokens per second)
      total time =    1215.90 ms /    93 tokens
slot      release: id  1 | task 31083 | stop processing: n_tokens = 12984, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.884 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31120 | processing task, is_child = 0
slot update_slots: id  1 | task 31120 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14656
slot update_slots: id  1 | task 31120 | n_tokens = 12958, memory_seq_rm [12958, end)
slot update_slots: id  1 | task 31120 | prompt processing progress, n_tokens = 14592, batch.n_tokens = 1634, progress = 0.995633
slot update_slots: id  1 | task 31120 | n_tokens = 14592, memory_seq_rm [14592, end)
slot update_slots: id  1 | task 31120 | prompt processing progress, n_tokens = 14656, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31120 | prompt done, n_tokens = 14656, batch.n_tokens = 64
slot init_sampler: id  1 | task 31120 | init sampler, took 2.77 ms, tokens: text = 14656, total = 14656
slot update_slots: id  1 | task 31120 | erasing old context checkpoint (pos_min = 8870, pos_max = 9512, size = 15.078 MiB)
slot update_slots: id  1 | task 31120 | created context checkpoint 8 of 8 (pos_min = 13949, pos_max = 14591, size = 15.078 MiB)
slot print_timing: id  1 | task 31120 | 
prompt eval time =    2988.41 ms /  1698 tokens (    1.76 ms per token,   568.19 tokens per second)
       eval time =    1032.80 ms /    35 tokens (   29.51 ms per token,    33.89 tokens per second)
      total time =    4021.21 ms /  1733 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 31120 | stop processing: n_tokens = 14690, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31157 | processing task, is_child = 0
slot update_slots: id  1 | task 31157 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15238
slot update_slots: id  1 | task 31157 | n_tokens = 14665, memory_seq_rm [14665, end)
slot update_slots: id  1 | task 31157 | prompt processing progress, n_tokens = 15174, batch.n_tokens = 509, progress = 0.995800
slot update_slots: id  1 | task 31157 | n_tokens = 15174, memory_seq_rm [15174, end)
slot update_slots: id  1 | task 31157 | prompt processing progress, n_tokens = 15238, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31157 | prompt done, n_tokens = 15238, batch.n_tokens = 64
slot init_sampler: id  1 | task 31157 | init sampler, took 3.03 ms, tokens: text = 15238, total = 15238
slot update_slots: id  1 | task 31157 | erasing old context checkpoint (pos_min = 9345, pos_max = 9987, size = 15.078 MiB)
slot update_slots: id  1 | task 31157 | created context checkpoint 8 of 8 (pos_min = 14531, pos_max = 15173, size = 15.078 MiB)
slot print_timing: id  1 | task 31157 | 
prompt eval time =    1050.22 ms /   573 tokens (    1.83 ms per token,   545.60 tokens per second)
       eval time =    1091.35 ms /    38 tokens (   28.72 ms per token,    34.82 tokens per second)
      total time =    2141.57 ms /   611 tokens
slot      release: id  1 | task 31157 | stop processing: n_tokens = 15275, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31197 | processing task, is_child = 0
slot update_slots: id  1 | task 31197 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15981
slot update_slots: id  1 | task 31197 | n_tokens = 15248, memory_seq_rm [15248, end)
slot update_slots: id  1 | task 31197 | prompt processing progress, n_tokens = 15917, batch.n_tokens = 669, progress = 0.995995
slot update_slots: id  1 | task 31197 | n_tokens = 15917, memory_seq_rm [15917, end)
slot update_slots: id  1 | task 31197 | prompt processing progress, n_tokens = 15981, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31197 | prompt done, n_tokens = 15981, batch.n_tokens = 64
slot init_sampler: id  1 | task 31197 | init sampler, took 2.24 ms, tokens: text = 15981, total = 15981
slot update_slots: id  1 | task 31197 | erasing old context checkpoint (pos_min = 9550, pos_max = 10192, size = 15.078 MiB)
slot update_slots: id  1 | task 31197 | created context checkpoint 8 of 8 (pos_min = 15274, pos_max = 15916, size = 15.078 MiB)
slot print_timing: id  1 | task 31197 | 
prompt eval time =    1415.22 ms /   733 tokens (    1.93 ms per token,   517.94 tokens per second)
       eval time =     977.45 ms /    34 tokens (   28.75 ms per token,    34.78 tokens per second)
      total time =    2392.67 ms /   767 tokens
slot      release: id  1 | task 31197 | stop processing: n_tokens = 16014, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31233 | processing task, is_child = 0
slot update_slots: id  1 | task 31233 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16471
slot update_slots: id  1 | task 31233 | n_tokens = 15990, memory_seq_rm [15990, end)
slot update_slots: id  1 | task 31233 | prompt processing progress, n_tokens = 16407, batch.n_tokens = 417, progress = 0.996114
slot update_slots: id  1 | task 31233 | n_tokens = 16407, memory_seq_rm [16407, end)
slot update_slots: id  1 | task 31233 | prompt processing progress, n_tokens = 16471, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31233 | prompt done, n_tokens = 16471, batch.n_tokens = 64
slot init_sampler: id  1 | task 31233 | init sampler, took 2.32 ms, tokens: text = 16471, total = 16471
slot update_slots: id  1 | task 31233 | erasing old context checkpoint (pos_min = 10350, pos_max = 10865, size = 12.100 MiB)
slot update_slots: id  1 | task 31233 | created context checkpoint 8 of 8 (pos_min = 15764, pos_max = 16406, size = 15.078 MiB)
slot print_timing: id  1 | task 31233 | 
prompt eval time =     912.25 ms /   481 tokens (    1.90 ms per token,   527.27 tokens per second)
       eval time =     953.40 ms /    34 tokens (   28.04 ms per token,    35.66 tokens per second)
      total time =    1865.65 ms /   515 tokens
slot      release: id  1 | task 31233 | stop processing: n_tokens = 16504, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31269 | processing task, is_child = 0
slot update_slots: id  1 | task 31269 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16551
slot update_slots: id  1 | task 31269 | n_tokens = 16480, memory_seq_rm [16480, end)
slot update_slots: id  1 | task 31269 | prompt processing progress, n_tokens = 16487, batch.n_tokens = 7, progress = 0.996133
slot update_slots: id  1 | task 31269 | n_tokens = 16487, memory_seq_rm [16487, end)
slot update_slots: id  1 | task 31269 | prompt processing progress, n_tokens = 16551, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 31269 | prompt done, n_tokens = 16551, batch.n_tokens = 64
slot init_sampler: id  1 | task 31269 | init sampler, took 2.33 ms, tokens: text = 16551, total = 16551
slot update_slots: id  1 | task 31269 | erasing old context checkpoint (pos_min = 10627, pos_max = 11142, size = 12.100 MiB)
slot update_slots: id  1 | task 31269 | created context checkpoint 8 of 8 (pos_min = 15861, pos_max = 16486, size = 14.679 MiB)
slot print_timing: id  1 | task 31269 | 
prompt eval time =     309.90 ms /    71 tokens (    4.36 ms per token,   229.11 tokens per second)
       eval time =    3198.38 ms /   111 tokens (   28.81 ms per token,    34.71 tokens per second)
      total time =    3508.28 ms /   182 tokens
slot      release: id  1 | task 31269 | stop processing: n_tokens = 16661, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.413 (> 0.100 thold), f_keep = 0.404
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 16661, total state size = 405.761 MiB
srv          load:  - looking for better prompt, base f_keep = 0.404, sim = 0.413
srv        update:  - cache state: 9 prompts, 3607.100 MiB (limits: 8192.000 MiB, 56064 tokens, 265472 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv  get_availabl: prompt cache update took 393.49 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 31382 | processing task, is_child = 0
slot update_slots: id  1 | task 31382 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16293
slot update_slots: id  1 | task 31382 | n_past = 6732, slot.prompt.tokens.size() = 16661, seq_id = 1, pos_min = 16018, n_swa = 128
slot update_slots: id  1 | task 31382 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 10758, pos_max = 11273, n_swa = 128, size = 12.100 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 12174, pos_max = 12816, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 12274, pos_max = 12891, n_swa = 128, size = 14.492 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 13949, pos_max = 14591, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 14531, pos_max = 15173, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 15274, pos_max = 15916, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 15764, pos_max = 16406, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 31382 | erased invalidated context checkpoint (pos_min = 15861, pos_max = 16486, n_swa = 128, size = 14.679 MiB)
slot update_slots: id  1 | task 31382 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 31382 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.125698
slot update_slots: id  1 | task 31382 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 31382 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.251396
slot update_slots: id  1 | task 31382 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 31382 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.377094
srv          stop: cancel task, id_task = 31382
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 31382 | stop processing: n_tokens = 6144, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.730
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31387 | processing task, is_child = 0
slot update_slots: id  0 | task 31387 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 725
slot update_slots: id  0 | task 31387 | n_past = 677, slot.prompt.tokens.size() = 928, seq_id = 0, pos_min = 801, n_swa = 128
slot update_slots: id  0 | task 31387 | restored context checkpoint (pos_min = 222, pos_max = 847, size = 14.679 MiB)
slot update_slots: id  0 | task 31387 | n_tokens = 677, memory_seq_rm [677, end)
slot update_slots: id  0 | task 31387 | prompt processing progress, n_tokens = 725, batch.n_tokens = 48, progress = 1.000000
slot update_slots: id  0 | task 31387 | prompt done, n_tokens = 725, batch.n_tokens = 48
slot init_sampler: id  0 | task 31387 | init sampler, took 0.16 ms, tokens: text = 725, total = 725
slot print_timing: id  0 | task 31387 | 
prompt eval time =     390.74 ms /    48 tokens (    8.14 ms per token,   122.84 tokens per second)
       eval time =    1121.95 ms /    41 tokens (   27.36 ms per token,    36.54 tokens per second)
      total time =    1512.69 ms /    89 tokens
slot      release: id  0 | task 31387 | stop processing: n_tokens = 765, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.487 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31429 | processing task, is_child = 0
slot update_slots: id  0 | task 31429 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1523
slot update_slots: id  0 | task 31429 | n_tokens = 741, memory_seq_rm [741, end)
slot update_slots: id  0 | task 31429 | prompt processing progress, n_tokens = 1459, batch.n_tokens = 718, progress = 0.957978
slot update_slots: id  0 | task 31429 | n_tokens = 1459, memory_seq_rm [1459, end)
slot update_slots: id  0 | task 31429 | prompt processing progress, n_tokens = 1523, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31429 | prompt done, n_tokens = 1523, batch.n_tokens = 64
slot init_sampler: id  0 | task 31429 | init sampler, took 0.22 ms, tokens: text = 1523, total = 1523
slot update_slots: id  0 | task 31429 | created context checkpoint 3 of 8 (pos_min = 816, pos_max = 1458, size = 15.078 MiB)
slot print_timing: id  0 | task 31429 | 
prompt eval time =    1339.32 ms /   782 tokens (    1.71 ms per token,   583.88 tokens per second)
       eval time =    1560.99 ms /    55 tokens (   28.38 ms per token,    35.23 tokens per second)
      total time =    2900.32 ms /   837 tokens
slot      release: id  0 | task 31429 | stop processing: n_tokens = 1577, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.472 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31486 | processing task, is_child = 0
slot update_slots: id  0 | task 31486 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3283
slot update_slots: id  0 | task 31486 | n_tokens = 1551, memory_seq_rm [1551, end)
slot update_slots: id  0 | task 31486 | prompt processing progress, n_tokens = 3219, batch.n_tokens = 1668, progress = 0.980506
slot update_slots: id  0 | task 31486 | n_tokens = 3219, memory_seq_rm [3219, end)
slot update_slots: id  0 | task 31486 | prompt processing progress, n_tokens = 3283, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31486 | prompt done, n_tokens = 3283, batch.n_tokens = 64
slot init_sampler: id  0 | task 31486 | init sampler, took 0.49 ms, tokens: text = 3283, total = 3283
slot update_slots: id  0 | task 31486 | created context checkpoint 4 of 8 (pos_min = 2576, pos_max = 3218, size = 15.078 MiB)
slot print_timing: id  0 | task 31486 | 
prompt eval time =    2817.74 ms /  1732 tokens (    1.63 ms per token,   614.68 tokens per second)
       eval time =    1133.94 ms /    39 tokens (   29.08 ms per token,    34.39 tokens per second)
      total time =    3951.67 ms /  1771 tokens
slot      release: id  0 | task 31486 | stop processing: n_tokens = 3321, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.567 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31527 | processing task, is_child = 0
slot update_slots: id  0 | task 31527 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5803
slot update_slots: id  0 | task 31527 | n_tokens = 3291, memory_seq_rm [3291, end)
slot update_slots: id  0 | task 31527 | prompt processing progress, n_tokens = 5339, batch.n_tokens = 2048, progress = 0.920041
slot update_slots: id  0 | task 31527 | n_tokens = 5339, memory_seq_rm [5339, end)
slot update_slots: id  0 | task 31527 | prompt processing progress, n_tokens = 5739, batch.n_tokens = 400, progress = 0.988971
slot update_slots: id  0 | task 31527 | n_tokens = 5739, memory_seq_rm [5739, end)
slot update_slots: id  0 | task 31527 | prompt processing progress, n_tokens = 5803, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31527 | prompt done, n_tokens = 5803, batch.n_tokens = 64
slot init_sampler: id  0 | task 31527 | init sampler, took 1.09 ms, tokens: text = 5803, total = 5803
slot update_slots: id  0 | task 31527 | created context checkpoint 5 of 8 (pos_min = 5096, pos_max = 5738, size = 15.078 MiB)
slot print_timing: id  0 | task 31527 | 
prompt eval time =    4023.13 ms /  2512 tokens (    1.60 ms per token,   624.39 tokens per second)
       eval time =    1076.63 ms /    35 tokens (   30.76 ms per token,    32.51 tokens per second)
      total time =    5099.76 ms /  2547 tokens
slot      release: id  0 | task 31527 | stop processing: n_tokens = 5837, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.773 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31565 | processing task, is_child = 0
slot update_slots: id  0 | task 31565 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7522
slot update_slots: id  0 | task 31565 | n_tokens = 5811, memory_seq_rm [5811, end)
slot update_slots: id  0 | task 31565 | prompt processing progress, n_tokens = 7458, batch.n_tokens = 1647, progress = 0.991492
slot update_slots: id  0 | task 31565 | n_tokens = 7458, memory_seq_rm [7458, end)
slot update_slots: id  0 | task 31565 | prompt processing progress, n_tokens = 7522, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31565 | prompt done, n_tokens = 7522, batch.n_tokens = 64
slot init_sampler: id  0 | task 31565 | init sampler, took 1.07 ms, tokens: text = 7522, total = 7522
slot update_slots: id  0 | task 31565 | created context checkpoint 6 of 8 (pos_min = 6815, pos_max = 7457, size = 15.078 MiB)
slot print_timing: id  0 | task 31565 | 
prompt eval time =    3022.88 ms /  1711 tokens (    1.77 ms per token,   566.02 tokens per second)
       eval time =    3374.93 ms /   111 tokens (   30.40 ms per token,    32.89 tokens per second)
      total time =    6397.81 ms /  1822 tokens
slot      release: id  0 | task 31565 | stop processing: n_tokens = 7632, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.943 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31678 | processing task, is_child = 0
slot update_slots: id  0 | task 31678 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8064
slot update_slots: id  0 | task 31678 | n_tokens = 7606, memory_seq_rm [7606, end)
slot update_slots: id  0 | task 31678 | prompt processing progress, n_tokens = 8000, batch.n_tokens = 394, progress = 0.992063
slot update_slots: id  0 | task 31678 | n_tokens = 8000, memory_seq_rm [8000, end)
slot update_slots: id  0 | task 31678 | prompt processing progress, n_tokens = 8064, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31678 | prompt done, n_tokens = 8064, batch.n_tokens = 64
slot init_sampler: id  0 | task 31678 | init sampler, took 1.25 ms, tokens: text = 8064, total = 8064
slot update_slots: id  0 | task 31678 | created context checkpoint 7 of 8 (pos_min = 7357, pos_max = 7999, size = 15.078 MiB)
slot print_timing: id  0 | task 31678 | 
prompt eval time =     943.49 ms /   458 tokens (    2.06 ms per token,   485.43 tokens per second)
       eval time =    1229.29 ms /    40 tokens (   30.73 ms per token,    32.54 tokens per second)
      total time =    2172.79 ms /   498 tokens
slot      release: id  0 | task 31678 | stop processing: n_tokens = 8103, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31720 | processing task, is_child = 0
slot update_slots: id  0 | task 31720 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8131
slot update_slots: id  0 | task 31720 | n_tokens = 8073, memory_seq_rm [8073, end)
slot update_slots: id  0 | task 31720 | prompt processing progress, n_tokens = 8131, batch.n_tokens = 58, progress = 1.000000
slot update_slots: id  0 | task 31720 | prompt done, n_tokens = 8131, batch.n_tokens = 58
slot init_sampler: id  0 | task 31720 | init sampler, took 1.16 ms, tokens: text = 8131, total = 8131
slot update_slots: id  0 | task 31720 | created context checkpoint 8 of 8 (pos_min = 7460, pos_max = 8072, size = 14.374 MiB)
slot print_timing: id  0 | task 31720 | 
prompt eval time =     218.80 ms /    58 tokens (    3.77 ms per token,   265.08 tokens per second)
       eval time =    1598.64 ms /    52 tokens (   30.74 ms per token,    32.53 tokens per second)
      total time =    1817.44 ms /   110 tokens
slot      release: id  0 | task 31720 | stop processing: n_tokens = 8182, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31773 | processing task, is_child = 0
slot update_slots: id  0 | task 31773 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8614
slot update_slots: id  0 | task 31773 | n_tokens = 8156, memory_seq_rm [8156, end)
slot update_slots: id  0 | task 31773 | prompt processing progress, n_tokens = 8550, batch.n_tokens = 394, progress = 0.992570
slot update_slots: id  0 | task 31773 | n_tokens = 8550, memory_seq_rm [8550, end)
slot update_slots: id  0 | task 31773 | prompt processing progress, n_tokens = 8614, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31773 | prompt done, n_tokens = 8614, batch.n_tokens = 64
slot init_sampler: id  0 | task 31773 | init sampler, took 1.63 ms, tokens: text = 8614, total = 8614
slot update_slots: id  0 | task 31773 | erasing old context checkpoint (pos_min = 0, pos_max = 619, size = 14.539 MiB)
slot update_slots: id  0 | task 31773 | created context checkpoint 8 of 8 (pos_min = 7907, pos_max = 8549, size = 15.078 MiB)
slot print_timing: id  0 | task 31773 | 
prompt eval time =     944.28 ms /   458 tokens (    2.06 ms per token,   485.02 tokens per second)
       eval time =    1843.76 ms /    61 tokens (   30.23 ms per token,    33.08 tokens per second)
      total time =    2788.05 ms /   519 tokens
slot      release: id  0 | task 31773 | stop processing: n_tokens = 8674, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31836 | processing task, is_child = 0
slot update_slots: id  0 | task 31836 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8841
slot update_slots: id  0 | task 31836 | n_tokens = 8645, memory_seq_rm [8645, end)
slot update_slots: id  0 | task 31836 | prompt processing progress, n_tokens = 8777, batch.n_tokens = 132, progress = 0.992761
slot update_slots: id  0 | task 31836 | n_tokens = 8777, memory_seq_rm [8777, end)
slot update_slots: id  0 | task 31836 | prompt processing progress, n_tokens = 8841, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31836 | prompt done, n_tokens = 8841, batch.n_tokens = 64
slot init_sampler: id  0 | task 31836 | init sampler, took 1.25 ms, tokens: text = 8841, total = 8841
slot update_slots: id  0 | task 31836 | erasing old context checkpoint (pos_min = 222, pos_max = 847, size = 14.679 MiB)
slot update_slots: id  0 | task 31836 | created context checkpoint 8 of 8 (pos_min = 8134, pos_max = 8776, size = 15.078 MiB)
slot print_timing: id  0 | task 31836 | 
prompt eval time =     650.18 ms /   196 tokens (    3.32 ms per token,   301.45 tokens per second)
       eval time =    2353.68 ms /    78 tokens (   30.18 ms per token,    33.14 tokens per second)
      total time =    3003.86 ms /   274 tokens
slot      release: id  0 | task 31836 | stop processing: n_tokens = 8918, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = 1227256082
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9923, total state size = 235.662 MiB
srv          load:  - looking for better prompt, base f_keep = 0.068, sim = 0.078
srv        update:  - cache state: 10 prompts, 3984.373 MiB (limits: 8192.000 MiB, 56064 tokens, 260737 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv  get_availabl: prompt cache update took 282.19 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 31916 | processing task, is_child = 0
slot update_slots: id  3 | task 31916 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8667
slot update_slots: id  3 | task 31916 | n_past = 677, slot.prompt.tokens.size() = 9923, seq_id = 3, pos_min = 9796, n_swa = 128
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
slot update_slots: id  3 | task 31916 | failed to restore context checkpoint (pos_min = 0, pos_max = 913, size = 21.433 MiB)
slot update_slots: id  3 | task 31916 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 31916 | erased invalidated context checkpoint (pos_min = 677, pos_max = 1397, n_swa = 128, size = 16.907 MiB)
slot update_slots: id  3 | task 31916 | erased invalidated context checkpoint (pos_min = 2140, pos_max = 3163, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 31916 | erased invalidated context checkpoint (pos_min = 4664, pos_max = 5687, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 31916 | erased invalidated context checkpoint (pos_min = 6378, pos_max = 7401, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 31916 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 31916 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.236299
slot update_slots: id  3 | task 31916 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 31916 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.472597
srv          stop: cancel task, id_task = 31916
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 31916 | stop processing: n_tokens = 4096, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = 2421353638
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8011, total state size = 190.828 MiB
srv          load:  - looking for better prompt, base f_keep = 0.011, sim = 0.099
srv        update:  - cache state: 11 prompts, 4294.815 MiB (limits: 8192.000 MiB, 56064 tokens, 257171 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv  get_availabl: prompt cache update took 227.91 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 31920 | processing task, is_child = 0
slot update_slots: id  2 | task 31920 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 869
slot update_slots: id  2 | task 31920 | n_past = 86, slot.prompt.tokens.size() = 8011, seq_id = 2, pos_min = 7884, n_swa = 128
slot update_slots: id  2 | task 31920 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 31920 | erased invalidated context checkpoint (pos_min = 492, pos_max = 1388, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 31920 | erased invalidated context checkpoint (pos_min = 2247, pos_max = 3143, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 31920 | erased invalidated context checkpoint (pos_min = 4738, pos_max = 5634, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 31920 | erased invalidated context checkpoint (pos_min = 6438, pos_max = 7334, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 31920 | erased invalidated context checkpoint (pos_min = 6839, pos_max = 7711, n_swa = 128, size = 20.471 MiB)
slot update_slots: id  2 | task 31920 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 31920 | prompt processing progress, n_tokens = 805, batch.n_tokens = 805, progress = 0.926352
slot update_slots: id  2 | task 31920 | n_tokens = 805, memory_seq_rm [805, end)
slot update_slots: id  2 | task 31920 | prompt processing progress, n_tokens = 869, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 31920 | prompt done, n_tokens = 869, batch.n_tokens = 64
slot init_sampler: id  2 | task 31920 | init sampler, took 0.13 ms, tokens: text = 869, total = 869
slot update_slots: id  2 | task 31920 | created context checkpoint 2 of 8 (pos_min = 162, pos_max = 804, size = 15.078 MiB)
slot print_timing: id  2 | task 31920 | 
prompt eval time =    1585.77 ms /   869 tokens (    1.82 ms per token,   548.00 tokens per second)
       eval time =    2383.85 ms /    82 tokens (   29.07 ms per token,    34.40 tokens per second)
      total time =    3969.61 ms /   951 tokens
slot      release: id  2 | task 31920 | stop processing: n_tokens = 950, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.952 (> 0.100 thold), f_keep = 0.863
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 32004 | processing task, is_child = 0
slot update_slots: id  2 | task 32004 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 861
slot update_slots: id  2 | task 32004 | n_tokens = 820, memory_seq_rm [820, end)
slot update_slots: id  2 | task 32004 | prompt processing progress, n_tokens = 861, batch.n_tokens = 41, progress = 1.000000
slot update_slots: id  2 | task 32004 | prompt done, n_tokens = 861, batch.n_tokens = 41
slot init_sampler: id  2 | task 32004 | init sampler, took 0.13 ms, tokens: text = 861, total = 861
slot print_timing: id  2 | task 32004 | 
prompt eval time =     246.43 ms /    41 tokens (    6.01 ms per token,   166.38 tokens per second)
       eval time =    1615.82 ms /    57 tokens (   28.35 ms per token,    35.28 tokens per second)
      total time =    1862.25 ms /    98 tokens
slot      release: id  2 | task 32004 | stop processing: n_tokens = 917, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.445 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 32062 | processing task, is_child = 0
slot update_slots: id  2 | task 32062 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2017
slot update_slots: id  2 | task 32062 | n_tokens = 898, memory_seq_rm [898, end)
slot update_slots: id  2 | task 32062 | prompt processing progress, n_tokens = 1953, batch.n_tokens = 1055, progress = 0.968270
slot update_slots: id  2 | task 32062 | n_tokens = 1953, memory_seq_rm [1953, end)
slot update_slots: id  2 | task 32062 | prompt processing progress, n_tokens = 2017, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 32062 | prompt done, n_tokens = 2017, batch.n_tokens = 64
slot init_sampler: id  2 | task 32062 | init sampler, took 0.41 ms, tokens: text = 2017, total = 2017
slot update_slots: id  2 | task 32062 | created context checkpoint 3 of 8 (pos_min = 1310, pos_max = 1952, size = 15.078 MiB)
slot print_timing: id  2 | task 32062 | 
prompt eval time =    1938.14 ms /  1119 tokens (    1.73 ms per token,   577.36 tokens per second)
       eval time =    1280.14 ms /    40 tokens (   32.00 ms per token,    31.25 tokens per second)
      total time =    3218.28 ms /  1159 tokens
slot      release: id  2 | task 32062 | stop processing: n_tokens = 2056, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.670 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 32104 | processing task, is_child = 0
slot update_slots: id  2 | task 32104 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3032
slot update_slots: id  2 | task 32104 | n_tokens = 2030, memory_seq_rm [2030, end)
slot update_slots: id  2 | task 32104 | prompt processing progress, n_tokens = 2968, batch.n_tokens = 938, progress = 0.978892
slot update_slots: id  2 | task 32104 | n_tokens = 2968, memory_seq_rm [2968, end)
slot update_slots: id  2 | task 32104 | prompt processing progress, n_tokens = 3032, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 32104 | prompt done, n_tokens = 3032, batch.n_tokens = 64
slot init_sampler: id  2 | task 32104 | init sampler, took 0.45 ms, tokens: text = 3032, total = 3032
slot update_slots: id  2 | task 32104 | created context checkpoint 4 of 8 (pos_min = 2325, pos_max = 2967, size = 15.078 MiB)
slot print_timing: id  2 | task 32104 | 
prompt eval time =    1760.98 ms /  1002 tokens (    1.76 ms per token,   569.00 tokens per second)
       eval time =    1419.17 ms /    47 tokens (   30.20 ms per token,    33.12 tokens per second)
      total time =    3180.15 ms /  1049 tokens
slot      release: id  2 | task 32104 | stop processing: n_tokens = 3078, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.611 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 32153 | processing task, is_child = 0
slot update_slots: id  2 | task 32153 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4988
slot update_slots: id  2 | task 32153 | n_tokens = 3047, memory_seq_rm [3047, end)
slot update_slots: id  2 | task 32153 | prompt processing progress, n_tokens = 4924, batch.n_tokens = 1877, progress = 0.987169
slot update_slots: id  2 | task 32153 | n_tokens = 4924, memory_seq_rm [4924, end)
slot update_slots: id  2 | task 32153 | prompt processing progress, n_tokens = 4988, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 32153 | prompt done, n_tokens = 4988, batch.n_tokens = 64
slot init_sampler: id  2 | task 32153 | init sampler, took 0.72 ms, tokens: text = 4988, total = 4988
slot update_slots: id  2 | task 32153 | created context checkpoint 5 of 8 (pos_min = 4281, pos_max = 4923, size = 15.078 MiB)
slot print_timing: id  2 | task 32153 | 
prompt eval time =    3354.61 ms /  1941 tokens (    1.73 ms per token,   578.61 tokens per second)
       eval time =    6026.12 ms /   196 tokens (   30.75 ms per token,    32.53 tokens per second)
      total time =    9380.74 ms /  2137 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 32153 | stop processing: n_tokens = 5183, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 32351 | processing task, is_child = 0
slot update_slots: id  2 | task 32351 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5495
slot update_slots: id  2 | task 32351 | n_tokens = 5158, memory_seq_rm [5158, end)
slot update_slots: id  2 | task 32351 | prompt processing progress, n_tokens = 5431, batch.n_tokens = 273, progress = 0.988353
slot update_slots: id  2 | task 32351 | n_tokens = 5431, memory_seq_rm [5431, end)
slot update_slots: id  2 | task 32351 | prompt processing progress, n_tokens = 5495, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 32351 | prompt done, n_tokens = 5495, batch.n_tokens = 64
slot init_sampler: id  2 | task 32351 | init sampler, took 1.24 ms, tokens: text = 5495, total = 5495
slot update_slots: id  2 | task 32351 | created context checkpoint 6 of 8 (pos_min = 4788, pos_max = 5430, size = 15.078 MiB)
slot print_timing: id  2 | task 32351 | 
prompt eval time =     791.53 ms /   337 tokens (    2.35 ms per token,   425.76 tokens per second)
       eval time =    1787.59 ms /    58 tokens (   30.82 ms per token,    32.45 tokens per second)
      total time =    2579.11 ms /   395 tokens
slot      release: id  2 | task 32351 | stop processing: n_tokens = 5552, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.907 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 32411 | processing task, is_child = 0
slot update_slots: id  2 | task 32411 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6083
slot update_slots: id  2 | task 32411 | n_tokens = 5518, memory_seq_rm [5518, end)
slot update_slots: id  2 | task 32411 | prompt processing progress, n_tokens = 6019, batch.n_tokens = 501, progress = 0.989479
slot update_slots: id  2 | task 32411 | n_tokens = 6019, memory_seq_rm [6019, end)
slot update_slots: id  2 | task 32411 | prompt processing progress, n_tokens = 6083, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 32411 | prompt done, n_tokens = 6083, batch.n_tokens = 64
slot init_sampler: id  2 | task 32411 | init sampler, took 0.91 ms, tokens: text = 6083, total = 6083
slot update_slots: id  2 | task 32411 | created context checkpoint 7 of 8 (pos_min = 5376, pos_max = 6018, size = 15.078 MiB)
slot print_timing: id  2 | task 32411 | 
prompt eval time =    1114.88 ms /   565 tokens (    1.97 ms per token,   506.78 tokens per second)
       eval time =   28286.41 ms /   964 tokens (   29.34 ms per token,    34.08 tokens per second)
      total time =   29401.29 ms /  1529 tokens
slot      release: id  2 | task 32411 | stop processing: n_tokens = 7046, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.936
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 33377 | processing task, is_child = 0
slot update_slots: id  2 | task 33377 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7133
slot update_slots: id  2 | task 33377 | n_tokens = 6593, memory_seq_rm [6593, end)
slot update_slots: id  2 | task 33377 | prompt processing progress, n_tokens = 7069, batch.n_tokens = 476, progress = 0.991028
slot update_slots: id  2 | task 33377 | n_tokens = 7069, memory_seq_rm [7069, end)
slot update_slots: id  2 | task 33377 | prompt processing progress, n_tokens = 7133, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 33377 | prompt done, n_tokens = 7133, batch.n_tokens = 64
slot init_sampler: id  2 | task 33377 | init sampler, took 1.01 ms, tokens: text = 7133, total = 7133
slot update_slots: id  2 | task 33377 | created context checkpoint 8 of 8 (pos_min = 6466, pos_max = 7068, size = 14.140 MiB)
slot print_timing: id  2 | task 33377 | 
prompt eval time =     993.12 ms /   540 tokens (    1.84 ms per token,   543.74 tokens per second)
       eval time =    1596.81 ms /    56 tokens (   28.51 ms per token,    35.07 tokens per second)
      total time =    2589.92 ms /   596 tokens
slot      release: id  2 | task 33377 | stop processing: n_tokens = 7188, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.881 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 33435 | processing task, is_child = 0
slot update_slots: id  2 | task 33435 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8127
slot update_slots: id  2 | task 33435 | n_tokens = 7161, memory_seq_rm [7161, end)
slot update_slots: id  2 | task 33435 | prompt processing progress, n_tokens = 8063, batch.n_tokens = 902, progress = 0.992125
slot update_slots: id  2 | task 33435 | n_tokens = 8063, memory_seq_rm [8063, end)
slot update_slots: id  2 | task 33435 | prompt processing progress, n_tokens = 8127, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 33435 | prompt done, n_tokens = 8127, batch.n_tokens = 64
slot init_sampler: id  2 | task 33435 | init sampler, took 1.31 ms, tokens: text = 8127, total = 8127
slot update_slots: id  2 | task 33435 | erasing old context checkpoint (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot update_slots: id  2 | task 33435 | created context checkpoint 8 of 8 (pos_min = 7420, pos_max = 8062, size = 15.078 MiB)
slot print_timing: id  2 | task 33435 | 
prompt eval time =    1614.68 ms /   966 tokens (    1.67 ms per token,   598.26 tokens per second)
       eval time =    9389.11 ms /   328 tokens (   28.63 ms per token,    34.93 tokens per second)
      total time =   11003.79 ms /  1294 tokens
slot      release: id  2 | task 33435 | stop processing: n_tokens = 8454, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 33765 | processing task, is_child = 0
slot update_slots: id  2 | task 33765 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8495
slot update_slots: id  2 | task 33765 | n_tokens = 8271, memory_seq_rm [8271, end)
slot update_slots: id  2 | task 33765 | prompt processing progress, n_tokens = 8431, batch.n_tokens = 160, progress = 0.992466
slot update_slots: id  2 | task 33765 | n_tokens = 8431, memory_seq_rm [8431, end)
slot update_slots: id  2 | task 33765 | prompt processing progress, n_tokens = 8495, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 33765 | prompt done, n_tokens = 8495, batch.n_tokens = 64
slot init_sampler: id  2 | task 33765 | init sampler, took 1.21 ms, tokens: text = 8495, total = 8495
slot update_slots: id  2 | task 33765 | erasing old context checkpoint (pos_min = 162, pos_max = 804, size = 15.078 MiB)
slot update_slots: id  2 | task 33765 | created context checkpoint 8 of 8 (pos_min = 7912, pos_max = 8430, size = 12.170 MiB)
slot print_timing: id  2 | task 33765 | 
prompt eval time =     593.06 ms /   224 tokens (    2.65 ms per token,   377.70 tokens per second)
       eval time =    7305.81 ms /   256 tokens (   28.54 ms per token,    35.04 tokens per second)
      total time =    7898.87 ms /   480 tokens
slot      release: id  2 | task 33765 | stop processing: n_tokens = 8750, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.967 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 34023 | processing task, is_child = 0
slot update_slots: id  2 | task 34023 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8796
slot update_slots: id  2 | task 34023 | n_tokens = 8507, memory_seq_rm [8507, end)
slot update_slots: id  2 | task 34023 | prompt processing progress, n_tokens = 8732, batch.n_tokens = 225, progress = 0.992724
slot update_slots: id  2 | task 34023 | n_tokens = 8732, memory_seq_rm [8732, end)
slot update_slots: id  2 | task 34023 | prompt processing progress, n_tokens = 8796, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 34023 | prompt done, n_tokens = 8796, batch.n_tokens = 64
slot init_sampler: id  2 | task 34023 | init sampler, took 1.25 ms, tokens: text = 8796, total = 8796
slot update_slots: id  2 | task 34023 | erasing old context checkpoint (pos_min = 1310, pos_max = 1952, size = 15.078 MiB)
slot update_slots: id  2 | task 34023 | created context checkpoint 8 of 8 (pos_min = 8231, pos_max = 8731, size = 11.748 MiB)
slot print_timing: id  2 | task 34023 | 
prompt eval time =     722.69 ms /   289 tokens (    2.50 ms per token,   399.90 tokens per second)
       eval time =    7774.27 ms /   270 tokens (   28.79 ms per token,    34.73 tokens per second)
      total time =    8496.96 ms /   559 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 34023 | stop processing: n_tokens = 9065, truncated = 0
srv  update_slots: all slots are idle
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv    operator(): got exception: {"error":{"code":500,"message":"\n------------\nWhile executing CallExpression at line 328, column 32 in source:\n... none %}↵            {{- raise_exception(\"Message has tool role, but there was n...\n                                           ^\nError: Jinja Exception: Message has tool role, but there was no previous assistant message with a tool call!","type":"server_error"}}
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.498 (> 0.100 thold), f_keep = 0.090
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 9065, total state size = 227.643 MiB
srv          load:  - looking for better prompt, base f_keep = 0.090, sim = 0.498
srv        update:  - cache state: 12 prompts, 4635.907 MiB (limits: 8192.000 MiB, 56064 tokens, 254268 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv        update:    - prompt 0x593d953daa30:    9065 tokens, checkpoints:  8,   341.091 MiB
srv  get_availabl: prompt cache update took 254.16 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 34295 | processing task, is_child = 0
slot update_slots: id  2 | task 34295 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1643
slot update_slots: id  2 | task 34295 | n_past = 819, slot.prompt.tokens.size() = 9065, seq_id = 2, pos_min = 8422, n_swa = 128
slot update_slots: id  2 | task 34295 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 34295 | erased invalidated context checkpoint (pos_min = 2325, pos_max = 2967, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 34295 | erased invalidated context checkpoint (pos_min = 4281, pos_max = 4923, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 34295 | erased invalidated context checkpoint (pos_min = 4788, pos_max = 5430, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 34295 | erased invalidated context checkpoint (pos_min = 5376, pos_max = 6018, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 34295 | erased invalidated context checkpoint (pos_min = 6466, pos_max = 7068, n_swa = 128, size = 14.140 MiB)
slot update_slots: id  2 | task 34295 | erased invalidated context checkpoint (pos_min = 7420, pos_max = 8062, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 34295 | erased invalidated context checkpoint (pos_min = 7912, pos_max = 8430, n_swa = 128, size = 12.170 MiB)
slot update_slots: id  2 | task 34295 | erased invalidated context checkpoint (pos_min = 8231, pos_max = 8731, n_swa = 128, size = 11.748 MiB)
slot update_slots: id  2 | task 34295 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 34295 | prompt processing progress, n_tokens = 1579, batch.n_tokens = 1579, progress = 0.961047
slot update_slots: id  2 | task 34295 | n_tokens = 1579, memory_seq_rm [1579, end)
slot update_slots: id  2 | task 34295 | prompt processing progress, n_tokens = 1643, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 34295 | prompt done, n_tokens = 1643, batch.n_tokens = 64
slot init_sampler: id  2 | task 34295 | init sampler, took 0.25 ms, tokens: text = 1643, total = 1643
slot update_slots: id  2 | task 34295 | created context checkpoint 1 of 8 (pos_min = 936, pos_max = 1578, size = 15.078 MiB)
slot print_timing: id  2 | task 34295 | 
prompt eval time =    2794.49 ms /  1643 tokens (    1.70 ms per token,   587.94 tokens per second)
       eval time =   28744.52 ms /   954 tokens (   30.13 ms per token,    33.19 tokens per second)
      total time =   31539.01 ms /  2597 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 34295 | stop processing: n_tokens = 2596, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.663 (> 0.100 thold), f_keep = 0.633
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 35251 | processing task, is_child = 0
slot update_slots: id  2 | task 35251 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2481
slot update_slots: id  2 | task 35251 | n_past = 1644, slot.prompt.tokens.size() = 2596, seq_id = 2, pos_min = 1953, n_swa = 128
slot update_slots: id  2 | task 35251 | restored context checkpoint (pos_min = 936, pos_max = 1578, size = 15.078 MiB)
slot update_slots: id  2 | task 35251 | n_tokens = 1578, memory_seq_rm [1578, end)
slot update_slots: id  2 | task 35251 | prompt processing progress, n_tokens = 2417, batch.n_tokens = 839, progress = 0.974204
slot update_slots: id  2 | task 35251 | n_tokens = 2417, memory_seq_rm [2417, end)
slot update_slots: id  2 | task 35251 | prompt processing progress, n_tokens = 2481, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 35251 | prompt done, n_tokens = 2481, batch.n_tokens = 64
slot init_sampler: id  2 | task 35251 | init sampler, took 0.37 ms, tokens: text = 2481, total = 2481
slot update_slots: id  2 | task 35251 | created context checkpoint 2 of 8 (pos_min = 1774, pos_max = 2416, size = 15.078 MiB)
slot print_timing: id  2 | task 35251 | 
prompt eval time =    1674.52 ms /   903 tokens (    1.85 ms per token,   539.26 tokens per second)
       eval time =    1033.91 ms /    37 tokens (   27.94 ms per token,    35.79 tokens per second)
      total time =    2708.43 ms /   940 tokens
slot      release: id  2 | task 35251 | stop processing: n_tokens = 2517, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.689 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 35290 | processing task, is_child = 0
slot update_slots: id  2 | task 35290 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3627
slot update_slots: id  2 | task 35290 | n_tokens = 2498, memory_seq_rm [2498, end)
slot update_slots: id  2 | task 35290 | prompt processing progress, n_tokens = 3563, batch.n_tokens = 1065, progress = 0.982355
slot update_slots: id  2 | task 35290 | n_tokens = 3563, memory_seq_rm [3563, end)
slot update_slots: id  2 | task 35290 | prompt processing progress, n_tokens = 3627, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 35290 | prompt done, n_tokens = 3627, batch.n_tokens = 64
slot init_sampler: id  2 | task 35290 | init sampler, took 0.54 ms, tokens: text = 3627, total = 3627
slot update_slots: id  2 | task 35290 | created context checkpoint 3 of 8 (pos_min = 2920, pos_max = 3562, size = 15.078 MiB)
slot print_timing: id  2 | task 35290 | 
prompt eval time =    1908.39 ms /  1129 tokens (    1.69 ms per token,   591.60 tokens per second)
       eval time =    3205.92 ms /   113 tokens (   28.37 ms per token,    35.25 tokens per second)
      total time =    5114.31 ms /  1242 tokens
slot      release: id  2 | task 35290 | stop processing: n_tokens = 3739, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.219
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3739, total state size = 102.754 MiB
srv          load:  - looking for better prompt, base f_keep = 0.219, sim = 0.986
srv        update:  - cache state: 13 prompts, 4783.894 MiB (limits: 8192.000 MiB, 56064 tokens, 252805 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv        update:    - prompt 0x593d953daa30:    9065 tokens, checkpoints:  8,   341.091 MiB
srv        update:    - prompt 0x593db7e3f750:    3739 tokens, checkpoints:  3,   147.988 MiB
srv  get_availabl: prompt cache update took 90.29 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 35405 | processing task, is_child = 0
slot update_slots: id  2 | task 35405 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 831
slot update_slots: id  2 | task 35405 | n_past = 819, slot.prompt.tokens.size() = 3739, seq_id = 2, pos_min = 3096, n_swa = 128
slot update_slots: id  2 | task 35405 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 35405 | erased invalidated context checkpoint (pos_min = 936, pos_max = 1578, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 35405 | erased invalidated context checkpoint (pos_min = 1774, pos_max = 2416, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 35405 | erased invalidated context checkpoint (pos_min = 2920, pos_max = 3562, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 35405 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 35405 | prompt processing progress, n_tokens = 767, batch.n_tokens = 767, progress = 0.922984
slot update_slots: id  2 | task 35405 | n_tokens = 767, memory_seq_rm [767, end)
slot update_slots: id  2 | task 35405 | prompt processing progress, n_tokens = 831, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 35405 | prompt done, n_tokens = 831, batch.n_tokens = 64
slot init_sampler: id  2 | task 35405 | init sampler, took 0.14 ms, tokens: text = 831, total = 831
slot update_slots: id  2 | task 35405 | created context checkpoint 1 of 8 (pos_min = 124, pos_max = 766, size = 15.078 MiB)
slot print_timing: id  2 | task 35405 | 
prompt eval time =    1574.87 ms /   831 tokens (    1.90 ms per token,   527.66 tokens per second)
       eval time =    1354.41 ms /    48 tokens (   28.22 ms per token,    35.44 tokens per second)
      total time =    2929.28 ms /   879 tokens
slot      release: id  2 | task 35405 | stop processing: n_tokens = 878, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.948 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 35455 | processing task, is_child = 0
slot update_slots: id  2 | task 35455 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 907
slot update_slots: id  2 | task 35455 | n_tokens = 860, memory_seq_rm [860, end)
slot update_slots: id  2 | task 35455 | prompt processing progress, n_tokens = 907, batch.n_tokens = 47, progress = 1.000000
slot update_slots: id  2 | task 35455 | prompt done, n_tokens = 907, batch.n_tokens = 47
slot init_sampler: id  2 | task 35455 | init sampler, took 0.14 ms, tokens: text = 907, total = 907
slot update_slots: id  2 | task 35455 | created context checkpoint 2 of 8 (pos_min = 235, pos_max = 859, size = 14.656 MiB)
slot print_timing: id  2 | task 35455 | 
prompt eval time =     180.46 ms /    47 tokens (    3.84 ms per token,   260.45 tokens per second)
       eval time =    1693.68 ms /    58 tokens (   29.20 ms per token,    34.24 tokens per second)
      total time =    1874.14 ms /   105 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 35455 | stop processing: n_tokens = 964, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.953 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 35514 | processing task, is_child = 0
slot update_slots: id  2 | task 35514 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 993
slot update_slots: id  2 | task 35514 | n_tokens = 946, memory_seq_rm [946, end)
slot update_slots: id  2 | task 35514 | prompt processing progress, n_tokens = 993, batch.n_tokens = 47, progress = 1.000000
slot update_slots: id  2 | task 35514 | prompt done, n_tokens = 993, batch.n_tokens = 47
slot init_sampler: id  2 | task 35514 | init sampler, took 0.21 ms, tokens: text = 993, total = 993
slot update_slots: id  2 | task 35514 | created context checkpoint 3 of 8 (pos_min = 321, pos_max = 945, size = 14.656 MiB)
slot print_timing: id  2 | task 35514 | 
prompt eval time =     180.33 ms /    47 tokens (    3.84 ms per token,   260.63 tokens per second)
       eval time =    3159.66 ms /   104 tokens (   30.38 ms per token,    32.91 tokens per second)
      total time =    3340.00 ms /   151 tokens
slot      release: id  2 | task 35514 | stop processing: n_tokens = 1096, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 35619 | processing task, is_child = 0
slot update_slots: id  2 | task 35619 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1122
slot update_slots: id  2 | task 35619 | n_tokens = 1075, memory_seq_rm [1075, end)
slot update_slots: id  2 | task 35619 | prompt processing progress, n_tokens = 1122, batch.n_tokens = 47, progress = 1.000000
slot update_slots: id  2 | task 35619 | prompt done, n_tokens = 1122, batch.n_tokens = 47
slot init_sampler: id  2 | task 35619 | init sampler, took 0.19 ms, tokens: text = 1122, total = 1122
slot update_slots: id  2 | task 35619 | created context checkpoint 4 of 8 (pos_min = 453, pos_max = 1074, size = 14.586 MiB)
slot print_timing: id  2 | task 35619 | 
prompt eval time =     182.19 ms /    47 tokens (    3.88 ms per token,   257.97 tokens per second)
       eval time =    5357.94 ms /   177 tokens (   30.27 ms per token,    33.04 tokens per second)
      total time =    5540.13 ms /   224 tokens
slot      release: id  2 | task 35619 | stop processing: n_tokens = 1298, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 35797 | processing task, is_child = 0
slot update_slots: id  2 | task 35797 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1331
slot update_slots: id  2 | task 35797 | n_tokens = 1284, memory_seq_rm [1284, end)
slot update_slots: id  2 | task 35797 | prompt processing progress, n_tokens = 1331, batch.n_tokens = 47, progress = 1.000000
slot update_slots: id  2 | task 35797 | prompt done, n_tokens = 1331, batch.n_tokens = 47
slot init_sampler: id  2 | task 35797 | init sampler, took 0.22 ms, tokens: text = 1331, total = 1331
slot update_slots: id  2 | task 35797 | created context checkpoint 5 of 8 (pos_min = 655, pos_max = 1283, size = 14.750 MiB)
slot print_timing: id  2 | task 35797 | 
prompt eval time =     194.49 ms /    47 tokens (    4.14 ms per token,   241.66 tokens per second)
       eval time =    6604.69 ms /   216 tokens (   30.58 ms per token,    32.70 tokens per second)
      total time =    6799.18 ms /   263 tokens
slot      release: id  2 | task 35797 | stop processing: n_tokens = 1546, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.530
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36014 | processing task, is_child = 0
slot update_slots: id  2 | task 36014 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 827
slot update_slots: id  2 | task 36014 | n_past = 819, slot.prompt.tokens.size() = 1546, seq_id = 2, pos_min = 905, n_swa = 128
slot update_slots: id  2 | task 36014 | restored context checkpoint (pos_min = 655, pos_max = 1283, size = 14.750 MiB)
slot update_slots: id  2 | task 36014 | n_tokens = 819, memory_seq_rm [819, end)
slot update_slots: id  2 | task 36014 | prompt processing progress, n_tokens = 827, batch.n_tokens = 8, progress = 1.000000
slot update_slots: id  2 | task 36014 | prompt done, n_tokens = 827, batch.n_tokens = 8
slot init_sampler: id  2 | task 36014 | init sampler, took 0.13 ms, tokens: text = 827, total = 827
slot print_timing: id  2 | task 36014 | 
prompt eval time =     223.96 ms /     8 tokens (   28.00 ms per token,    35.72 tokens per second)
       eval time =     839.96 ms /    31 tokens (   27.10 ms per token,    36.91 tokens per second)
      total time =    1063.92 ms /    39 tokens
slot      release: id  2 | task 36014 | stop processing: n_tokens = 857, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.836 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36046 | processing task, is_child = 0
slot update_slots: id  2 | task 36046 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1004
slot update_slots: id  2 | task 36046 | n_tokens = 839, memory_seq_rm [839, end)
slot update_slots: id  2 | task 36046 | prompt processing progress, n_tokens = 940, batch.n_tokens = 101, progress = 0.936255
slot update_slots: id  2 | task 36046 | n_tokens = 940, memory_seq_rm [940, end)
slot update_slots: id  2 | task 36046 | prompt processing progress, n_tokens = 1004, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36046 | prompt done, n_tokens = 1004, batch.n_tokens = 64
slot init_sampler: id  2 | task 36046 | init sampler, took 0.16 ms, tokens: text = 1004, total = 1004
slot print_timing: id  2 | task 36046 | 
prompt eval time =     517.96 ms /   165 tokens (    3.14 ms per token,   318.56 tokens per second)
       eval time =    3004.16 ms /   107 tokens (   28.08 ms per token,    35.62 tokens per second)
      total time =    3522.11 ms /   272 tokens
slot      release: id  2 | task 36046 | stop processing: n_tokens = 1110, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.742 (> 0.100 thold), f_keep = 0.746
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36155 | processing task, is_child = 0
slot update_slots: id  2 | task 36155 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1116
slot update_slots: id  2 | task 36155 | n_tokens = 828, memory_seq_rm [828, end)
slot update_slots: id  2 | task 36155 | prompt processing progress, n_tokens = 1052, batch.n_tokens = 224, progress = 0.942652
slot update_slots: id  2 | task 36155 | n_tokens = 1052, memory_seq_rm [1052, end)
slot update_slots: id  2 | task 36155 | prompt processing progress, n_tokens = 1116, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36155 | prompt done, n_tokens = 1116, batch.n_tokens = 64
slot init_sampler: id  2 | task 36155 | init sampler, took 0.23 ms, tokens: text = 1116, total = 1116
slot print_timing: id  2 | task 36155 | 
prompt eval time =     742.86 ms /   288 tokens (    2.58 ms per token,   387.69 tokens per second)
       eval time =    1151.01 ms /    40 tokens (   28.78 ms per token,    34.75 tokens per second)
      total time =    1893.87 ms /   328 tokens
slot      release: id  2 | task 36155 | stop processing: n_tokens = 1155, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.873 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36197 | processing task, is_child = 0
slot update_slots: id  2 | task 36197 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1302
slot update_slots: id  2 | task 36197 | n_tokens = 1137, memory_seq_rm [1137, end)
slot update_slots: id  2 | task 36197 | prompt processing progress, n_tokens = 1238, batch.n_tokens = 101, progress = 0.950845
slot update_slots: id  2 | task 36197 | n_tokens = 1238, memory_seq_rm [1238, end)
slot update_slots: id  2 | task 36197 | prompt processing progress, n_tokens = 1302, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36197 | prompt done, n_tokens = 1302, batch.n_tokens = 64
slot init_sampler: id  2 | task 36197 | init sampler, took 0.20 ms, tokens: text = 1302, total = 1302
slot print_timing: id  2 | task 36197 | 
prompt eval time =     539.97 ms /   165 tokens (    3.27 ms per token,   305.57 tokens per second)
       eval time =    3337.85 ms /   117 tokens (   28.53 ms per token,    35.05 tokens per second)
      total time =    3877.82 ms /   282 tokens
slot      release: id  2 | task 36197 | stop processing: n_tokens = 1418, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.792 (> 0.100 thold), f_keep = 0.787
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36316 | processing task, is_child = 0
slot update_slots: id  2 | task 36316 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1409
slot update_slots: id  2 | task 36316 | n_tokens = 1116, memory_seq_rm [1116, end)
slot update_slots: id  2 | task 36316 | prompt processing progress, n_tokens = 1345, batch.n_tokens = 229, progress = 0.954578
slot update_slots: id  2 | task 36316 | n_tokens = 1345, memory_seq_rm [1345, end)
slot update_slots: id  2 | task 36316 | prompt processing progress, n_tokens = 1409, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36316 | prompt done, n_tokens = 1409, batch.n_tokens = 64
slot init_sampler: id  2 | task 36316 | init sampler, took 0.21 ms, tokens: text = 1409, total = 1409
slot print_timing: id  2 | task 36316 | 
prompt eval time =     805.20 ms /   293 tokens (    2.75 ms per token,   363.89 tokens per second)
       eval time =   11002.76 ms /   367 tokens (   29.98 ms per token,    33.36 tokens per second)
      total time =   11807.95 ms /   660 tokens
slot      release: id  2 | task 36316 | stop processing: n_tokens = 1775, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.705 (> 0.100 thold), f_keep = 0.794
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36685 | processing task, is_child = 0
slot update_slots: id  2 | task 36685 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1999
slot update_slots: id  2 | task 36685 | n_tokens = 1410, memory_seq_rm [1410, end)
slot update_slots: id  2 | task 36685 | prompt processing progress, n_tokens = 1935, batch.n_tokens = 525, progress = 0.967984
slot update_slots: id  2 | task 36685 | n_tokens = 1935, memory_seq_rm [1935, end)
slot update_slots: id  2 | task 36685 | prompt processing progress, n_tokens = 1999, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36685 | prompt done, n_tokens = 1999, batch.n_tokens = 64
slot init_sampler: id  2 | task 36685 | init sampler, took 0.29 ms, tokens: text = 1999, total = 1999
slot update_slots: id  2 | task 36685 | created context checkpoint 6 of 8 (pos_min = 1419, pos_max = 1934, size = 12.100 MiB)
slot print_timing: id  2 | task 36685 | 
prompt eval time =    1145.02 ms /   589 tokens (    1.94 ms per token,   514.40 tokens per second)
       eval time =    1947.66 ms /    66 tokens (   29.51 ms per token,    33.89 tokens per second)
      total time =    3092.68 ms /   655 tokens
slot      release: id  2 | task 36685 | stop processing: n_tokens = 2064, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.925 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36753 | processing task, is_child = 0
slot update_slots: id  2 | task 36753 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2208
slot update_slots: id  2 | task 36753 | n_tokens = 2043, memory_seq_rm [2043, end)
slot update_slots: id  2 | task 36753 | prompt processing progress, n_tokens = 2144, batch.n_tokens = 101, progress = 0.971014
slot update_slots: id  2 | task 36753 | n_tokens = 2144, memory_seq_rm [2144, end)
slot update_slots: id  2 | task 36753 | prompt processing progress, n_tokens = 2208, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36753 | prompt done, n_tokens = 2208, batch.n_tokens = 64
slot init_sampler: id  2 | task 36753 | init sampler, took 0.33 ms, tokens: text = 2208, total = 2208
slot update_slots: id  2 | task 36753 | created context checkpoint 7 of 8 (pos_min = 1577, pos_max = 2143, size = 13.296 MiB)
slot print_timing: id  2 | task 36753 | 
prompt eval time =     557.04 ms /   165 tokens (    3.38 ms per token,   296.21 tokens per second)
       eval time =    1255.25 ms /    44 tokens (   28.53 ms per token,    35.05 tokens per second)
      total time =    1812.29 ms /   209 tokens
slot      release: id  2 | task 36753 | stop processing: n_tokens = 2251, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.536 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36799 | processing task, is_child = 0
slot update_slots: id  2 | task 36799 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4140
slot update_slots: id  2 | task 36799 | n_tokens = 2220, memory_seq_rm [2220, end)
slot update_slots: id  2 | task 36799 | prompt processing progress, n_tokens = 4076, batch.n_tokens = 1856, progress = 0.984541
slot update_slots: id  2 | task 36799 | n_tokens = 4076, memory_seq_rm [4076, end)
slot update_slots: id  2 | task 36799 | prompt processing progress, n_tokens = 4140, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36799 | prompt done, n_tokens = 4140, batch.n_tokens = 64
slot init_sampler: id  2 | task 36799 | init sampler, took 0.59 ms, tokens: text = 4140, total = 4140
slot update_slots: id  2 | task 36799 | created context checkpoint 8 of 8 (pos_min = 3433, pos_max = 4075, size = 15.078 MiB)
slot print_timing: id  2 | task 36799 | 
prompt eval time =    3194.92 ms /  1920 tokens (    1.66 ms per token,   600.95 tokens per second)
       eval time =    1600.48 ms /    55 tokens (   29.10 ms per token,    34.36 tokens per second)
      total time =    4795.40 ms /  1975 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 36799 | stop processing: n_tokens = 4194, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36856 | processing task, is_child = 0
slot update_slots: id  2 | task 36856 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4223
slot update_slots: id  2 | task 36856 | n_tokens = 4163, memory_seq_rm [4163, end)
slot update_slots: id  2 | task 36856 | prompt processing progress, n_tokens = 4223, batch.n_tokens = 60, progress = 1.000000
slot update_slots: id  2 | task 36856 | prompt done, n_tokens = 4223, batch.n_tokens = 60
slot init_sampler: id  2 | task 36856 | init sampler, took 0.80 ms, tokens: text = 4223, total = 4223
slot update_slots: id  2 | task 36856 | erasing old context checkpoint (pos_min = 124, pos_max = 766, size = 15.078 MiB)
slot update_slots: id  2 | task 36856 | created context checkpoint 8 of 8 (pos_min = 3551, pos_max = 4162, size = 14.351 MiB)
slot print_timing: id  2 | task 36856 | 
prompt eval time =     210.16 ms /    60 tokens (    3.50 ms per token,   285.50 tokens per second)
       eval time =    2076.55 ms /    69 tokens (   30.09 ms per token,    33.23 tokens per second)
      total time =    2286.72 ms /   129 tokens
slot      release: id  2 | task 36856 | stop processing: n_tokens = 4291, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.993
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36926 | processing task, is_child = 0
slot update_slots: id  2 | task 36926 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4428
slot update_slots: id  2 | task 36926 | n_tokens = 4263, memory_seq_rm [4263, end)
slot update_slots: id  2 | task 36926 | prompt processing progress, n_tokens = 4364, batch.n_tokens = 101, progress = 0.985547
slot update_slots: id  2 | task 36926 | n_tokens = 4364, memory_seq_rm [4364, end)
slot update_slots: id  2 | task 36926 | prompt processing progress, n_tokens = 4428, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36926 | prompt done, n_tokens = 4428, batch.n_tokens = 64
slot init_sampler: id  2 | task 36926 | init sampler, took 0.93 ms, tokens: text = 4428, total = 4428
slot update_slots: id  2 | task 36926 | erasing old context checkpoint (pos_min = 235, pos_max = 859, size = 14.656 MiB)
slot update_slots: id  2 | task 36926 | created context checkpoint 8 of 8 (pos_min = 3721, pos_max = 4363, size = 15.078 MiB)
slot print_timing: id  2 | task 36926 | 
prompt eval time =     551.15 ms /   165 tokens (    3.34 ms per token,   299.38 tokens per second)
       eval time =     835.53 ms /    28 tokens (   29.84 ms per token,    33.51 tokens per second)
      total time =    1386.68 ms /   193 tokens
slot      release: id  2 | task 36926 | stop processing: n_tokens = 4455, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 36956 | processing task, is_child = 0
slot update_slots: id  2 | task 36956 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4751
slot update_slots: id  2 | task 36956 | n_tokens = 4428, memory_seq_rm [4428, end)
slot update_slots: id  2 | task 36956 | prompt processing progress, n_tokens = 4687, batch.n_tokens = 259, progress = 0.986529
slot update_slots: id  2 | task 36956 | n_tokens = 4687, memory_seq_rm [4687, end)
slot update_slots: id  2 | task 36956 | prompt processing progress, n_tokens = 4751, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 36956 | prompt done, n_tokens = 4751, batch.n_tokens = 64
slot init_sampler: id  2 | task 36956 | init sampler, took 0.68 ms, tokens: text = 4751, total = 4751
slot update_slots: id  2 | task 36956 | erasing old context checkpoint (pos_min = 321, pos_max = 945, size = 14.656 MiB)
slot update_slots: id  2 | task 36956 | created context checkpoint 8 of 8 (pos_min = 4044, pos_max = 4686, size = 15.078 MiB)
slot print_timing: id  2 | task 36956 | 
prompt eval time =     724.94 ms /   323 tokens (    2.24 ms per token,   445.55 tokens per second)
       eval time =    2263.81 ms /    75 tokens (   30.18 ms per token,    33.13 tokens per second)
      total time =    2988.76 ms /   398 tokens
slot      release: id  2 | task 36956 | stop processing: n_tokens = 4825, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.703 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 37033 | processing task, is_child = 0
slot update_slots: id  2 | task 37033 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6818
slot update_slots: id  2 | task 37033 | n_tokens = 4796, memory_seq_rm [4796, end)
slot update_slots: id  2 | task 37033 | prompt processing progress, n_tokens = 6754, batch.n_tokens = 1958, progress = 0.990613
slot update_slots: id  2 | task 37033 | n_tokens = 6754, memory_seq_rm [6754, end)
slot update_slots: id  2 | task 37033 | prompt processing progress, n_tokens = 6818, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 37033 | prompt done, n_tokens = 6818, batch.n_tokens = 64
slot init_sampler: id  2 | task 37033 | init sampler, took 0.97 ms, tokens: text = 6818, total = 6818
slot update_slots: id  2 | task 37033 | erasing old context checkpoint (pos_min = 453, pos_max = 1074, size = 14.586 MiB)
slot update_slots: id  2 | task 37033 | created context checkpoint 8 of 8 (pos_min = 6111, pos_max = 6753, size = 15.078 MiB)
slot print_timing: id  2 | task 37033 | 
prompt eval time =    3501.59 ms /  2022 tokens (    1.73 ms per token,   577.45 tokens per second)
       eval time =    1220.62 ms /    40 tokens (   30.52 ms per token,    32.77 tokens per second)
      total time =    4722.21 ms /  2062 tokens
slot      release: id  2 | task 37033 | stop processing: n_tokens = 6857, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.818 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 37075 | processing task, is_child = 0
slot update_slots: id  2 | task 37075 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8351
slot update_slots: id  2 | task 37075 | n_tokens = 6827, memory_seq_rm [6827, end)
slot update_slots: id  2 | task 37075 | prompt processing progress, n_tokens = 8287, batch.n_tokens = 1460, progress = 0.992336
slot update_slots: id  2 | task 37075 | n_tokens = 8287, memory_seq_rm [8287, end)
slot update_slots: id  2 | task 37075 | prompt processing progress, n_tokens = 8351, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 37075 | prompt done, n_tokens = 8351, batch.n_tokens = 64
slot init_sampler: id  2 | task 37075 | init sampler, took 1.61 ms, tokens: text = 8351, total = 8351
slot update_slots: id  2 | task 37075 | erasing old context checkpoint (pos_min = 655, pos_max = 1283, size = 14.750 MiB)
slot update_slots: id  2 | task 37075 | created context checkpoint 8 of 8 (pos_min = 7644, pos_max = 8286, size = 15.078 MiB)
slot print_timing: id  2 | task 37075 | 
prompt eval time =    2756.57 ms /  1524 tokens (    1.81 ms per token,   552.86 tokens per second)
       eval time =    5454.44 ms /   177 tokens (   30.82 ms per token,    32.45 tokens per second)
      total time =    8211.01 ms /  1701 tokens
slot      release: id  2 | task 37075 | stop processing: n_tokens = 8527, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.947 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 37254 | processing task, is_child = 0
slot update_slots: id  2 | task 37254 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8977
slot update_slots: id  2 | task 37254 | n_tokens = 8498, memory_seq_rm [8498, end)
slot update_slots: id  2 | task 37254 | prompt processing progress, n_tokens = 8913, batch.n_tokens = 415, progress = 0.992871
slot update_slots: id  2 | task 37254 | n_tokens = 8913, memory_seq_rm [8913, end)
slot update_slots: id  2 | task 37254 | prompt processing progress, n_tokens = 8977, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 37254 | prompt done, n_tokens = 8977, batch.n_tokens = 64
slot init_sampler: id  2 | task 37254 | init sampler, took 1.80 ms, tokens: text = 8977, total = 8977
slot update_slots: id  2 | task 37254 | erasing old context checkpoint (pos_min = 1419, pos_max = 1934, size = 12.100 MiB)
slot update_slots: id  2 | task 37254 | created context checkpoint 8 of 8 (pos_min = 8270, pos_max = 8912, size = 15.078 MiB)
slot print_timing: id  2 | task 37254 | 
prompt eval time =     973.51 ms /   479 tokens (    2.03 ms per token,   492.03 tokens per second)
       eval time =   18827.60 ms /   631 tokens (   29.84 ms per token,    33.51 tokens per second)
      total time =   19801.11 ms /  1110 tokens
slot      release: id  2 | task 37254 | stop processing: n_tokens = 9607, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 37887 | processing task, is_child = 0
slot update_slots: id  2 | task 37887 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9651
slot update_slots: id  2 | task 37887 | n_tokens = 9515, memory_seq_rm [9515, end)
slot update_slots: id  2 | task 37887 | prompt processing progress, n_tokens = 9587, batch.n_tokens = 72, progress = 0.993369
slot update_slots: id  2 | task 37887 | n_tokens = 9587, memory_seq_rm [9587, end)
slot update_slots: id  2 | task 37887 | prompt processing progress, n_tokens = 9651, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 37887 | prompt done, n_tokens = 9651, batch.n_tokens = 64
slot init_sampler: id  2 | task 37887 | init sampler, took 1.37 ms, tokens: text = 9651, total = 9651
slot update_slots: id  2 | task 37887 | erasing old context checkpoint (pos_min = 1577, pos_max = 2143, size = 13.296 MiB)
slot update_slots: id  2 | task 37887 | created context checkpoint 8 of 8 (pos_min = 8964, pos_max = 9586, size = 14.609 MiB)
slot print_timing: id  2 | task 37887 | 
prompt eval time =     563.20 ms /   136 tokens (    4.14 ms per token,   241.48 tokens per second)
       eval time =   13630.53 ms /   481 tokens (   28.34 ms per token,    35.29 tokens per second)
      total time =   14193.73 ms /   617 tokens
slot      release: id  2 | task 37887 | stop processing: n_tokens = 10131, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.081
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 10131, total state size = 252.640 MiB
srv          load:  - looking for better prompt, base f_keep = 0.081, sim = 0.986
srv        update:  - cache state: 14 prompts, 5155.962 MiB (limits: 8192.000 MiB, 56064 tokens, 250658 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv        update:    - prompt 0x593d953daa30:    9065 tokens, checkpoints:  8,   341.091 MiB
srv        update:    - prompt 0x593db7e3f750:    3739 tokens, checkpoints:  3,   147.988 MiB
srv        update:    - prompt 0x593d95120cb0:   10131 tokens, checkpoints:  8,   372.067 MiB
srv  get_availabl: prompt cache update took 264.49 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 38370 | processing task, is_child = 0
slot update_slots: id  2 | task 38370 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 831
slot update_slots: id  2 | task 38370 | n_past = 819, slot.prompt.tokens.size() = 10131, seq_id = 2, pos_min = 9488, n_swa = 128
slot update_slots: id  2 | task 38370 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 38370 | erased invalidated context checkpoint (pos_min = 3433, pos_max = 4075, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 38370 | erased invalidated context checkpoint (pos_min = 3551, pos_max = 4162, n_swa = 128, size = 14.351 MiB)
slot update_slots: id  2 | task 38370 | erased invalidated context checkpoint (pos_min = 3721, pos_max = 4363, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 38370 | erased invalidated context checkpoint (pos_min = 4044, pos_max = 4686, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 38370 | erased invalidated context checkpoint (pos_min = 6111, pos_max = 6753, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 38370 | erased invalidated context checkpoint (pos_min = 7644, pos_max = 8286, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 38370 | erased invalidated context checkpoint (pos_min = 8270, pos_max = 8912, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 38370 | erased invalidated context checkpoint (pos_min = 8964, pos_max = 9586, n_swa = 128, size = 14.609 MiB)
slot update_slots: id  2 | task 38370 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 38370 | prompt processing progress, n_tokens = 767, batch.n_tokens = 767, progress = 0.922984
slot update_slots: id  2 | task 38370 | n_tokens = 767, memory_seq_rm [767, end)
slot update_slots: id  2 | task 38370 | prompt processing progress, n_tokens = 831, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 38370 | prompt done, n_tokens = 831, batch.n_tokens = 64
slot init_sampler: id  2 | task 38370 | init sampler, took 0.13 ms, tokens: text = 831, total = 831
slot update_slots: id  2 | task 38370 | created context checkpoint 1 of 8 (pos_min = 124, pos_max = 766, size = 15.078 MiB)
slot print_timing: id  2 | task 38370 | 
prompt eval time =    1579.83 ms /   831 tokens (    1.90 ms per token,   526.01 tokens per second)
       eval time =   11344.43 ms /   388 tokens (   29.24 ms per token,    34.20 tokens per second)
      total time =   12924.26 ms /  1219 tokens
slot      release: id  2 | task 38370 | stop processing: n_tokens = 1218, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.697 (> 0.100 thold), f_keep = 0.716
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 38760 | processing task, is_child = 0
slot update_slots: id  2 | task 38760 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1251
slot update_slots: id  2 | task 38760 | n_tokens = 872, memory_seq_rm [872, end)
slot update_slots: id  2 | task 38760 | prompt processing progress, n_tokens = 1187, batch.n_tokens = 315, progress = 0.948841
slot update_slots: id  2 | task 38760 | n_tokens = 1187, memory_seq_rm [1187, end)
slot update_slots: id  2 | task 38760 | prompt processing progress, n_tokens = 1251, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 38760 | prompt done, n_tokens = 1251, batch.n_tokens = 64
slot init_sampler: id  2 | task 38760 | init sampler, took 0.20 ms, tokens: text = 1251, total = 1251
slot update_slots: id  2 | task 38760 | created context checkpoint 2 of 8 (pos_min = 575, pos_max = 1186, size = 14.351 MiB)
slot print_timing: id  2 | task 38760 | 
prompt eval time =     844.93 ms /   379 tokens (    2.23 ms per token,   448.56 tokens per second)
       eval time =    1368.09 ms /    46 tokens (   29.74 ms per token,    33.62 tokens per second)
      total time =    2213.02 ms /   425 tokens
slot      release: id  2 | task 38760 | stop processing: n_tokens = 1296, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.652 (> 0.100 thold), f_keep = 0.641
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 38808 | processing task, is_child = 0
slot update_slots: id  2 | task 38808 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1275
slot update_slots: id  2 | task 38808 | n_tokens = 831, memory_seq_rm [831, end)
slot update_slots: id  2 | task 38808 | prompt processing progress, n_tokens = 1211, batch.n_tokens = 380, progress = 0.949804
slot update_slots: id  2 | task 38808 | n_tokens = 1211, memory_seq_rm [1211, end)
slot update_slots: id  2 | task 38808 | prompt processing progress, n_tokens = 1275, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 38808 | prompt done, n_tokens = 1275, batch.n_tokens = 64
slot init_sampler: id  2 | task 38808 | init sampler, took 0.18 ms, tokens: text = 1275, total = 1275
slot print_timing: id  2 | task 38808 | 
prompt eval time =     969.27 ms /   444 tokens (    2.18 ms per token,   458.07 tokens per second)
       eval time =    3149.39 ms /   108 tokens (   29.16 ms per token,    34.29 tokens per second)
      total time =    4118.67 ms /   552 tokens
slot      release: id  2 | task 38808 | stop processing: n_tokens = 1382, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 38918 | processing task, is_child = 0
slot update_slots: id  2 | task 38918 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1408
slot update_slots: id  2 | task 38918 | n_tokens = 1353, memory_seq_rm [1353, end)
slot update_slots: id  2 | task 38918 | prompt processing progress, n_tokens = 1408, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  2 | task 38918 | prompt done, n_tokens = 1408, batch.n_tokens = 55
slot init_sampler: id  2 | task 38918 | init sampler, took 0.24 ms, tokens: text = 1408, total = 1408
slot update_slots: id  2 | task 38918 | created context checkpoint 3 of 8 (pos_min = 866, pos_max = 1352, size = 11.420 MiB)
slot print_timing: id  2 | task 38918 | 
prompt eval time =     205.26 ms /    55 tokens (    3.73 ms per token,   267.95 tokens per second)
       eval time =   10954.59 ms /   368 tokens (   29.77 ms per token,    33.59 tokens per second)
      total time =   11159.85 ms /   423 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 38918 | stop processing: n_tokens = 1775, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.461
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1775, total state size = 56.700 MiB
srv          load:  - looking for better prompt, base f_keep = 0.461, sim = 0.960
srv        update:  - cache state: 15 prompts, 5253.511 MiB (limits: 8192.000 MiB, 56064 tokens, 248772 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv        update:    - prompt 0x593d953daa30:    9065 tokens, checkpoints:  8,   341.091 MiB
srv        update:    - prompt 0x593db7e3f750:    3739 tokens, checkpoints:  3,   147.988 MiB
srv        update:    - prompt 0x593d95120cb0:   10131 tokens, checkpoints:  8,   372.067 MiB
srv        update:    - prompt 0x593dc061f040:    1775 tokens, checkpoints:  3,    97.549 MiB
srv  get_availabl: prompt cache update took 46.85 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 39287 | processing task, is_child = 0
slot update_slots: id  2 | task 39287 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 853
slot update_slots: id  2 | task 39287 | n_past = 819, slot.prompt.tokens.size() = 1775, seq_id = 2, pos_min = 1132, n_swa = 128
slot update_slots: id  2 | task 39287 | restored context checkpoint (pos_min = 575, pos_max = 1186, size = 14.351 MiB)
slot update_slots: id  2 | task 39287 | erased invalidated context checkpoint (pos_min = 866, pos_max = 1352, n_swa = 128, size = 11.420 MiB)
slot update_slots: id  2 | task 39287 | n_tokens = 819, memory_seq_rm [819, end)
slot update_slots: id  2 | task 39287 | prompt processing progress, n_tokens = 853, batch.n_tokens = 34, progress = 1.000000
slot update_slots: id  2 | task 39287 | prompt done, n_tokens = 853, batch.n_tokens = 34
slot init_sampler: id  2 | task 39287 | init sampler, took 0.13 ms, tokens: text = 853, total = 853
slot print_timing: id  2 | task 39287 | 
prompt eval time =     345.25 ms /    34 tokens (   10.15 ms per token,    98.48 tokens per second)
       eval time =    2199.33 ms /    78 tokens (   28.20 ms per token,    35.47 tokens per second)
      total time =    2544.59 ms /   112 tokens
slot      release: id  2 | task 39287 | stop processing: n_tokens = 930, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.438 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 39366 | processing task, is_child = 0
slot update_slots: id  2 | task 39366 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2068
slot update_slots: id  2 | task 39366 | n_tokens = 906, memory_seq_rm [906, end)
slot update_slots: id  2 | task 39366 | prompt processing progress, n_tokens = 2004, batch.n_tokens = 1098, progress = 0.969052
slot update_slots: id  2 | task 39366 | n_tokens = 2004, memory_seq_rm [2004, end)
slot update_slots: id  2 | task 39366 | prompt processing progress, n_tokens = 2068, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 39366 | prompt done, n_tokens = 2068, batch.n_tokens = 64
slot init_sampler: id  2 | task 39366 | init sampler, took 0.31 ms, tokens: text = 2068, total = 2068
slot update_slots: id  2 | task 39366 | created context checkpoint 3 of 8 (pos_min = 1361, pos_max = 2003, size = 15.078 MiB)
slot print_timing: id  2 | task 39366 | 
prompt eval time =    2029.81 ms /  1162 tokens (    1.75 ms per token,   572.47 tokens per second)
       eval time =    1109.05 ms /    38 tokens (   29.19 ms per token,    34.26 tokens per second)
      total time =    3138.86 ms /  1200 tokens
slot      release: id  2 | task 39366 | stop processing: n_tokens = 2105, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.583 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 39406 | processing task, is_child = 0
slot update_slots: id  2 | task 39406 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3562
slot update_slots: id  2 | task 39406 | n_tokens = 2078, memory_seq_rm [2078, end)
slot update_slots: id  2 | task 39406 | prompt processing progress, n_tokens = 3498, batch.n_tokens = 1420, progress = 0.982033
slot update_slots: id  2 | task 39406 | n_tokens = 3498, memory_seq_rm [3498, end)
slot update_slots: id  2 | task 39406 | prompt processing progress, n_tokens = 3562, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 39406 | prompt done, n_tokens = 3562, batch.n_tokens = 64
slot init_sampler: id  2 | task 39406 | init sampler, took 0.70 ms, tokens: text = 3562, total = 3562
slot update_slots: id  2 | task 39406 | created context checkpoint 4 of 8 (pos_min = 2855, pos_max = 3497, size = 15.078 MiB)
slot print_timing: id  2 | task 39406 | 
prompt eval time =    2495.37 ms /  1484 tokens (    1.68 ms per token,   594.70 tokens per second)
       eval time =     803.03 ms /    28 tokens (   28.68 ms per token,    34.87 tokens per second)
      total time =    3298.39 ms /  1512 tokens
slot      release: id  2 | task 39406 | stop processing: n_tokens = 3589, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 39436 | processing task, is_child = 0
slot update_slots: id  2 | task 39436 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3621
slot update_slots: id  2 | task 39436 | n_tokens = 3562, memory_seq_rm [3562, end)
slot update_slots: id  2 | task 39436 | prompt processing progress, n_tokens = 3621, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  2 | task 39436 | prompt done, n_tokens = 3621, batch.n_tokens = 59
slot init_sampler: id  2 | task 39436 | init sampler, took 0.71 ms, tokens: text = 3621, total = 3621
slot print_timing: id  2 | task 39436 | 
prompt eval time =     201.29 ms /    59 tokens (    3.41 ms per token,   293.11 tokens per second)
       eval time =    1315.73 ms /    44 tokens (   29.90 ms per token,    33.44 tokens per second)
      total time =    1517.01 ms /   103 tokens
slot      release: id  2 | task 39436 | stop processing: n_tokens = 3664, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.896 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 39481 | processing task, is_child = 0
slot update_slots: id  2 | task 39481 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4056
slot update_slots: id  2 | task 39481 | n_tokens = 3634, memory_seq_rm [3634, end)
slot update_slots: id  2 | task 39481 | prompt processing progress, n_tokens = 3992, batch.n_tokens = 358, progress = 0.984221
slot update_slots: id  2 | task 39481 | n_tokens = 3992, memory_seq_rm [3992, end)
slot update_slots: id  2 | task 39481 | prompt processing progress, n_tokens = 4056, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 39481 | prompt done, n_tokens = 4056, batch.n_tokens = 64
slot init_sampler: id  2 | task 39481 | init sampler, took 0.59 ms, tokens: text = 4056, total = 4056
slot update_slots: id  2 | task 39481 | created context checkpoint 5 of 8 (pos_min = 3349, pos_max = 3991, size = 15.078 MiB)
slot print_timing: id  2 | task 39481 | 
prompt eval time =     892.85 ms /   422 tokens (    2.12 ms per token,   472.64 tokens per second)
       eval time =   34330.91 ms /  1140 tokens (   30.11 ms per token,    33.21 tokens per second)
      total time =   35223.76 ms /  1562 tokens
slot      release: id  2 | task 39481 | stop processing: n_tokens = 5195, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.179 (> 0.100 thold), f_keep = 0.164
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5195, total state size = 136.896 MiB
srv          load:  - looking for better prompt, base f_keep = 0.164, sim = 0.179
srv        update:  - cache state: 16 prompts, 5465.069 MiB (limits: 8192.000 MiB, 56064 tokens, 246929 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv        update:    - prompt 0x593d953daa30:    9065 tokens, checkpoints:  8,   341.091 MiB
srv        update:    - prompt 0x593db7e3f750:    3739 tokens, checkpoints:  3,   147.988 MiB
srv        update:    - prompt 0x593d95120cb0:   10131 tokens, checkpoints:  8,   372.067 MiB
srv        update:    - prompt 0x593dc061f040:    1775 tokens, checkpoints:  3,    97.549 MiB
srv        update:    - prompt 0x593db7f29c60:    5195 tokens, checkpoints:  5,   211.558 MiB
srv  get_availabl: prompt cache update took 161.29 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 40623 | processing task, is_child = 0
slot update_slots: id  2 | task 40623 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4760
slot update_slots: id  2 | task 40623 | n_past = 853, slot.prompt.tokens.size() = 5195, seq_id = 2, pos_min = 4552, n_swa = 128
slot update_slots: id  2 | task 40623 | restored context checkpoint (pos_min = 575, pos_max = 1186, size = 14.351 MiB)
slot update_slots: id  2 | task 40623 | erased invalidated context checkpoint (pos_min = 1361, pos_max = 2003, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 40623 | erased invalidated context checkpoint (pos_min = 2855, pos_max = 3497, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 40623 | erased invalidated context checkpoint (pos_min = 3349, pos_max = 3991, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 40623 | n_tokens = 853, memory_seq_rm [853, end)
slot update_slots: id  2 | task 40623 | prompt processing progress, n_tokens = 2901, batch.n_tokens = 2048, progress = 0.609454
slot update_slots: id  2 | task 40623 | n_tokens = 2901, memory_seq_rm [2901, end)
slot update_slots: id  2 | task 40623 | prompt processing progress, n_tokens = 4696, batch.n_tokens = 1795, progress = 0.986555
slot update_slots: id  2 | task 40623 | n_tokens = 4696, memory_seq_rm [4696, end)
slot update_slots: id  2 | task 40623 | prompt processing progress, n_tokens = 4760, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 40623 | prompt done, n_tokens = 4760, batch.n_tokens = 64
slot init_sampler: id  2 | task 40623 | init sampler, took 0.94 ms, tokens: text = 4760, total = 4760
slot update_slots: id  2 | task 40623 | created context checkpoint 3 of 8 (pos_min = 4053, pos_max = 4695, size = 15.078 MiB)
slot print_timing: id  2 | task 40623 | 
prompt eval time =    6038.05 ms /  3907 tokens (    1.55 ms per token,   647.06 tokens per second)
       eval time =    9685.74 ms /   339 tokens (   28.57 ms per token,    35.00 tokens per second)
      total time =   15723.78 ms /  4246 tokens
slot      release: id  2 | task 40623 | stop processing: n_tokens = 5098, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.942 (> 0.100 thold), f_keep = 0.948
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 40965 | processing task, is_child = 0
slot update_slots: id  2 | task 40965 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5132
slot update_slots: id  2 | task 40965 | n_tokens = 4833, memory_seq_rm [4833, end)
slot update_slots: id  2 | task 40965 | prompt processing progress, n_tokens = 5068, batch.n_tokens = 235, progress = 0.987529
slot update_slots: id  2 | task 40965 | n_tokens = 5068, memory_seq_rm [5068, end)
slot update_slots: id  2 | task 40965 | prompt processing progress, n_tokens = 5132, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 40965 | prompt done, n_tokens = 5132, batch.n_tokens = 64
slot init_sampler: id  2 | task 40965 | init sampler, took 0.80 ms, tokens: text = 5132, total = 5132
slot update_slots: id  2 | task 40965 | created context checkpoint 4 of 8 (pos_min = 4455, pos_max = 5067, size = 14.374 MiB)
slot print_timing: id  2 | task 40965 | 
prompt eval time =     739.16 ms /   299 tokens (    2.47 ms per token,   404.51 tokens per second)
       eval time =    7283.06 ms /   245 tokens (   29.73 ms per token,    33.64 tokens per second)
      total time =    8022.22 ms /   544 tokens
slot      release: id  2 | task 40965 | stop processing: n_tokens = 5376, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.954 (> 0.100 thold), f_keep = 0.960
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 41212 | processing task, is_child = 0
slot update_slots: id  2 | task 41212 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5414
slot update_slots: id  2 | task 41212 | n_tokens = 5163, memory_seq_rm [5163, end)
slot update_slots: id  2 | task 41212 | prompt processing progress, n_tokens = 5350, batch.n_tokens = 187, progress = 0.988179
slot update_slots: id  2 | task 41212 | n_tokens = 5350, memory_seq_rm [5350, end)
slot update_slots: id  2 | task 41212 | prompt processing progress, n_tokens = 5414, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 41212 | prompt done, n_tokens = 5414, batch.n_tokens = 64
slot init_sampler: id  2 | task 41212 | init sampler, took 0.79 ms, tokens: text = 5414, total = 5414
slot update_slots: id  2 | task 41212 | created context checkpoint 5 of 8 (pos_min = 4733, pos_max = 5349, size = 14.468 MiB)
slot print_timing: id  2 | task 41212 | 
prompt eval time =     659.93 ms /   251 tokens (    2.63 ms per token,   380.34 tokens per second)
       eval time =   14650.56 ms /   483 tokens (   30.33 ms per token,    32.97 tokens per second)
      total time =   15310.49 ms /   734 tokens
slot      release: id  2 | task 41212 | stop processing: n_tokens = 5896, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.920 (> 0.100 thold), f_keep = 0.928
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 41697 | processing task, is_child = 0
slot update_slots: id  2 | task 41697 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5942
slot update_slots: id  2 | task 41697 | n_tokens = 5469, memory_seq_rm [5469, end)
slot update_slots: id  2 | task 41697 | prompt processing progress, n_tokens = 5878, batch.n_tokens = 409, progress = 0.989229
slot update_slots: id  2 | task 41697 | n_tokens = 5878, memory_seq_rm [5878, end)
slot update_slots: id  2 | task 41697 | prompt processing progress, n_tokens = 5942, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 41697 | prompt done, n_tokens = 5942, batch.n_tokens = 64
slot init_sampler: id  2 | task 41697 | init sampler, took 0.87 ms, tokens: text = 5942, total = 5942
slot update_slots: id  2 | task 41697 | created context checkpoint 6 of 8 (pos_min = 5342, pos_max = 5877, size = 12.569 MiB)
slot print_timing: id  2 | task 41697 | 
prompt eval time =     992.00 ms /   473 tokens (    2.10 ms per token,   476.82 tokens per second)
       eval time =    3112.95 ms /   103 tokens (   30.22 ms per token,    33.09 tokens per second)
      total time =    4104.94 ms /   576 tokens
slot      release: id  2 | task 41697 | stop processing: n_tokens = 6044, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.818 (> 0.100 thold), f_keep = 0.788
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 41802 | processing task, is_child = 0
slot update_slots: id  2 | task 41802 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5817
slot update_slots: id  2 | task 41802 | n_past = 4760, slot.prompt.tokens.size() = 6044, seq_id = 2, pos_min = 5528, n_swa = 128
slot update_slots: id  2 | task 41802 | restored context checkpoint (pos_min = 4455, pos_max = 5067, size = 14.374 MiB)
slot update_slots: id  2 | task 41802 | erased invalidated context checkpoint (pos_min = 4733, pos_max = 5349, n_swa = 128, size = 14.468 MiB)
slot update_slots: id  2 | task 41802 | erased invalidated context checkpoint (pos_min = 5342, pos_max = 5877, n_swa = 128, size = 12.569 MiB)
slot update_slots: id  2 | task 41802 | n_tokens = 4760, memory_seq_rm [4760, end)
slot update_slots: id  2 | task 41802 | prompt processing progress, n_tokens = 5753, batch.n_tokens = 993, progress = 0.988998
slot update_slots: id  2 | task 41802 | n_tokens = 5753, memory_seq_rm [5753, end)
slot update_slots: id  2 | task 41802 | prompt processing progress, n_tokens = 5817, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 41802 | prompt done, n_tokens = 5817, batch.n_tokens = 64
slot init_sampler: id  2 | task 41802 | init sampler, took 0.83 ms, tokens: text = 5817, total = 5817
slot update_slots: id  2 | task 41802 | created context checkpoint 5 of 8 (pos_min = 5110, pos_max = 5752, size = 15.078 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 41802
slot      release: id  2 | task 41802 | stop processing: n_tokens = 5835, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.141
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5835, total state size = 151.903 MiB
srv          load:  - looking for better prompt, base f_keep = 0.141, sim = 0.974
srv        update:  - cache state: 17 prompts, 5690.931 MiB (limits: 8192.000 MiB, 56064 tokens, 245528 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv        update:    - prompt 0x593d953daa30:    9065 tokens, checkpoints:  8,   341.091 MiB
srv        update:    - prompt 0x593db7e3f750:    3739 tokens, checkpoints:  3,   147.988 MiB
srv        update:    - prompt 0x593d95120cb0:   10131 tokens, checkpoints:  8,   372.067 MiB
srv        update:    - prompt 0x593dc061f040:    1775 tokens, checkpoints:  3,    97.549 MiB
srv        update:    - prompt 0x593db7f29c60:    5195 tokens, checkpoints:  5,   211.558 MiB
srv        update:    - prompt 0x593d95dc86d0:    5835 tokens, checkpoints:  5,   225.862 MiB
srv  get_availabl: prompt cache update took 174.82 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 41824 | processing task, is_child = 0
slot update_slots: id  2 | task 41824 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 845
slot update_slots: id  2 | task 41824 | n_past = 823, slot.prompt.tokens.size() = 5835, seq_id = 2, pos_min = 5192, n_swa = 128
slot update_slots: id  2 | task 41824 | restored context checkpoint (pos_min = 575, pos_max = 1186, size = 14.351 MiB)
slot update_slots: id  2 | task 41824 | erased invalidated context checkpoint (pos_min = 4053, pos_max = 4695, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 41824 | erased invalidated context checkpoint (pos_min = 4455, pos_max = 5067, n_swa = 128, size = 14.374 MiB)
slot update_slots: id  2 | task 41824 | erased invalidated context checkpoint (pos_min = 5110, pos_max = 5752, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 41824 | n_tokens = 823, memory_seq_rm [823, end)
slot update_slots: id  2 | task 41824 | prompt processing progress, n_tokens = 845, batch.n_tokens = 22, progress = 1.000000
slot update_slots: id  2 | task 41824 | prompt done, n_tokens = 845, batch.n_tokens = 22
slot init_sampler: id  2 | task 41824 | init sampler, took 0.13 ms, tokens: text = 845, total = 845
slot print_timing: id  2 | task 41824 | 
prompt eval time =     270.21 ms /    22 tokens (   12.28 ms per token,    81.42 tokens per second)
       eval time =    3167.02 ms /   112 tokens (   28.28 ms per token,    35.36 tokens per second)
      total time =    3437.23 ms /   134 tokens
slot      release: id  2 | task 41824 | stop processing: n_tokens = 956, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 41937 | processing task, is_child = 0
slot update_slots: id  2 | task 41937 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 999
slot update_slots: id  2 | task 41937 | n_tokens = 930, memory_seq_rm [930, end)
slot update_slots: id  2 | task 41937 | prompt processing progress, n_tokens = 935, batch.n_tokens = 5, progress = 0.935936
slot update_slots: id  2 | task 41937 | n_tokens = 935, memory_seq_rm [935, end)
slot update_slots: id  2 | task 41937 | prompt processing progress, n_tokens = 999, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 41937 | prompt done, n_tokens = 999, batch.n_tokens = 64
slot init_sampler: id  2 | task 41937 | init sampler, took 0.19 ms, tokens: text = 999, total = 999
slot print_timing: id  2 | task 41937 | 
prompt eval time =     273.08 ms /    69 tokens (    3.96 ms per token,   252.67 tokens per second)
       eval time =   15382.85 ms /   522 tokens (   29.47 ms per token,    33.93 tokens per second)
      total time =   15655.93 ms /   591 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 41937 | stop processing: n_tokens = 1520, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.846 (> 0.100 thold), f_keep = 0.539
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 42461 | processing task, is_child = 0
slot update_slots: id  2 | task 42461 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 968
slot update_slots: id  2 | task 42461 | n_past = 819, slot.prompt.tokens.size() = 1520, seq_id = 2, pos_min = 877, n_swa = 128
slot update_slots: id  2 | task 42461 | restored context checkpoint (pos_min = 575, pos_max = 1186, size = 14.351 MiB)
slot update_slots: id  2 | task 42461 | n_tokens = 819, memory_seq_rm [819, end)
slot update_slots: id  2 | task 42461 | prompt processing progress, n_tokens = 904, batch.n_tokens = 85, progress = 0.933884
slot update_slots: id  2 | task 42461 | n_tokens = 904, memory_seq_rm [904, end)
slot update_slots: id  2 | task 42461 | prompt processing progress, n_tokens = 968, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 42461 | prompt done, n_tokens = 968, batch.n_tokens = 64
slot init_sampler: id  2 | task 42461 | init sampler, took 0.15 ms, tokens: text = 968, total = 968
slot print_timing: id  2 | task 42461 | 
prompt eval time =     724.55 ms /   149 tokens (    4.86 ms per token,   205.64 tokens per second)
       eval time =    5896.55 ms /   203 tokens (   29.05 ms per token,    34.43 tokens per second)
      total time =    6621.11 ms /   352 tokens
slot      release: id  2 | task 42461 | stop processing: n_tokens = 1170, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.486 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 42666 | processing task, is_child = 0
slot update_slots: id  2 | task 42666 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2358
slot update_slots: id  2 | task 42666 | n_tokens = 1146, memory_seq_rm [1146, end)
slot update_slots: id  2 | task 42666 | prompt processing progress, n_tokens = 2294, batch.n_tokens = 1148, progress = 0.972858
slot update_slots: id  2 | task 42666 | n_tokens = 2294, memory_seq_rm [2294, end)
slot update_slots: id  2 | task 42666 | prompt processing progress, n_tokens = 2358, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 42666 | prompt done, n_tokens = 2358, batch.n_tokens = 64
slot init_sampler: id  2 | task 42666 | init sampler, took 0.34 ms, tokens: text = 2358, total = 2358
slot update_slots: id  2 | task 42666 | created context checkpoint 3 of 8 (pos_min = 1651, pos_max = 2293, size = 15.078 MiB)
slot print_timing: id  2 | task 42666 | 
prompt eval time =    2222.49 ms /  1212 tokens (    1.83 ms per token,   545.33 tokens per second)
       eval time =    1479.25 ms /    49 tokens (   30.19 ms per token,    33.12 tokens per second)
      total time =    3701.75 ms /  1261 tokens
slot      release: id  2 | task 42666 | stop processing: n_tokens = 2406, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.812 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 42717 | processing task, is_child = 0
slot update_slots: id  2 | task 42717 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2924
slot update_slots: id  2 | task 42717 | n_tokens = 2373, memory_seq_rm [2373, end)
slot update_slots: id  2 | task 42717 | prompt processing progress, n_tokens = 2860, batch.n_tokens = 487, progress = 0.978112
slot update_slots: id  2 | task 42717 | n_tokens = 2860, memory_seq_rm [2860, end)
slot update_slots: id  2 | task 42717 | prompt processing progress, n_tokens = 2924, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 42717 | prompt done, n_tokens = 2924, batch.n_tokens = 64
slot init_sampler: id  2 | task 42717 | init sampler, took 0.43 ms, tokens: text = 2924, total = 2924
slot update_slots: id  2 | task 42717 | created context checkpoint 4 of 8 (pos_min = 2217, pos_max = 2859, size = 15.078 MiB)
slot print_timing: id  2 | task 42717 | 
prompt eval time =    1048.93 ms /   551 tokens (    1.90 ms per token,   525.30 tokens per second)
       eval time =   11012.64 ms /   360 tokens (   30.59 ms per token,    32.69 tokens per second)
      total time =   12061.57 ms /   911 tokens
slot      release: id  2 | task 42717 | stop processing: n_tokens = 3283, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.937 (> 0.100 thold), f_keep = 0.950
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43079 | processing task, is_child = 0
slot update_slots: id  2 | task 43079 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3329
slot update_slots: id  2 | task 43079 | n_tokens = 3119, memory_seq_rm [3119, end)
slot update_slots: id  2 | task 43079 | prompt processing progress, n_tokens = 3265, batch.n_tokens = 146, progress = 0.980775
slot update_slots: id  2 | task 43079 | n_tokens = 3265, memory_seq_rm [3265, end)
slot update_slots: id  2 | task 43079 | prompt processing progress, n_tokens = 3329, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 43079 | prompt done, n_tokens = 3329, batch.n_tokens = 64
slot init_sampler: id  2 | task 43079 | init sampler, took 0.48 ms, tokens: text = 3329, total = 3329
slot update_slots: id  2 | task 43079 | created context checkpoint 5 of 8 (pos_min = 2718, pos_max = 3264, size = 12.827 MiB)
slot print_timing: id  2 | task 43079 | 
prompt eval time =     628.05 ms /   210 tokens (    2.99 ms per token,   334.37 tokens per second)
       eval time =    2793.26 ms /    91 tokens (   30.70 ms per token,    32.58 tokens per second)
      total time =    3421.31 ms /   301 tokens
slot      release: id  2 | task 43079 | stop processing: n_tokens = 3419, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.672 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43172 | processing task, is_child = 0
slot update_slots: id  2 | task 43172 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5040
slot update_slots: id  2 | task 43172 | n_tokens = 3389, memory_seq_rm [3389, end)
slot update_slots: id  2 | task 43172 | prompt processing progress, n_tokens = 4976, batch.n_tokens = 1587, progress = 0.987302
slot update_slots: id  2 | task 43172 | n_tokens = 4976, memory_seq_rm [4976, end)
slot update_slots: id  2 | task 43172 | prompt processing progress, n_tokens = 5040, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 43172 | prompt done, n_tokens = 5040, batch.n_tokens = 64
slot init_sampler: id  2 | task 43172 | init sampler, took 0.97 ms, tokens: text = 5040, total = 5040
slot update_slots: id  2 | task 43172 | created context checkpoint 6 of 8 (pos_min = 4333, pos_max = 4975, size = 15.078 MiB)
slot print_timing: id  2 | task 43172 | 
prompt eval time =    2997.39 ms /  1651 tokens (    1.82 ms per token,   550.81 tokens per second)
       eval time =    7796.66 ms /   259 tokens (   30.10 ms per token,    33.22 tokens per second)
      total time =   10794.06 ms /  1910 tokens
slot      release: id  2 | task 43172 | stop processing: n_tokens = 5298, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.155
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5298, total state size = 139.311 MiB
srv          load:  - looking for better prompt, base f_keep = 0.155, sim = 0.989
srv        update:  - cache state: 18 prompts, 5917.732 MiB (limits: 8192.000 MiB, 56064 tokens, 243452 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv        update:    - prompt 0x593d953daa30:    9065 tokens, checkpoints:  8,   341.091 MiB
srv        update:    - prompt 0x593db7e3f750:    3739 tokens, checkpoints:  3,   147.988 MiB
srv        update:    - prompt 0x593d95120cb0:   10131 tokens, checkpoints:  8,   372.067 MiB
srv        update:    - prompt 0x593dc061f040:    1775 tokens, checkpoints:  3,    97.549 MiB
srv        update:    - prompt 0x593db7f29c60:    5195 tokens, checkpoints:  5,   211.558 MiB
srv        update:    - prompt 0x593d95dc86d0:    5835 tokens, checkpoints:  5,   225.862 MiB
srv        update:    - prompt 0x593d962d8bb0:    5298 tokens, checkpoints:  6,   226.801 MiB
srv  get_availabl: prompt cache update took 172.77 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43433 | processing task, is_child = 0
slot update_slots: id  2 | task 43433 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 828
slot update_slots: id  2 | task 43433 | n_past = 819, slot.prompt.tokens.size() = 5298, seq_id = 2, pos_min = 4655, n_swa = 128
slot update_slots: id  2 | task 43433 | restored context checkpoint (pos_min = 575, pos_max = 1186, size = 14.351 MiB)
slot update_slots: id  2 | task 43433 | erased invalidated context checkpoint (pos_min = 1651, pos_max = 2293, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 43433 | erased invalidated context checkpoint (pos_min = 2217, pos_max = 2859, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 43433 | erased invalidated context checkpoint (pos_min = 2718, pos_max = 3264, n_swa = 128, size = 12.827 MiB)
slot update_slots: id  2 | task 43433 | erased invalidated context checkpoint (pos_min = 4333, pos_max = 4975, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 43433 | n_tokens = 819, memory_seq_rm [819, end)
slot update_slots: id  2 | task 43433 | prompt processing progress, n_tokens = 828, batch.n_tokens = 9, progress = 1.000000
slot update_slots: id  2 | task 43433 | prompt done, n_tokens = 828, batch.n_tokens = 9
slot init_sampler: id  2 | task 43433 | init sampler, took 0.13 ms, tokens: text = 828, total = 828
slot print_timing: id  2 | task 43433 | 
prompt eval time =     238.51 ms /     9 tokens (   26.50 ms per token,    37.73 tokens per second)
       eval time =    1338.89 ms /    48 tokens (   27.89 ms per token,    35.85 tokens per second)
      total time =    1577.41 ms /    57 tokens
slot      release: id  2 | task 43433 | stop processing: n_tokens = 875, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.839 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43482 | processing task, is_child = 0
slot update_slots: id  2 | task 43482 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1022
slot update_slots: id  2 | task 43482 | n_tokens = 857, memory_seq_rm [857, end)
slot update_slots: id  2 | task 43482 | prompt processing progress, n_tokens = 958, batch.n_tokens = 101, progress = 0.937378
slot update_slots: id  2 | task 43482 | n_tokens = 958, memory_seq_rm [958, end)
slot update_slots: id  2 | task 43482 | prompt processing progress, n_tokens = 1022, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 43482 | prompt done, n_tokens = 1022, batch.n_tokens = 64
slot init_sampler: id  2 | task 43482 | init sampler, took 0.16 ms, tokens: text = 1022, total = 1022
slot print_timing: id  2 | task 43482 | 
prompt eval time =     519.94 ms /   165 tokens (    3.15 ms per token,   317.34 tokens per second)
       eval time =    3682.37 ms /   130 tokens (   28.33 ms per token,    35.30 tokens per second)
      total time =    4202.31 ms /   295 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 43482 | stop processing: n_tokens = 1151, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.689 (> 0.100 thold), f_keep = 0.719
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43614 | processing task, is_child = 0
slot update_slots: id  2 | task 43614 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1202
slot update_slots: id  2 | task 43614 | n_tokens = 828, memory_seq_rm [828, end)
slot update_slots: id  2 | task 43614 | prompt processing progress, n_tokens = 1138, batch.n_tokens = 310, progress = 0.946755
slot update_slots: id  2 | task 43614 | n_tokens = 1138, memory_seq_rm [1138, end)
slot update_slots: id  2 | task 43614 | prompt processing progress, n_tokens = 1202, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 43614 | prompt done, n_tokens = 1202, batch.n_tokens = 64
slot init_sampler: id  2 | task 43614 | init sampler, took 0.19 ms, tokens: text = 1202, total = 1202
slot print_timing: id  2 | task 43614 | 
prompt eval time =     835.07 ms /   374 tokens (    2.23 ms per token,   447.87 tokens per second)
       eval time =    1183.20 ms /    42 tokens (   28.17 ms per token,    35.50 tokens per second)
      total time =    2018.27 ms /   416 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 43614 | stop processing: n_tokens = 1243, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.503 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43658 | processing task, is_child = 0
slot update_slots: id  2 | task 43658 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2431
slot update_slots: id  2 | task 43658 | n_tokens = 1223, memory_seq_rm [1223, end)
slot update_slots: id  2 | task 43658 | prompt processing progress, n_tokens = 2367, batch.n_tokens = 1144, progress = 0.973673
slot update_slots: id  2 | task 43658 | n_tokens = 2367, memory_seq_rm [2367, end)
slot update_slots: id  2 | task 43658 | prompt processing progress, n_tokens = 2431, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 43658 | prompt done, n_tokens = 2431, batch.n_tokens = 64
slot init_sampler: id  2 | task 43658 | init sampler, took 0.48 ms, tokens: text = 2431, total = 2431
slot update_slots: id  2 | task 43658 | created context checkpoint 3 of 8 (pos_min = 1724, pos_max = 2366, size = 15.078 MiB)
slot print_timing: id  2 | task 43658 | 
prompt eval time =    2124.17 ms /  1208 tokens (    1.76 ms per token,   568.69 tokens per second)
       eval time =    7476.81 ms /   250 tokens (   29.91 ms per token,    33.44 tokens per second)
      total time =    9600.98 ms /  1458 tokens
slot      release: id  2 | task 43658 | stop processing: n_tokens = 2680, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.872 (> 0.100 thold), f_keep = 0.911
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43910 | processing task, is_child = 0
slot update_slots: id  2 | task 43910 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2800
slot update_slots: id  2 | task 43910 | n_tokens = 2442, memory_seq_rm [2442, end)
slot update_slots: id  2 | task 43910 | prompt processing progress, n_tokens = 2736, batch.n_tokens = 294, progress = 0.977143
slot update_slots: id  2 | task 43910 | n_tokens = 2736, memory_seq_rm [2736, end)
slot update_slots: id  2 | task 43910 | prompt processing progress, n_tokens = 2800, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 43910 | prompt done, n_tokens = 2800, batch.n_tokens = 64
slot init_sampler: id  2 | task 43910 | init sampler, took 0.42 ms, tokens: text = 2800, total = 2800
slot update_slots: id  2 | task 43910 | created context checkpoint 4 of 8 (pos_min = 2223, pos_max = 2735, size = 12.030 MiB)
slot print_timing: id  2 | task 43910 | 
prompt eval time =     792.53 ms /   358 tokens (    2.21 ms per token,   451.72 tokens per second)
       eval time =    1293.09 ms /    43 tokens (   30.07 ms per token,    33.25 tokens per second)
      total time =    2085.62 ms /   401 tokens
slot      release: id  2 | task 43910 | stop processing: n_tokens = 2842, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.594 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43955 | processing task, is_child = 0
slot update_slots: id  2 | task 43955 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4733
slot update_slots: id  2 | task 43955 | n_tokens = 2812, memory_seq_rm [2812, end)
slot update_slots: id  2 | task 43955 | prompt processing progress, n_tokens = 4669, batch.n_tokens = 1857, progress = 0.986478
slot update_slots: id  2 | task 43955 | n_tokens = 4669, memory_seq_rm [4669, end)
slot update_slots: id  2 | task 43955 | prompt processing progress, n_tokens = 4733, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 43955 | prompt done, n_tokens = 4733, batch.n_tokens = 64
slot init_sampler: id  2 | task 43955 | init sampler, took 0.95 ms, tokens: text = 4733, total = 4733
slot update_slots: id  2 | task 43955 | created context checkpoint 5 of 8 (pos_min = 4026, pos_max = 4668, size = 15.078 MiB)
slot print_timing: id  2 | task 43955 | 
prompt eval time =    3357.72 ms /  1921 tokens (    1.75 ms per token,   572.11 tokens per second)
       eval time =    1286.13 ms /    42 tokens (   30.62 ms per token,    32.66 tokens per second)
      total time =    4643.85 ms /  1963 tokens
slot      release: id  2 | task 43955 | stop processing: n_tokens = 4774, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.700 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 43999 | processing task, is_child = 0
slot update_slots: id  2 | task 43999 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6781
slot update_slots: id  2 | task 43999 | n_tokens = 4747, memory_seq_rm [4747, end)
slot update_slots: id  2 | task 43999 | prompt processing progress, n_tokens = 6717, batch.n_tokens = 1970, progress = 0.990562
slot update_slots: id  2 | task 43999 | n_tokens = 6717, memory_seq_rm [6717, end)
slot update_slots: id  2 | task 43999 | prompt processing progress, n_tokens = 6781, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 43999 | prompt done, n_tokens = 6781, batch.n_tokens = 64
slot init_sampler: id  2 | task 43999 | init sampler, took 1.11 ms, tokens: text = 6781, total = 6781
slot update_slots: id  2 | task 43999 | created context checkpoint 6 of 8 (pos_min = 6074, pos_max = 6716, size = 15.078 MiB)
slot print_timing: id  2 | task 43999 | 
prompt eval time =    3545.12 ms /  2034 tokens (    1.74 ms per token,   573.75 tokens per second)
       eval time =    1395.14 ms /    45 tokens (   31.00 ms per token,    32.25 tokens per second)
      total time =    4940.27 ms /  2079 tokens
slot      release: id  2 | task 43999 | stop processing: n_tokens = 6825, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.773 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44046 | processing task, is_child = 0
slot update_slots: id  2 | task 44046 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8800
slot update_slots: id  2 | task 44046 | n_tokens = 6799, memory_seq_rm [6799, end)
slot update_slots: id  2 | task 44046 | prompt processing progress, n_tokens = 8736, batch.n_tokens = 1937, progress = 0.992727
slot update_slots: id  2 | task 44046 | n_tokens = 8736, memory_seq_rm [8736, end)
slot update_slots: id  2 | task 44046 | prompt processing progress, n_tokens = 8800, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44046 | prompt done, n_tokens = 8800, batch.n_tokens = 64
slot init_sampler: id  2 | task 44046 | init sampler, took 1.25 ms, tokens: text = 8800, total = 8800
slot update_slots: id  2 | task 44046 | created context checkpoint 7 of 8 (pos_min = 8093, pos_max = 8735, size = 15.078 MiB)
slot print_timing: id  2 | task 44046 | 
prompt eval time =    3556.03 ms /  2001 tokens (    1.78 ms per token,   562.71 tokens per second)
       eval time =     823.51 ms /    27 tokens (   30.50 ms per token,    32.79 tokens per second)
      total time =    4379.54 ms /  2028 tokens
slot      release: id  2 | task 44046 | stop processing: n_tokens = 8826, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.853 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44075 | processing task, is_child = 0
slot update_slots: id  2 | task 44075 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10318
slot update_slots: id  2 | task 44075 | n_tokens = 8800, memory_seq_rm [8800, end)
slot update_slots: id  2 | task 44075 | prompt processing progress, n_tokens = 10254, batch.n_tokens = 1454, progress = 0.993797
slot update_slots: id  2 | task 44075 | n_tokens = 10254, memory_seq_rm [10254, end)
slot update_slots: id  2 | task 44075 | prompt processing progress, n_tokens = 10318, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44075 | prompt done, n_tokens = 10318, batch.n_tokens = 64
slot init_sampler: id  2 | task 44075 | init sampler, took 1.99 ms, tokens: text = 10318, total = 10318
slot update_slots: id  2 | task 44075 | created context checkpoint 8 of 8 (pos_min = 9611, pos_max = 10253, size = 15.078 MiB)
slot print_timing: id  2 | task 44075 | 
prompt eval time =    2667.92 ms /  1518 tokens (    1.76 ms per token,   568.98 tokens per second)
       eval time =   20313.70 ms /   694 tokens (   29.27 ms per token,    34.16 tokens per second)
      total time =   22981.62 ms /  2212 tokens
slot      release: id  2 | task 44075 | stop processing: n_tokens = 11011, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 44771 | processing task, is_child = 0
slot update_slots: id  2 | task 44771 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11052
slot update_slots: id  2 | task 44771 | n_tokens = 10677, memory_seq_rm [10677, end)
slot update_slots: id  2 | task 44771 | prompt processing progress, n_tokens = 10988, batch.n_tokens = 311, progress = 0.994209
slot update_slots: id  2 | task 44771 | n_tokens = 10988, memory_seq_rm [10988, end)
slot update_slots: id  2 | task 44771 | prompt processing progress, n_tokens = 11052, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 44771 | prompt done, n_tokens = 11052, batch.n_tokens = 64
slot init_sampler: id  2 | task 44771 | init sampler, took 2.01 ms, tokens: text = 11052, total = 11052
slot update_slots: id  2 | task 44771 | erasing old context checkpoint (pos_min = 124, pos_max = 766, size = 15.078 MiB)
slot update_slots: id  2 | task 44771 | created context checkpoint 8 of 8 (pos_min = 10469, pos_max = 10987, size = 12.170 MiB)
slot print_timing: id  2 | task 44771 | 
prompt eval time =     797.85 ms /   375 tokens (    2.13 ms per token,   470.01 tokens per second)
       eval time =    7093.05 ms /   248 tokens (   28.60 ms per token,    34.96 tokens per second)
      total time =    7890.90 ms /   623 tokens
slot      release: id  2 | task 44771 | stop processing: n_tokens = 11299, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 45021 | processing task, is_child = 0
slot update_slots: id  2 | task 45021 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11343
slot update_slots: id  2 | task 45021 | n_tokens = 11070, memory_seq_rm [11070, end)
slot update_slots: id  2 | task 45021 | prompt processing progress, n_tokens = 11279, batch.n_tokens = 209, progress = 0.994358
slot update_slots: id  2 | task 45021 | n_tokens = 11279, memory_seq_rm [11279, end)
slot update_slots: id  2 | task 45021 | prompt processing progress, n_tokens = 11343, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 45021 | prompt done, n_tokens = 11343, batch.n_tokens = 64
slot init_sampler: id  2 | task 45021 | init sampler, took 2.27 ms, tokens: text = 11343, total = 11343
slot update_slots: id  2 | task 45021 | erasing old context checkpoint (pos_min = 575, pos_max = 1186, size = 14.351 MiB)
slot update_slots: id  2 | task 45021 | created context checkpoint 8 of 8 (pos_min = 10677, pos_max = 11278, size = 14.117 MiB)
slot print_timing: id  2 | task 45021 | 
prompt eval time =     674.02 ms /   273 tokens (    2.47 ms per token,   405.03 tokens per second)
       eval time =    9278.13 ms /   327 tokens (   28.37 ms per token,    35.24 tokens per second)
      total time =    9952.15 ms /   600 tokens
slot      release: id  2 | task 45021 | stop processing: n_tokens = 11669, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 45350 | processing task, is_child = 0
slot update_slots: id  2 | task 45350 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11713
slot update_slots: id  2 | task 45350 | n_tokens = 11356, memory_seq_rm [11356, end)
slot update_slots: id  2 | task 45350 | prompt processing progress, n_tokens = 11649, batch.n_tokens = 293, progress = 0.994536
slot update_slots: id  2 | task 45350 | n_tokens = 11649, memory_seq_rm [11649, end)
slot update_slots: id  2 | task 45350 | prompt processing progress, n_tokens = 11713, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 45350 | prompt done, n_tokens = 11713, batch.n_tokens = 64
slot init_sampler: id  2 | task 45350 | init sampler, took 2.24 ms, tokens: text = 11713, total = 11713
slot update_slots: id  2 | task 45350 | erasing old context checkpoint (pos_min = 1724, pos_max = 2366, size = 15.078 MiB)
slot update_slots: id  2 | task 45350 | created context checkpoint 8 of 8 (pos_min = 11026, pos_max = 11648, size = 14.609 MiB)
slot print_timing: id  2 | task 45350 | 
prompt eval time =     770.54 ms /   357 tokens (    2.16 ms per token,   463.31 tokens per second)
       eval time =    8010.30 ms /   280 tokens (   28.61 ms per token,    34.96 tokens per second)
      total time =    8780.84 ms /   637 tokens
slot      release: id  2 | task 45350 | stop processing: n_tokens = 11992, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 45632 | processing task, is_child = 0
slot update_slots: id  2 | task 45632 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12031
slot update_slots: id  2 | task 45632 | n_tokens = 11849, memory_seq_rm [11849, end)
slot update_slots: id  2 | task 45632 | prompt processing progress, n_tokens = 11967, batch.n_tokens = 118, progress = 0.994680
slot update_slots: id  2 | task 45632 | n_tokens = 11967, memory_seq_rm [11967, end)
slot update_slots: id  2 | task 45632 | prompt processing progress, n_tokens = 12031, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 45632 | prompt done, n_tokens = 12031, batch.n_tokens = 64
slot init_sampler: id  2 | task 45632 | init sampler, took 1.92 ms, tokens: text = 12031, total = 12031
slot update_slots: id  2 | task 45632 | erasing old context checkpoint (pos_min = 2223, pos_max = 2735, size = 12.030 MiB)
slot update_slots: id  2 | task 45632 | created context checkpoint 8 of 8 (pos_min = 11438, pos_max = 11966, size = 12.405 MiB)
slot print_timing: id  2 | task 45632 | 
prompt eval time =     609.44 ms /   182 tokens (    3.35 ms per token,   298.63 tokens per second)
       eval time =    3821.41 ms /   132 tokens (   28.95 ms per token,    34.54 tokens per second)
      total time =    4430.85 ms /   314 tokens
slot      release: id  2 | task 45632 | stop processing: n_tokens = 12162, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 45766 | processing task, is_child = 0
slot update_slots: id  2 | task 45766 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12206
slot update_slots: id  2 | task 45766 | n_tokens = 12045, memory_seq_rm [12045, end)
slot update_slots: id  2 | task 45766 | prompt processing progress, n_tokens = 12142, batch.n_tokens = 97, progress = 0.994757
slot update_slots: id  2 | task 45766 | n_tokens = 12142, memory_seq_rm [12142, end)
slot update_slots: id  2 | task 45766 | prompt processing progress, n_tokens = 12206, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 45766 | prompt done, n_tokens = 12206, batch.n_tokens = 64
slot init_sampler: id  2 | task 45766 | init sampler, took 1.72 ms, tokens: text = 12206, total = 12206
slot update_slots: id  2 | task 45766 | erasing old context checkpoint (pos_min = 4026, pos_max = 4668, size = 15.078 MiB)
slot update_slots: id  2 | task 45766 | created context checkpoint 8 of 8 (pos_min = 11633, pos_max = 12141, size = 11.936 MiB)
slot print_timing: id  2 | task 45766 | 
prompt eval time =     528.01 ms /   161 tokens (    3.28 ms per token,   304.92 tokens per second)
       eval time =    4741.38 ms /   164 tokens (   28.91 ms per token,    34.59 tokens per second)
      total time =    5269.39 ms /   325 tokens
slot      release: id  2 | task 45766 | stop processing: n_tokens = 12369, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 45932 | processing task, is_child = 0
slot update_slots: id  2 | task 45932 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12413
slot update_slots: id  2 | task 45932 | n_tokens = 12219, memory_seq_rm [12219, end)
slot update_slots: id  2 | task 45932 | prompt processing progress, n_tokens = 12349, batch.n_tokens = 130, progress = 0.994844
slot update_slots: id  2 | task 45932 | n_tokens = 12349, memory_seq_rm [12349, end)
slot update_slots: id  2 | task 45932 | prompt processing progress, n_tokens = 12413, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 45932 | prompt done, n_tokens = 12413, batch.n_tokens = 64
slot init_sampler: id  2 | task 45932 | init sampler, took 1.77 ms, tokens: text = 12413, total = 12413
slot update_slots: id  2 | task 45932 | erasing old context checkpoint (pos_min = 6074, pos_max = 6716, size = 15.078 MiB)
slot update_slots: id  2 | task 45932 | created context checkpoint 8 of 8 (pos_min = 11840, pos_max = 12348, size = 11.936 MiB)
slot print_timing: id  2 | task 45932 | 
prompt eval time =     584.97 ms /   194 tokens (    3.02 ms per token,   331.64 tokens per second)
       eval time =    7239.66 ms /   246 tokens (   29.43 ms per token,    33.98 tokens per second)
      total time =    7824.64 ms /   440 tokens
slot      release: id  2 | task 45932 | stop processing: n_tokens = 12658, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 46180 | processing task, is_child = 0
slot update_slots: id  2 | task 46180 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12696
slot update_slots: id  2 | task 46180 | n_tokens = 12446, memory_seq_rm [12446, end)
slot update_slots: id  2 | task 46180 | prompt processing progress, n_tokens = 12632, batch.n_tokens = 186, progress = 0.994959
slot update_slots: id  2 | task 46180 | n_tokens = 12632, memory_seq_rm [12632, end)
slot update_slots: id  2 | task 46180 | prompt processing progress, n_tokens = 12696, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 46180 | prompt done, n_tokens = 12696, batch.n_tokens = 64
slot init_sampler: id  2 | task 46180 | init sampler, took 1.89 ms, tokens: text = 12696, total = 12696
slot update_slots: id  2 | task 46180 | erasing old context checkpoint (pos_min = 8093, pos_max = 8735, size = 15.078 MiB)
slot update_slots: id  2 | task 46180 | created context checkpoint 8 of 8 (pos_min = 12035, pos_max = 12631, size = 13.999 MiB)
slot print_timing: id  2 | task 46180 | 
prompt eval time =     647.29 ms /   250 tokens (    2.59 ms per token,   386.23 tokens per second)
       eval time =    3887.61 ms /   132 tokens (   29.45 ms per token,    33.95 tokens per second)
      total time =    4534.90 ms /   382 tokens
slot      release: id  2 | task 46180 | stop processing: n_tokens = 12827, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 46314 | processing task, is_child = 0
slot update_slots: id  2 | task 46314 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12871
slot update_slots: id  2 | task 46314 | n_tokens = 12728, memory_seq_rm [12728, end)
slot update_slots: id  2 | task 46314 | prompt processing progress, n_tokens = 12807, batch.n_tokens = 79, progress = 0.995028
slot update_slots: id  2 | task 46314 | n_tokens = 12807, memory_seq_rm [12807, end)
slot update_slots: id  2 | task 46314 | prompt processing progress, n_tokens = 12871, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 46314 | prompt done, n_tokens = 12871, batch.n_tokens = 64
slot init_sampler: id  2 | task 46314 | init sampler, took 1.82 ms, tokens: text = 12871, total = 12871
slot update_slots: id  2 | task 46314 | erasing old context checkpoint (pos_min = 9611, pos_max = 10253, size = 15.078 MiB)
slot update_slots: id  2 | task 46314 | created context checkpoint 8 of 8 (pos_min = 12230, pos_max = 12806, size = 13.530 MiB)
slot print_timing: id  2 | task 46314 | 
prompt eval time =     491.66 ms /   143 tokens (    3.44 ms per token,   290.85 tokens per second)
       eval time =    6422.29 ms /   217 tokens (   29.60 ms per token,    33.79 tokens per second)
      total time =    6913.95 ms /   360 tokens
slot      release: id  2 | task 46314 | stop processing: n_tokens = 13087, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 46533 | processing task, is_child = 0
slot update_slots: id  2 | task 46533 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13125
slot update_slots: id  2 | task 46533 | n_tokens = 12922, memory_seq_rm [12922, end)
slot update_slots: id  2 | task 46533 | prompt processing progress, n_tokens = 13061, batch.n_tokens = 139, progress = 0.995124
slot update_slots: id  2 | task 46533 | n_tokens = 13061, memory_seq_rm [13061, end)
slot update_slots: id  2 | task 46533 | prompt processing progress, n_tokens = 13125, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 46533 | prompt done, n_tokens = 13125, batch.n_tokens = 64
slot init_sampler: id  2 | task 46533 | init sampler, took 2.15 ms, tokens: text = 13125, total = 13125
slot update_slots: id  2 | task 46533 | erasing old context checkpoint (pos_min = 10469, pos_max = 10987, size = 12.170 MiB)
slot update_slots: id  2 | task 46533 | created context checkpoint 8 of 8 (pos_min = 12446, pos_max = 13060, size = 14.421 MiB)
slot print_timing: id  2 | task 46533 | 
prompt eval time =     570.02 ms /   203 tokens (    2.81 ms per token,   356.13 tokens per second)
       eval time =    5137.89 ms /   175 tokens (   29.36 ms per token,    34.06 tokens per second)
      total time =    5707.91 ms /   378 tokens
slot      release: id  2 | task 46533 | stop processing: n_tokens = 13299, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 46710 | processing task, is_child = 0
slot update_slots: id  2 | task 46710 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13343
slot update_slots: id  2 | task 46710 | n_tokens = 13139, memory_seq_rm [13139, end)
slot update_slots: id  2 | task 46710 | prompt processing progress, n_tokens = 13279, batch.n_tokens = 140, progress = 0.995203
slot update_slots: id  2 | task 46710 | n_tokens = 13279, memory_seq_rm [13279, end)
slot update_slots: id  2 | task 46710 | prompt processing progress, n_tokens = 13343, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 46710 | prompt done, n_tokens = 13343, batch.n_tokens = 64
slot init_sampler: id  2 | task 46710 | init sampler, took 1.87 ms, tokens: text = 13343, total = 13343
slot update_slots: id  2 | task 46710 | erasing old context checkpoint (pos_min = 10677, pos_max = 11278, size = 14.117 MiB)
slot update_slots: id  2 | task 46710 | created context checkpoint 8 of 8 (pos_min = 12656, pos_max = 13278, size = 14.609 MiB)
slot print_timing: id  2 | task 46710 | 
prompt eval time =     551.09 ms /   204 tokens (    2.70 ms per token,   370.18 tokens per second)
       eval time =    4666.68 ms /   159 tokens (   29.35 ms per token,    34.07 tokens per second)
      total time =    5217.77 ms /   363 tokens
slot      release: id  2 | task 46710 | stop processing: n_tokens = 13501, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.869 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 46871 | processing task, is_child = 0
slot update_slots: id  2 | task 46871 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15506
slot update_slots: id  2 | task 46871 | n_tokens = 13475, memory_seq_rm [13475, end)
slot update_slots: id  2 | task 46871 | prompt processing progress, n_tokens = 15442, batch.n_tokens = 1967, progress = 0.995873
slot update_slots: id  2 | task 46871 | n_tokens = 15442, memory_seq_rm [15442, end)
slot update_slots: id  2 | task 46871 | prompt processing progress, n_tokens = 15506, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 46871 | prompt done, n_tokens = 15506, batch.n_tokens = 64
slot init_sampler: id  2 | task 46871 | init sampler, took 2.32 ms, tokens: text = 15506, total = 15506
slot update_slots: id  2 | task 46871 | erasing old context checkpoint (pos_min = 11026, pos_max = 11648, size = 14.609 MiB)
slot update_slots: id  2 | task 46871 | created context checkpoint 8 of 8 (pos_min = 14799, pos_max = 15441, size = 15.078 MiB)
slot print_timing: id  2 | task 46871 | 
prompt eval time =    3378.41 ms /  2031 tokens (    1.66 ms per token,   601.17 tokens per second)
       eval time =    1073.47 ms /    37 tokens (   29.01 ms per token,    34.47 tokens per second)
      total time =    4451.88 ms /  2068 tokens
slot      release: id  2 | task 46871 | stop processing: n_tokens = 15542, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.901 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 46910 | processing task, is_child = 0
slot update_slots: id  2 | task 46910 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17224
slot update_slots: id  2 | task 46910 | n_tokens = 15516, memory_seq_rm [15516, end)
slot update_slots: id  2 | task 46910 | prompt processing progress, n_tokens = 17160, batch.n_tokens = 1644, progress = 0.996284
slot update_slots: id  2 | task 46910 | n_tokens = 17160, memory_seq_rm [17160, end)
slot update_slots: id  2 | task 46910 | prompt processing progress, n_tokens = 17224, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 46910 | prompt done, n_tokens = 17224, batch.n_tokens = 64
slot init_sampler: id  2 | task 46910 | init sampler, took 2.42 ms, tokens: text = 17224, total = 17224
slot update_slots: id  2 | task 46910 | erasing old context checkpoint (pos_min = 11438, pos_max = 11966, size = 12.405 MiB)
slot update_slots: id  2 | task 46910 | created context checkpoint 8 of 8 (pos_min = 16517, pos_max = 17159, size = 15.078 MiB)
slot print_timing: id  2 | task 46910 | 
prompt eval time =    3041.74 ms /  1708 tokens (    1.78 ms per token,   561.52 tokens per second)
       eval time =    6683.66 ms /   226 tokens (   29.57 ms per token,    33.81 tokens per second)
      total time =    9725.40 ms /  1934 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 46910 | stop processing: n_tokens = 17449, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47138 | processing task, is_child = 0
slot update_slots: id  2 | task 47138 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17486
slot update_slots: id  2 | task 47138 | n_tokens = 17292, memory_seq_rm [17292, end)
slot update_slots: id  2 | task 47138 | prompt processing progress, n_tokens = 17422, batch.n_tokens = 130, progress = 0.996340
slot update_slots: id  2 | task 47138 | n_tokens = 17422, memory_seq_rm [17422, end)
slot update_slots: id  2 | task 47138 | prompt processing progress, n_tokens = 17486, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47138 | prompt done, n_tokens = 17486, batch.n_tokens = 64
slot init_sampler: id  2 | task 47138 | init sampler, took 3.36 ms, tokens: text = 17486, total = 17486
slot update_slots: id  2 | task 47138 | erasing old context checkpoint (pos_min = 11633, pos_max = 12141, size = 11.936 MiB)
slot update_slots: id  2 | task 47138 | created context checkpoint 8 of 8 (pos_min = 16806, pos_max = 17421, size = 14.445 MiB)
slot print_timing: id  2 | task 47138 | 
prompt eval time =     628.12 ms /   194 tokens (    3.24 ms per token,   308.86 tokens per second)
       eval time =    1262.48 ms /    43 tokens (   29.36 ms per token,    34.06 tokens per second)
      total time =    1890.60 ms /   237 tokens
slot      release: id  2 | task 47138 | stop processing: n_tokens = 17528, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47183 | processing task, is_child = 0
slot update_slots: id  2 | task 47183 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17895
slot update_slots: id  2 | task 47183 | n_tokens = 17502, memory_seq_rm [17502, end)
slot update_slots: id  2 | task 47183 | prompt processing progress, n_tokens = 17831, batch.n_tokens = 329, progress = 0.996424
slot update_slots: id  2 | task 47183 | n_tokens = 17831, memory_seq_rm [17831, end)
slot update_slots: id  2 | task 47183 | prompt processing progress, n_tokens = 17895, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47183 | prompt done, n_tokens = 17895, batch.n_tokens = 64
slot init_sampler: id  2 | task 47183 | init sampler, took 2.60 ms, tokens: text = 17895, total = 17895
slot update_slots: id  2 | task 47183 | erasing old context checkpoint (pos_min = 11840, pos_max = 12348, size = 11.936 MiB)
slot update_slots: id  2 | task 47183 | created context checkpoint 8 of 8 (pos_min = 17188, pos_max = 17830, size = 15.078 MiB)
slot print_timing: id  2 | task 47183 | 
prompt eval time =     832.18 ms /   393 tokens (    2.12 ms per token,   472.25 tokens per second)
       eval time =    2943.84 ms /   100 tokens (   29.44 ms per token,    33.97 tokens per second)
      total time =    3776.02 ms /   493 tokens
slot      release: id  2 | task 47183 | stop processing: n_tokens = 17994, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47285 | processing task, is_child = 0
slot update_slots: id  2 | task 47285 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18295
slot update_slots: id  2 | task 47285 | n_tokens = 17968, memory_seq_rm [17968, end)
slot update_slots: id  2 | task 47285 | prompt processing progress, n_tokens = 18231, batch.n_tokens = 263, progress = 0.996502
slot update_slots: id  2 | task 47285 | n_tokens = 18231, memory_seq_rm [18231, end)
slot update_slots: id  2 | task 47285 | prompt processing progress, n_tokens = 18295, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47285 | prompt done, n_tokens = 18295, batch.n_tokens = 64
slot init_sampler: id  2 | task 47285 | init sampler, took 2.63 ms, tokens: text = 18295, total = 18295
slot update_slots: id  2 | task 47285 | erasing old context checkpoint (pos_min = 12035, pos_max = 12631, size = 13.999 MiB)
slot update_slots: id  2 | task 47285 | created context checkpoint 8 of 8 (pos_min = 17588, pos_max = 18230, size = 15.078 MiB)
slot print_timing: id  2 | task 47285 | 
prompt eval time =     747.90 ms /   327 tokens (    2.29 ms per token,   437.22 tokens per second)
       eval time =    1840.70 ms /    63 tokens (   29.22 ms per token,    34.23 tokens per second)
      total time =    2588.60 ms /   390 tokens
slot      release: id  2 | task 47285 | stop processing: n_tokens = 18357, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47350 | processing task, is_child = 0
slot update_slots: id  2 | task 47350 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18763
slot update_slots: id  2 | task 47350 | n_tokens = 18331, memory_seq_rm [18331, end)
slot update_slots: id  2 | task 47350 | prompt processing progress, n_tokens = 18699, batch.n_tokens = 368, progress = 0.996589
slot update_slots: id  2 | task 47350 | n_tokens = 18699, memory_seq_rm [18699, end)
slot update_slots: id  2 | task 47350 | prompt processing progress, n_tokens = 18763, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47350 | prompt done, n_tokens = 18763, batch.n_tokens = 64
slot init_sampler: id  2 | task 47350 | init sampler, took 2.65 ms, tokens: text = 18763, total = 18763
slot update_slots: id  2 | task 47350 | erasing old context checkpoint (pos_min = 12230, pos_max = 12806, size = 13.530 MiB)
slot update_slots: id  2 | task 47350 | created context checkpoint 8 of 8 (pos_min = 18056, pos_max = 18698, size = 15.078 MiB)
slot print_timing: id  2 | task 47350 | 
prompt eval time =     923.12 ms /   432 tokens (    2.14 ms per token,   467.98 tokens per second)
       eval time =    1047.25 ms /    35 tokens (   29.92 ms per token,    33.42 tokens per second)
      total time =    1970.37 ms /   467 tokens
slot      release: id  2 | task 47350 | stop processing: n_tokens = 18797, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47387 | processing task, is_child = 0
slot update_slots: id  2 | task 47387 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19092
slot update_slots: id  2 | task 47387 | n_tokens = 18771, memory_seq_rm [18771, end)
slot update_slots: id  2 | task 47387 | prompt processing progress, n_tokens = 19028, batch.n_tokens = 257, progress = 0.996648
slot update_slots: id  2 | task 47387 | n_tokens = 19028, memory_seq_rm [19028, end)
slot update_slots: id  2 | task 47387 | prompt processing progress, n_tokens = 19092, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47387 | prompt done, n_tokens = 19092, batch.n_tokens = 64
slot init_sampler: id  2 | task 47387 | init sampler, took 3.63 ms, tokens: text = 19092, total = 19092
slot update_slots: id  2 | task 47387 | erasing old context checkpoint (pos_min = 12446, pos_max = 13060, size = 14.421 MiB)
slot update_slots: id  2 | task 47387 | created context checkpoint 8 of 8 (pos_min = 18385, pos_max = 19027, size = 15.078 MiB)
slot print_timing: id  2 | task 47387 | 
prompt eval time =     740.77 ms /   321 tokens (    2.31 ms per token,   433.33 tokens per second)
       eval time =    1496.45 ms /    51 tokens (   29.34 ms per token,    34.08 tokens per second)
      total time =    2237.23 ms /   372 tokens
slot      release: id  2 | task 47387 | stop processing: n_tokens = 19142, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47440 | processing task, is_child = 0
slot update_slots: id  2 | task 47440 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19482
slot update_slots: id  2 | task 47440 | n_tokens = 19116, memory_seq_rm [19116, end)
slot update_slots: id  2 | task 47440 | prompt processing progress, n_tokens = 19418, batch.n_tokens = 302, progress = 0.996715
slot update_slots: id  2 | task 47440 | n_tokens = 19418, memory_seq_rm [19418, end)
slot update_slots: id  2 | task 47440 | prompt processing progress, n_tokens = 19482, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47440 | prompt done, n_tokens = 19482, batch.n_tokens = 64
slot init_sampler: id  2 | task 47440 | init sampler, took 2.74 ms, tokens: text = 19482, total = 19482
slot update_slots: id  2 | task 47440 | erasing old context checkpoint (pos_min = 12656, pos_max = 13278, size = 14.609 MiB)
slot update_slots: id  2 | task 47440 | created context checkpoint 8 of 8 (pos_min = 18775, pos_max = 19417, size = 15.078 MiB)
slot print_timing: id  2 | task 47440 | 
prompt eval time =     819.12 ms /   366 tokens (    2.24 ms per token,   446.82 tokens per second)
       eval time =    1054.15 ms /    36 tokens (   29.28 ms per token,    34.15 tokens per second)
      total time =    1873.27 ms /   402 tokens
slot      release: id  2 | task 47440 | stop processing: n_tokens = 19517, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47478 | processing task, is_child = 0
slot update_slots: id  2 | task 47478 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19818
slot update_slots: id  2 | task 47478 | n_tokens = 19491, memory_seq_rm [19491, end)
slot update_slots: id  2 | task 47478 | prompt processing progress, n_tokens = 19754, batch.n_tokens = 263, progress = 0.996771
slot update_slots: id  2 | task 47478 | n_tokens = 19754, memory_seq_rm [19754, end)
slot update_slots: id  2 | task 47478 | prompt processing progress, n_tokens = 19818, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47478 | prompt done, n_tokens = 19818, batch.n_tokens = 64
slot init_sampler: id  2 | task 47478 | init sampler, took 2.84 ms, tokens: text = 19818, total = 19818
slot update_slots: id  2 | task 47478 | erasing old context checkpoint (pos_min = 14799, pos_max = 15441, size = 15.078 MiB)
slot update_slots: id  2 | task 47478 | created context checkpoint 8 of 8 (pos_min = 19111, pos_max = 19753, size = 15.078 MiB)
slot print_timing: id  2 | task 47478 | 
prompt eval time =     753.05 ms /   327 tokens (    2.30 ms per token,   434.23 tokens per second)
       eval time =    1174.60 ms /    40 tokens (   29.36 ms per token,    34.05 tokens per second)
      total time =    1927.65 ms /   367 tokens
slot      release: id  2 | task 47478 | stop processing: n_tokens = 19857, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47520 | processing task, is_child = 0
slot update_slots: id  2 | task 47520 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 20122
slot update_slots: id  2 | task 47520 | n_tokens = 19835, memory_seq_rm [19835, end)
slot update_slots: id  2 | task 47520 | prompt processing progress, n_tokens = 20058, batch.n_tokens = 223, progress = 0.996819
slot update_slots: id  2 | task 47520 | n_tokens = 20058, memory_seq_rm [20058, end)
slot update_slots: id  2 | task 47520 | prompt processing progress, n_tokens = 20122, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47520 | prompt done, n_tokens = 20122, batch.n_tokens = 64
slot init_sampler: id  2 | task 47520 | init sampler, took 2.88 ms, tokens: text = 20122, total = 20122
slot update_slots: id  2 | task 47520 | erasing old context checkpoint (pos_min = 16517, pos_max = 17159, size = 15.078 MiB)
slot update_slots: id  2 | task 47520 | created context checkpoint 8 of 8 (pos_min = 19415, pos_max = 20057, size = 15.078 MiB)
slot print_timing: id  2 | task 47520 | 
prompt eval time =     732.01 ms /   287 tokens (    2.55 ms per token,   392.07 tokens per second)
       eval time =    6561.73 ms /   220 tokens (   29.83 ms per token,    33.53 tokens per second)
      total time =    7293.74 ms /   507 tokens
slot      release: id  2 | task 47520 | stop processing: n_tokens = 20341, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.040
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 20341, total state size = 492.053 MiB
srv          load:  - looking for better prompt, base f_keep = 0.040, sim = 0.992
srv        update:  - cache state: 19 prompts, 6529.776 MiB (limits: 8192.000 MiB, 56064 tokens, 246152 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv        update:    - prompt 0x593d953daa30:    9065 tokens, checkpoints:  8,   341.091 MiB
srv        update:    - prompt 0x593db7e3f750:    3739 tokens, checkpoints:  3,   147.988 MiB
srv        update:    - prompt 0x593d95120cb0:   10131 tokens, checkpoints:  8,   372.067 MiB
srv        update:    - prompt 0x593dc061f040:    1775 tokens, checkpoints:  3,    97.549 MiB
srv        update:    - prompt 0x593db7f29c60:    5195 tokens, checkpoints:  5,   211.558 MiB
srv        update:    - prompt 0x593d95dc86d0:    5835 tokens, checkpoints:  5,   225.862 MiB
srv        update:    - prompt 0x593d962d8bb0:    5298 tokens, checkpoints:  6,   226.801 MiB
srv        update:    - prompt 0x593dedc488f0:   20341 tokens, checkpoints:  8,   612.044 MiB
srv  get_availabl: prompt cache update took 644.92 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47742 | processing task, is_child = 0
slot update_slots: id  2 | task 47742 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 826
slot update_slots: id  2 | task 47742 | n_past = 819, slot.prompt.tokens.size() = 20341, seq_id = 2, pos_min = 19698, n_swa = 128
slot update_slots: id  2 | task 47742 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 47742 | erased invalidated context checkpoint (pos_min = 16806, pos_max = 17421, n_swa = 128, size = 14.445 MiB)
slot update_slots: id  2 | task 47742 | erased invalidated context checkpoint (pos_min = 17188, pos_max = 17830, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 47742 | erased invalidated context checkpoint (pos_min = 17588, pos_max = 18230, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 47742 | erased invalidated context checkpoint (pos_min = 18056, pos_max = 18698, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 47742 | erased invalidated context checkpoint (pos_min = 18385, pos_max = 19027, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 47742 | erased invalidated context checkpoint (pos_min = 18775, pos_max = 19417, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 47742 | erased invalidated context checkpoint (pos_min = 19111, pos_max = 19753, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 47742 | erased invalidated context checkpoint (pos_min = 19415, pos_max = 20057, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 47742 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 47742 | prompt processing progress, n_tokens = 762, batch.n_tokens = 762, progress = 0.922518
slot update_slots: id  2 | task 47742 | n_tokens = 762, memory_seq_rm [762, end)
slot update_slots: id  2 | task 47742 | prompt processing progress, n_tokens = 826, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47742 | prompt done, n_tokens = 826, batch.n_tokens = 64
slot init_sampler: id  2 | task 47742 | init sampler, took 0.13 ms, tokens: text = 826, total = 826
slot update_slots: id  2 | task 47742 | created context checkpoint 1 of 8 (pos_min = 119, pos_max = 761, size = 15.078 MiB)
slot print_timing: id  2 | task 47742 | 
prompt eval time =    1558.87 ms /   826 tokens (    1.89 ms per token,   529.87 tokens per second)
       eval time =    1618.73 ms /    58 tokens (   27.91 ms per token,    35.83 tokens per second)
      total time =    3177.60 ms /   884 tokens
slot      release: id  2 | task 47742 | stop processing: n_tokens = 883, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.838 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47802 | processing task, is_child = 0
slot update_slots: id  2 | task 47802 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1031
slot update_slots: id  2 | task 47802 | n_tokens = 864, memory_seq_rm [864, end)
slot update_slots: id  2 | task 47802 | prompt processing progress, n_tokens = 967, batch.n_tokens = 103, progress = 0.937924
slot update_slots: id  2 | task 47802 | n_tokens = 967, memory_seq_rm [967, end)
slot update_slots: id  2 | task 47802 | prompt processing progress, n_tokens = 1031, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47802 | prompt done, n_tokens = 1031, batch.n_tokens = 64
slot init_sampler: id  2 | task 47802 | init sampler, took 0.15 ms, tokens: text = 1031, total = 1031
slot update_slots: id  2 | task 47802 | created context checkpoint 2 of 8 (pos_min = 324, pos_max = 966, size = 15.078 MiB)
slot print_timing: id  2 | task 47802 | 
prompt eval time =     529.21 ms /   167 tokens (    3.17 ms per token,   315.57 tokens per second)
       eval time =    3749.09 ms /   132 tokens (   28.40 ms per token,    35.21 tokens per second)
      total time =    4278.30 ms /   299 tokens
slot      release: id  2 | task 47802 | stop processing: n_tokens = 1162, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.651 (> 0.100 thold), f_keep = 0.705
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47936 | processing task, is_child = 0
slot update_slots: id  2 | task 47936 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1259
slot update_slots: id  2 | task 47936 | n_tokens = 819, memory_seq_rm [819, end)
slot update_slots: id  2 | task 47936 | prompt processing progress, n_tokens = 1195, batch.n_tokens = 376, progress = 0.949166
slot update_slots: id  2 | task 47936 | n_tokens = 1195, memory_seq_rm [1195, end)
slot update_slots: id  2 | task 47936 | prompt processing progress, n_tokens = 1259, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47936 | prompt done, n_tokens = 1259, batch.n_tokens = 64
slot init_sampler: id  2 | task 47936 | init sampler, took 0.24 ms, tokens: text = 1259, total = 1259
slot update_slots: id  2 | task 47936 | created context checkpoint 3 of 8 (pos_min = 552, pos_max = 1194, size = 15.078 MiB)
slot print_timing: id  2 | task 47936 | 
prompt eval time =     933.51 ms /   440 tokens (    2.12 ms per token,   471.34 tokens per second)
       eval time =    1152.00 ms /    39 tokens (   29.54 ms per token,    33.85 tokens per second)
      total time =    2085.51 ms /   479 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 47936 | stop processing: n_tokens = 1297, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.513 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 47977 | processing task, is_child = 0
slot update_slots: id  2 | task 47977 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2486
slot update_slots: id  2 | task 47977 | n_tokens = 1275, memory_seq_rm [1275, end)
slot update_slots: id  2 | task 47977 | prompt processing progress, n_tokens = 2422, batch.n_tokens = 1147, progress = 0.974256
slot update_slots: id  2 | task 47977 | n_tokens = 2422, memory_seq_rm [2422, end)
slot update_slots: id  2 | task 47977 | prompt processing progress, n_tokens = 2486, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 47977 | prompt done, n_tokens = 2486, batch.n_tokens = 64
slot init_sampler: id  2 | task 47977 | init sampler, took 0.35 ms, tokens: text = 2486, total = 2486
slot update_slots: id  2 | task 47977 | created context checkpoint 4 of 8 (pos_min = 1899, pos_max = 2421, size = 12.264 MiB)
slot print_timing: id  2 | task 47977 | 
prompt eval time =    2208.11 ms /  1211 tokens (    1.82 ms per token,   548.43 tokens per second)
       eval time =    1862.28 ms /    64 tokens (   29.10 ms per token,    34.37 tokens per second)
      total time =    4070.39 ms /  1275 tokens
slot      release: id  2 | task 47977 | stop processing: n_tokens = 2549, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 48043 | processing task, is_child = 0
slot update_slots: id  2 | task 48043 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2585
slot update_slots: id  2 | task 48043 | n_tokens = 2496, memory_seq_rm [2496, end)
slot update_slots: id  2 | task 48043 | prompt processing progress, n_tokens = 2521, batch.n_tokens = 25, progress = 0.975242
slot update_slots: id  2 | task 48043 | n_tokens = 2521, memory_seq_rm [2521, end)
slot update_slots: id  2 | task 48043 | prompt processing progress, n_tokens = 2585, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 48043 | prompt done, n_tokens = 2585, batch.n_tokens = 64
slot init_sampler: id  2 | task 48043 | init sampler, took 0.38 ms, tokens: text = 2585, total = 2585
slot update_slots: id  2 | task 48043 | created context checkpoint 5 of 8 (pos_min = 2026, pos_max = 2520, size = 11.608 MiB)
slot print_timing: id  2 | task 48043 | 
prompt eval time =     388.69 ms /    89 tokens (    4.37 ms per token,   228.97 tokens per second)
       eval time =    1206.46 ms /    41 tokens (   29.43 ms per token,    33.98 tokens per second)
      total time =    1595.15 ms /   130 tokens
slot      release: id  2 | task 48043 | stop processing: n_tokens = 2625, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.560 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 48086 | processing task, is_child = 0
slot update_slots: id  2 | task 48086 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4632
slot update_slots: id  2 | task 48086 | n_tokens = 2595, memory_seq_rm [2595, end)
slot update_slots: id  2 | task 48086 | prompt processing progress, n_tokens = 4568, batch.n_tokens = 1973, progress = 0.986183
slot update_slots: id  2 | task 48086 | n_tokens = 4568, memory_seq_rm [4568, end)
slot update_slots: id  2 | task 48086 | prompt processing progress, n_tokens = 4632, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 48086 | prompt done, n_tokens = 4632, batch.n_tokens = 64
slot init_sampler: id  2 | task 48086 | init sampler, took 0.67 ms, tokens: text = 4632, total = 4632
slot update_slots: id  2 | task 48086 | created context checkpoint 6 of 8 (pos_min = 3925, pos_max = 4567, size = 15.078 MiB)
slot print_timing: id  2 | task 48086 | 
prompt eval time =    3427.08 ms /  2037 tokens (    1.68 ms per token,   594.38 tokens per second)
       eval time =    1054.09 ms /    35 tokens (   30.12 ms per token,    33.20 tokens per second)
      total time =    4481.17 ms /  2072 tokens
slot      release: id  2 | task 48086 | stop processing: n_tokens = 4666, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.731 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 48123 | processing task, is_child = 0
slot update_slots: id  2 | task 48123 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6348
slot update_slots: id  2 | task 48123 | n_tokens = 4640, memory_seq_rm [4640, end)
slot update_slots: id  2 | task 48123 | prompt processing progress, n_tokens = 6284, batch.n_tokens = 1644, progress = 0.989918
slot update_slots: id  2 | task 48123 | n_tokens = 6284, memory_seq_rm [6284, end)
slot update_slots: id  2 | task 48123 | prompt processing progress, n_tokens = 6348, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 48123 | prompt done, n_tokens = 6348, batch.n_tokens = 64
slot init_sampler: id  2 | task 48123 | init sampler, took 1.31 ms, tokens: text = 6348, total = 6348
slot update_slots: id  2 | task 48123 | created context checkpoint 7 of 8 (pos_min = 5641, pos_max = 6283, size = 15.078 MiB)
slot print_timing: id  2 | task 48123 | 
prompt eval time =    3119.43 ms /  1708 tokens (    1.83 ms per token,   547.54 tokens per second)
       eval time =    7518.36 ms /   245 tokens (   30.69 ms per token,    32.59 tokens per second)
      total time =   10637.79 ms /  1953 tokens
slot      release: id  2 | task 48123 | stop processing: n_tokens = 6592, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.773 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 48370 | processing task, is_child = 0
slot update_slots: id  2 | task 48370 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8486
slot update_slots: id  2 | task 48370 | n_tokens = 6561, memory_seq_rm [6561, end)
slot update_slots: id  2 | task 48370 | prompt processing progress, n_tokens = 8422, batch.n_tokens = 1861, progress = 0.992458
slot update_slots: id  2 | task 48370 | n_tokens = 8422, memory_seq_rm [8422, end)
slot update_slots: id  2 | task 48370 | prompt processing progress, n_tokens = 8486, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 48370 | prompt done, n_tokens = 8486, batch.n_tokens = 64
slot init_sampler: id  2 | task 48370 | init sampler, took 1.58 ms, tokens: text = 8486, total = 8486
slot update_slots: id  2 | task 48370 | created context checkpoint 8 of 8 (pos_min = 7779, pos_max = 8421, size = 15.078 MiB)
slot print_timing: id  2 | task 48370 | 
prompt eval time =    3404.23 ms /  1925 tokens (    1.77 ms per token,   565.47 tokens per second)
       eval time =    6376.55 ms /   210 tokens (   30.36 ms per token,    32.93 tokens per second)
      total time =    9780.78 ms /  2135 tokens
slot      release: id  2 | task 48370 | stop processing: n_tokens = 8695, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.946 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 48582 | processing task, is_child = 0
slot update_slots: id  2 | task 48582 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9161
slot update_slots: id  2 | task 48582 | n_tokens = 8662, memory_seq_rm [8662, end)
slot update_slots: id  2 | task 48582 | prompt processing progress, n_tokens = 9097, batch.n_tokens = 435, progress = 0.993014
slot update_slots: id  2 | task 48582 | n_tokens = 9097, memory_seq_rm [9097, end)
slot update_slots: id  2 | task 48582 | prompt processing progress, n_tokens = 9161, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 48582 | prompt done, n_tokens = 9161, batch.n_tokens = 64
slot init_sampler: id  2 | task 48582 | init sampler, took 1.30 ms, tokens: text = 9161, total = 9161
slot update_slots: id  2 | task 48582 | erasing old context checkpoint (pos_min = 119, pos_max = 761, size = 15.078 MiB)
slot update_slots: id  2 | task 48582 | created context checkpoint 8 of 8 (pos_min = 8454, pos_max = 9096, size = 15.078 MiB)
slot print_timing: id  2 | task 48582 | 
prompt eval time =     979.55 ms /   499 tokens (    1.96 ms per token,   509.42 tokens per second)
       eval time =   14481.59 ms /   496 tokens (   29.20 ms per token,    34.25 tokens per second)
      total time =   15461.14 ms /   995 tokens
slot      release: id  2 | task 48582 | stop processing: n_tokens = 9656, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 49080 | processing task, is_child = 0
slot update_slots: id  2 | task 49080 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9700
slot update_slots: id  2 | task 49080 | n_tokens = 9474, memory_seq_rm [9474, end)
slot update_slots: id  2 | task 49080 | prompt processing progress, n_tokens = 9636, batch.n_tokens = 162, progress = 0.993402
slot update_slots: id  2 | task 49080 | n_tokens = 9636, memory_seq_rm [9636, end)
slot update_slots: id  2 | task 49080 | prompt processing progress, n_tokens = 9700, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 49080 | prompt done, n_tokens = 9700, batch.n_tokens = 64
slot init_sampler: id  2 | task 49080 | init sampler, took 1.38 ms, tokens: text = 9700, total = 9700
slot update_slots: id  2 | task 49080 | erasing old context checkpoint (pos_min = 324, pos_max = 966, size = 15.078 MiB)
slot update_slots: id  2 | task 49080 | created context checkpoint 8 of 8 (pos_min = 9013, pos_max = 9635, size = 14.609 MiB)
slot print_timing: id  2 | task 49080 | 
prompt eval time =     606.65 ms /   226 tokens (    2.68 ms per token,   372.54 tokens per second)
       eval time =    5384.12 ms /   188 tokens (   28.64 ms per token,    34.92 tokens per second)
      total time =    5990.77 ms /   414 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 49080 | stop processing: n_tokens = 9887, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 49270 | processing task, is_child = 0
slot update_slots: id  2 | task 49270 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9931
slot update_slots: id  2 | task 49270 | n_tokens = 9711, memory_seq_rm [9711, end)
slot update_slots: id  2 | task 49270 | prompt processing progress, n_tokens = 9867, batch.n_tokens = 156, progress = 0.993556
slot update_slots: id  2 | task 49270 | n_tokens = 9867, memory_seq_rm [9867, end)
slot update_slots: id  2 | task 49270 | prompt processing progress, n_tokens = 9931, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 49270 | prompt done, n_tokens = 9931, batch.n_tokens = 64
slot init_sampler: id  2 | task 49270 | init sampler, took 2.04 ms, tokens: text = 9931, total = 9931
slot update_slots: id  2 | task 49270 | erasing old context checkpoint (pos_min = 552, pos_max = 1194, size = 15.078 MiB)
slot update_slots: id  2 | task 49270 | created context checkpoint 8 of 8 (pos_min = 9244, pos_max = 9866, size = 14.609 MiB)
slot print_timing: id  2 | task 49270 | 
prompt eval time =     566.64 ms /   220 tokens (    2.58 ms per token,   388.25 tokens per second)
       eval time =    8665.87 ms /   305 tokens (   28.41 ms per token,    35.20 tokens per second)
      total time =    9232.51 ms /   525 tokens
slot      release: id  2 | task 49270 | stop processing: n_tokens = 10235, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.967 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 49577 | processing task, is_child = 0
slot update_slots: id  2 | task 49577 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10279
slot update_slots: id  2 | task 49577 | n_tokens = 9941, memory_seq_rm [9941, end)
slot update_slots: id  2 | task 49577 | prompt processing progress, n_tokens = 10215, batch.n_tokens = 274, progress = 0.993774
slot update_slots: id  2 | task 49577 | n_tokens = 10215, memory_seq_rm [10215, end)
slot update_slots: id  2 | task 49577 | prompt processing progress, n_tokens = 10279, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 49577 | prompt done, n_tokens = 10279, batch.n_tokens = 64
slot init_sampler: id  2 | task 49577 | init sampler, took 1.45 ms, tokens: text = 10279, total = 10279
slot update_slots: id  2 | task 49577 | erasing old context checkpoint (pos_min = 1899, pos_max = 2421, size = 12.264 MiB)
slot update_slots: id  2 | task 49577 | created context checkpoint 8 of 8 (pos_min = 9661, pos_max = 10214, size = 12.991 MiB)
slot print_timing: id  2 | task 49577 | 
prompt eval time =     732.05 ms /   338 tokens (    2.17 ms per token,   461.72 tokens per second)
       eval time =    8908.12 ms /   311 tokens (   28.64 ms per token,    34.91 tokens per second)
      total time =    9640.17 ms /   649 tokens
slot      release: id  2 | task 49577 | stop processing: n_tokens = 10589, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.973
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 49890 | processing task, is_child = 0
slot update_slots: id  2 | task 49890 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10629
slot update_slots: id  2 | task 49890 | n_tokens = 10307, memory_seq_rm [10307, end)
slot update_slots: id  2 | task 49890 | prompt processing progress, n_tokens = 10565, batch.n_tokens = 258, progress = 0.993979
slot update_slots: id  2 | task 49890 | n_tokens = 10565, memory_seq_rm [10565, end)
slot update_slots: id  2 | task 49890 | prompt processing progress, n_tokens = 10629, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 49890 | prompt done, n_tokens = 10629, batch.n_tokens = 64
slot init_sampler: id  2 | task 49890 | init sampler, took 1.58 ms, tokens: text = 10629, total = 10629
slot update_slots: id  2 | task 49890 | erasing old context checkpoint (pos_min = 2026, pos_max = 2520, size = 11.608 MiB)
slot update_slots: id  2 | task 49890 | created context checkpoint 8 of 8 (pos_min = 10180, pos_max = 10564, size = 9.028 MiB)
slot print_timing: id  2 | task 49890 | 
prompt eval time =     691.56 ms /   322 tokens (    2.15 ms per token,   465.62 tokens per second)
       eval time =    5086.58 ms /   177 tokens (   28.74 ms per token,    34.80 tokens per second)
      total time =    5778.13 ms /   499 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 49890 | stop processing: n_tokens = 10805, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 50069 | processing task, is_child = 0
slot update_slots: id  2 | task 50069 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10845
slot update_slots: id  2 | task 50069 | n_tokens = 10641, memory_seq_rm [10641, end)
slot update_slots: id  2 | task 50069 | prompt processing progress, n_tokens = 10781, batch.n_tokens = 140, progress = 0.994099
slot update_slots: id  2 | task 50069 | n_tokens = 10781, memory_seq_rm [10781, end)
slot update_slots: id  2 | task 50069 | prompt processing progress, n_tokens = 10845, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 50069 | prompt done, n_tokens = 10845, batch.n_tokens = 64
slot init_sampler: id  2 | task 50069 | init sampler, took 2.03 ms, tokens: text = 10845, total = 10845
slot update_slots: id  2 | task 50069 | erasing old context checkpoint (pos_min = 3925, pos_max = 4567, size = 15.078 MiB)
slot update_slots: id  2 | task 50069 | created context checkpoint 8 of 8 (pos_min = 10180, pos_max = 10780, size = 14.093 MiB)
slot print_timing: id  2 | task 50069 | 
prompt eval time =     547.96 ms /   204 tokens (    2.69 ms per token,   372.29 tokens per second)
       eval time =    9229.54 ms /   319 tokens (   28.93 ms per token,    34.56 tokens per second)
      total time =    9777.50 ms /   523 tokens
slot      release: id  2 | task 50069 | stop processing: n_tokens = 11163, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 50390 | processing task, is_child = 0
slot update_slots: id  2 | task 50390 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11201
slot update_slots: id  2 | task 50390 | n_tokens = 10852, memory_seq_rm [10852, end)
slot update_slots: id  2 | task 50390 | prompt processing progress, n_tokens = 11137, batch.n_tokens = 285, progress = 0.994286
slot update_slots: id  2 | task 50390 | n_tokens = 11137, memory_seq_rm [11137, end)
slot update_slots: id  2 | task 50390 | prompt processing progress, n_tokens = 11201, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 50390 | prompt done, n_tokens = 11201, batch.n_tokens = 64
slot init_sampler: id  2 | task 50390 | init sampler, took 1.60 ms, tokens: text = 11201, total = 11201
slot update_slots: id  2 | task 50390 | erasing old context checkpoint (pos_min = 5641, pos_max = 6283, size = 15.078 MiB)
slot update_slots: id  2 | task 50390 | created context checkpoint 8 of 8 (pos_min = 10546, pos_max = 11136, size = 13.859 MiB)
slot print_timing: id  2 | task 50390 | 
prompt eval time =     766.56 ms /   349 tokens (    2.20 ms per token,   455.28 tokens per second)
       eval time =    5095.87 ms /   174 tokens (   29.29 ms per token,    34.15 tokens per second)
      total time =    5862.43 ms /   523 tokens
slot      release: id  2 | task 50390 | stop processing: n_tokens = 11374, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 50566 | processing task, is_child = 0
slot update_slots: id  2 | task 50566 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11418
slot update_slots: id  2 | task 50566 | n_tokens = 11218, memory_seq_rm [11218, end)
slot update_slots: id  2 | task 50566 | prompt processing progress, n_tokens = 11354, batch.n_tokens = 136, progress = 0.994395
slot update_slots: id  2 | task 50566 | n_tokens = 11354, memory_seq_rm [11354, end)
slot update_slots: id  2 | task 50566 | prompt processing progress, n_tokens = 11418, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 50566 | prompt done, n_tokens = 11418, batch.n_tokens = 64
slot init_sampler: id  2 | task 50566 | init sampler, took 1.62 ms, tokens: text = 11418, total = 11418
slot update_slots: id  2 | task 50566 | erasing old context checkpoint (pos_min = 7779, pos_max = 8421, size = 15.078 MiB)
slot update_slots: id  2 | task 50566 | created context checkpoint 8 of 8 (pos_min = 10731, pos_max = 11353, size = 14.609 MiB)
slot print_timing: id  2 | task 50566 | 
prompt eval time =     546.62 ms /   200 tokens (    2.73 ms per token,   365.88 tokens per second)
       eval time =    3732.07 ms /   128 tokens (   29.16 ms per token,    34.30 tokens per second)
      total time =    4278.69 ms /   328 tokens
slot      release: id  2 | task 50566 | stop processing: n_tokens = 11545, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 50696 | processing task, is_child = 0
slot update_slots: id  2 | task 50696 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11589
slot update_slots: id  2 | task 50696 | n_tokens = 11435, memory_seq_rm [11435, end)
slot update_slots: id  2 | task 50696 | prompt processing progress, n_tokens = 11525, batch.n_tokens = 90, progress = 0.994478
slot update_slots: id  2 | task 50696 | n_tokens = 11525, memory_seq_rm [11525, end)
slot update_slots: id  2 | task 50696 | prompt processing progress, n_tokens = 11589, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 50696 | prompt done, n_tokens = 11589, batch.n_tokens = 64
slot init_sampler: id  2 | task 50696 | init sampler, took 1.64 ms, tokens: text = 11589, total = 11589
slot update_slots: id  2 | task 50696 | erasing old context checkpoint (pos_min = 8454, pos_max = 9096, size = 15.078 MiB)
slot update_slots: id  2 | task 50696 | created context checkpoint 8 of 8 (pos_min = 10913, pos_max = 11524, size = 14.351 MiB)
slot print_timing: id  2 | task 50696 | 
prompt eval time =     511.68 ms /   154 tokens (    3.32 ms per token,   300.97 tokens per second)
       eval time =    3753.46 ms /   129 tokens (   29.10 ms per token,    34.37 tokens per second)
      total time =    4265.14 ms /   283 tokens
slot      release: id  2 | task 50696 | stop processing: n_tokens = 11717, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 50827 | processing task, is_child = 0
slot update_slots: id  2 | task 50827 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11757
slot update_slots: id  2 | task 50827 | n_tokens = 11603, memory_seq_rm [11603, end)
slot update_slots: id  2 | task 50827 | prompt processing progress, n_tokens = 11693, batch.n_tokens = 90, progress = 0.994556
slot update_slots: id  2 | task 50827 | n_tokens = 11693, memory_seq_rm [11693, end)
slot update_slots: id  2 | task 50827 | prompt processing progress, n_tokens = 11757, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 50827 | prompt done, n_tokens = 11757, batch.n_tokens = 64
slot init_sampler: id  2 | task 50827 | init sampler, took 2.15 ms, tokens: text = 11757, total = 11757
slot update_slots: id  2 | task 50827 | erasing old context checkpoint (pos_min = 9013, pos_max = 9635, size = 14.609 MiB)
slot update_slots: id  2 | task 50827 | created context checkpoint 8 of 8 (pos_min = 11105, pos_max = 11692, size = 13.788 MiB)
slot print_timing: id  2 | task 50827 | 
prompt eval time =     515.41 ms /   154 tokens (    3.35 ms per token,   298.79 tokens per second)
       eval time =    2135.73 ms /    72 tokens (   29.66 ms per token,    33.71 tokens per second)
      total time =    2651.14 ms /   226 tokens
slot      release: id  2 | task 50827 | stop processing: n_tokens = 11828, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 50901 | processing task, is_child = 0
slot update_slots: id  2 | task 50901 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11874
slot update_slots: id  2 | task 50901 | n_tokens = 11772, memory_seq_rm [11772, end)
slot update_slots: id  2 | task 50901 | prompt processing progress, n_tokens = 11810, batch.n_tokens = 38, progress = 0.994610
slot update_slots: id  2 | task 50901 | n_tokens = 11810, memory_seq_rm [11810, end)
slot update_slots: id  2 | task 50901 | prompt processing progress, n_tokens = 11874, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 50901 | prompt done, n_tokens = 11874, batch.n_tokens = 64
slot init_sampler: id  2 | task 50901 | init sampler, took 2.19 ms, tokens: text = 11874, total = 11874
slot update_slots: id  2 | task 50901 | erasing old context checkpoint (pos_min = 9244, pos_max = 9866, size = 14.609 MiB)
slot update_slots: id  2 | task 50901 | created context checkpoint 8 of 8 (pos_min = 11216, pos_max = 11809, size = 13.929 MiB)
slot print_timing: id  2 | task 50901 | 
prompt eval time =     418.61 ms /   102 tokens (    4.10 ms per token,   243.66 tokens per second)
       eval time =    5423.96 ms /   184 tokens (   29.48 ms per token,    33.92 tokens per second)
      total time =    5842.57 ms /   286 tokens
slot      release: id  2 | task 50901 | stop processing: n_tokens = 12057, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 51087 | processing task, is_child = 0
slot update_slots: id  2 | task 51087 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12113
slot update_slots: id  2 | task 51087 | n_tokens = 12048, memory_seq_rm [12048, end)
slot update_slots: id  2 | task 51087 | prompt processing progress, n_tokens = 12049, batch.n_tokens = 1, progress = 0.994716
slot update_slots: id  2 | task 51087 | n_tokens = 12049, memory_seq_rm [12049, end)
slot update_slots: id  2 | task 51087 | prompt processing progress, n_tokens = 12113, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 51087 | prompt done, n_tokens = 12113, batch.n_tokens = 64
slot init_sampler: id  2 | task 51087 | init sampler, took 1.72 ms, tokens: text = 12113, total = 12113
slot update_slots: id  2 | task 51087 | erasing old context checkpoint (pos_min = 9661, pos_max = 10214, size = 12.991 MiB)
slot update_slots: id  2 | task 51087 | created context checkpoint 8 of 8 (pos_min = 11435, pos_max = 12048, size = 14.398 MiB)
slot print_timing: id  2 | task 51087 | 
prompt eval time =     330.76 ms /    65 tokens (    5.09 ms per token,   196.51 tokens per second)
       eval time =    3609.69 ms /   124 tokens (   29.11 ms per token,    34.35 tokens per second)
      total time =    3940.46 ms /   189 tokens
slot      release: id  2 | task 51087 | stop processing: n_tokens = 12236, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.067
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 12236, total state size = 302.000 MiB
srv          load:  - looking for better prompt, base f_keep = 0.067, sim = 0.986
srv        update:  - cache state: 20 prompts, 6939.830 MiB (limits: 8192.000 MiB, 56064 tokens, 246051 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv        update:    - prompt 0x593d953daa30:    9065 tokens, checkpoints:  8,   341.091 MiB
srv        update:    - prompt 0x593db7e3f750:    3739 tokens, checkpoints:  3,   147.988 MiB
srv        update:    - prompt 0x593d95120cb0:   10131 tokens, checkpoints:  8,   372.067 MiB
srv        update:    - prompt 0x593dc061f040:    1775 tokens, checkpoints:  3,    97.549 MiB
srv        update:    - prompt 0x593db7f29c60:    5195 tokens, checkpoints:  5,   211.558 MiB
srv        update:    - prompt 0x593d95dc86d0:    5835 tokens, checkpoints:  5,   225.862 MiB
srv        update:    - prompt 0x593d962d8bb0:    5298 tokens, checkpoints:  6,   226.801 MiB
srv        update:    - prompt 0x593dedc488f0:   20341 tokens, checkpoints:  8,   612.044 MiB
srv        update:    - prompt 0x593d954be4f0:   12236 tokens, checkpoints:  8,   410.055 MiB
srv  get_availabl: prompt cache update took 368.38 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 51213 | processing task, is_child = 0
slot update_slots: id  2 | task 51213 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 831
slot update_slots: id  2 | task 51213 | n_past = 819, slot.prompt.tokens.size() = 12236, seq_id = 2, pos_min = 11593, n_swa = 128
slot update_slots: id  2 | task 51213 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 51213 | erased invalidated context checkpoint (pos_min = 10180, pos_max = 10564, n_swa = 128, size = 9.028 MiB)
slot update_slots: id  2 | task 51213 | erased invalidated context checkpoint (pos_min = 10180, pos_max = 10780, n_swa = 128, size = 14.093 MiB)
slot update_slots: id  2 | task 51213 | erased invalidated context checkpoint (pos_min = 10546, pos_max = 11136, n_swa = 128, size = 13.859 MiB)
slot update_slots: id  2 | task 51213 | erased invalidated context checkpoint (pos_min = 10731, pos_max = 11353, n_swa = 128, size = 14.609 MiB)
slot update_slots: id  2 | task 51213 | erased invalidated context checkpoint (pos_min = 10913, pos_max = 11524, n_swa = 128, size = 14.351 MiB)
slot update_slots: id  2 | task 51213 | erased invalidated context checkpoint (pos_min = 11105, pos_max = 11692, n_swa = 128, size = 13.788 MiB)
slot update_slots: id  2 | task 51213 | erased invalidated context checkpoint (pos_min = 11216, pos_max = 11809, n_swa = 128, size = 13.929 MiB)
slot update_slots: id  2 | task 51213 | erased invalidated context checkpoint (pos_min = 11435, pos_max = 12048, n_swa = 128, size = 14.398 MiB)
slot update_slots: id  2 | task 51213 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 51213 | prompt processing progress, n_tokens = 767, batch.n_tokens = 767, progress = 0.922984
slot update_slots: id  2 | task 51213 | n_tokens = 767, memory_seq_rm [767, end)
slot update_slots: id  2 | task 51213 | prompt processing progress, n_tokens = 831, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 51213 | prompt done, n_tokens = 831, batch.n_tokens = 64
slot init_sampler: id  2 | task 51213 | init sampler, took 0.13 ms, tokens: text = 831, total = 831
slot update_slots: id  2 | task 51213 | created context checkpoint 1 of 8 (pos_min = 124, pos_max = 766, size = 15.078 MiB)
slot print_timing: id  2 | task 51213 | 
prompt eval time =    1522.30 ms /   831 tokens (    1.83 ms per token,   545.89 tokens per second)
       eval time =     878.44 ms /    32 tokens (   27.45 ms per token,    36.43 tokens per second)
      total time =    2400.73 ms /   863 tokens
slot      release: id  2 | task 51213 | stop processing: n_tokens = 862, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.836 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 51247 | processing task, is_child = 0
slot update_slots: id  2 | task 51247 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1009
slot update_slots: id  2 | task 51247 | n_tokens = 844, memory_seq_rm [844, end)
slot update_slots: id  2 | task 51247 | prompt processing progress, n_tokens = 945, batch.n_tokens = 101, progress = 0.936571
slot update_slots: id  2 | task 51247 | n_tokens = 945, memory_seq_rm [945, end)
slot update_slots: id  2 | task 51247 | prompt processing progress, n_tokens = 1009, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 51247 | prompt done, n_tokens = 1009, batch.n_tokens = 64
slot init_sampler: id  2 | task 51247 | init sampler, took 0.15 ms, tokens: text = 1009, total = 1009
slot update_slots: id  2 | task 51247 | created context checkpoint 2 of 8 (pos_min = 302, pos_max = 944, size = 15.078 MiB)
slot print_timing: id  2 | task 51247 | 
prompt eval time =     534.90 ms /   165 tokens (    3.24 ms per token,   308.47 tokens per second)
       eval time =    3023.73 ms /   107 tokens (   28.26 ms per token,    35.39 tokens per second)
      total time =    3558.63 ms /   272 tokens
slot      release: id  2 | task 51247 | stop processing: n_tokens = 1115, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.744 (> 0.100 thold), f_keep = 0.745
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 51356 | processing task, is_child = 0
slot update_slots: id  2 | task 51356 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1117
slot update_slots: id  2 | task 51356 | n_tokens = 831, memory_seq_rm [831, end)
slot update_slots: id  2 | task 51356 | prompt processing progress, n_tokens = 1053, batch.n_tokens = 222, progress = 0.942704
slot update_slots: id  2 | task 51356 | n_tokens = 1053, memory_seq_rm [1053, end)
slot update_slots: id  2 | task 51356 | prompt processing progress, n_tokens = 1117, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 51356 | prompt done, n_tokens = 1117, batch.n_tokens = 64
slot init_sampler: id  2 | task 51356 | init sampler, took 0.17 ms, tokens: text = 1117, total = 1117
slot update_slots: id  2 | task 51356 | created context checkpoint 3 of 8 (pos_min = 472, pos_max = 1052, size = 13.624 MiB)
slot print_timing: id  2 | task 51356 | 
prompt eval time =     767.22 ms /   286 tokens (    2.68 ms per token,   372.77 tokens per second)
       eval time =    1859.50 ms /    66 tokens (   28.17 ms per token,    35.49 tokens per second)
      total time =    2626.72 ms /   352 tokens
slot      release: id  2 | task 51356 | stop processing: n_tokens = 1182, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.946
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 51424 | processing task, is_child = 0
slot update_slots: id  2 | task 51424 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1159
slot update_slots: id  2 | task 51424 | n_tokens = 1118, memory_seq_rm [1118, end)
slot update_slots: id  2 | task 51424 | prompt processing progress, n_tokens = 1159, batch.n_tokens = 41, progress = 1.000000
slot update_slots: id  2 | task 51424 | prompt done, n_tokens = 1159, batch.n_tokens = 41
slot init_sampler: id  2 | task 51424 | init sampler, took 0.24 ms, tokens: text = 1159, total = 1159
slot update_slots: id  2 | task 51424 | created context checkpoint 4 of 8 (pos_min = 539, pos_max = 1117, size = 13.577 MiB)
slot print_timing: id  2 | task 51424 | 
prompt eval time =     280.34 ms /    41 tokens (    6.84 ms per token,   146.25 tokens per second)
       eval time =    4935.99 ms /   170 tokens (   29.04 ms per token,    34.44 tokens per second)
      total time =    5216.33 ms /   211 tokens
slot      release: id  2 | task 51424 | stop processing: n_tokens = 1328, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.902 (> 0.100 thold), f_keep = 0.918
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 51595 | processing task, is_child = 0
slot update_slots: id  2 | task 51595 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1352
slot update_slots: id  2 | task 51595 | n_tokens = 1219, memory_seq_rm [1219, end)
slot update_slots: id  2 | task 51595 | prompt processing progress, n_tokens = 1288, batch.n_tokens = 69, progress = 0.952663
slot update_slots: id  2 | task 51595 | n_tokens = 1288, memory_seq_rm [1288, end)
slot update_slots: id  2 | task 51595 | prompt processing progress, n_tokens = 1352, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 51595 | prompt done, n_tokens = 1352, batch.n_tokens = 64
slot init_sampler: id  2 | task 51595 | init sampler, took 0.20 ms, tokens: text = 1352, total = 1352
slot update_slots: id  2 | task 51595 | created context checkpoint 5 of 8 (pos_min = 712, pos_max = 1287, size = 13.507 MiB)
slot print_timing: id  2 | task 51595 | 
prompt eval time =     510.18 ms /   133 tokens (    3.84 ms per token,   260.69 tokens per second)
       eval time =    2835.46 ms /    95 tokens (   29.85 ms per token,    33.50 tokens per second)
      total time =    3345.64 ms /   228 tokens
slot      release: id  2 | task 51595 | stop processing: n_tokens = 1446, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.823 (> 0.100 thold), f_keep = 0.802
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 51692 | processing task, is_child = 0
slot update_slots: id  2 | task 51692 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1408
slot update_slots: id  2 | task 51692 | n_tokens = 1159, memory_seq_rm [1159, end)
slot update_slots: id  2 | task 51692 | prompt processing progress, n_tokens = 1344, batch.n_tokens = 185, progress = 0.954545
slot update_slots: id  2 | task 51692 | n_tokens = 1344, memory_seq_rm [1344, end)
slot update_slots: id  2 | task 51692 | prompt processing progress, n_tokens = 1408, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 51692 | prompt done, n_tokens = 1408, batch.n_tokens = 64
slot init_sampler: id  2 | task 51692 | init sampler, took 0.21 ms, tokens: text = 1408, total = 1408
slot print_timing: id  2 | task 51692 | 
prompt eval time =     710.21 ms /   249 tokens (    2.85 ms per token,   350.60 tokens per second)
       eval time =    9911.17 ms /   329 tokens (   30.13 ms per token,    33.19 tokens per second)
      total time =   10621.38 ms /   578 tokens
slot      release: id  2 | task 51692 | stop processing: n_tokens = 1736, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.812
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 52023 | processing task, is_child = 0
slot update_slots: id  2 | task 52023 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1525
slot update_slots: id  2 | task 52023 | n_tokens = 1409, memory_seq_rm [1409, end)
slot update_slots: id  2 | task 52023 | prompt processing progress, n_tokens = 1461, batch.n_tokens = 52, progress = 0.958033
slot update_slots: id  2 | task 52023 | n_tokens = 1461, memory_seq_rm [1461, end)
slot update_slots: id  2 | task 52023 | prompt processing progress, n_tokens = 1525, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 52023 | prompt done, n_tokens = 1525, batch.n_tokens = 64
slot init_sampler: id  2 | task 52023 | init sampler, took 0.23 ms, tokens: text = 1525, total = 1525
slot update_slots: id  2 | task 52023 | created context checkpoint 6 of 8 (pos_min = 1159, pos_max = 1460, size = 7.082 MiB)
slot print_timing: id  2 | task 52023 | 
prompt eval time =     547.43 ms /   116 tokens (    4.72 ms per token,   211.90 tokens per second)
       eval time =    5853.26 ms /   202 tokens (   28.98 ms per token,    34.51 tokens per second)
      total time =    6400.69 ms /   318 tokens
slot      release: id  2 | task 52023 | stop processing: n_tokens = 1726, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.925 (> 0.100 thold), f_keep = 0.884
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 52227 | processing task, is_child = 0
slot update_slots: id  2 | task 52227 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1650
slot update_slots: id  2 | task 52227 | n_tokens = 1526, memory_seq_rm [1526, end)
slot update_slots: id  2 | task 52227 | prompt processing progress, n_tokens = 1586, batch.n_tokens = 60, progress = 0.961212
slot update_slots: id  2 | task 52227 | n_tokens = 1586, memory_seq_rm [1586, end)
slot update_slots: id  2 | task 52227 | prompt processing progress, n_tokens = 1650, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 52227 | prompt done, n_tokens = 1650, batch.n_tokens = 64
slot init_sampler: id  2 | task 52227 | init sampler, took 0.24 ms, tokens: text = 1650, total = 1650
slot update_slots: id  2 | task 52227 | created context checkpoint 7 of 8 (pos_min = 1159, pos_max = 1585, size = 10.013 MiB)
slot print_timing: id  2 | task 52227 | 
prompt eval time =     560.46 ms /   124 tokens (    4.52 ms per token,   221.25 tokens per second)
       eval time =   23214.03 ms /   786 tokens (   29.53 ms per token,    33.86 tokens per second)
      total time =   23774.49 ms /   910 tokens
slot      release: id  2 | task 52227 | stop processing: n_tokens = 2435, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.678
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 53015 | processing task, is_child = 0
slot update_slots: id  2 | task 53015 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1759
slot update_slots: id  2 | task 53015 | n_past = 1651, slot.prompt.tokens.size() = 2435, seq_id = 2, pos_min = 1792, n_swa = 128
slot update_slots: id  2 | task 53015 | restored context checkpoint (pos_min = 1159, pos_max = 1585, size = 10.013 MiB)
slot update_slots: id  2 | task 53015 | n_tokens = 1585, memory_seq_rm [1585, end)
slot update_slots: id  2 | task 53015 | prompt processing progress, n_tokens = 1695, batch.n_tokens = 110, progress = 0.963616
slot update_slots: id  2 | task 53015 | n_tokens = 1695, memory_seq_rm [1695, end)
slot update_slots: id  2 | task 53015 | prompt processing progress, n_tokens = 1759, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 53015 | prompt done, n_tokens = 1759, batch.n_tokens = 64
slot init_sampler: id  2 | task 53015 | init sampler, took 0.28 ms, tokens: text = 1759, total = 1759
slot update_slots: id  2 | task 53015 | created context checkpoint 8 of 8 (pos_min = 1159, pos_max = 1694, size = 12.569 MiB)
slot print_timing: id  2 | task 53015 | 
prompt eval time =     750.25 ms /   174 tokens (    4.31 ms per token,   231.92 tokens per second)
       eval time =   14903.96 ms /   508 tokens (   29.34 ms per token,    34.08 tokens per second)
      total time =   15654.21 ms /   682 tokens
slot      release: id  2 | task 53015 | stop processing: n_tokens = 2266, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.777
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 53525 | processing task, is_child = 0
slot update_slots: id  2 | task 53525 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1905
slot update_slots: id  2 | task 53525 | n_tokens = 1760, memory_seq_rm [1760, end)
slot update_slots: id  2 | task 53525 | prompt processing progress, n_tokens = 1841, batch.n_tokens = 81, progress = 0.966404
slot update_slots: id  2 | task 53525 | n_tokens = 1841, memory_seq_rm [1841, end)
slot update_slots: id  2 | task 53525 | prompt processing progress, n_tokens = 1905, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 53525 | prompt done, n_tokens = 1905, batch.n_tokens = 64
slot init_sampler: id  2 | task 53525 | init sampler, took 0.28 ms, tokens: text = 1905, total = 1905
slot update_slots: id  2 | task 53525 | erasing old context checkpoint (pos_min = 124, pos_max = 766, size = 15.078 MiB)
slot update_slots: id  2 | task 53525 | created context checkpoint 8 of 8 (pos_min = 1623, pos_max = 1840, size = 5.112 MiB)
slot print_timing: id  2 | task 53525 | 
prompt eval time =     604.76 ms /   145 tokens (    4.17 ms per token,   239.76 tokens per second)
       eval time =   30851.84 ms /  1043 tokens (   29.58 ms per token,    33.81 tokens per second)
      total time =   31456.60 ms /  1188 tokens
slot      release: id  2 | task 53525 | stop processing: n_tokens = 2947, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.907 (> 0.100 thold), f_keep = 0.647
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 54570 | processing task, is_child = 0
slot update_slots: id  2 | task 54570 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2101
slot update_slots: id  2 | task 54570 | n_past = 1906, slot.prompt.tokens.size() = 2947, seq_id = 2, pos_min = 2304, n_swa = 128
slot update_slots: id  2 | task 54570 | restored context checkpoint (pos_min = 1623, pos_max = 1840, size = 5.112 MiB)
slot update_slots: id  2 | task 54570 | n_tokens = 1840, memory_seq_rm [1840, end)
slot update_slots: id  2 | task 54570 | prompt processing progress, n_tokens = 2037, batch.n_tokens = 197, progress = 0.969538
slot update_slots: id  2 | task 54570 | n_tokens = 2037, memory_seq_rm [2037, end)
slot update_slots: id  2 | task 54570 | prompt processing progress, n_tokens = 2101, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 54570 | prompt done, n_tokens = 2101, batch.n_tokens = 64
slot init_sampler: id  2 | task 54570 | init sampler, took 0.41 ms, tokens: text = 2101, total = 2101
slot update_slots: id  2 | task 54570 | erasing old context checkpoint (pos_min = 302, pos_max = 944, size = 15.078 MiB)
slot update_slots: id  2 | task 54570 | created context checkpoint 8 of 8 (pos_min = 1713, pos_max = 2036, size = 7.598 MiB)
slot print_timing: id  2 | task 54570 | 
prompt eval time =     794.51 ms /   261 tokens (    3.04 ms per token,   328.50 tokens per second)
       eval time =    3832.18 ms /   137 tokens (   27.97 ms per token,    35.75 tokens per second)
      total time =    4626.70 ms /   398 tokens
slot      release: id  2 | task 54570 | stop processing: n_tokens = 2237, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.940
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 54709 | processing task, is_child = 0
slot update_slots: id  2 | task 54709 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2167
slot update_slots: id  2 | task 54709 | n_tokens = 2102, memory_seq_rm [2102, end)
slot update_slots: id  2 | task 54709 | prompt processing progress, n_tokens = 2103, batch.n_tokens = 1, progress = 0.970466
slot update_slots: id  2 | task 54709 | n_tokens = 2103, memory_seq_rm [2103, end)
slot update_slots: id  2 | task 54709 | prompt processing progress, n_tokens = 2167, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 54709 | prompt done, n_tokens = 2167, batch.n_tokens = 64
slot init_sampler: id  2 | task 54709 | init sampler, took 0.43 ms, tokens: text = 2167, total = 2167
slot update_slots: id  2 | task 54709 | erasing old context checkpoint (pos_min = 472, pos_max = 1052, size = 13.624 MiB)
slot update_slots: id  2 | task 54709 | created context checkpoint 8 of 8 (pos_min = 1713, pos_max = 2102, size = 9.145 MiB)
slot print_timing: id  2 | task 54709 | 
prompt eval time =     334.37 ms /    65 tokens (    5.14 ms per token,   194.40 tokens per second)
       eval time =    6067.57 ms /   213 tokens (   28.49 ms per token,    35.10 tokens per second)
      total time =    6401.94 ms /   278 tokens
slot      release: id  2 | task 54709 | stop processing: n_tokens = 2379, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.911 (> 0.100 thold), f_keep = 0.929
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 54924 | processing task, is_child = 0
slot update_slots: id  2 | task 54924 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2424
slot update_slots: id  2 | task 54924 | n_tokens = 2209, memory_seq_rm [2209, end)
slot update_slots: id  2 | task 54924 | prompt processing progress, n_tokens = 2360, batch.n_tokens = 151, progress = 0.973597
slot update_slots: id  2 | task 54924 | n_tokens = 2360, memory_seq_rm [2360, end)
slot update_slots: id  2 | task 54924 | prompt processing progress, n_tokens = 2424, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 54924 | prompt done, n_tokens = 2424, batch.n_tokens = 64
slot init_sampler: id  2 | task 54924 | init sampler, took 0.35 ms, tokens: text = 2424, total = 2424
slot update_slots: id  2 | task 54924 | erasing old context checkpoint (pos_min = 539, pos_max = 1117, size = 13.577 MiB)
slot update_slots: id  2 | task 54924 | created context checkpoint 8 of 8 (pos_min = 1911, pos_max = 2359, size = 10.529 MiB)
slot print_timing: id  2 | task 54924 | 
prompt eval time =     603.72 ms /   215 tokens (    2.81 ms per token,   356.13 tokens per second)
       eval time =    5257.07 ms /   179 tokens (   29.37 ms per token,    34.05 tokens per second)
      total time =    5860.80 ms /   394 tokens
slot      release: id  2 | task 54924 | stop processing: n_tokens = 2602, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.882 (> 0.100 thold), f_keep = 0.833
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 55105 | processing task, is_child = 0
slot update_slots: id  2 | task 55105 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2458
slot update_slots: id  2 | task 55105 | n_past = 2167, slot.prompt.tokens.size() = 2602, seq_id = 2, pos_min = 2106, n_swa = 128
slot update_slots: id  2 | task 55105 | restored context checkpoint (pos_min = 1911, pos_max = 2359, size = 10.529 MiB)
slot update_slots: id  2 | task 55105 | n_tokens = 2167, memory_seq_rm [2167, end)
slot update_slots: id  2 | task 55105 | prompt processing progress, n_tokens = 2394, batch.n_tokens = 227, progress = 0.973963
slot update_slots: id  2 | task 55105 | n_tokens = 2394, memory_seq_rm [2394, end)
slot update_slots: id  2 | task 55105 | prompt processing progress, n_tokens = 2458, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 55105 | prompt done, n_tokens = 2458, batch.n_tokens = 64
slot init_sampler: id  2 | task 55105 | init sampler, took 0.35 ms, tokens: text = 2458, total = 2458
slot print_timing: id  2 | task 55105 | 
prompt eval time =     922.30 ms /   291 tokens (    3.17 ms per token,   315.52 tokens per second)
       eval time =    5375.21 ms /   182 tokens (   29.53 ms per token,    33.86 tokens per second)
      total time =    6297.51 ms /   473 tokens
slot      release: id  2 | task 55105 | stop processing: n_tokens = 2639, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.932
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 55289 | processing task, is_child = 0
slot update_slots: id  2 | task 55289 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2592
slot update_slots: id  2 | task 55289 | n_tokens = 2459, memory_seq_rm [2459, end)
slot update_slots: id  2 | task 55289 | prompt processing progress, n_tokens = 2528, batch.n_tokens = 69, progress = 0.975309
slot update_slots: id  2 | task 55289 | n_tokens = 2528, memory_seq_rm [2528, end)
slot update_slots: id  2 | task 55289 | prompt processing progress, n_tokens = 2592, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 55289 | prompt done, n_tokens = 2592, batch.n_tokens = 64
slot init_sampler: id  2 | task 55289 | init sampler, took 0.39 ms, tokens: text = 2592, total = 2592
slot update_slots: id  2 | task 55289 | erasing old context checkpoint (pos_min = 712, pos_max = 1287, size = 13.507 MiB)
slot update_slots: id  2 | task 55289 | created context checkpoint 8 of 8 (pos_min = 2167, pos_max = 2527, size = 8.465 MiB)
slot print_timing: id  2 | task 55289 | 
prompt eval time =     622.25 ms /   133 tokens (    4.68 ms per token,   213.74 tokens per second)
       eval time =   42038.49 ms /  1403 tokens (   29.96 ms per token,    33.37 tokens per second)
      total time =   42660.74 ms /  1536 tokens
slot      release: id  2 | task 55289 | stop processing: n_tokens = 3994, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.205
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3994, total state size = 108.733 MiB
srv          load:  - looking for better prompt, base f_keep = 0.205, sim = 0.987
srv        update:  - cache state: 21 prompts, 7119.077 MiB (limits: 8192.000 MiB, 56064 tokens, 244452 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv        update:    - prompt 0x593d953daa30:    9065 tokens, checkpoints:  8,   341.091 MiB
srv        update:    - prompt 0x593db7e3f750:    3739 tokens, checkpoints:  3,   147.988 MiB
srv        update:    - prompt 0x593d95120cb0:   10131 tokens, checkpoints:  8,   372.067 MiB
srv        update:    - prompt 0x593dc061f040:    1775 tokens, checkpoints:  3,    97.549 MiB
srv        update:    - prompt 0x593db7f29c60:    5195 tokens, checkpoints:  5,   211.558 MiB
srv        update:    - prompt 0x593d95dc86d0:    5835 tokens, checkpoints:  5,   225.862 MiB
srv        update:    - prompt 0x593d962d8bb0:    5298 tokens, checkpoints:  6,   226.801 MiB
srv        update:    - prompt 0x593dedc488f0:   20341 tokens, checkpoints:  8,   612.044 MiB
srv        update:    - prompt 0x593d954be4f0:   12236 tokens, checkpoints:  8,   410.055 MiB
srv        update:    - prompt 0x593d9511ce20:    3994 tokens, checkpoints:  8,   179.247 MiB
srv  get_availabl: prompt cache update took 124.58 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 56694 | processing task, is_child = 0
slot update_slots: id  2 | task 56694 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 830
slot update_slots: id  2 | task 56694 | n_past = 819, slot.prompt.tokens.size() = 3994, seq_id = 2, pos_min = 3351, n_swa = 128
slot update_slots: id  2 | task 56694 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 56694 | erased invalidated context checkpoint (pos_min = 1159, pos_max = 1460, n_swa = 128, size = 7.082 MiB)
slot update_slots: id  2 | task 56694 | erased invalidated context checkpoint (pos_min = 1159, pos_max = 1585, n_swa = 128, size = 10.013 MiB)
slot update_slots: id  2 | task 56694 | erased invalidated context checkpoint (pos_min = 1159, pos_max = 1694, n_swa = 128, size = 12.569 MiB)
slot update_slots: id  2 | task 56694 | erased invalidated context checkpoint (pos_min = 1623, pos_max = 1840, n_swa = 128, size = 5.112 MiB)
slot update_slots: id  2 | task 56694 | erased invalidated context checkpoint (pos_min = 1713, pos_max = 2036, n_swa = 128, size = 7.598 MiB)
slot update_slots: id  2 | task 56694 | erased invalidated context checkpoint (pos_min = 1713, pos_max = 2102, n_swa = 128, size = 9.145 MiB)
slot update_slots: id  2 | task 56694 | erased invalidated context checkpoint (pos_min = 1911, pos_max = 2359, n_swa = 128, size = 10.529 MiB)
slot update_slots: id  2 | task 56694 | erased invalidated context checkpoint (pos_min = 2167, pos_max = 2527, n_swa = 128, size = 8.465 MiB)
slot update_slots: id  2 | task 56694 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 56694 | prompt processing progress, n_tokens = 766, batch.n_tokens = 766, progress = 0.922892
slot update_slots: id  2 | task 56694 | n_tokens = 766, memory_seq_rm [766, end)
slot update_slots: id  2 | task 56694 | prompt processing progress, n_tokens = 830, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 56694 | prompt done, n_tokens = 830, batch.n_tokens = 64
slot init_sampler: id  2 | task 56694 | init sampler, took 0.17 ms, tokens: text = 830, total = 830
slot update_slots: id  2 | task 56694 | created context checkpoint 1 of 8 (pos_min = 123, pos_max = 765, size = 15.078 MiB)
slot print_timing: id  2 | task 56694 | 
prompt eval time =    1535.47 ms /   830 tokens (    1.85 ms per token,   540.55 tokens per second)
       eval time =   13416.49 ms /   471 tokens (   28.49 ms per token,    35.11 tokens per second)
      total time =   14951.96 ms /  1301 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 56694 | stop processing: n_tokens = 1300, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.880 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 57167 | processing task, is_child = 0
slot update_slots: id  2 | task 57167 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1447
slot update_slots: id  2 | task 57167 | n_tokens = 1273, memory_seq_rm [1273, end)
slot update_slots: id  2 | task 57167 | prompt processing progress, n_tokens = 1383, batch.n_tokens = 110, progress = 0.955771
slot update_slots: id  2 | task 57167 | n_tokens = 1383, memory_seq_rm [1383, end)
slot update_slots: id  2 | task 57167 | prompt processing progress, n_tokens = 1447, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 57167 | prompt done, n_tokens = 1447, batch.n_tokens = 64
slot init_sampler: id  2 | task 57167 | init sampler, took 0.26 ms, tokens: text = 1447, total = 1447
slot update_slots: id  2 | task 57167 | created context checkpoint 2 of 8 (pos_min = 753, pos_max = 1382, size = 14.773 MiB)
slot print_timing: id  2 | task 57167 | 
prompt eval time =     556.67 ms /   174 tokens (    3.20 ms per token,   312.57 tokens per second)
       eval time =    4122.76 ms /   141 tokens (   29.24 ms per token,    34.20 tokens per second)
      total time =    4679.42 ms /   315 tokens
slot      release: id  2 | task 57167 | stop processing: n_tokens = 1587, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.900 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 57310 | processing task, is_child = 0
slot update_slots: id  2 | task 57310 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1734
slot update_slots: id  2 | task 57310 | n_tokens = 1560, memory_seq_rm [1560, end)
slot update_slots: id  2 | task 57310 | prompt processing progress, n_tokens = 1670, batch.n_tokens = 110, progress = 0.963091
slot update_slots: id  2 | task 57310 | n_tokens = 1670, memory_seq_rm [1670, end)
slot update_slots: id  2 | task 57310 | prompt processing progress, n_tokens = 1734, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 57310 | prompt done, n_tokens = 1734, batch.n_tokens = 64
slot init_sampler: id  2 | task 57310 | init sampler, took 0.27 ms, tokens: text = 1734, total = 1734
slot update_slots: id  2 | task 57310 | created context checkpoint 3 of 8 (pos_min = 1040, pos_max = 1669, size = 14.773 MiB)
slot print_timing: id  2 | task 57310 | 
prompt eval time =     571.65 ms /   174 tokens (    3.29 ms per token,   304.38 tokens per second)
       eval time =    1166.67 ms /    39 tokens (   29.91 ms per token,    33.43 tokens per second)
      total time =    1738.32 ms /   213 tokens
slot      release: id  2 | task 57310 | stop processing: n_tokens = 1772, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.909 (> 0.100 thold), f_keep = 0.985
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 57351 | processing task, is_child = 0
slot update_slots: id  2 | task 57351 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1919
slot update_slots: id  2 | task 57351 | n_tokens = 1745, memory_seq_rm [1745, end)
slot update_slots: id  2 | task 57351 | prompt processing progress, n_tokens = 1855, batch.n_tokens = 110, progress = 0.966649
slot update_slots: id  2 | task 57351 | n_tokens = 1855, memory_seq_rm [1855, end)
slot update_slots: id  2 | task 57351 | prompt processing progress, n_tokens = 1919, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 57351 | prompt done, n_tokens = 1919, batch.n_tokens = 64
slot init_sampler: id  2 | task 57351 | init sampler, took 0.30 ms, tokens: text = 1919, total = 1919
slot update_slots: id  2 | task 57351 | created context checkpoint 4 of 8 (pos_min = 1225, pos_max = 1854, size = 14.773 MiB)
slot print_timing: id  2 | task 57351 | 
prompt eval time =     574.14 ms /   174 tokens (    3.30 ms per token,   303.06 tokens per second)
       eval time =    1142.55 ms /    38 tokens (   30.07 ms per token,    33.26 tokens per second)
      total time =    1716.69 ms /   212 tokens
slot      release: id  2 | task 57351 | stop processing: n_tokens = 1956, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.917 (> 0.100 thold), f_keep = 0.986
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 57391 | processing task, is_child = 0
slot update_slots: id  2 | task 57391 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2103
slot update_slots: id  2 | task 57391 | n_tokens = 1929, memory_seq_rm [1929, end)
slot update_slots: id  2 | task 57391 | prompt processing progress, n_tokens = 2039, batch.n_tokens = 110, progress = 0.969567
slot update_slots: id  2 | task 57391 | n_tokens = 2039, memory_seq_rm [2039, end)
slot update_slots: id  2 | task 57391 | prompt processing progress, n_tokens = 2103, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 57391 | prompt done, n_tokens = 2103, batch.n_tokens = 64
slot init_sampler: id  2 | task 57391 | init sampler, took 0.32 ms, tokens: text = 2103, total = 2103
slot update_slots: id  2 | task 57391 | created context checkpoint 5 of 8 (pos_min = 1396, pos_max = 2038, size = 15.078 MiB)
slot print_timing: id  2 | task 57391 | 
prompt eval time =     570.37 ms /   174 tokens (    3.28 ms per token,   305.06 tokens per second)
       eval time =    1139.87 ms /    38 tokens (   30.00 ms per token,    33.34 tokens per second)
      total time =    1710.24 ms /   212 tokens
slot      release: id  2 | task 57391 | stop processing: n_tokens = 2140, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 57431 | processing task, is_child = 0
slot update_slots: id  2 | task 57431 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2287
slot update_slots: id  2 | task 57431 | n_tokens = 2113, memory_seq_rm [2113, end)
slot update_slots: id  2 | task 57431 | prompt processing progress, n_tokens = 2223, batch.n_tokens = 110, progress = 0.972016
slot update_slots: id  2 | task 57431 | n_tokens = 2223, memory_seq_rm [2223, end)
slot update_slots: id  2 | task 57431 | prompt processing progress, n_tokens = 2287, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 57431 | prompt done, n_tokens = 2287, batch.n_tokens = 64
slot init_sampler: id  2 | task 57431 | init sampler, took 0.37 ms, tokens: text = 2287, total = 2287
slot update_slots: id  2 | task 57431 | created context checkpoint 6 of 8 (pos_min = 1580, pos_max = 2222, size = 15.078 MiB)
slot print_timing: id  2 | task 57431 | 
prompt eval time =     582.08 ms /   174 tokens (    3.35 ms per token,   298.93 tokens per second)
       eval time =    1145.56 ms /    38 tokens (   30.15 ms per token,    33.17 tokens per second)
      total time =    1727.63 ms /   212 tokens
slot      release: id  2 | task 57431 | stop processing: n_tokens = 2324, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.930 (> 0.100 thold), f_keep = 0.988
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 57471 | processing task, is_child = 0
slot update_slots: id  2 | task 57471 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2471
slot update_slots: id  2 | task 57471 | n_tokens = 2297, memory_seq_rm [2297, end)
slot update_slots: id  2 | task 57471 | prompt processing progress, n_tokens = 2407, batch.n_tokens = 110, progress = 0.974100
slot update_slots: id  2 | task 57471 | n_tokens = 2407, memory_seq_rm [2407, end)
slot update_slots: id  2 | task 57471 | prompt processing progress, n_tokens = 2471, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 57471 | prompt done, n_tokens = 2471, batch.n_tokens = 64
slot init_sampler: id  2 | task 57471 | init sampler, took 0.49 ms, tokens: text = 2471, total = 2471
slot update_slots: id  2 | task 57471 | created context checkpoint 7 of 8 (pos_min = 1764, pos_max = 2406, size = 15.078 MiB)
slot print_timing: id  2 | task 57471 | 
prompt eval time =     580.21 ms /   174 tokens (    3.33 ms per token,   299.89 tokens per second)
       eval time =    1129.44 ms /    38 tokens (   29.72 ms per token,    33.64 tokens per second)
      total time =    1709.65 ms /   212 tokens
slot      release: id  2 | task 57471 | stop processing: n_tokens = 2508, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 57511 | processing task, is_child = 0
slot update_slots: id  2 | task 57511 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2655
slot update_slots: id  2 | task 57511 | n_tokens = 2481, memory_seq_rm [2481, end)
slot update_slots: id  2 | task 57511 | prompt processing progress, n_tokens = 2591, batch.n_tokens = 110, progress = 0.975895
slot update_slots: id  2 | task 57511 | n_tokens = 2591, memory_seq_rm [2591, end)
slot update_slots: id  2 | task 57511 | prompt processing progress, n_tokens = 2655, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 57511 | prompt done, n_tokens = 2655, batch.n_tokens = 64
slot init_sampler: id  2 | task 57511 | init sampler, took 0.54 ms, tokens: text = 2655, total = 2655
slot update_slots: id  2 | task 57511 | created context checkpoint 8 of 8 (pos_min = 1948, pos_max = 2590, size = 15.078 MiB)
slot print_timing: id  2 | task 57511 | 
prompt eval time =     586.94 ms /   174 tokens (    3.37 ms per token,   296.45 tokens per second)
       eval time =    8861.89 ms /   291 tokens (   30.45 ms per token,    32.84 tokens per second)
      total time =    9448.82 ms /   465 tokens
slot      release: id  2 | task 57511 | stop processing: n_tokens = 2945, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.351 (> 0.100 thold), f_keep = 0.282
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2945, total state size = 84.135 MiB
srv          load:  - looking for better prompt, base f_keep = 0.282, sim = 0.351
srv        update:  - cache state: 22 prompts, 7322.921 MiB (limits: 8192.000 MiB, 56064 tokens, 240942 est)
srv        update:    - prompt 0x593d95405f20:   11043 tokens, checkpoints:  8,   413.337 MiB
srv        update:    - prompt 0x593d95173480:    8176 tokens, checkpoints:  2,   245.324 MiB
srv        update:    - prompt 0x593d9556f720:   15600 tokens, checkpoints:  4,   439.388 MiB
srv        update:    - prompt 0x593d9519a190:    8477 tokens, checkpoints:  8,   332.884 MiB
srv        update:    - prompt 0x593d94af56a0:   20280 tokens, checkpoints:  8,   602.336 MiB
srv        update:    - prompt 0x593db7d76170:   21357 tokens, checkpoints:  2,   546.033 MiB
srv        update:    - prompt 0x593d9596b300:    8461 tokens, checkpoints:  3,   252.078 MiB
srv        update:    - prompt 0x593d95506090:    6838 tokens, checkpoints:  5,   253.298 MiB
srv        update:    - prompt 0x593d952d5730:   16661 tokens, checkpoints:  8,   522.422 MiB
srv        update:    - prompt 0x593d9523bb00:    9923 tokens, checkpoints:  7,   377.273 MiB
srv        update:    - prompt 0x593d8e0b2d90:    8011 tokens, checkpoints:  6,   310.443 MiB
srv        update:    - prompt 0x593d953daa30:    9065 tokens, checkpoints:  8,   341.091 MiB
srv        update:    - prompt 0x593db7e3f750:    3739 tokens, checkpoints:  3,   147.988 MiB
srv        update:    - prompt 0x593d95120cb0:   10131 tokens, checkpoints:  8,   372.067 MiB
srv        update:    - prompt 0x593dc061f040:    1775 tokens, checkpoints:  3,    97.549 MiB
srv        update:    - prompt 0x593db7f29c60:    5195 tokens, checkpoints:  5,   211.558 MiB
srv        update:    - prompt 0x593d95dc86d0:    5835 tokens, checkpoints:  5,   225.862 MiB
srv        update:    - prompt 0x593d962d8bb0:    5298 tokens, checkpoints:  6,   226.801 MiB
srv        update:    - prompt 0x593dedc488f0:   20341 tokens, checkpoints:  8,   612.044 MiB
srv        update:    - prompt 0x593d954be4f0:   12236 tokens, checkpoints:  8,   410.055 MiB
srv        update:    - prompt 0x593d9511ce20:    3994 tokens, checkpoints:  8,   179.247 MiB
srv        update:    - prompt 0x593d8e0abfd0:    2945 tokens, checkpoints:  8,   203.845 MiB
srv  get_availabl: prompt cache update took 187.97 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 57804 | processing task, is_child = 0
slot update_slots: id  2 | task 57804 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2362
slot update_slots: id  2 | task 57804 | n_past = 830, slot.prompt.tokens.size() = 2945, seq_id = 2, pos_min = 2302, n_swa = 128
slot update_slots: id  2 | task 57804 | restored context checkpoint (pos_min = 123, pos_max = 765, size = 15.078 MiB)
slot update_slots: id  2 | task 57804 | erased invalidated context checkpoint (pos_min = 753, pos_max = 1382, n_swa = 128, size = 14.773 MiB)
slot update_slots: id  2 | task 57804 | erased invalidated context checkpoint (pos_min = 1040, pos_max = 1669, n_swa = 128, size = 14.773 MiB)
slot update_slots: id  2 | task 57804 | erased invalidated context checkpoint (pos_min = 1225, pos_max = 1854, n_swa = 128, size = 14.773 MiB)
slot update_slots: id  2 | task 57804 | erased invalidated context checkpoint (pos_min = 1396, pos_max = 2038, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 57804 | erased invalidated context checkpoint (pos_min = 1580, pos_max = 2222, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 57804 | erased invalidated context checkpoint (pos_min = 1764, pos_max = 2406, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 57804 | erased invalidated context checkpoint (pos_min = 1948, pos_max = 2590, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  2 | task 57804 | n_tokens = 765, memory_seq_rm [765, end)
slot update_slots: id  2 | task 57804 | prompt processing progress, n_tokens = 2298, batch.n_tokens = 1533, progress = 0.972904
slot update_slots: id  2 | task 57804 | n_tokens = 2298, memory_seq_rm [2298, end)
slot update_slots: id  2 | task 57804 | prompt processing progress, n_tokens = 2362, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 57804 | prompt done, n_tokens = 2362, batch.n_tokens = 64
slot init_sampler: id  2 | task 57804 | init sampler, took 0.44 ms, tokens: text = 2362, total = 2362
slot update_slots: id  2 | task 57804 | created context checkpoint 2 of 8 (pos_min = 1655, pos_max = 2297, size = 15.078 MiB)
slot print_timing: id  2 | task 57804 | 
prompt eval time =    2654.22 ms /  1597 tokens (    1.66 ms per token,   601.68 tokens per second)
       eval time =   22248.62 ms /   758 tokens (   29.35 ms per token,    34.07 tokens per second)
      total time =   24902.85 ms /  2355 tokens
slot      release: id  2 | task 57804 | stop processing: n_tokens = 3119, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.797 (> 0.100 thold), f_keep = 0.758
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 58564 | processing task, is_child = 0
slot update_slots: id  2 | task 58564 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2964
slot update_slots: id  2 | task 58564 | n_past = 2363, slot.prompt.tokens.size() = 3119, seq_id = 2, pos_min = 2476, n_swa = 128
slot update_slots: id  2 | task 58564 | restored context checkpoint (pos_min = 1655, pos_max = 2297, size = 15.078 MiB)
slot update_slots: id  2 | task 58564 | n_tokens = 2297, memory_seq_rm [2297, end)
slot update_slots: id  2 | task 58564 | prompt processing progress, n_tokens = 2900, batch.n_tokens = 603, progress = 0.978408
slot update_slots: id  2 | task 58564 | n_tokens = 2900, memory_seq_rm [2900, end)
slot update_slots: id  2 | task 58564 | prompt processing progress, n_tokens = 2964, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 58564 | prompt done, n_tokens = 2964, batch.n_tokens = 64
slot init_sampler: id  2 | task 58564 | init sampler, took 0.42 ms, tokens: text = 2964, total = 2964
slot update_slots: id  2 | task 58564 | created context checkpoint 3 of 8 (pos_min = 2257, pos_max = 2899, size = 15.078 MiB)
slot print_timing: id  2 | task 58564 | 
prompt eval time =    1481.99 ms /   667 tokens (    2.22 ms per token,   450.07 tokens per second)
       eval time =   10661.82 ms /   370 tokens (   28.82 ms per token,    34.70 tokens per second)
      total time =   12143.81 ms /  1037 tokens
slot      release: id  2 | task 58564 | stop processing: n_tokens = 3333, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
