ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: using cached file: /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14807 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1763 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 56064 -> need 1767 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.96 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14807 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
srv  log_server_r: request: GET /health 127.0.0.1 503
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 56064
llama_context: n_ctx_seq     = 56064
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (56064) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 56064 cells
llama_kv_cache:      CUDA0 KV buffer size =  1314.00 MiB
llama_kv_cache: size = 1314.00 MiB ( 56064 cells,  12 layers,  4/1 seqs), K (f16):  657.00 MiB, V (f16):  657.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
srv  log_server_r: request: GET /health 127.0.0.1 503
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   117.15 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 78.09 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  1 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  2 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  3 | task -1 | new slot, n_ctx = 56064
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-12

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 704
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 640, batch.n_tokens = 640, progress = 0.909091
slot update_slots: id  3 | task 0 | n_tokens = 640, memory_seq_rm [640, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 704, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 704, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.17 ms, tokens: text = 704, total = 704
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =    1233.53 ms /   704 tokens (    1.75 ms per token,   570.72 tokens per second)
       eval time =     791.14 ms /    33 tokens (   23.97 ms per token,    41.71 tokens per second)
      total time =    2024.67 ms /   737 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 736, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.810 (> 0.100 thold), f_keep = 0.957
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 35 | processing task, is_child = 0
slot update_slots: id  3 | task 35 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 869
slot update_slots: id  3 | task 35 | n_tokens = 704, memory_seq_rm [704, end)
slot update_slots: id  3 | task 35 | prompt processing progress, n_tokens = 805, batch.n_tokens = 101, progress = 0.926352
slot update_slots: id  3 | task 35 | n_tokens = 805, memory_seq_rm [805, end)
slot update_slots: id  3 | task 35 | prompt processing progress, n_tokens = 869, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 35 | prompt done, n_tokens = 869, batch.n_tokens = 64
slot init_sampler: id  3 | task 35 | init sampler, took 0.16 ms, tokens: text = 869, total = 869
slot update_slots: id  3 | task 35 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 804, size = 18.877 MiB)
slot print_timing: id  3 | task 35 | 
prompt eval time =     400.61 ms /   165 tokens (    2.43 ms per token,   411.87 tokens per second)
       eval time =    3050.52 ms /   121 tokens (   25.21 ms per token,    39.67 tokens per second)
      total time =    3451.13 ms /   286 tokens
slot      release: id  3 | task 35 | stop processing: n_tokens = 989, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.727 (> 0.100 thold), f_keep = 0.683
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 158 | processing task, is_child = 0
slot update_slots: id  3 | task 158 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 928
slot update_slots: id  3 | task 158 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  3 | task 158 | prompt processing progress, n_tokens = 864, batch.n_tokens = 189, progress = 0.931035
slot update_slots: id  3 | task 158 | n_tokens = 864, memory_seq_rm [864, end)
slot update_slots: id  3 | task 158 | prompt processing progress, n_tokens = 928, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 158 | prompt done, n_tokens = 928, batch.n_tokens = 64
slot init_sampler: id  3 | task 158 | init sampler, took 0.18 ms, tokens: text = 928, total = 928
slot print_timing: id  3 | task 158 | 
prompt eval time =     570.75 ms /   253 tokens (    2.26 ms per token,   443.27 tokens per second)
       eval time =    2980.28 ms /   116 tokens (   25.69 ms per token,    38.92 tokens per second)
      total time =    3551.04 ms /   369 tokens
slot      release: id  3 | task 158 | stop processing: n_tokens = 1043, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.937 (> 0.100 thold), f_keep = 0.890
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 276 | processing task, is_child = 0
slot update_slots: id  3 | task 276 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 990
slot update_slots: id  3 | task 276 | n_tokens = 928, memory_seq_rm [928, end)
slot update_slots: id  3 | task 276 | prompt processing progress, n_tokens = 990, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 276 | prompt done, n_tokens = 990, batch.n_tokens = 62
slot init_sampler: id  3 | task 276 | init sampler, took 0.22 ms, tokens: text = 990, total = 990
slot update_slots: id  3 | task 276 | created context checkpoint 3 of 8 (pos_min = 19, pos_max = 927, size = 21.315 MiB)
slot print_timing: id  3 | task 276 | 
prompt eval time =     259.74 ms /    62 tokens (    4.19 ms per token,   238.70 tokens per second)
       eval time =    2783.11 ms /   106 tokens (   26.26 ms per token,    38.09 tokens per second)
      total time =    3042.85 ms /   168 tokens
slot      release: id  3 | task 276 | stop processing: n_tokens = 1095, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.930 (> 0.100 thold), f_keep = 0.904
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 383 | processing task, is_child = 0
slot update_slots: id  3 | task 383 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1065
slot update_slots: id  3 | task 383 | n_tokens = 990, memory_seq_rm [990, end)
slot update_slots: id  3 | task 383 | prompt processing progress, n_tokens = 1001, batch.n_tokens = 11, progress = 0.939906
slot update_slots: id  3 | task 383 | n_tokens = 1001, memory_seq_rm [1001, end)
slot update_slots: id  3 | task 383 | prompt processing progress, n_tokens = 1065, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 383 | prompt done, n_tokens = 1065, batch.n_tokens = 64
slot init_sampler: id  3 | task 383 | init sampler, took 0.20 ms, tokens: text = 1065, total = 1065
slot update_slots: id  3 | task 383 | created context checkpoint 4 of 8 (pos_min = 167, pos_max = 1000, size = 19.557 MiB)
slot print_timing: id  3 | task 383 | 
prompt eval time =     356.82 ms /    75 tokens (    4.76 ms per token,   210.19 tokens per second)
       eval time =    3169.05 ms /   116 tokens (   27.32 ms per token,    36.60 tokens per second)
      total time =    3525.87 ms /   191 tokens
slot      release: id  3 | task 383 | stop processing: n_tokens = 1180, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.903
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 501 | processing task, is_child = 0
slot update_slots: id  3 | task 501 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1144
slot update_slots: id  3 | task 501 | n_tokens = 1065, memory_seq_rm [1065, end)
slot update_slots: id  3 | task 501 | prompt processing progress, n_tokens = 1080, batch.n_tokens = 15, progress = 0.944056
slot update_slots: id  3 | task 501 | n_tokens = 1080, memory_seq_rm [1080, end)
slot update_slots: id  3 | task 501 | prompt processing progress, n_tokens = 1144, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 501 | prompt done, n_tokens = 1144, batch.n_tokens = 64
slot init_sampler: id  3 | task 501 | init sampler, took 0.20 ms, tokens: text = 1144, total = 1144
slot update_slots: id  3 | task 501 | created context checkpoint 5 of 8 (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot print_timing: id  3 | task 501 | 
prompt eval time =     345.48 ms /    79 tokens (    4.37 ms per token,   228.67 tokens per second)
       eval time =    3481.67 ms /   128 tokens (   27.20 ms per token,    36.76 tokens per second)
      total time =    3827.15 ms /   207 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 501 | stop processing: n_tokens = 1271, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.900
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 631 | processing task, is_child = 0
slot update_slots: id  3 | task 631 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1206
slot update_slots: id  3 | task 631 | n_tokens = 1144, memory_seq_rm [1144, end)
slot update_slots: id  3 | task 631 | prompt processing progress, n_tokens = 1206, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 631 | prompt done, n_tokens = 1206, batch.n_tokens = 62
slot init_sampler: id  3 | task 631 | init sampler, took 0.20 ms, tokens: text = 1206, total = 1206
slot print_timing: id  3 | task 631 | 
prompt eval time =     232.42 ms /    62 tokens (    3.75 ms per token,   266.76 tokens per second)
       eval time =    5218.02 ms /   188 tokens (   27.76 ms per token,    36.03 tokens per second)
      total time =    5450.44 ms /   250 tokens
slot      release: id  3 | task 631 | stop processing: n_tokens = 1393, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.330 (> 0.100 thold), f_keep = 0.866
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 820 | processing task, is_child = 0
slot update_slots: id  3 | task 820 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3658
slot update_slots: id  3 | task 820 | n_tokens = 1206, memory_seq_rm [1206, end)
slot update_slots: id  3 | task 820 | prompt processing progress, n_tokens = 3254, batch.n_tokens = 2048, progress = 0.889557
slot update_slots: id  3 | task 820 | n_tokens = 3254, memory_seq_rm [3254, end)
slot update_slots: id  3 | task 820 | prompt processing progress, n_tokens = 3594, batch.n_tokens = 340, progress = 0.982504
slot update_slots: id  3 | task 820 | n_tokens = 3594, memory_seq_rm [3594, end)
slot update_slots: id  3 | task 820 | prompt processing progress, n_tokens = 3658, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 820 | prompt done, n_tokens = 3658, batch.n_tokens = 64
slot init_sampler: id  3 | task 820 | init sampler, took 0.76 ms, tokens: text = 3658, total = 3658
slot update_slots: id  3 | task 820 | created context checkpoint 6 of 8 (pos_min = 2570, pos_max = 3593, size = 24.012 MiB)
slot print_timing: id  3 | task 820 | 
prompt eval time =    2850.32 ms /  2452 tokens (    1.16 ms per token,   860.25 tokens per second)
       eval time =    5826.35 ms /   198 tokens (   29.43 ms per token,    33.98 tokens per second)
      total time =    8676.67 ms /  2650 tokens
slot      release: id  3 | task 820 | stop processing: n_tokens = 3855, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.909 (> 0.100 thold), f_keep = 0.949
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1021 | processing task, is_child = 0
slot update_slots: id  3 | task 1021 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4025
slot update_slots: id  3 | task 1021 | n_tokens = 3658, memory_seq_rm [3658, end)
slot update_slots: id  3 | task 1021 | prompt processing progress, n_tokens = 3961, batch.n_tokens = 303, progress = 0.984099
slot update_slots: id  3 | task 1021 | n_tokens = 3961, memory_seq_rm [3961, end)
slot update_slots: id  3 | task 1021 | prompt processing progress, n_tokens = 4025, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1021 | prompt done, n_tokens = 4025, batch.n_tokens = 64
slot init_sampler: id  3 | task 1021 | init sampler, took 0.59 ms, tokens: text = 4025, total = 4025
slot update_slots: id  3 | task 1021 | created context checkpoint 7 of 8 (pos_min = 2937, pos_max = 3960, size = 24.012 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 1021
slot      release: id  3 | task 1021 | stop processing: n_tokens = 4127, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.956 (> 0.100 thold), f_keep = 0.164
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4127, total state size = 120.786 MiB
srv          load:  - looking for better prompt, base f_keep = 0.164, sim = 0.956
srv        update:  - cache state: 1 prompts, 262.983 MiB (limits: 8192.000 MiB, 56064 tokens, 128557 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv  get_availabl: prompt cache update took 200.92 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1127 | processing task, is_child = 0
slot update_slots: id  3 | task 1127 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 706
slot update_slots: id  3 | task 1127 | n_past = 675, slot.prompt.tokens.size() = 4127, seq_id = 3, pos_min = 3103, n_swa = 128
slot update_slots: id  3 | task 1127 | restored context checkpoint (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot update_slots: id  3 | task 1127 | erased invalidated context checkpoint (pos_min = 2570, pos_max = 3593, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1127 | erased invalidated context checkpoint (pos_min = 2937, pos_max = 3960, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1127 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  3 | task 1127 | prompt processing progress, n_tokens = 706, batch.n_tokens = 31, progress = 1.000000
slot update_slots: id  3 | task 1127 | prompt done, n_tokens = 706, batch.n_tokens = 31
slot init_sampler: id  3 | task 1127 | init sampler, took 0.12 ms, tokens: text = 706, total = 706
slot print_timing: id  3 | task 1127 | 
prompt eval time =     189.00 ms /    31 tokens (    6.10 ms per token,   164.02 tokens per second)
       eval time =   13677.85 ms /   526 tokens (   26.00 ms per token,    38.46 tokens per second)
      total time =   13866.86 ms /   557 tokens
slot      release: id  3 | task 1127 | stop processing: n_tokens = 1231, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.432 (> 0.100 thold), f_keep = 0.548
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1654 | processing task, is_child = 0
slot update_slots: id  3 | task 1654 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1562
slot update_slots: id  3 | task 1654 | n_past = 675, slot.prompt.tokens.size() = 1231, seq_id = 3, pos_min = 656, n_swa = 128
slot update_slots: id  3 | task 1654 | restored context checkpoint (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot update_slots: id  3 | task 1654 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  3 | task 1654 | prompt processing progress, n_tokens = 1498, batch.n_tokens = 823, progress = 0.959027
slot update_slots: id  3 | task 1654 | n_tokens = 1498, memory_seq_rm [1498, end)
slot update_slots: id  3 | task 1654 | prompt processing progress, n_tokens = 1562, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1654 | prompt done, n_tokens = 1562, batch.n_tokens = 64
slot init_sampler: id  3 | task 1654 | init sampler, took 0.34 ms, tokens: text = 1562, total = 1562
slot update_slots: id  3 | task 1654 | created context checkpoint 6 of 8 (pos_min = 548, pos_max = 1497, size = 22.277 MiB)
slot print_timing: id  3 | task 1654 | 
prompt eval time =    1165.60 ms /   887 tokens (    1.31 ms per token,   760.98 tokens per second)
       eval time =   10953.55 ms /   396 tokens (   27.66 ms per token,    36.15 tokens per second)
      total time =   12119.15 ms /  1283 tokens
slot      release: id  3 | task 1654 | stop processing: n_tokens = 1957, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.798
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2052 | processing task, is_child = 0
slot update_slots: id  3 | task 2052 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1643
slot update_slots: id  3 | task 2052 | n_tokens = 1562, memory_seq_rm [1562, end)
slot update_slots: id  3 | task 2052 | prompt processing progress, n_tokens = 1579, batch.n_tokens = 17, progress = 0.961047
slot update_slots: id  3 | task 2052 | n_tokens = 1579, memory_seq_rm [1579, end)
slot update_slots: id  3 | task 2052 | prompt processing progress, n_tokens = 1643, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2052 | prompt done, n_tokens = 1643, batch.n_tokens = 64
slot init_sampler: id  3 | task 2052 | init sampler, took 0.28 ms, tokens: text = 1643, total = 1643
slot update_slots: id  3 | task 2052 | created context checkpoint 7 of 8 (pos_min = 1060, pos_max = 1578, size = 12.170 MiB)
slot print_timing: id  3 | task 2052 | 
prompt eval time =     349.39 ms /    81 tokens (    4.31 ms per token,   231.83 tokens per second)
       eval time =    3599.11 ms /   135 tokens (   26.66 ms per token,    37.51 tokens per second)
      total time =    3948.51 ms /   216 tokens
slot      release: id  3 | task 2052 | stop processing: n_tokens = 1777, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.956 (> 0.100 thold), f_keep = 0.925
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2189 | processing task, is_child = 0
slot update_slots: id  3 | task 2189 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1718
slot update_slots: id  3 | task 2189 | n_tokens = 1643, memory_seq_rm [1643, end)
slot update_slots: id  3 | task 2189 | prompt processing progress, n_tokens = 1654, batch.n_tokens = 11, progress = 0.962747
slot update_slots: id  3 | task 2189 | n_tokens = 1654, memory_seq_rm [1654, end)
slot update_slots: id  3 | task 2189 | prompt processing progress, n_tokens = 1718, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2189 | prompt done, n_tokens = 1718, batch.n_tokens = 64
slot init_sampler: id  3 | task 2189 | init sampler, took 0.32 ms, tokens: text = 1718, total = 1718
slot update_slots: id  3 | task 2189 | created context checkpoint 8 of 8 (pos_min = 1060, pos_max = 1653, size = 13.929 MiB)
slot print_timing: id  3 | task 2189 | 
prompt eval time =     337.70 ms /    75 tokens (    4.50 ms per token,   222.09 tokens per second)
       eval time =    1309.37 ms /    50 tokens (   26.19 ms per token,    38.19 tokens per second)
      total time =    1647.07 ms /   125 tokens
slot      release: id  3 | task 2189 | stop processing: n_tokens = 1767, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2241 | processing task, is_child = 0
slot update_slots: id  3 | task 2241 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1780
slot update_slots: id  3 | task 2241 | n_tokens = 1718, memory_seq_rm [1718, end)
slot update_slots: id  3 | task 2241 | prompt processing progress, n_tokens = 1780, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 2241 | prompt done, n_tokens = 1780, batch.n_tokens = 62
slot init_sampler: id  3 | task 2241 | init sampler, took 0.35 ms, tokens: text = 1780, total = 1780
slot print_timing: id  3 | task 2241 | 
prompt eval time =     254.25 ms /    62 tokens (    4.10 ms per token,   243.86 tokens per second)
       eval time =    4489.37 ms /   174 tokens (   25.80 ms per token,    38.76 tokens per second)
      total time =    4743.62 ms /   236 tokens
slot      release: id  3 | task 2241 | stop processing: n_tokens = 1953, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.911
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2416 | processing task, is_child = 0
slot update_slots: id  3 | task 2416 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1844
slot update_slots: id  3 | task 2416 | n_tokens = 1780, memory_seq_rm [1780, end)
slot update_slots: id  3 | task 2416 | prompt processing progress, n_tokens = 1844, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2416 | prompt done, n_tokens = 1844, batch.n_tokens = 64
slot init_sampler: id  3 | task 2416 | init sampler, took 0.35 ms, tokens: text = 1844, total = 1844
slot update_slots: id  3 | task 2416 | erasing old context checkpoint (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot update_slots: id  3 | task 2416 | created context checkpoint 8 of 8 (pos_min = 1066, pos_max = 1779, size = 16.743 MiB)
slot print_timing: id  3 | task 2416 | 
prompt eval time =     239.37 ms /    64 tokens (    3.74 ms per token,   267.36 tokens per second)
       eval time =    2794.49 ms /   106 tokens (   26.36 ms per token,    37.93 tokens per second)
      total time =    3033.86 ms /   170 tokens
slot      release: id  3 | task 2416 | stop processing: n_tokens = 1949, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.959 (> 0.100 thold), f_keep = 0.946
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2523 | processing task, is_child = 0
slot update_slots: id  3 | task 2523 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1922
slot update_slots: id  3 | task 2523 | n_tokens = 1844, memory_seq_rm [1844, end)
slot update_slots: id  3 | task 2523 | prompt processing progress, n_tokens = 1858, batch.n_tokens = 14, progress = 0.966701
slot update_slots: id  3 | task 2523 | n_tokens = 1858, memory_seq_rm [1858, end)
slot update_slots: id  3 | task 2523 | prompt processing progress, n_tokens = 1922, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2523 | prompt done, n_tokens = 1922, batch.n_tokens = 64
slot init_sampler: id  3 | task 2523 | init sampler, took 0.30 ms, tokens: text = 1922, total = 1922
slot update_slots: id  3 | task 2523 | erasing old context checkpoint (pos_min = 0, pos_max = 804, size = 18.877 MiB)
slot update_slots: id  3 | task 2523 | created context checkpoint 8 of 8 (pos_min = 1066, pos_max = 1857, size = 18.572 MiB)
slot print_timing: id  3 | task 2523 | 
prompt eval time =     316.51 ms /    78 tokens (    4.06 ms per token,   246.44 tokens per second)
       eval time =    2090.62 ms /    78 tokens (   26.80 ms per token,    37.31 tokens per second)
      total time =    2407.13 ms /   156 tokens
slot      release: id  3 | task 2523 | stop processing: n_tokens = 1999, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.961
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2603 | processing task, is_child = 0
slot update_slots: id  3 | task 2603 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2000
slot update_slots: id  3 | task 2603 | n_tokens = 1922, memory_seq_rm [1922, end)
slot update_slots: id  3 | task 2603 | prompt processing progress, n_tokens = 1936, batch.n_tokens = 14, progress = 0.968000
slot update_slots: id  3 | task 2603 | n_tokens = 1936, memory_seq_rm [1936, end)
slot update_slots: id  3 | task 2603 | prompt processing progress, n_tokens = 2000, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2603 | prompt done, n_tokens = 2000, batch.n_tokens = 64
slot init_sampler: id  3 | task 2603 | init sampler, took 0.39 ms, tokens: text = 2000, total = 2000
slot update_slots: id  3 | task 2603 | erasing old context checkpoint (pos_min = 19, pos_max = 927, size = 21.315 MiB)
slot update_slots: id  3 | task 2603 | created context checkpoint 8 of 8 (pos_min = 1112, pos_max = 1935, size = 19.322 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 2603
slot      release: id  3 | task 2603 | stop processing: n_tokens = 2108, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.524 (> 0.100 thold), f_keep = 0.522
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2715 | processing task, is_child = 0
slot update_slots: id  3 | task 2715 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2098
slot update_slots: id  3 | task 2715 | n_past = 1100, slot.prompt.tokens.size() = 2108, seq_id = 3, pos_min = 1123, n_swa = 128
slot update_slots: id  3 | task 2715 | restored context checkpoint (pos_min = 548, pos_max = 1497, size = 22.277 MiB)
slot update_slots: id  3 | task 2715 | erased invalidated context checkpoint (pos_min = 1060, pos_max = 1578, n_swa = 128, size = 12.170 MiB)
slot update_slots: id  3 | task 2715 | erased invalidated context checkpoint (pos_min = 1060, pos_max = 1653, n_swa = 128, size = 13.929 MiB)
slot update_slots: id  3 | task 2715 | erased invalidated context checkpoint (pos_min = 1066, pos_max = 1779, n_swa = 128, size = 16.743 MiB)
slot update_slots: id  3 | task 2715 | erased invalidated context checkpoint (pos_min = 1066, pos_max = 1857, n_swa = 128, size = 18.572 MiB)
slot update_slots: id  3 | task 2715 | erased invalidated context checkpoint (pos_min = 1112, pos_max = 1935, n_swa = 128, size = 19.322 MiB)
slot update_slots: id  3 | task 2715 | n_tokens = 1100, memory_seq_rm [1100, end)
slot update_slots: id  3 | task 2715 | prompt processing progress, n_tokens = 2034, batch.n_tokens = 934, progress = 0.969495
slot update_slots: id  3 | task 2715 | n_tokens = 2034, memory_seq_rm [2034, end)
slot update_slots: id  3 | task 2715 | prompt processing progress, n_tokens = 2098, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2715 | prompt done, n_tokens = 2098, batch.n_tokens = 64
slot init_sampler: id  3 | task 2715 | init sampler, took 0.38 ms, tokens: text = 2098, total = 2098
slot update_slots: id  3 | task 2715 | created context checkpoint 4 of 8 (pos_min = 1010, pos_max = 2033, size = 24.012 MiB)
slot print_timing: id  3 | task 2715 | 
prompt eval time =    1244.34 ms /   998 tokens (    1.25 ms per token,   802.03 tokens per second)
       eval time =   16686.83 ms /   588 tokens (   28.38 ms per token,    35.24 tokens per second)
      total time =   17931.17 ms /  1586 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 2715 | stop processing: n_tokens = 2685, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.504 (> 0.100 thold), f_keep = 0.508
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3305 | processing task, is_child = 0
slot update_slots: id  3 | task 3305 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2708
slot update_slots: id  3 | task 3305 | n_past = 1364, slot.prompt.tokens.size() = 2685, seq_id = 3, pos_min = 1661, n_swa = 128
slot update_slots: id  3 | task 3305 | restored context checkpoint (pos_min = 1010, pos_max = 2033, size = 24.012 MiB)
slot update_slots: id  3 | task 3305 | n_tokens = 1364, memory_seq_rm [1364, end)
slot update_slots: id  3 | task 3305 | prompt processing progress, n_tokens = 2644, batch.n_tokens = 1280, progress = 0.976366
slot update_slots: id  3 | task 3305 | n_tokens = 2644, memory_seq_rm [2644, end)
slot update_slots: id  3 | task 3305 | prompt processing progress, n_tokens = 2708, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3305 | prompt done, n_tokens = 2708, batch.n_tokens = 64
slot init_sampler: id  3 | task 3305 | init sampler, took 0.55 ms, tokens: text = 2708, total = 2708
slot update_slots: id  3 | task 3305 | created context checkpoint 5 of 8 (pos_min = 1620, pos_max = 2643, size = 24.012 MiB)
slot print_timing: id  3 | task 3305 | 
prompt eval time =    1640.30 ms /  1344 tokens (    1.22 ms per token,   819.36 tokens per second)
       eval time =    5460.23 ms /   214 tokens (   25.52 ms per token,    39.19 tokens per second)
      total time =    7100.53 ms /  1558 tokens
slot      release: id  3 | task 3305 | stop processing: n_tokens = 2921, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.927
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3521 | processing task, is_child = 0
slot update_slots: id  3 | task 3521 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2790
slot update_slots: id  3 | task 3521 | n_tokens = 2708, memory_seq_rm [2708, end)
slot update_slots: id  3 | task 3521 | prompt processing progress, n_tokens = 2726, batch.n_tokens = 18, progress = 0.977061
slot update_slots: id  3 | task 3521 | n_tokens = 2726, memory_seq_rm [2726, end)
slot update_slots: id  3 | task 3521 | prompt processing progress, n_tokens = 2790, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3521 | prompt done, n_tokens = 2790, batch.n_tokens = 64
slot init_sampler: id  3 | task 3521 | init sampler, took 0.42 ms, tokens: text = 2790, total = 2790
slot update_slots: id  3 | task 3521 | created context checkpoint 6 of 8 (pos_min = 1897, pos_max = 2725, size = 19.439 MiB)
slot print_timing: id  3 | task 3521 | 
prompt eval time =     334.19 ms /    82 tokens (    4.08 ms per token,   245.37 tokens per second)
       eval time =    4462.25 ms /   167 tokens (   26.72 ms per token,    37.43 tokens per second)
      total time =    4796.44 ms /   249 tokens
slot      release: id  3 | task 3521 | stop processing: n_tokens = 2956, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.944
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3690 | processing task, is_child = 0
slot update_slots: id  3 | task 3690 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2868
slot update_slots: id  3 | task 3690 | n_tokens = 2790, memory_seq_rm [2790, end)
slot update_slots: id  3 | task 3690 | prompt processing progress, n_tokens = 2804, batch.n_tokens = 14, progress = 0.977685
slot update_slots: id  3 | task 3690 | n_tokens = 2804, memory_seq_rm [2804, end)
slot update_slots: id  3 | task 3690 | prompt processing progress, n_tokens = 2868, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3690 | prompt done, n_tokens = 2868, batch.n_tokens = 64
slot init_sampler: id  3 | task 3690 | init sampler, took 0.55 ms, tokens: text = 2868, total = 2868
slot update_slots: id  3 | task 3690 | created context checkpoint 7 of 8 (pos_min = 1932, pos_max = 2803, size = 20.448 MiB)
slot print_timing: id  3 | task 3690 | 
prompt eval time =     365.66 ms /    78 tokens (    4.69 ms per token,   213.31 tokens per second)
       eval time =    7951.49 ms /   288 tokens (   27.61 ms per token,    36.22 tokens per second)
      total time =    8317.15 ms /   366 tokens
slot      release: id  3 | task 3690 | stop processing: n_tokens = 3155, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.909
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3980 | processing task, is_child = 0
slot update_slots: id  3 | task 3980 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2958
slot update_slots: id  3 | task 3980 | n_tokens = 2868, memory_seq_rm [2868, end)
slot update_slots: id  3 | task 3980 | prompt processing progress, n_tokens = 2894, batch.n_tokens = 26, progress = 0.978364
slot update_slots: id  3 | task 3980 | n_tokens = 2894, memory_seq_rm [2894, end)
slot update_slots: id  3 | task 3980 | prompt processing progress, n_tokens = 2958, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3980 | prompt done, n_tokens = 2958, batch.n_tokens = 64
slot init_sampler: id  3 | task 3980 | init sampler, took 0.48 ms, tokens: text = 2958, total = 2958
slot update_slots: id  3 | task 3980 | created context checkpoint 8 of 8 (pos_min = 2131, pos_max = 2893, size = 17.892 MiB)
slot print_timing: id  3 | task 3980 | 
prompt eval time =     365.55 ms /    90 tokens (    4.06 ms per token,   246.21 tokens per second)
       eval time =    4260.52 ms /   152 tokens (   28.03 ms per token,    35.68 tokens per second)
      total time =    4626.07 ms /   242 tokens
slot      release: id  3 | task 3980 | stop processing: n_tokens = 3109, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.951
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4134 | processing task, is_child = 0
slot update_slots: id  3 | task 4134 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3021
slot update_slots: id  3 | task 4134 | n_tokens = 2958, memory_seq_rm [2958, end)
slot update_slots: id  3 | task 4134 | prompt processing progress, n_tokens = 3021, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 4134 | prompt done, n_tokens = 3021, batch.n_tokens = 63
slot init_sampler: id  3 | task 4134 | init sampler, took 0.54 ms, tokens: text = 3021, total = 3021
slot print_timing: id  3 | task 4134 | 
prompt eval time =     253.03 ms /    63 tokens (    4.02 ms per token,   248.99 tokens per second)
       eval time =    2924.87 ms /   108 tokens (   27.08 ms per token,    36.92 tokens per second)
      total time =    3177.89 ms /   171 tokens
slot      release: id  3 | task 4134 | stop processing: n_tokens = 3128, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4243 | processing task, is_child = 0
slot update_slots: id  3 | task 4243 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3099
slot update_slots: id  3 | task 4243 | n_tokens = 3021, memory_seq_rm [3021, end)
slot update_slots: id  3 | task 4243 | prompt processing progress, n_tokens = 3035, batch.n_tokens = 14, progress = 0.979348
slot update_slots: id  3 | task 4243 | n_tokens = 3035, memory_seq_rm [3035, end)
slot update_slots: id  3 | task 4243 | prompt processing progress, n_tokens = 3099, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4243 | prompt done, n_tokens = 3099, batch.n_tokens = 64
slot init_sampler: id  3 | task 4243 | init sampler, took 0.47 ms, tokens: text = 3099, total = 3099
slot update_slots: id  3 | task 4243 | erasing old context checkpoint (pos_min = 167, pos_max = 1000, size = 19.557 MiB)
slot update_slots: id  3 | task 4243 | created context checkpoint 8 of 8 (pos_min = 2131, pos_max = 3034, size = 21.198 MiB)
slot print_timing: id  3 | task 4243 | 
prompt eval time =     337.13 ms /    78 tokens (    4.32 ms per token,   231.36 tokens per second)
       eval time =    3777.04 ms /   142 tokens (   26.60 ms per token,    37.60 tokens per second)
      total time =    4114.17 ms /   220 tokens
slot      release: id  3 | task 4243 | stop processing: n_tokens = 3240, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.268 (> 0.100 thold), f_keep = 0.210
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3240, total state size = 99.987 MiB
srv          load:  - looking for better prompt, base f_keep = 0.210, sim = 0.268
srv        update:  - cache state: 2 prompts, 531.663 MiB (limits: 8192.000 MiB, 56064 tokens, 113512 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv  get_availabl: prompt cache update took 187.18 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4387 | processing task, is_child = 0
slot update_slots: id  3 | task 4387 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2531
slot update_slots: id  3 | task 4387 | n_past = 679, slot.prompt.tokens.size() = 3240, seq_id = 3, pos_min = 2216, n_swa = 128
slot update_slots: id  3 | task 4387 | restored context checkpoint (pos_min = 548, pos_max = 1497, size = 22.277 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 1010, pos_max = 2033, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 1620, pos_max = 2643, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 1897, pos_max = 2725, n_swa = 128, size = 19.439 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 1932, pos_max = 2803, n_swa = 128, size = 20.448 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 2131, pos_max = 2893, n_swa = 128, size = 17.892 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 2131, pos_max = 3034, n_swa = 128, size = 21.198 MiB)
slot update_slots: id  3 | task 4387 | n_tokens = 679, memory_seq_rm [679, end)
slot update_slots: id  3 | task 4387 | prompt processing progress, n_tokens = 2467, batch.n_tokens = 1788, progress = 0.974714
slot update_slots: id  3 | task 4387 | n_tokens = 2467, memory_seq_rm [2467, end)
slot update_slots: id  3 | task 4387 | prompt processing progress, n_tokens = 2531, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4387 | prompt done, n_tokens = 2531, batch.n_tokens = 64
slot init_sampler: id  3 | task 4387 | init sampler, took 0.45 ms, tokens: text = 2531, total = 2531
slot update_slots: id  3 | task 4387 | created context checkpoint 3 of 8 (pos_min = 1443, pos_max = 2466, size = 24.012 MiB)
slot print_timing: id  3 | task 4387 | 
prompt eval time =    2157.28 ms /  1852 tokens (    1.16 ms per token,   858.49 tokens per second)
       eval time =    4500.48 ms /   173 tokens (   26.01 ms per token,    38.44 tokens per second)
      total time =    6657.76 ms /  2025 tokens
slot      release: id  3 | task 4387 | stop processing: n_tokens = 2703, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.936
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4562 | processing task, is_child = 0
slot update_slots: id  3 | task 4562 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2594
slot update_slots: id  3 | task 4562 | n_tokens = 2531, memory_seq_rm [2531, end)
slot update_slots: id  3 | task 4562 | prompt processing progress, n_tokens = 2594, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 4562 | prompt done, n_tokens = 2594, batch.n_tokens = 63
slot init_sampler: id  3 | task 4562 | init sampler, took 0.75 ms, tokens: text = 2594, total = 2594
slot print_timing: id  3 | task 4562 | 
prompt eval time =     229.97 ms /    63 tokens (    3.65 ms per token,   273.95 tokens per second)
       eval time =    4439.49 ms /   165 tokens (   26.91 ms per token,    37.17 tokens per second)
      total time =    4669.46 ms /   228 tokens
slot      release: id  3 | task 4562 | stop processing: n_tokens = 2758, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4728 | processing task, is_child = 0
slot update_slots: id  3 | task 4728 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2709
slot update_slots: id  3 | task 4728 | n_tokens = 2594, memory_seq_rm [2594, end)
slot update_slots: id  3 | task 4728 | prompt processing progress, n_tokens = 2645, batch.n_tokens = 51, progress = 0.976375
slot update_slots: id  3 | task 4728 | n_tokens = 2645, memory_seq_rm [2645, end)
slot update_slots: id  3 | task 4728 | prompt processing progress, n_tokens = 2709, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4728 | prompt done, n_tokens = 2709, batch.n_tokens = 64
slot init_sampler: id  3 | task 4728 | init sampler, took 0.41 ms, tokens: text = 2709, total = 2709
slot update_slots: id  3 | task 4728 | created context checkpoint 4 of 8 (pos_min = 1803, pos_max = 2644, size = 19.744 MiB)
slot print_timing: id  3 | task 4728 | 
prompt eval time =     465.17 ms /   115 tokens (    4.04 ms per token,   247.22 tokens per second)
       eval time =    8104.02 ms /   286 tokens (   28.34 ms per token,    35.29 tokens per second)
      total time =    8569.19 ms /   401 tokens
slot      release: id  3 | task 4728 | stop processing: n_tokens = 2994, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.905
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5016 | processing task, is_child = 0
slot update_slots: id  3 | task 5016 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2787
slot update_slots: id  3 | task 5016 | n_tokens = 2709, memory_seq_rm [2709, end)
slot update_slots: id  3 | task 5016 | prompt processing progress, n_tokens = 2723, batch.n_tokens = 14, progress = 0.977036
slot update_slots: id  3 | task 5016 | n_tokens = 2723, memory_seq_rm [2723, end)
slot update_slots: id  3 | task 5016 | prompt processing progress, n_tokens = 2787, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5016 | prompt done, n_tokens = 2787, batch.n_tokens = 64
slot init_sampler: id  3 | task 5016 | init sampler, took 0.50 ms, tokens: text = 2787, total = 2787
slot update_slots: id  3 | task 5016 | created context checkpoint 5 of 8 (pos_min = 2039, pos_max = 2722, size = 16.039 MiB)
slot print_timing: id  3 | task 5016 | 
prompt eval time =     327.67 ms /    78 tokens (    4.20 ms per token,   238.05 tokens per second)
       eval time =    4553.99 ms /   158 tokens (   28.82 ms per token,    34.69 tokens per second)
      total time =    4881.66 ms /   236 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5016 | stop processing: n_tokens = 2944, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.915 (> 0.100 thold), f_keep = 0.947
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5176 | processing task, is_child = 0
slot update_slots: id  3 | task 5176 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3047
slot update_slots: id  3 | task 5176 | n_tokens = 2787, memory_seq_rm [2787, end)
slot update_slots: id  3 | task 5176 | prompt processing progress, n_tokens = 2983, batch.n_tokens = 196, progress = 0.978996
slot update_slots: id  3 | task 5176 | n_tokens = 2983, memory_seq_rm [2983, end)
slot update_slots: id  3 | task 5176 | prompt processing progress, n_tokens = 3047, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5176 | prompt done, n_tokens = 3047, batch.n_tokens = 64
slot init_sampler: id  3 | task 5176 | init sampler, took 0.48 ms, tokens: text = 3047, total = 3047
slot update_slots: id  3 | task 5176 | created context checkpoint 6 of 8 (pos_min = 2039, pos_max = 2982, size = 22.136 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 5176
slot      release: id  3 | task 5176 | stop processing: n_tokens = 3108, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.273 (> 0.100 thold), f_keep = 0.218
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3108, total state size = 95.274 MiB
srv          load:  - looking for better prompt, base f_keep = 0.218, sim = 0.273
srv        update:  - cache state: 3 prompts, 750.562 MiB (limits: 8192.000 MiB, 56064 tokens, 114329 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv  get_availabl: prompt cache update took 154.01 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5241 | processing task, is_child = 0
slot update_slots: id  3 | task 5241 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2472
slot update_slots: id  3 | task 5241 | n_past = 676, slot.prompt.tokens.size() = 3108, seq_id = 3, pos_min = 2153, n_swa = 128
slot update_slots: id  3 | task 5241 | restored context checkpoint (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot update_slots: id  3 | task 5241 | erased invalidated context checkpoint (pos_min = 1443, pos_max = 2466, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5241 | erased invalidated context checkpoint (pos_min = 1803, pos_max = 2644, n_swa = 128, size = 19.744 MiB)
slot update_slots: id  3 | task 5241 | erased invalidated context checkpoint (pos_min = 2039, pos_max = 2722, n_swa = 128, size = 16.039 MiB)
slot update_slots: id  3 | task 5241 | erased invalidated context checkpoint (pos_min = 2039, pos_max = 2982, n_swa = 128, size = 22.136 MiB)
slot update_slots: id  3 | task 5241 | n_tokens = 676, memory_seq_rm [676, end)
slot update_slots: id  3 | task 5241 | prompt processing progress, n_tokens = 2408, batch.n_tokens = 1732, progress = 0.974110
slot update_slots: id  3 | task 5241 | n_tokens = 2408, memory_seq_rm [2408, end)
slot update_slots: id  3 | task 5241 | prompt processing progress, n_tokens = 2472, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5241 | prompt done, n_tokens = 2472, batch.n_tokens = 64
slot init_sampler: id  3 | task 5241 | init sampler, took 0.43 ms, tokens: text = 2472, total = 2472
slot update_slots: id  3 | task 5241 | created context checkpoint 3 of 8 (pos_min = 1384, pos_max = 2407, size = 24.012 MiB)
slot print_timing: id  3 | task 5241 | 
prompt eval time =    2097.48 ms /  1796 tokens (    1.17 ms per token,   856.26 tokens per second)
       eval time =    7991.59 ms /   294 tokens (   27.18 ms per token,    36.79 tokens per second)
      total time =   10089.07 ms /  2090 tokens
slot      release: id  3 | task 5241 | stop processing: n_tokens = 2765, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.811 (> 0.100 thold), f_keep = 0.244
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2765, total state size = 88.849 MiB
srv          load:  - looking for better prompt, base f_keep = 0.244, sim = 0.811
srv        update:  - cache state: 4 prompts, 905.115 MiB (limits: 8192.000 MiB, 56064 tokens, 119832 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv  get_availabl: prompt cache update took 110.33 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5537 | processing task, is_child = 0
slot update_slots: id  3 | task 5537 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 832
slot update_slots: id  3 | task 5537 | n_past = 675, slot.prompt.tokens.size() = 2765, seq_id = 3, pos_min = 1741, n_swa = 128
slot update_slots: id  3 | task 5537 | restored context checkpoint (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot update_slots: id  3 | task 5537 | erased invalidated context checkpoint (pos_min = 548, pos_max = 1497, n_swa = 128, size = 22.277 MiB)
slot update_slots: id  3 | task 5537 | erased invalidated context checkpoint (pos_min = 1384, pos_max = 2407, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5537 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  3 | task 5537 | prompt processing progress, n_tokens = 768, batch.n_tokens = 93, progress = 0.923077
slot update_slots: id  3 | task 5537 | n_tokens = 768, memory_seq_rm [768, end)
slot update_slots: id  3 | task 5537 | prompt processing progress, n_tokens = 832, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5537 | prompt done, n_tokens = 832, batch.n_tokens = 64
slot init_sampler: id  3 | task 5537 | init sampler, took 0.15 ms, tokens: text = 832, total = 832
slot print_timing: id  3 | task 5537 | 
prompt eval time =     497.80 ms /   157 tokens (    3.17 ms per token,   315.39 tokens per second)
       eval time =    1008.68 ms /    40 tokens (   25.22 ms per token,    39.66 tokens per second)
      total time =    1506.49 ms /   197 tokens
slot      release: id  3 | task 5537 | stop processing: n_tokens = 871, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.626 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5579 | processing task, is_child = 0
slot update_slots: id  3 | task 5579 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1330
slot update_slots: id  3 | task 5579 | n_tokens = 832, memory_seq_rm [832, end)
slot update_slots: id  3 | task 5579 | prompt processing progress, n_tokens = 1266, batch.n_tokens = 434, progress = 0.951880
slot update_slots: id  3 | task 5579 | n_tokens = 1266, memory_seq_rm [1266, end)
slot update_slots: id  3 | task 5579 | prompt processing progress, n_tokens = 1330, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5579 | prompt done, n_tokens = 1330, batch.n_tokens = 64
slot init_sampler: id  3 | task 5579 | init sampler, took 0.22 ms, tokens: text = 1330, total = 1330
slot update_slots: id  3 | task 5579 | created context checkpoint 2 of 8 (pos_min = 675, pos_max = 1265, size = 13.859 MiB)
slot print_timing: id  3 | task 5579 | 
prompt eval time =     598.12 ms /   498 tokens (    1.20 ms per token,   832.62 tokens per second)
       eval time =    1187.39 ms /    45 tokens (   26.39 ms per token,    37.90 tokens per second)
      total time =    1785.50 ms /   543 tokens
slot      release: id  3 | task 5579 | stop processing: n_tokens = 1374, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.459 (> 0.100 thold), f_keep = 0.968
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5626 | processing task, is_child = 0
slot update_slots: id  3 | task 5626 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2898
slot update_slots: id  3 | task 5626 | n_tokens = 1330, memory_seq_rm [1330, end)
slot update_slots: id  3 | task 5626 | prompt processing progress, n_tokens = 2834, batch.n_tokens = 1504, progress = 0.977916
slot update_slots: id  3 | task 5626 | n_tokens = 2834, memory_seq_rm [2834, end)
slot update_slots: id  3 | task 5626 | prompt processing progress, n_tokens = 2898, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5626 | prompt done, n_tokens = 2898, batch.n_tokens = 64
slot init_sampler: id  3 | task 5626 | init sampler, took 0.55 ms, tokens: text = 2898, total = 2898
slot update_slots: id  3 | task 5626 | created context checkpoint 3 of 8 (pos_min = 1810, pos_max = 2833, size = 24.012 MiB)
slot print_timing: id  3 | task 5626 | 
prompt eval time =    1753.27 ms /  1568 tokens (    1.12 ms per token,   894.33 tokens per second)
       eval time =    3547.19 ms /   131 tokens (   27.08 ms per token,    36.93 tokens per second)
      total time =    5300.46 ms /  1699 tokens
slot      release: id  3 | task 5626 | stop processing: n_tokens = 3028, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.957
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5759 | processing task, is_child = 0
slot update_slots: id  3 | task 5759 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2961
slot update_slots: id  3 | task 5759 | n_tokens = 2898, memory_seq_rm [2898, end)
slot update_slots: id  3 | task 5759 | prompt processing progress, n_tokens = 2961, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 5759 | prompt done, n_tokens = 2961, batch.n_tokens = 63
slot init_sampler: id  3 | task 5759 | init sampler, took 0.62 ms, tokens: text = 2961, total = 2961
slot print_timing: id  3 | task 5759 | 
prompt eval time =     252.33 ms /    63 tokens (    4.01 ms per token,   249.67 tokens per second)
       eval time =    2610.25 ms /    97 tokens (   26.91 ms per token,    37.16 tokens per second)
      total time =    2862.58 ms /   160 tokens
slot      release: id  3 | task 5759 | stop processing: n_tokens = 3057, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5857 | processing task, is_child = 0
slot update_slots: id  3 | task 5857 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3069
slot update_slots: id  3 | task 5857 | n_tokens = 2961, memory_seq_rm [2961, end)
slot update_slots: id  3 | task 5857 | prompt processing progress, n_tokens = 3005, batch.n_tokens = 44, progress = 0.979146
slot update_slots: id  3 | task 5857 | n_tokens = 3005, memory_seq_rm [3005, end)
slot update_slots: id  3 | task 5857 | prompt processing progress, n_tokens = 3069, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5857 | prompt done, n_tokens = 3069, batch.n_tokens = 64
slot init_sampler: id  3 | task 5857 | init sampler, took 0.68 ms, tokens: text = 3069, total = 3069
slot update_slots: id  3 | task 5857 | created context checkpoint 4 of 8 (pos_min = 2033, pos_max = 3004, size = 22.793 MiB)
slot print_timing: id  3 | task 5857 | 
prompt eval time =     325.31 ms /   108 tokens (    3.01 ms per token,   331.99 tokens per second)
       eval time =    7445.88 ms /   269 tokens (   27.68 ms per token,    36.13 tokens per second)
      total time =    7771.19 ms /   377 tokens
slot      release: id  3 | task 5857 | stop processing: n_tokens = 3337, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6128 | processing task, is_child = 0
slot update_slots: id  3 | task 6128 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3124
slot update_slots: id  3 | task 6128 | n_tokens = 3069, memory_seq_rm [3069, end)
slot update_slots: id  3 | task 6128 | prompt processing progress, n_tokens = 3124, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 6128 | prompt done, n_tokens = 3124, batch.n_tokens = 55
slot init_sampler: id  3 | task 6128 | init sampler, took 0.63 ms, tokens: text = 3124, total = 3124
slot print_timing: id  3 | task 6128 | 
prompt eval time =     182.90 ms /    55 tokens (    3.33 ms per token,   300.71 tokens per second)
       eval time =   27471.33 ms /   960 tokens (   28.62 ms per token,    34.95 tokens per second)
      total time =   27654.23 ms /  1015 tokens
slot      release: id  3 | task 6128 | stop processing: n_tokens = 4083, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.333 (> 0.100 thold), f_keep = 0.165
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4083, total state size = 119.754 MiB
srv          load:  - looking for better prompt, base f_keep = 0.165, sim = 0.333
srv        update:  - cache state: 5 prompts, 1104.949 MiB (limits: 8192.000 MiB, 56064 tokens, 128431 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv  get_availabl: prompt cache update took 157.07 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7089 | processing task, is_child = 0
slot update_slots: id  3 | task 7089 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2024
slot update_slots: id  3 | task 7089 | n_past = 675, slot.prompt.tokens.size() = 4083, seq_id = 3, pos_min = 3059, n_swa = 128
slot update_slots: id  3 | task 7089 | restored context checkpoint (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot update_slots: id  3 | task 7089 | erased invalidated context checkpoint (pos_min = 675, pos_max = 1265, n_swa = 128, size = 13.859 MiB)
slot update_slots: id  3 | task 7089 | erased invalidated context checkpoint (pos_min = 1810, pos_max = 2833, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 7089 | erased invalidated context checkpoint (pos_min = 2033, pos_max = 3004, n_swa = 128, size = 22.793 MiB)
slot update_slots: id  3 | task 7089 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  3 | task 7089 | prompt processing progress, n_tokens = 1960, batch.n_tokens = 1285, progress = 0.968379
slot update_slots: id  3 | task 7089 | n_tokens = 1960, memory_seq_rm [1960, end)
slot update_slots: id  3 | task 7089 | prompt processing progress, n_tokens = 2024, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7089 | prompt done, n_tokens = 2024, batch.n_tokens = 64
slot init_sampler: id  3 | task 7089 | init sampler, took 0.35 ms, tokens: text = 2024, total = 2024
slot update_slots: id  3 | task 7089 | created context checkpoint 2 of 8 (pos_min = 1063, pos_max = 1959, size = 21.034 MiB)
slot print_timing: id  3 | task 7089 | 
prompt eval time =    1552.36 ms /  1349 tokens (    1.15 ms per token,   869.00 tokens per second)
       eval time =   15507.73 ms /   604 tokens (   25.68 ms per token,    38.95 tokens per second)
      total time =   17060.08 ms /  1953 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 7089 | stop processing: n_tokens = 2627, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.450 (> 0.100 thold), f_keep = 0.508
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7695 | processing task, is_child = 0
slot update_slots: id  3 | task 7695 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2966
slot update_slots: id  3 | task 7695 | n_past = 1334, slot.prompt.tokens.size() = 2627, seq_id = 3, pos_min = 1603, n_swa = 128
slot update_slots: id  3 | task 7695 | restored context checkpoint (pos_min = 1063, pos_max = 1959, size = 21.034 MiB)
slot update_slots: id  3 | task 7695 | n_tokens = 1334, memory_seq_rm [1334, end)
slot update_slots: id  3 | task 7695 | prompt processing progress, n_tokens = 2902, batch.n_tokens = 1568, progress = 0.978422
slot update_slots: id  3 | task 7695 | n_tokens = 2902, memory_seq_rm [2902, end)
slot update_slots: id  3 | task 7695 | prompt processing progress, n_tokens = 2966, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7695 | prompt done, n_tokens = 2966, batch.n_tokens = 64
slot init_sampler: id  3 | task 7695 | init sampler, took 0.48 ms, tokens: text = 2966, total = 2966
slot update_slots: id  3 | task 7695 | created context checkpoint 3 of 8 (pos_min = 1878, pos_max = 2901, size = 24.012 MiB)
slot print_timing: id  3 | task 7695 | 
prompt eval time =    1996.91 ms /  1632 tokens (    1.22 ms per token,   817.26 tokens per second)
       eval time =    5986.18 ms /   215 tokens (   27.84 ms per token,    35.92 tokens per second)
      total time =    7983.09 ms /  1847 tokens
slot      release: id  3 | task 7695 | stop processing: n_tokens = 3180, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.654 (> 0.100 thold), f_keep = 0.933
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7912 | processing task, is_child = 0
slot update_slots: id  3 | task 7912 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4534
slot update_slots: id  3 | task 7912 | n_tokens = 2966, memory_seq_rm [2966, end)
slot update_slots: id  3 | task 7912 | prompt processing progress, n_tokens = 4470, batch.n_tokens = 1504, progress = 0.985884
slot update_slots: id  3 | task 7912 | n_tokens = 4470, memory_seq_rm [4470, end)
slot update_slots: id  3 | task 7912 | prompt processing progress, n_tokens = 4534, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7912 | prompt done, n_tokens = 4534, batch.n_tokens = 64
slot init_sampler: id  3 | task 7912 | init sampler, took 0.72 ms, tokens: text = 4534, total = 4534
slot update_slots: id  3 | task 7912 | created context checkpoint 4 of 8 (pos_min = 3446, pos_max = 4469, size = 24.012 MiB)
slot print_timing: id  3 | task 7912 | 
prompt eval time =    1942.80 ms /  1568 tokens (    1.24 ms per token,   807.08 tokens per second)
       eval time =   42060.06 ms /  1511 tokens (   27.84 ms per token,    35.92 tokens per second)
      total time =   44002.86 ms /  3079 tokens
slot      release: id  3 | task 7912 | stop processing: n_tokens = 6044, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.787 (> 0.100 thold), f_keep = 0.750
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9425 | processing task, is_child = 0
slot update_slots: id  3 | task 9425 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5761
slot update_slots: id  3 | task 9425 | n_past = 4534, slot.prompt.tokens.size() = 6044, seq_id = 3, pos_min = 5020, n_swa = 128
slot update_slots: id  3 | task 9425 | restored context checkpoint (pos_min = 3446, pos_max = 4469, size = 24.012 MiB)
slot update_slots: id  3 | task 9425 | n_tokens = 4469, memory_seq_rm [4469, end)
slot update_slots: id  3 | task 9425 | prompt processing progress, n_tokens = 5697, batch.n_tokens = 1228, progress = 0.988891
slot update_slots: id  3 | task 9425 | n_tokens = 5697, memory_seq_rm [5697, end)
slot update_slots: id  3 | task 9425 | prompt processing progress, n_tokens = 5761, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9425 | prompt done, n_tokens = 5761, batch.n_tokens = 64
slot init_sampler: id  3 | task 9425 | init sampler, took 1.09 ms, tokens: text = 5761, total = 5761
slot update_slots: id  3 | task 9425 | created context checkpoint 5 of 8 (pos_min = 4673, pos_max = 5696, size = 24.012 MiB)
slot print_timing: id  3 | task 9425 | 
prompt eval time =    1584.60 ms /  1292 tokens (    1.23 ms per token,   815.35 tokens per second)
       eval time =    3376.44 ms /   130 tokens (   25.97 ms per token,    38.50 tokens per second)
      total time =    4961.04 ms /  1422 tokens
slot      release: id  3 | task 9425 | stop processing: n_tokens = 5890, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.868 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9557 | processing task, is_child = 0
slot update_slots: id  3 | task 9557 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6639
slot update_slots: id  3 | task 9557 | n_tokens = 5761, memory_seq_rm [5761, end)
slot update_slots: id  3 | task 9557 | prompt processing progress, n_tokens = 6575, batch.n_tokens = 814, progress = 0.990360
slot update_slots: id  3 | task 9557 | n_tokens = 6575, memory_seq_rm [6575, end)
slot update_slots: id  3 | task 9557 | prompt processing progress, n_tokens = 6639, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9557 | prompt done, n_tokens = 6639, batch.n_tokens = 64
slot init_sampler: id  3 | task 9557 | init sampler, took 1.06 ms, tokens: text = 6639, total = 6639
slot update_slots: id  3 | task 9557 | created context checkpoint 6 of 8 (pos_min = 5551, pos_max = 6574, size = 24.012 MiB)
slot print_timing: id  3 | task 9557 | 
prompt eval time =    1150.85 ms /   878 tokens (    1.31 ms per token,   762.92 tokens per second)
       eval time =   45669.90 ms /  1697 tokens (   26.91 ms per token,    37.16 tokens per second)
      total time =   46820.75 ms /  2575 tokens
slot      release: id  3 | task 9557 | stop processing: n_tokens = 8335, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.856 (> 0.100 thold), f_keep = 0.797
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11256 | processing task, is_child = 0
slot update_slots: id  3 | task 11256 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7755
slot update_slots: id  3 | task 11256 | n_past = 6639, slot.prompt.tokens.size() = 8335, seq_id = 3, pos_min = 7311, n_swa = 128
slot update_slots: id  3 | task 11256 | restored context checkpoint (pos_min = 5551, pos_max = 6574, size = 24.012 MiB)
slot update_slots: id  3 | task 11256 | n_tokens = 6574, memory_seq_rm [6574, end)
slot update_slots: id  3 | task 11256 | prompt processing progress, n_tokens = 7691, batch.n_tokens = 1117, progress = 0.991747
slot update_slots: id  3 | task 11256 | n_tokens = 7691, memory_seq_rm [7691, end)
slot update_slots: id  3 | task 11256 | prompt processing progress, n_tokens = 7755, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11256 | prompt done, n_tokens = 7755, batch.n_tokens = 64
slot init_sampler: id  3 | task 11256 | init sampler, took 1.56 ms, tokens: text = 7755, total = 7755
slot update_slots: id  3 | task 11256 | created context checkpoint 7 of 8 (pos_min = 6667, pos_max = 7690, size = 24.012 MiB)
slot print_timing: id  3 | task 11256 | 
prompt eval time =    1623.34 ms /  1181 tokens (    1.37 ms per token,   727.51 tokens per second)
       eval time =    3491.99 ms /   125 tokens (   27.94 ms per token,    35.80 tokens per second)
      total time =    5115.33 ms /  1306 tokens
slot      release: id  3 | task 11256 | stop processing: n_tokens = 7879, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.868 (> 0.100 thold), f_keep = 0.007
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7879, total state size = 208.767 MiB
srv          load:  - looking for better prompt, base f_keep = 0.007, sim = 0.868
srv        update:  - cache state: 6 prompts, 1474.225 MiB (limits: 8192.000 MiB, 56064 tokens, 140042 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv  get_availabl: prompt cache update took 303.31 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11383 | processing task, is_child = 0
slot update_slots: id  3 | task 11383 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 68
slot update_slots: id  3 | task 11383 | n_past = 59, slot.prompt.tokens.size() = 7879, seq_id = 3, pos_min = 6855, n_swa = 128
slot update_slots: id  3 | task 11383 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 252, pos_max = 1079, n_swa = 128, size = 19.416 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 1063, pos_max = 1959, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 1878, pos_max = 2901, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 3446, pos_max = 4469, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 4673, pos_max = 5696, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 5551, pos_max = 6574, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 6667, pos_max = 7690, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11383 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 11383 | prompt processing progress, n_tokens = 4, batch.n_tokens = 4, progress = 0.058824
slot update_slots: id  3 | task 11383 | n_tokens = 4, memory_seq_rm [4, end)
slot update_slots: id  3 | task 11383 | prompt processing progress, n_tokens = 68, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11383 | prompt done, n_tokens = 68, batch.n_tokens = 64
slot init_sampler: id  3 | task 11383 | init sampler, took 0.01 ms, tokens: text = 68, total = 68
slot print_timing: id  3 | task 11383 | 
prompt eval time =     302.25 ms /    68 tokens (    4.44 ms per token,   224.98 tokens per second)
       eval time =     873.62 ms /    35 tokens (   24.96 ms per token,    40.06 tokens per second)
      total time =    1175.87 ms /   103 tokens
slot      release: id  3 | task 11383 | stop processing: n_tokens = 102, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.928 (> 0.100 thold), f_keep = 0.627
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11420 | processing task, is_child = 0
slot update_slots: id  3 | task 11420 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 69
slot update_slots: id  3 | task 11420 | n_tokens = 64, memory_seq_rm [64, end)
slot update_slots: id  3 | task 11420 | prompt processing progress, n_tokens = 69, batch.n_tokens = 5, progress = 1.000000
slot update_slots: id  3 | task 11420 | prompt done, n_tokens = 69, batch.n_tokens = 5
slot init_sampler: id  3 | task 11420 | init sampler, took 0.01 ms, tokens: text = 69, total = 69
slot print_timing: id  3 | task 11420 | 
prompt eval time =      61.61 ms /     5 tokens (   12.32 ms per token,    81.15 tokens per second)
       eval time =     721.65 ms /    29 tokens (   24.88 ms per token,    40.19 tokens per second)
      total time =     783.26 ms /    34 tokens
slot      release: id  3 | task 11420 | stop processing: n_tokens = 97, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11450 | processing task, is_child = 0
slot update_slots: id  2 | task 11450 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8201
slot update_slots: id  2 | task 11450 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 11450 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.249726
slot update_slots: id  2 | task 11450 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 11450 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.499451
slot update_slots: id  2 | task 11450 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 11450 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.749177
slot update_slots: id  2 | task 11450 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 11450 | prompt processing progress, n_tokens = 8137, batch.n_tokens = 1993, progress = 0.992196
slot update_slots: id  2 | task 11450 | n_tokens = 8137, memory_seq_rm [8137, end)
slot update_slots: id  2 | task 11450 | prompt processing progress, n_tokens = 8201, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11450 | prompt done, n_tokens = 8201, batch.n_tokens = 64
slot init_sampler: id  2 | task 11450 | init sampler, took 1.27 ms, tokens: text = 8201, total = 8201
slot update_slots: id  2 | task 11450 | created context checkpoint 1 of 8 (pos_min = 7210, pos_max = 8136, size = 21.737 MiB)
slot print_timing: id  2 | task 11450 | 
prompt eval time =    8849.27 ms /  8201 tokens (    1.08 ms per token,   926.74 tokens per second)
       eval time =    1296.92 ms /    48 tokens (   27.02 ms per token,    37.01 tokens per second)
      total time =   10146.19 ms /  8249 tokens
slot      release: id  2 | task 11450 | stop processing: n_tokens = 8248, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.884 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11503 | processing task, is_child = 0
slot update_slots: id  2 | task 11503 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9278
slot update_slots: id  2 | task 11503 | n_tokens = 8201, memory_seq_rm [8201, end)
slot update_slots: id  2 | task 11503 | prompt processing progress, n_tokens = 9214, batch.n_tokens = 1013, progress = 0.993102
slot update_slots: id  2 | task 11503 | n_tokens = 9214, memory_seq_rm [9214, end)
slot update_slots: id  2 | task 11503 | prompt processing progress, n_tokens = 9278, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11503 | prompt done, n_tokens = 9278, batch.n_tokens = 64
slot init_sampler: id  2 | task 11503 | init sampler, took 1.39 ms, tokens: text = 9278, total = 9278
slot update_slots: id  2 | task 11503 | created context checkpoint 2 of 8 (pos_min = 8287, pos_max = 9213, size = 21.737 MiB)
slot print_timing: id  2 | task 11503 | 
prompt eval time =    1414.80 ms /  1077 tokens (    1.31 ms per token,   761.24 tokens per second)
       eval time =    2287.95 ms /    84 tokens (   27.24 ms per token,    36.71 tokens per second)
      total time =    3702.75 ms /  1161 tokens
slot      release: id  2 | task 11503 | stop processing: n_tokens = 9361, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11589 | processing task, is_child = 0
slot update_slots: id  2 | task 11589 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9601
slot update_slots: id  2 | task 11589 | n_tokens = 9278, memory_seq_rm [9278, end)
slot update_slots: id  2 | task 11589 | prompt processing progress, n_tokens = 9537, batch.n_tokens = 259, progress = 0.993334
slot update_slots: id  2 | task 11589 | n_tokens = 9537, memory_seq_rm [9537, end)
slot update_slots: id  2 | task 11589 | prompt processing progress, n_tokens = 9601, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11589 | prompt done, n_tokens = 9601, batch.n_tokens = 64
slot init_sampler: id  2 | task 11589 | init sampler, took 1.39 ms, tokens: text = 9601, total = 9601
slot update_slots: id  2 | task 11589 | created context checkpoint 3 of 8 (pos_min = 8610, pos_max = 9536, size = 21.737 MiB)
slot print_timing: id  2 | task 11589 | 
prompt eval time =     593.04 ms /   323 tokens (    1.84 ms per token,   544.65 tokens per second)
       eval time =    2516.80 ms /    91 tokens (   27.66 ms per token,    36.16 tokens per second)
      total time =    3109.84 ms /   414 tokens
slot      release: id  2 | task 11589 | stop processing: n_tokens = 9691, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11682 | processing task, is_child = 0
slot update_slots: id  2 | task 11682 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9879
slot update_slots: id  2 | task 11682 | n_tokens = 9601, memory_seq_rm [9601, end)
slot update_slots: id  2 | task 11682 | prompt processing progress, n_tokens = 9815, batch.n_tokens = 214, progress = 0.993522
slot update_slots: id  2 | task 11682 | n_tokens = 9815, memory_seq_rm [9815, end)
slot update_slots: id  2 | task 11682 | prompt processing progress, n_tokens = 9879, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11682 | prompt done, n_tokens = 9879, batch.n_tokens = 64
slot init_sampler: id  2 | task 11682 | init sampler, took 1.85 ms, tokens: text = 9879, total = 9879
slot update_slots: id  2 | task 11682 | created context checkpoint 4 of 8 (pos_min = 8888, pos_max = 9814, size = 21.737 MiB)
slot print_timing: id  2 | task 11682 | 
prompt eval time =     558.76 ms /   278 tokens (    2.01 ms per token,   497.53 tokens per second)
       eval time =    4986.97 ms /   183 tokens (   27.25 ms per token,    36.70 tokens per second)
      total time =    5545.73 ms /   461 tokens
slot      release: id  2 | task 11682 | stop processing: n_tokens = 10061, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11867 | processing task, is_child = 0
slot update_slots: id  2 | task 11867 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10463
slot update_slots: id  2 | task 11867 | n_tokens = 9879, memory_seq_rm [9879, end)
slot update_slots: id  2 | task 11867 | prompt processing progress, n_tokens = 10399, batch.n_tokens = 520, progress = 0.993883
slot update_slots: id  2 | task 11867 | n_tokens = 10399, memory_seq_rm [10399, end)
slot update_slots: id  2 | task 11867 | prompt processing progress, n_tokens = 10463, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11867 | prompt done, n_tokens = 10463, batch.n_tokens = 64
slot init_sampler: id  2 | task 11867 | init sampler, took 1.53 ms, tokens: text = 10463, total = 10463
slot update_slots: id  2 | task 11867 | created context checkpoint 5 of 8 (pos_min = 9472, pos_max = 10398, size = 21.737 MiB)
slot print_timing: id  2 | task 11867 | 
prompt eval time =     908.67 ms /   584 tokens (    1.56 ms per token,   642.70 tokens per second)
       eval time =   11826.28 ms /   433 tokens (   27.31 ms per token,    36.61 tokens per second)
      total time =   12734.95 ms /  1017 tokens
slot      release: id  2 | task 11867 | stop processing: n_tokens = 10895, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.960
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12302 | processing task, is_child = 0
slot update_slots: id  2 | task 12302 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10590
slot update_slots: id  2 | task 12302 | n_tokens = 10463, memory_seq_rm [10463, end)
slot update_slots: id  2 | task 12302 | prompt processing progress, n_tokens = 10526, batch.n_tokens = 63, progress = 0.993957
slot update_slots: id  2 | task 12302 | n_tokens = 10526, memory_seq_rm [10526, end)
slot update_slots: id  2 | task 12302 | prompt processing progress, n_tokens = 10590, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12302 | prompt done, n_tokens = 10590, batch.n_tokens = 64
slot init_sampler: id  2 | task 12302 | init sampler, took 1.50 ms, tokens: text = 10590, total = 10590
slot update_slots: id  2 | task 12302 | created context checkpoint 6 of 8 (pos_min = 9968, pos_max = 10525, size = 13.085 MiB)
slot print_timing: id  2 | task 12302 | 
prompt eval time =     386.90 ms /   127 tokens (    3.05 ms per token,   328.25 tokens per second)
       eval time =   10693.25 ms /   392 tokens (   27.28 ms per token,    36.66 tokens per second)
      total time =   11080.15 ms /   519 tokens
slot      release: id  2 | task 12302 | stop processing: n_tokens = 10981, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12696 | processing task, is_child = 0
slot update_slots: id  2 | task 12696 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10774
slot update_slots: id  2 | task 12696 | n_tokens = 10590, memory_seq_rm [10590, end)
slot update_slots: id  2 | task 12696 | prompt processing progress, n_tokens = 10710, batch.n_tokens = 120, progress = 0.994060
slot update_slots: id  2 | task 12696 | n_tokens = 10710, memory_seq_rm [10710, end)
slot update_slots: id  2 | task 12696 | prompt processing progress, n_tokens = 10774, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12696 | prompt done, n_tokens = 10774, batch.n_tokens = 64
slot init_sampler: id  2 | task 12696 | init sampler, took 1.56 ms, tokens: text = 10774, total = 10774
slot update_slots: id  2 | task 12696 | created context checkpoint 7 of 8 (pos_min = 10054, pos_max = 10709, size = 15.383 MiB)
slot print_timing: id  2 | task 12696 | 
prompt eval time =     518.59 ms /   184 tokens (    2.82 ms per token,   354.81 tokens per second)
       eval time =   14393.74 ms /   530 tokens (   27.16 ms per token,    36.82 tokens per second)
      total time =   14912.33 ms /   714 tokens
slot      release: id  2 | task 12696 | stop processing: n_tokens = 11303, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13228 | processing task, is_child = 0
slot update_slots: id  2 | task 13228 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10975
slot update_slots: id  2 | task 13228 | n_tokens = 10774, memory_seq_rm [10774, end)
slot update_slots: id  2 | task 13228 | prompt processing progress, n_tokens = 10911, batch.n_tokens = 137, progress = 0.994169
slot update_slots: id  2 | task 13228 | n_tokens = 10911, memory_seq_rm [10911, end)
slot update_slots: id  2 | task 13228 | prompt processing progress, n_tokens = 10975, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13228 | prompt done, n_tokens = 10975, batch.n_tokens = 64
slot init_sampler: id  2 | task 13228 | init sampler, took 1.67 ms, tokens: text = 10975, total = 10975
slot update_slots: id  2 | task 13228 | created context checkpoint 8 of 8 (pos_min = 10376, pos_max = 10910, size = 12.545 MiB)
slot print_timing: id  2 | task 13228 | 
prompt eval time =     472.77 ms /   201 tokens (    2.35 ms per token,   425.16 tokens per second)
       eval time =   17315.05 ms /   636 tokens (   27.22 ms per token,    36.73 tokens per second)
      total time =   17787.82 ms /   837 tokens
slot      release: id  2 | task 13228 | stop processing: n_tokens = 11610, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.945
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13866 | processing task, is_child = 0
slot update_slots: id  2 | task 13866 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11183
slot update_slots: id  2 | task 13866 | n_tokens = 10975, memory_seq_rm [10975, end)
slot update_slots: id  2 | task 13866 | prompt processing progress, n_tokens = 11119, batch.n_tokens = 144, progress = 0.994277
slot update_slots: id  2 | task 13866 | n_tokens = 11119, memory_seq_rm [11119, end)
slot update_slots: id  2 | task 13866 | prompt processing progress, n_tokens = 11183, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13866 | prompt done, n_tokens = 11183, batch.n_tokens = 64
slot init_sampler: id  2 | task 13866 | init sampler, took 1.59 ms, tokens: text = 11183, total = 11183
slot update_slots: id  2 | task 13866 | erasing old context checkpoint (pos_min = 7210, pos_max = 8136, size = 21.737 MiB)
slot update_slots: id  2 | task 13866 | created context checkpoint 8 of 8 (pos_min = 10774, pos_max = 11118, size = 8.090 MiB)
slot print_timing: id  2 | task 13866 | 
prompt eval time =     482.19 ms /   208 tokens (    2.32 ms per token,   431.37 tokens per second)
       eval time =   19830.68 ms /   728 tokens (   27.24 ms per token,    36.71 tokens per second)
      total time =   20312.87 ms /   936 tokens
slot      release: id  2 | task 13866 | stop processing: n_tokens = 11910, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 14596 | processing task, is_child = 0
slot update_slots: id  2 | task 14596 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12012
slot update_slots: id  2 | task 14596 | n_tokens = 11183, memory_seq_rm [11183, end)
slot update_slots: id  2 | task 14596 | prompt processing progress, n_tokens = 11948, batch.n_tokens = 765, progress = 0.994672
slot update_slots: id  2 | task 14596 | n_tokens = 11948, memory_seq_rm [11948, end)
slot update_slots: id  2 | task 14596 | prompt processing progress, n_tokens = 12012, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 14596 | prompt done, n_tokens = 12012, batch.n_tokens = 64
slot init_sampler: id  2 | task 14596 | init sampler, took 1.73 ms, tokens: text = 12012, total = 12012
slot update_slots: id  2 | task 14596 | erasing old context checkpoint (pos_min = 8287, pos_max = 9213, size = 21.737 MiB)
slot update_slots: id  2 | task 14596 | created context checkpoint 8 of 8 (pos_min = 11056, pos_max = 11947, size = 20.917 MiB)
slot print_timing: id  2 | task 14596 | 
prompt eval time =    1265.94 ms /   829 tokens (    1.53 ms per token,   654.85 tokens per second)
       eval time =    1303.54 ms /    48 tokens (   27.16 ms per token,    36.82 tokens per second)
      total time =    2569.48 ms /   877 tokens
slot      release: id  2 | task 14596 | stop processing: n_tokens = 12059, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.913 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 14646 | processing task, is_child = 0
slot update_slots: id  2 | task 14646 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13163
slot update_slots: id  2 | task 14646 | n_tokens = 12012, memory_seq_rm [12012, end)
slot update_slots: id  2 | task 14646 | prompt processing progress, n_tokens = 13099, batch.n_tokens = 1087, progress = 0.995138
slot update_slots: id  2 | task 14646 | n_tokens = 13099, memory_seq_rm [13099, end)
slot update_slots: id  2 | task 14646 | prompt processing progress, n_tokens = 13163, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 14646 | prompt done, n_tokens = 13163, batch.n_tokens = 64
slot init_sampler: id  2 | task 14646 | init sampler, took 1.94 ms, tokens: text = 13163, total = 13163
slot update_slots: id  2 | task 14646 | erasing old context checkpoint (pos_min = 8610, pos_max = 9536, size = 21.737 MiB)
slot update_slots: id  2 | task 14646 | created context checkpoint 8 of 8 (pos_min = 12172, pos_max = 13098, size = 21.737 MiB)
slot print_timing: id  2 | task 14646 | 
prompt eval time =    1660.95 ms /  1151 tokens (    1.44 ms per token,   692.98 tokens per second)
       eval time =   31451.53 ms /  1140 tokens (   27.59 ms per token,    36.25 tokens per second)
      total time =   33112.49 ms /  2291 tokens
slot      release: id  2 | task 14646 | stop processing: n_tokens = 14302, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 15788 | processing task, is_child = 0
slot update_slots: id  2 | task 15788 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14124
slot update_slots: id  2 | task 15788 | n_past = 13163, slot.prompt.tokens.size() = 14302, seq_id = 2, pos_min = 13375, n_swa = 128
slot update_slots: id  2 | task 15788 | restored context checkpoint (pos_min = 12172, pos_max = 13098, size = 21.737 MiB)
slot update_slots: id  2 | task 15788 | n_tokens = 13098, memory_seq_rm [13098, end)
slot update_slots: id  2 | task 15788 | prompt processing progress, n_tokens = 14060, batch.n_tokens = 962, progress = 0.995469
slot update_slots: id  2 | task 15788 | n_tokens = 14060, memory_seq_rm [14060, end)
slot update_slots: id  2 | task 15788 | prompt processing progress, n_tokens = 14124, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 15788 | prompt done, n_tokens = 14124, batch.n_tokens = 64
slot init_sampler: id  2 | task 15788 | init sampler, took 2.83 ms, tokens: text = 14124, total = 14124
slot update_slots: id  2 | task 15788 | erasing old context checkpoint (pos_min = 8888, pos_max = 9814, size = 21.737 MiB)
slot update_slots: id  2 | task 15788 | created context checkpoint 8 of 8 (pos_min = 13133, pos_max = 14059, size = 21.737 MiB)
slot print_timing: id  2 | task 15788 | 
prompt eval time =    1450.78 ms /  1026 tokens (    1.41 ms per token,   707.21 tokens per second)
       eval time =   17141.24 ms /   618 tokens (   27.74 ms per token,    36.05 tokens per second)
      total time =   18592.02 ms /  1644 tokens
slot      release: id  2 | task 15788 | stop processing: n_tokens = 14741, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16408 | processing task, is_child = 0
slot update_slots: id  2 | task 16408 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14966
slot update_slots: id  2 | task 16408 | n_tokens = 14124, memory_seq_rm [14124, end)
slot update_slots: id  2 | task 16408 | prompt processing progress, n_tokens = 14902, batch.n_tokens = 778, progress = 0.995724
slot update_slots: id  2 | task 16408 | n_tokens = 14902, memory_seq_rm [14902, end)
slot update_slots: id  2 | task 16408 | prompt processing progress, n_tokens = 14966, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16408 | prompt done, n_tokens = 14966, batch.n_tokens = 64
slot init_sampler: id  2 | task 16408 | init sampler, took 2.15 ms, tokens: text = 14966, total = 14966
slot update_slots: id  2 | task 16408 | erasing old context checkpoint (pos_min = 9472, pos_max = 10398, size = 21.737 MiB)
slot update_slots: id  2 | task 16408 | created context checkpoint 8 of 8 (pos_min = 13975, pos_max = 14901, size = 21.737 MiB)
slot print_timing: id  2 | task 16408 | 
prompt eval time =    1258.54 ms /   842 tokens (    1.49 ms per token,   669.03 tokens per second)
       eval time =    1505.27 ms /    54 tokens (   27.88 ms per token,    35.87 tokens per second)
      total time =    2763.81 ms /   896 tokens
slot      release: id  2 | task 16408 | stop processing: n_tokens = 15019, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16464 | processing task, is_child = 0
slot update_slots: id  2 | task 16464 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15042
slot update_slots: id  2 | task 16464 | n_tokens = 14966, memory_seq_rm [14966, end)
slot update_slots: id  2 | task 16464 | prompt processing progress, n_tokens = 14978, batch.n_tokens = 12, progress = 0.995745
slot update_slots: id  2 | task 16464 | n_tokens = 14978, memory_seq_rm [14978, end)
slot update_slots: id  2 | task 16464 | prompt processing progress, n_tokens = 15042, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16464 | prompt done, n_tokens = 15042, batch.n_tokens = 64
slot init_sampler: id  2 | task 16464 | init sampler, took 2.15 ms, tokens: text = 15042, total = 15042
slot update_slots: id  2 | task 16464 | erasing old context checkpoint (pos_min = 9968, pos_max = 10525, size = 13.085 MiB)
slot update_slots: id  2 | task 16464 | created context checkpoint 8 of 8 (pos_min = 14092, pos_max = 14977, size = 20.776 MiB)
slot print_timing: id  2 | task 16464 | 
prompt eval time =     283.48 ms /    76 tokens (    3.73 ms per token,   268.10 tokens per second)
       eval time =    1444.51 ms /    52 tokens (   27.78 ms per token,    36.00 tokens per second)
      total time =    1727.99 ms /   128 tokens
slot      release: id  2 | task 16464 | stop processing: n_tokens = 15093, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16518 | processing task, is_child = 0
slot update_slots: id  2 | task 16518 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15435
slot update_slots: id  2 | task 16518 | n_tokens = 15042, memory_seq_rm [15042, end)
slot update_slots: id  2 | task 16518 | prompt processing progress, n_tokens = 15371, batch.n_tokens = 329, progress = 0.995854
slot update_slots: id  2 | task 16518 | n_tokens = 15371, memory_seq_rm [15371, end)
slot update_slots: id  2 | task 16518 | prompt processing progress, n_tokens = 15435, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16518 | prompt done, n_tokens = 15435, batch.n_tokens = 64
slot init_sampler: id  2 | task 16518 | init sampler, took 2.21 ms, tokens: text = 15435, total = 15435
slot update_slots: id  2 | task 16518 | erasing old context checkpoint (pos_min = 10054, pos_max = 10709, size = 15.383 MiB)
slot update_slots: id  2 | task 16518 | created context checkpoint 8 of 8 (pos_min = 14444, pos_max = 15370, size = 21.737 MiB)
slot print_timing: id  2 | task 16518 | 
prompt eval time =     679.02 ms /   393 tokens (    1.73 ms per token,   578.77 tokens per second)
       eval time =   13717.06 ms /   496 tokens (   27.66 ms per token,    36.16 tokens per second)
      total time =   14396.08 ms /   889 tokens
slot      release: id  2 | task 16518 | stop processing: n_tokens = 15930, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17016 | processing task, is_child = 0
slot update_slots: id  2 | task 17016 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16120
slot update_slots: id  2 | task 17016 | n_tokens = 15435, memory_seq_rm [15435, end)
slot update_slots: id  2 | task 17016 | prompt processing progress, n_tokens = 16056, batch.n_tokens = 621, progress = 0.996030
slot update_slots: id  2 | task 17016 | n_tokens = 16056, memory_seq_rm [16056, end)
slot update_slots: id  2 | task 17016 | prompt processing progress, n_tokens = 16120, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17016 | prompt done, n_tokens = 16120, batch.n_tokens = 64
slot init_sampler: id  2 | task 17016 | init sampler, took 2.52 ms, tokens: text = 16120, total = 16120
slot update_slots: id  2 | task 17016 | erasing old context checkpoint (pos_min = 10376, pos_max = 10910, size = 12.545 MiB)
slot update_slots: id  2 | task 17016 | created context checkpoint 8 of 8 (pos_min = 15385, pos_max = 16055, size = 15.735 MiB)
slot print_timing: id  2 | task 17016 | 
prompt eval time =    1136.35 ms /   685 tokens (    1.66 ms per token,   602.81 tokens per second)
       eval time =    1306.62 ms /    47 tokens (   27.80 ms per token,    35.97 tokens per second)
      total time =    2442.96 ms /   732 tokens
slot      release: id  2 | task 17016 | stop processing: n_tokens = 16166, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17065 | processing task, is_child = 0
slot update_slots: id  2 | task 17065 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17271
slot update_slots: id  2 | task 17065 | n_tokens = 16120, memory_seq_rm [16120, end)
slot update_slots: id  2 | task 17065 | prompt processing progress, n_tokens = 17207, batch.n_tokens = 1087, progress = 0.996294
slot update_slots: id  2 | task 17065 | n_tokens = 17207, memory_seq_rm [17207, end)
slot update_slots: id  2 | task 17065 | prompt processing progress, n_tokens = 17271, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17065 | prompt done, n_tokens = 17271, batch.n_tokens = 64
slot init_sampler: id  2 | task 17065 | init sampler, took 3.70 ms, tokens: text = 17271, total = 17271
slot update_slots: id  2 | task 17065 | erasing old context checkpoint (pos_min = 10774, pos_max = 11118, size = 8.090 MiB)
slot update_slots: id  2 | task 17065 | created context checkpoint 8 of 8 (pos_min = 16280, pos_max = 17206, size = 21.737 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 17065
slot      release: id  2 | task 17065 | stop processing: n_tokens = 17282, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.446 (> 0.100 thold), f_keep = 0.018
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 17282, total state size = 426.982 MiB
srv          load:  - looking for better prompt, base f_keep = 0.018, sim = 0.446
srv        update:  - cache state: 7 prompts, 2067.322 MiB (limits: 8192.000 MiB, 56064 tokens, 168347 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv  get_availabl: prompt cache update took 621.77 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17080 | processing task, is_child = 0
slot update_slots: id  2 | task 17080 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 700
slot update_slots: id  2 | task 17080 | n_past = 312, slot.prompt.tokens.size() = 17282, seq_id = 2, pos_min = 16355, n_swa = 128
slot update_slots: id  2 | task 17080 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 17080 | erased invalidated context checkpoint (pos_min = 11056, pos_max = 11947, n_swa = 128, size = 20.917 MiB)
slot update_slots: id  2 | task 17080 | erased invalidated context checkpoint (pos_min = 12172, pos_max = 13098, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 17080 | erased invalidated context checkpoint (pos_min = 13133, pos_max = 14059, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 17080 | erased invalidated context checkpoint (pos_min = 13975, pos_max = 14901, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 17080 | erased invalidated context checkpoint (pos_min = 14092, pos_max = 14977, n_swa = 128, size = 20.776 MiB)
slot update_slots: id  2 | task 17080 | erased invalidated context checkpoint (pos_min = 14444, pos_max = 15370, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 17080 | erased invalidated context checkpoint (pos_min = 15385, pos_max = 16055, n_swa = 128, size = 15.735 MiB)
slot update_slots: id  2 | task 17080 | erased invalidated context checkpoint (pos_min = 16280, pos_max = 17206, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 17080 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 17080 | prompt processing progress, n_tokens = 636, batch.n_tokens = 636, progress = 0.908571
slot update_slots: id  2 | task 17080 | n_tokens = 636, memory_seq_rm [636, end)
slot update_slots: id  2 | task 17080 | prompt processing progress, n_tokens = 700, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17080 | prompt done, n_tokens = 700, batch.n_tokens = 64
slot init_sampler: id  2 | task 17080 | init sampler, took 0.13 ms, tokens: text = 700, total = 700
slot update_slots: id  2 | task 17080 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 635, size = 14.914 MiB)
slot print_timing: id  2 | task 17080 | 
prompt eval time =    1037.51 ms /   700 tokens (    1.48 ms per token,   674.69 tokens per second)
       eval time =    2369.32 ms /    88 tokens (   26.92 ms per token,    37.14 tokens per second)
      total time =    3406.82 ms /   788 tokens
slot      release: id  2 | task 17080 | stop processing: n_tokens = 787, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.895 (> 0.100 thold), f_keep = 0.889
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17170 | processing task, is_child = 0
slot update_slots: id  2 | task 17170 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 782
slot update_slots: id  2 | task 17170 | n_tokens = 700, memory_seq_rm [700, end)
slot update_slots: id  2 | task 17170 | prompt processing progress, n_tokens = 718, batch.n_tokens = 18, progress = 0.918159
slot update_slots: id  2 | task 17170 | n_tokens = 718, memory_seq_rm [718, end)
slot update_slots: id  2 | task 17170 | prompt processing progress, n_tokens = 782, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17170 | prompt done, n_tokens = 782, batch.n_tokens = 64
slot init_sampler: id  2 | task 17170 | init sampler, took 0.14 ms, tokens: text = 782, total = 782
slot update_slots: id  2 | task 17170 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 717, size = 16.837 MiB)
slot print_timing: id  2 | task 17170 | 
prompt eval time =     346.03 ms /    82 tokens (    4.22 ms per token,   236.97 tokens per second)
       eval time =    3603.41 ms /   135 tokens (   26.69 ms per token,    37.46 tokens per second)
      total time =    3949.44 ms /   217 tokens
slot      release: id  2 | task 17170 | stop processing: n_tokens = 916, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 17307 | processing task, is_child = 0
slot update_slots: id  1 | task 17307 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 521228
srv    send_error: task id = 17307, error: request (521228 tokens) exceeds the available context size (56064 tokens), try increasing it
slot      release: id  1 | task 17307 | stop processing: n_tokens = 0, truncated = 0
srv  update_slots: no tokens to decode
srv  update_slots: all slots are idle
srv          stop: cancel task, id_task = 17307
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 400
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.784 (> 0.100 thold), f_keep = 0.733
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17310 | processing task, is_child = 0
slot update_slots: id  2 | task 17310 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 856
slot update_slots: id  2 | task 17310 | n_tokens = 671, memory_seq_rm [671, end)
slot update_slots: id  2 | task 17310 | prompt processing progress, n_tokens = 792, batch.n_tokens = 121, progress = 0.925234
slot update_slots: id  2 | task 17310 | n_tokens = 792, memory_seq_rm [792, end)
slot update_slots: id  2 | task 17310 | prompt processing progress, n_tokens = 856, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17310 | prompt done, n_tokens = 856, batch.n_tokens = 64
slot init_sampler: id  2 | task 17310 | init sampler, took 0.17 ms, tokens: text = 856, total = 856
slot update_slots: id  2 | task 17310 | created context checkpoint 3 of 8 (pos_min = 0, pos_max = 791, size = 18.572 MiB)
slot print_timing: id  2 | task 17310 | 
prompt eval time =     542.11 ms /   185 tokens (    2.93 ms per token,   341.26 tokens per second)
       eval time =    3000.99 ms /   109 tokens (   27.53 ms per token,    36.32 tokens per second)
      total time =    3543.10 ms /   294 tokens
slot      release: id  2 | task 17310 | stop processing: n_tokens = 964, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.913 (> 0.100 thold), f_keep = 0.888
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17421 | processing task, is_child = 0
slot update_slots: id  2 | task 17421 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 938
slot update_slots: id  2 | task 17421 | n_tokens = 856, memory_seq_rm [856, end)
slot update_slots: id  2 | task 17421 | prompt processing progress, n_tokens = 874, batch.n_tokens = 18, progress = 0.931770
slot update_slots: id  2 | task 17421 | n_tokens = 874, memory_seq_rm [874, end)
slot update_slots: id  2 | task 17421 | prompt processing progress, n_tokens = 938, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17421 | prompt done, n_tokens = 938, batch.n_tokens = 64
slot init_sampler: id  2 | task 17421 | init sampler, took 0.18 ms, tokens: text = 938, total = 938
slot update_slots: id  2 | task 17421 | created context checkpoint 4 of 8 (pos_min = 37, pos_max = 873, size = 19.627 MiB)
slot print_timing: id  2 | task 17421 | 
prompt eval time =     359.79 ms /    82 tokens (    4.39 ms per token,   227.91 tokens per second)
       eval time =    7274.88 ms /   270 tokens (   26.94 ms per token,    37.11 tokens per second)
      total time =    7634.66 ms /   352 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 17421 | stop processing: n_tokens = 1207, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.924 (> 0.100 thold), f_keep = 0.777
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17693 | processing task, is_child = 0
slot update_slots: id  2 | task 17693 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1015
slot update_slots: id  2 | task 17693 | n_tokens = 938, memory_seq_rm [938, end)
slot update_slots: id  2 | task 17693 | prompt processing progress, n_tokens = 951, batch.n_tokens = 13, progress = 0.936946
slot update_slots: id  2 | task 17693 | n_tokens = 951, memory_seq_rm [951, end)
slot update_slots: id  2 | task 17693 | prompt processing progress, n_tokens = 1015, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17693 | prompt done, n_tokens = 1015, batch.n_tokens = 64
slot init_sampler: id  2 | task 17693 | init sampler, took 0.24 ms, tokens: text = 1015, total = 1015
slot update_slots: id  2 | task 17693 | created context checkpoint 5 of 8 (pos_min = 351, pos_max = 950, size = 14.070 MiB)
slot print_timing: id  2 | task 17693 | 
prompt eval time =     359.01 ms /    77 tokens (    4.66 ms per token,   214.48 tokens per second)
       eval time =    2805.16 ms /   105 tokens (   26.72 ms per token,    37.43 tokens per second)
      total time =    3164.16 ms /   182 tokens
slot      release: id  2 | task 17693 | stop processing: n_tokens = 1119, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.444 (> 0.100 thold), f_keep = 0.279
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1119, total state size = 44.249 MiB
srv          load:  - looking for better prompt, base f_keep = 0.279, sim = 0.444
srv        update:  - cache state: 8 prompts, 2195.590 MiB (limits: 8192.000 MiB, 56064 tokens, 162687 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv  get_availabl: prompt cache update took 76.00 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17800 | processing task, is_child = 0
slot update_slots: id  2 | task 17800 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 702
slot update_slots: id  2 | task 17800 | n_past = 312, slot.prompt.tokens.size() = 1119, seq_id = 2, pos_min = 351, n_swa = 128
slot update_slots: id  2 | task 17800 | restored context checkpoint (pos_min = 37, pos_max = 873, size = 19.627 MiB)
slot update_slots: id  2 | task 17800 | erased invalidated context checkpoint (pos_min = 351, pos_max = 950, n_swa = 128, size = 14.070 MiB)
slot update_slots: id  2 | task 17800 | n_tokens = 312, memory_seq_rm [312, end)
slot update_slots: id  2 | task 17800 | prompt processing progress, n_tokens = 638, batch.n_tokens = 326, progress = 0.908832
slot update_slots: id  2 | task 17800 | n_tokens = 638, memory_seq_rm [638, end)
slot update_slots: id  2 | task 17800 | prompt processing progress, n_tokens = 702, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17800 | prompt done, n_tokens = 702, batch.n_tokens = 64
slot init_sampler: id  2 | task 17800 | init sampler, took 0.13 ms, tokens: text = 702, total = 702
slot print_timing: id  2 | task 17800 | 
prompt eval time =     616.96 ms /   390 tokens (    1.58 ms per token,   632.13 tokens per second)
       eval time =    5125.56 ms /   188 tokens (   27.26 ms per token,    36.68 tokens per second)
      total time =    5742.52 ms /   578 tokens
slot      release: id  2 | task 17800 | stop processing: n_tokens = 889, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.322 (> 0.100 thold), f_keep = 0.790
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17990 | processing task, is_child = 0
slot update_slots: id  2 | task 17990 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2177
slot update_slots: id  2 | task 17990 | n_tokens = 702, memory_seq_rm [702, end)
slot update_slots: id  2 | task 17990 | prompt processing progress, n_tokens = 2113, batch.n_tokens = 1411, progress = 0.970602
slot update_slots: id  2 | task 17990 | n_tokens = 2113, memory_seq_rm [2113, end)
slot update_slots: id  2 | task 17990 | prompt processing progress, n_tokens = 2177, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17990 | prompt done, n_tokens = 2177, batch.n_tokens = 64
slot init_sampler: id  2 | task 17990 | init sampler, took 0.39 ms, tokens: text = 2177, total = 2177
slot update_slots: id  2 | task 17990 | created context checkpoint 5 of 8 (pos_min = 1186, pos_max = 2112, size = 21.737 MiB)
slot print_timing: id  2 | task 17990 | 
prompt eval time =    1696.18 ms /  1475 tokens (    1.15 ms per token,   869.60 tokens per second)
       eval time =    9504.19 ms /   334 tokens (   28.46 ms per token,    35.14 tokens per second)
      total time =   11200.37 ms /  1809 tokens
slot      release: id  2 | task 17990 | stop processing: n_tokens = 2510, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.596 (> 0.100 thold), f_keep = 0.867
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18326 | processing task, is_child = 0
slot update_slots: id  2 | task 18326 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3652
slot update_slots: id  2 | task 18326 | n_tokens = 2177, memory_seq_rm [2177, end)
slot update_slots: id  2 | task 18326 | prompt processing progress, n_tokens = 3588, batch.n_tokens = 1411, progress = 0.982475
slot update_slots: id  2 | task 18326 | n_tokens = 3588, memory_seq_rm [3588, end)
slot update_slots: id  2 | task 18326 | prompt processing progress, n_tokens = 3652, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 18326 | prompt done, n_tokens = 3652, batch.n_tokens = 64
slot init_sampler: id  2 | task 18326 | init sampler, took 0.70 ms, tokens: text = 3652, total = 3652
slot update_slots: id  2 | task 18326 | created context checkpoint 6 of 8 (pos_min = 2661, pos_max = 3587, size = 21.737 MiB)
slot print_timing: id  2 | task 18326 | 
prompt eval time =    1678.55 ms /  1475 tokens (    1.14 ms per token,   878.73 tokens per second)
       eval time =    3676.13 ms /   137 tokens (   26.83 ms per token,    37.27 tokens per second)
      total time =    5354.68 ms /  1612 tokens
slot      release: id  2 | task 18326 | stop processing: n_tokens = 3788, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.712 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18465 | processing task, is_child = 0
slot update_slots: id  2 | task 18465 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5127
slot update_slots: id  2 | task 18465 | n_tokens = 3652, memory_seq_rm [3652, end)
slot update_slots: id  2 | task 18465 | prompt processing progress, n_tokens = 5063, batch.n_tokens = 1411, progress = 0.987517
slot update_slots: id  2 | task 18465 | n_tokens = 5063, memory_seq_rm [5063, end)
slot update_slots: id  2 | task 18465 | prompt processing progress, n_tokens = 5127, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 18465 | prompt done, n_tokens = 5127, batch.n_tokens = 64
slot init_sampler: id  2 | task 18465 | init sampler, took 0.98 ms, tokens: text = 5127, total = 5127
slot update_slots: id  2 | task 18465 | created context checkpoint 7 of 8 (pos_min = 4136, pos_max = 5062, size = 21.737 MiB)
slot print_timing: id  2 | task 18465 | 
prompt eval time =    1672.35 ms /  1475 tokens (    1.13 ms per token,   881.99 tokens per second)
       eval time =    3943.95 ms /   149 tokens (   26.47 ms per token,    37.78 tokens per second)
      total time =    5616.30 ms /  1624 tokens
slot      release: id  2 | task 18465 | stop processing: n_tokens = 5275, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.777 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18616 | processing task, is_child = 0
slot update_slots: id  2 | task 18616 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6602
slot update_slots: id  2 | task 18616 | n_tokens = 5127, memory_seq_rm [5127, end)
slot update_slots: id  2 | task 18616 | prompt processing progress, n_tokens = 6538, batch.n_tokens = 1411, progress = 0.990306
slot update_slots: id  2 | task 18616 | n_tokens = 6538, memory_seq_rm [6538, end)
slot update_slots: id  2 | task 18616 | prompt processing progress, n_tokens = 6602, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 18616 | prompt done, n_tokens = 6602, batch.n_tokens = 64
slot init_sampler: id  2 | task 18616 | init sampler, took 0.98 ms, tokens: text = 6602, total = 6602
slot update_slots: id  2 | task 18616 | created context checkpoint 8 of 8 (pos_min = 5611, pos_max = 6537, size = 21.737 MiB)
slot print_timing: id  2 | task 18616 | 
prompt eval time =    1716.13 ms /  1475 tokens (    1.16 ms per token,   859.49 tokens per second)
       eval time =   34637.59 ms /  1242 tokens (   27.89 ms per token,    35.86 tokens per second)
      total time =   36353.72 ms /  2717 tokens
slot      release: id  2 | task 18616 | stop processing: n_tokens = 7843, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.198 (> 0.100 thold), f_keep = 0.086
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7843, total state size = 205.648 MiB
srv          load:  - looking for better prompt, base f_keep = 0.086, sim = 0.198
srv        update:  - cache state: 9 prompts, 2558.137 MiB (limits: 8192.000 MiB, 56064 tokens, 164747 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv        update:    - prompt 0x5a03d440fde0:    7843 tokens, checkpoints:  8,   362.547 MiB
srv  get_availabl: prompt cache update took 288.20 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 19860 | processing task, is_child = 0
slot update_slots: id  2 | task 19860 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3414
slot update_slots: id  2 | task 19860 | n_past = 675, slot.prompt.tokens.size() = 7843, seq_id = 2, pos_min = 6916, n_swa = 128
slot update_slots: id  2 | task 19860 | restored context checkpoint (pos_min = 37, pos_max = 873, size = 19.627 MiB)
slot update_slots: id  2 | task 19860 | erased invalidated context checkpoint (pos_min = 1186, pos_max = 2112, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 19860 | erased invalidated context checkpoint (pos_min = 2661, pos_max = 3587, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 19860 | erased invalidated context checkpoint (pos_min = 4136, pos_max = 5062, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 19860 | erased invalidated context checkpoint (pos_min = 5611, pos_max = 6537, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 19860 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  2 | task 19860 | prompt processing progress, n_tokens = 2723, batch.n_tokens = 2048, progress = 0.797598
slot update_slots: id  2 | task 19860 | n_tokens = 2723, memory_seq_rm [2723, end)
slot update_slots: id  2 | task 19860 | prompt processing progress, n_tokens = 3350, batch.n_tokens = 627, progress = 0.981254
slot update_slots: id  2 | task 19860 | n_tokens = 3350, memory_seq_rm [3350, end)
slot update_slots: id  2 | task 19860 | prompt processing progress, n_tokens = 3414, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 19860 | prompt done, n_tokens = 3414, batch.n_tokens = 64
slot init_sampler: id  2 | task 19860 | init sampler, took 0.55 ms, tokens: text = 3414, total = 3414
slot update_slots: id  2 | task 19860 | created context checkpoint 5 of 8 (pos_min = 2423, pos_max = 3349, size = 21.737 MiB)
slot print_timing: id  2 | task 19860 | 
prompt eval time =    2906.71 ms /  2739 tokens (    1.06 ms per token,   942.30 tokens per second)
       eval time =    4014.67 ms /   160 tokens (   25.09 ms per token,    39.85 tokens per second)
      total time =    6921.38 ms /  2899 tokens
slot      release: id  2 | task 19860 | stop processing: n_tokens = 3573, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.773 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20023 | processing task, is_child = 0
slot update_slots: id  2 | task 20023 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4414
slot update_slots: id  2 | task 20023 | n_tokens = 3414, memory_seq_rm [3414, end)
slot update_slots: id  2 | task 20023 | prompt processing progress, n_tokens = 4350, batch.n_tokens = 936, progress = 0.985501
slot update_slots: id  2 | task 20023 | n_tokens = 4350, memory_seq_rm [4350, end)
slot update_slots: id  2 | task 20023 | prompt processing progress, n_tokens = 4414, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 20023 | prompt done, n_tokens = 4414, batch.n_tokens = 64
slot init_sampler: id  2 | task 20023 | init sampler, took 0.80 ms, tokens: text = 4414, total = 4414
slot update_slots: id  2 | task 20023 | created context checkpoint 6 of 8 (pos_min = 3423, pos_max = 4349, size = 21.737 MiB)
slot print_timing: id  2 | task 20023 | 
prompt eval time =    1174.49 ms /  1000 tokens (    1.17 ms per token,   851.43 tokens per second)
       eval time =    2146.13 ms /    86 tokens (   24.96 ms per token,    40.07 tokens per second)
      total time =    3320.62 ms /  1086 tokens
slot      release: id  2 | task 20023 | stop processing: n_tokens = 4499, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20111 | processing task, is_child = 0
slot update_slots: id  2 | task 20111 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4524
slot update_slots: id  2 | task 20111 | n_tokens = 4414, memory_seq_rm [4414, end)
slot update_slots: id  2 | task 20111 | prompt processing progress, n_tokens = 4460, batch.n_tokens = 46, progress = 0.985853
slot update_slots: id  2 | task 20111 | n_tokens = 4460, memory_seq_rm [4460, end)
slot update_slots: id  2 | task 20111 | prompt processing progress, n_tokens = 4524, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 20111 | prompt done, n_tokens = 4524, batch.n_tokens = 64
slot init_sampler: id  2 | task 20111 | init sampler, took 0.93 ms, tokens: text = 4524, total = 4524
slot update_slots: id  2 | task 20111 | created context checkpoint 7 of 8 (pos_min = 3572, pos_max = 4459, size = 20.823 MiB)
slot print_timing: id  2 | task 20111 | 
prompt eval time =     390.87 ms /   110 tokens (    3.55 ms per token,   281.42 tokens per second)
       eval time =    8779.06 ms /   345 tokens (   25.45 ms per token,    39.30 tokens per second)
      total time =    9169.93 ms /   455 tokens
slot      release: id  2 | task 20111 | stop processing: n_tokens = 4868, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.517 (> 0.100 thold), f_keep = 0.416
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4868, total state size = 135.887 MiB
srv          load:  - looking for better prompt, base f_keep = 0.416, sim = 0.517
srv        update:  - cache state: 10 prompts, 2828.272 MiB (limits: 8192.000 MiB, 56064 tokens, 163111 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv        update:    - prompt 0x5a03d440fde0:    7843 tokens, checkpoints:  8,   362.547 MiB
srv        update:    - prompt 0x5a03cd584ab0:    4868 tokens, checkpoints:  7,   270.134 MiB
srv  get_availabl: prompt cache update took 193.69 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20458 | processing task, is_child = 0
slot update_slots: id  2 | task 20458 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3920
slot update_slots: id  2 | task 20458 | n_past = 2027, slot.prompt.tokens.size() = 4868, seq_id = 2, pos_min = 3941, n_swa = 128
slot update_slots: id  2 | task 20458 | restored context checkpoint (pos_min = 37, pos_max = 873, size = 19.627 MiB)
slot update_slots: id  2 | task 20458 | erased invalidated context checkpoint (pos_min = 2423, pos_max = 3349, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 20458 | erased invalidated context checkpoint (pos_min = 3423, pos_max = 4349, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 20458 | erased invalidated context checkpoint (pos_min = 3572, pos_max = 4459, n_swa = 128, size = 20.823 MiB)
slot update_slots: id  2 | task 20458 | n_tokens = 873, memory_seq_rm [873, end)
slot update_slots: id  2 | task 20458 | prompt processing progress, n_tokens = 2921, batch.n_tokens = 2048, progress = 0.745153
slot update_slots: id  2 | task 20458 | n_tokens = 2921, memory_seq_rm [2921, end)
slot update_slots: id  2 | task 20458 | prompt processing progress, n_tokens = 3856, batch.n_tokens = 935, progress = 0.983673
slot update_slots: id  2 | task 20458 | n_tokens = 3856, memory_seq_rm [3856, end)
slot update_slots: id  2 | task 20458 | prompt processing progress, n_tokens = 3920, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 20458 | prompt done, n_tokens = 3920, batch.n_tokens = 64
slot init_sampler: id  2 | task 20458 | init sampler, took 0.60 ms, tokens: text = 3920, total = 3920
slot update_slots: id  2 | task 20458 | created context checkpoint 5 of 8 (pos_min = 2929, pos_max = 3855, size = 21.737 MiB)
slot print_timing: id  2 | task 20458 | 
prompt eval time =    3265.71 ms /  3047 tokens (    1.07 ms per token,   933.03 tokens per second)
       eval time =    3238.32 ms /   121 tokens (   26.76 ms per token,    37.37 tokens per second)
      total time =    6504.03 ms /  3168 tokens
slot      release: id  2 | task 20458 | stop processing: n_tokens = 4040, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20582 | processing task, is_child = 0
slot update_slots: id  2 | task 20582 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4030
slot update_slots: id  2 | task 20582 | n_tokens = 3920, memory_seq_rm [3920, end)
slot update_slots: id  2 | task 20582 | prompt processing progress, n_tokens = 3966, batch.n_tokens = 46, progress = 0.984119
slot update_slots: id  2 | task 20582 | n_tokens = 3966, memory_seq_rm [3966, end)
slot update_slots: id  2 | task 20582 | prompt processing progress, n_tokens = 4030, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 20582 | prompt done, n_tokens = 4030, batch.n_tokens = 64
slot init_sampler: id  2 | task 20582 | init sampler, took 0.78 ms, tokens: text = 4030, total = 4030
slot update_slots: id  2 | task 20582 | created context checkpoint 6 of 8 (pos_min = 3113, pos_max = 3965, size = 20.002 MiB)
slot print_timing: id  2 | task 20582 | 
prompt eval time =     443.24 ms /   110 tokens (    4.03 ms per token,   248.17 tokens per second)
       eval time =    2376.56 ms /    85 tokens (   27.96 ms per token,    35.77 tokens per second)
      total time =    2819.80 ms /   195 tokens
slot      release: id  2 | task 20582 | stop processing: n_tokens = 4114, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20669 | processing task, is_child = 0
slot update_slots: id  2 | task 20669 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4144
slot update_slots: id  2 | task 20669 | n_tokens = 4030, memory_seq_rm [4030, end)
slot update_slots: id  2 | task 20669 | prompt processing progress, n_tokens = 4080, batch.n_tokens = 50, progress = 0.984556
slot update_slots: id  2 | task 20669 | n_tokens = 4080, memory_seq_rm [4080, end)
slot update_slots: id  2 | task 20669 | prompt processing progress, n_tokens = 4144, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 20669 | prompt done, n_tokens = 4144, batch.n_tokens = 64
slot init_sampler: id  2 | task 20669 | init sampler, took 0.73 ms, tokens: text = 4144, total = 4144
slot update_slots: id  2 | task 20669 | created context checkpoint 7 of 8 (pos_min = 3187, pos_max = 4079, size = 20.940 MiB)
slot print_timing: id  2 | task 20669 | 
prompt eval time =     467.42 ms /   114 tokens (    4.10 ms per token,   243.89 tokens per second)
       eval time =     412.73 ms /    16 tokens (   25.80 ms per token,    38.77 tokens per second)
      total time =     880.15 ms /   130 tokens
slot      release: id  2 | task 20669 | stop processing: n_tokens = 4159, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.185 (> 0.100 thold), f_keep = 0.183
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4159, total state size = 119.262 MiB
srv          load:  - looking for better prompt, base f_keep = 0.183, sim = 0.185
srv        update:  - cache state: 11 prompts, 3080.163 MiB (limits: 8192.000 MiB, 56064 tokens, 160833 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv        update:    - prompt 0x5a03d440fde0:    7843 tokens, checkpoints:  8,   362.547 MiB
srv        update:    - prompt 0x5a03cd584ab0:    4868 tokens, checkpoints:  7,   270.134 MiB
srv        update:    - prompt 0x5a03cf145a50:    4159 tokens, checkpoints:  7,   251.891 MiB
srv  get_availabl: prompt cache update took 181.66 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20687 | processing task, is_child = 0
slot update_slots: id  2 | task 20687 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4118
slot update_slots: id  2 | task 20687 | n_past = 761, slot.prompt.tokens.size() = 4159, seq_id = 2, pos_min = 3232, n_swa = 128
slot update_slots: id  2 | task 20687 | restored context checkpoint (pos_min = 37, pos_max = 873, size = 19.627 MiB)
slot update_slots: id  2 | task 20687 | erased invalidated context checkpoint (pos_min = 2929, pos_max = 3855, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 20687 | erased invalidated context checkpoint (pos_min = 3113, pos_max = 3965, n_swa = 128, size = 20.002 MiB)
slot update_slots: id  2 | task 20687 | erased invalidated context checkpoint (pos_min = 3187, pos_max = 4079, n_swa = 128, size = 20.940 MiB)
slot update_slots: id  2 | task 20687 | n_tokens = 761, memory_seq_rm [761, end)
slot update_slots: id  2 | task 20687 | prompt processing progress, n_tokens = 2809, batch.n_tokens = 2048, progress = 0.682127
slot update_slots: id  2 | task 20687 | n_tokens = 2809, memory_seq_rm [2809, end)
slot update_slots: id  2 | task 20687 | prompt processing progress, n_tokens = 4054, batch.n_tokens = 1245, progress = 0.984458
slot update_slots: id  2 | task 20687 | n_tokens = 4054, memory_seq_rm [4054, end)
slot update_slots: id  2 | task 20687 | prompt processing progress, n_tokens = 4118, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 20687 | prompt done, n_tokens = 4118, batch.n_tokens = 64
slot init_sampler: id  2 | task 20687 | init sampler, took 0.77 ms, tokens: text = 4118, total = 4118
slot update_slots: id  2 | task 20687 | created context checkpoint 5 of 8 (pos_min = 3127, pos_max = 4053, size = 21.737 MiB)
slot print_timing: id  2 | task 20687 | 
prompt eval time =    3717.25 ms /  3357 tokens (    1.11 ms per token,   903.09 tokens per second)
       eval time =    4115.92 ms /   152 tokens (   27.08 ms per token,    36.93 tokens per second)
      total time =    7833.17 ms /  3509 tokens
slot      release: id  2 | task 20687 | stop processing: n_tokens = 4269, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20842 | processing task, is_child = 0
slot update_slots: id  2 | task 20842 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4216
slot update_slots: id  2 | task 20842 | n_tokens = 4118, memory_seq_rm [4118, end)
slot update_slots: id  2 | task 20842 | prompt processing progress, n_tokens = 4152, batch.n_tokens = 34, progress = 0.984820
slot update_slots: id  2 | task 20842 | n_tokens = 4152, memory_seq_rm [4152, end)
slot update_slots: id  2 | task 20842 | prompt processing progress, n_tokens = 4216, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 20842 | prompt done, n_tokens = 4216, batch.n_tokens = 64
slot init_sampler: id  2 | task 20842 | init sampler, took 0.63 ms, tokens: text = 4216, total = 4216
slot update_slots: id  2 | task 20842 | created context checkpoint 6 of 8 (pos_min = 3342, pos_max = 4151, size = 18.994 MiB)
slot print_timing: id  2 | task 20842 | 
prompt eval time =     379.03 ms /    98 tokens (    3.87 ms per token,   258.56 tokens per second)
       eval time =    2926.41 ms /   105 tokens (   27.87 ms per token,    35.88 tokens per second)
      total time =    3305.44 ms /   203 tokens
slot      release: id  2 | task 20842 | stop processing: n_tokens = 4320, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.976
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 20949 | processing task, is_child = 0
slot update_slots: id  2 | task 20949 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4330
slot update_slots: id  2 | task 20949 | n_tokens = 4216, memory_seq_rm [4216, end)
slot update_slots: id  2 | task 20949 | prompt processing progress, n_tokens = 4266, batch.n_tokens = 50, progress = 0.985219
slot update_slots: id  2 | task 20949 | n_tokens = 4266, memory_seq_rm [4266, end)
slot update_slots: id  2 | task 20949 | prompt processing progress, n_tokens = 4330, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 20949 | prompt done, n_tokens = 4330, batch.n_tokens = 64
slot init_sampler: id  2 | task 20949 | init sampler, took 0.64 ms, tokens: text = 4330, total = 4330
slot update_slots: id  2 | task 20949 | created context checkpoint 7 of 8 (pos_min = 3393, pos_max = 4265, size = 20.471 MiB)
slot print_timing: id  2 | task 20949 | 
prompt eval time =     436.36 ms /   114 tokens (    3.83 ms per token,   261.25 tokens per second)
       eval time =    2302.81 ms /    86 tokens (   26.78 ms per token,    37.35 tokens per second)
      total time =    2739.17 ms /   200 tokens
slot      release: id  2 | task 20949 | stop processing: n_tokens = 4415, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 21037 | processing task, is_child = 0
slot update_slots: id  2 | task 21037 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4441
slot update_slots: id  2 | task 21037 | n_tokens = 4330, memory_seq_rm [4330, end)
slot update_slots: id  2 | task 21037 | prompt processing progress, n_tokens = 4377, batch.n_tokens = 47, progress = 0.985589
slot update_slots: id  2 | task 21037 | n_tokens = 4377, memory_seq_rm [4377, end)
slot update_slots: id  2 | task 21037 | prompt processing progress, n_tokens = 4441, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 21037 | prompt done, n_tokens = 4441, batch.n_tokens = 64
slot init_sampler: id  2 | task 21037 | init sampler, took 0.85 ms, tokens: text = 4441, total = 4441
slot update_slots: id  2 | task 21037 | created context checkpoint 8 of 8 (pos_min = 3488, pos_max = 4376, size = 20.846 MiB)
slot print_timing: id  2 | task 21037 | 
prompt eval time =     420.82 ms /   111 tokens (    3.79 ms per token,   263.77 tokens per second)
       eval time =    5630.57 ms /   215 tokens (   26.19 ms per token,    38.18 tokens per second)
      total time =    6051.38 ms /   326 tokens
slot      release: id  2 | task 21037 | stop processing: n_tokens = 4655, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.810 (> 0.100 thold), f_keep = 0.954
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 21254 | processing task, is_child = 0
slot update_slots: id  2 | task 21254 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5480
slot update_slots: id  2 | task 21254 | n_tokens = 4441, memory_seq_rm [4441, end)
slot update_slots: id  2 | task 21254 | prompt processing progress, n_tokens = 5416, batch.n_tokens = 975, progress = 0.988321
slot update_slots: id  2 | task 21254 | n_tokens = 5416, memory_seq_rm [5416, end)
slot update_slots: id  2 | task 21254 | prompt processing progress, n_tokens = 5480, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 21254 | prompt done, n_tokens = 5480, batch.n_tokens = 64
slot init_sampler: id  2 | task 21254 | init sampler, took 1.07 ms, tokens: text = 5480, total = 5480
slot update_slots: id  2 | task 21254 | erasing old context checkpoint (pos_min = 0, pos_max = 635, size = 14.914 MiB)
slot update_slots: id  2 | task 21254 | created context checkpoint 8 of 8 (pos_min = 4489, pos_max = 5415, size = 21.737 MiB)
slot print_timing: id  2 | task 21254 | 
prompt eval time =    1300.67 ms /  1039 tokens (    1.25 ms per token,   798.82 tokens per second)
       eval time =    5703.64 ms /   210 tokens (   27.16 ms per token,    36.82 tokens per second)
      total time =    7004.31 ms /  1249 tokens
slot      release: id  2 | task 21254 | stop processing: n_tokens = 5689, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.201 (> 0.100 thold), f_keep = 0.122
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5689, total state size = 155.139 MiB
srv          load:  - looking for better prompt, base f_keep = 0.122, sim = 0.201
srv        update:  - cache state: 12 prompts, 3394.124 MiB (limits: 8192.000 MiB, 56064 tokens, 159687 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv        update:    - prompt 0x5a03d440fde0:    7843 tokens, checkpoints:  8,   362.547 MiB
srv        update:    - prompt 0x5a03cd584ab0:    4868 tokens, checkpoints:  7,   270.134 MiB
srv        update:    - prompt 0x5a03cf145a50:    4159 tokens, checkpoints:  7,   251.891 MiB
srv        update:    - prompt 0x5a03d42d4e40:    5689 tokens, checkpoints:  8,   313.961 MiB
srv  get_availabl: prompt cache update took 246.12 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 21466 | processing task, is_child = 0
slot update_slots: id  2 | task 21466 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3464
slot update_slots: id  2 | task 21466 | n_past = 695, slot.prompt.tokens.size() = 5689, seq_id = 2, pos_min = 4762, n_swa = 128
slot update_slots: id  2 | task 21466 | restored context checkpoint (pos_min = 37, pos_max = 873, size = 19.627 MiB)
slot update_slots: id  2 | task 21466 | erased invalidated context checkpoint (pos_min = 3127, pos_max = 4053, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 21466 | erased invalidated context checkpoint (pos_min = 3342, pos_max = 4151, n_swa = 128, size = 18.994 MiB)
slot update_slots: id  2 | task 21466 | erased invalidated context checkpoint (pos_min = 3393, pos_max = 4265, n_swa = 128, size = 20.471 MiB)
slot update_slots: id  2 | task 21466 | erased invalidated context checkpoint (pos_min = 3488, pos_max = 4376, n_swa = 128, size = 20.846 MiB)
slot update_slots: id  2 | task 21466 | erased invalidated context checkpoint (pos_min = 4489, pos_max = 5415, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 21466 | n_tokens = 695, memory_seq_rm [695, end)
slot update_slots: id  2 | task 21466 | prompt processing progress, n_tokens = 2743, batch.n_tokens = 2048, progress = 0.791859
slot update_slots: id  2 | task 21466 | n_tokens = 2743, memory_seq_rm [2743, end)
slot update_slots: id  2 | task 21466 | prompt processing progress, n_tokens = 3400, batch.n_tokens = 657, progress = 0.981524
slot update_slots: id  2 | task 21466 | n_tokens = 3400, memory_seq_rm [3400, end)
slot update_slots: id  2 | task 21466 | prompt processing progress, n_tokens = 3464, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 21466 | prompt done, n_tokens = 3464, batch.n_tokens = 64
slot init_sampler: id  2 | task 21466 | init sampler, took 0.58 ms, tokens: text = 3464, total = 3464
slot update_slots: id  2 | task 21466 | created context checkpoint 4 of 8 (pos_min = 2473, pos_max = 3399, size = 21.737 MiB)
slot print_timing: id  2 | task 21466 | 
prompt eval time =    3092.07 ms /  2769 tokens (    1.12 ms per token,   895.52 tokens per second)
       eval time =    3044.90 ms /   114 tokens (   26.71 ms per token,    37.44 tokens per second)
      total time =    6136.97 ms /  2883 tokens
slot      release: id  2 | task 21466 | stop processing: n_tokens = 3577, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.968
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 21583 | processing task, is_child = 0
slot update_slots: id  2 | task 21583 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3574
slot update_slots: id  2 | task 21583 | n_tokens = 3464, memory_seq_rm [3464, end)
slot update_slots: id  2 | task 21583 | prompt processing progress, n_tokens = 3510, batch.n_tokens = 46, progress = 0.982093
slot update_slots: id  2 | task 21583 | n_tokens = 3510, memory_seq_rm [3510, end)
slot update_slots: id  2 | task 21583 | prompt processing progress, n_tokens = 3574, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 21583 | prompt done, n_tokens = 3574, batch.n_tokens = 64
slot init_sampler: id  2 | task 21583 | init sampler, took 0.69 ms, tokens: text = 3574, total = 3574
slot update_slots: id  2 | task 21583 | created context checkpoint 5 of 8 (pos_min = 2650, pos_max = 3509, size = 20.166 MiB)
slot print_timing: id  2 | task 21583 | 
prompt eval time =     411.96 ms /   110 tokens (    3.75 ms per token,   267.02 tokens per second)
       eval time =    1715.58 ms /    64 tokens (   26.81 ms per token,    37.31 tokens per second)
      total time =    2127.54 ms /   174 tokens
slot      release: id  2 | task 21583 | stop processing: n_tokens = 3637, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.769 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 21649 | processing task, is_child = 0
slot update_slots: id  2 | task 21649 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4650
slot update_slots: id  2 | task 21649 | n_tokens = 3574, memory_seq_rm [3574, end)
slot update_slots: id  2 | task 21649 | prompt processing progress, n_tokens = 4586, batch.n_tokens = 1012, progress = 0.986237
slot update_slots: id  2 | task 21649 | n_tokens = 4586, memory_seq_rm [4586, end)
slot update_slots: id  2 | task 21649 | prompt processing progress, n_tokens = 4650, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 21649 | prompt done, n_tokens = 4650, batch.n_tokens = 64
slot init_sampler: id  2 | task 21649 | init sampler, took 0.80 ms, tokens: text = 4650, total = 4650
slot update_slots: id  2 | task 21649 | created context checkpoint 6 of 8 (pos_min = 3659, pos_max = 4585, size = 21.737 MiB)
slot print_timing: id  2 | task 21649 | 
prompt eval time =    1363.55 ms /  1076 tokens (    1.27 ms per token,   789.12 tokens per second)
       eval time =    5536.26 ms /   192 tokens (   28.83 ms per token,    34.68 tokens per second)
      total time =    6899.80 ms /  1268 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 21649 | stop processing: n_tokens = 4841, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.804 (> 0.100 thold), f_keep = 0.961
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 21843 | processing task, is_child = 0
slot update_slots: id  2 | task 21843 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5783
slot update_slots: id  2 | task 21843 | n_tokens = 4650, memory_seq_rm [4650, end)
slot update_slots: id  2 | task 21843 | prompt processing progress, n_tokens = 5719, batch.n_tokens = 1069, progress = 0.988933
slot update_slots: id  2 | task 21843 | n_tokens = 5719, memory_seq_rm [5719, end)
slot update_slots: id  2 | task 21843 | prompt processing progress, n_tokens = 5783, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 21843 | prompt done, n_tokens = 5783, batch.n_tokens = 64
slot init_sampler: id  2 | task 21843 | init sampler, took 0.90 ms, tokens: text = 5783, total = 5783
slot update_slots: id  2 | task 21843 | created context checkpoint 7 of 8 (pos_min = 4792, pos_max = 5718, size = 21.737 MiB)
slot print_timing: id  2 | task 21843 | 
prompt eval time =    1511.65 ms /  1133 tokens (    1.33 ms per token,   749.51 tokens per second)
       eval time =    3474.84 ms /   122 tokens (   28.48 ms per token,    35.11 tokens per second)
      total time =    4986.48 ms /  1255 tokens
slot      release: id  2 | task 21843 | stop processing: n_tokens = 5904, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.980
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 21967 | processing task, is_child = 0
slot update_slots: id  2 | task 21967 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5879
slot update_slots: id  2 | task 21967 | n_tokens = 5783, memory_seq_rm [5783, end)
slot update_slots: id  2 | task 21967 | prompt processing progress, n_tokens = 5815, batch.n_tokens = 32, progress = 0.989114
slot update_slots: id  2 | task 21967 | n_tokens = 5815, memory_seq_rm [5815, end)
slot update_slots: id  2 | task 21967 | prompt processing progress, n_tokens = 5879, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 21967 | prompt done, n_tokens = 5879, batch.n_tokens = 64
slot init_sampler: id  2 | task 21967 | init sampler, took 1.10 ms, tokens: text = 5879, total = 5879
slot update_slots: id  2 | task 21967 | created context checkpoint 8 of 8 (pos_min = 4977, pos_max = 5814, size = 19.651 MiB)
slot print_timing: id  2 | task 21967 | 
prompt eval time =     389.45 ms /    96 tokens (    4.06 ms per token,   246.50 tokens per second)
       eval time =   10811.08 ms /   402 tokens (   26.89 ms per token,    37.18 tokens per second)
      total time =   11200.52 ms /   498 tokens
slot      release: id  2 | task 21967 | stop processing: n_tokens = 6280, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.152 (> 0.100 thold), f_keep = 0.108
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 6280, total state size = 168.997 MiB
srv          load:  - looking for better prompt, base f_keep = 0.108, sim = 0.152
srv        update:  - cache state: 13 prompts, 3723.185 MiB (limits: 8192.000 MiB, 56064 tokens, 159391 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv        update:    - prompt 0x5a03d440fde0:    7843 tokens, checkpoints:  8,   362.547 MiB
srv        update:    - prompt 0x5a03cd584ab0:    4868 tokens, checkpoints:  7,   270.134 MiB
srv        update:    - prompt 0x5a03cf145a50:    4159 tokens, checkpoints:  7,   251.891 MiB
srv        update:    - prompt 0x5a03d42d4e40:    5689 tokens, checkpoints:  8,   313.961 MiB
srv        update:    - prompt 0x5a0400041a00:    6280 tokens, checkpoints:  8,   329.062 MiB
srv  get_availabl: prompt cache update took 236.77 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 22371 | processing task, is_child = 0
slot update_slots: id  2 | task 22371 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4433
slot update_slots: id  2 | task 22371 | n_past = 676, slot.prompt.tokens.size() = 6280, seq_id = 2, pos_min = 5353, n_swa = 128
slot update_slots: id  2 | task 22371 | restored context checkpoint (pos_min = 37, pos_max = 873, size = 19.627 MiB)
slot update_slots: id  2 | task 22371 | erased invalidated context checkpoint (pos_min = 2473, pos_max = 3399, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 22371 | erased invalidated context checkpoint (pos_min = 2650, pos_max = 3509, n_swa = 128, size = 20.166 MiB)
slot update_slots: id  2 | task 22371 | erased invalidated context checkpoint (pos_min = 3659, pos_max = 4585, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 22371 | erased invalidated context checkpoint (pos_min = 4792, pos_max = 5718, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 22371 | erased invalidated context checkpoint (pos_min = 4977, pos_max = 5814, n_swa = 128, size = 19.651 MiB)
slot update_slots: id  2 | task 22371 | n_tokens = 676, memory_seq_rm [676, end)
slot update_slots: id  2 | task 22371 | prompt processing progress, n_tokens = 2724, batch.n_tokens = 2048, progress = 0.614482
slot update_slots: id  2 | task 22371 | n_tokens = 2724, memory_seq_rm [2724, end)
slot update_slots: id  2 | task 22371 | prompt processing progress, n_tokens = 4369, batch.n_tokens = 1645, progress = 0.985563
slot update_slots: id  2 | task 22371 | n_tokens = 4369, memory_seq_rm [4369, end)
slot update_slots: id  2 | task 22371 | prompt processing progress, n_tokens = 4433, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 22371 | prompt done, n_tokens = 4433, batch.n_tokens = 64
slot init_sampler: id  2 | task 22371 | init sampler, took 0.78 ms, tokens: text = 4433, total = 4433
slot update_slots: id  2 | task 22371 | created context checkpoint 4 of 8 (pos_min = 3442, pos_max = 4368, size = 21.737 MiB)
slot print_timing: id  2 | task 22371 | 
prompt eval time =    4029.89 ms /  3757 tokens (    1.07 ms per token,   932.28 tokens per second)
       eval time =    2592.04 ms /    99 tokens (   26.18 ms per token,    38.19 tokens per second)
      total time =    6621.93 ms /  3856 tokens
slot      release: id  2 | task 22371 | stop processing: n_tokens = 4531, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.967 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 22473 | processing task, is_child = 0
slot update_slots: id  2 | task 22473 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4586
slot update_slots: id  2 | task 22473 | n_tokens = 4433, memory_seq_rm [4433, end)
slot update_slots: id  2 | task 22473 | prompt processing progress, n_tokens = 4522, batch.n_tokens = 89, progress = 0.986044
slot update_slots: id  2 | task 22473 | n_tokens = 4522, memory_seq_rm [4522, end)
slot update_slots: id  2 | task 22473 | prompt processing progress, n_tokens = 4586, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 22473 | prompt done, n_tokens = 4586, batch.n_tokens = 64
slot init_sampler: id  2 | task 22473 | init sampler, took 0.68 ms, tokens: text = 4586, total = 4586
slot update_slots: id  2 | task 22473 | created context checkpoint 5 of 8 (pos_min = 3604, pos_max = 4521, size = 21.526 MiB)
slot print_timing: id  2 | task 22473 | 
prompt eval time =     492.40 ms /   153 tokens (    3.22 ms per token,   310.72 tokens per second)
       eval time =      76.42 ms /     4 tokens (   19.10 ms per token,    52.34 tokens per second)
      total time =     568.82 ms /   157 tokens
slot      release: id  2 | task 22473 | stop processing: n_tokens = 4589, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.152 (> 0.100 thold), f_keep = 0.147
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4589, total state size = 129.345 MiB
srv          load:  - looking for better prompt, base f_keep = 0.147, sim = 0.152
srv        update:  - cache state: 14 prompts, 3950.830 MiB (limits: 8192.000 MiB, 56064 tokens, 159722 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv        update:    - prompt 0x5a03d440fde0:    7843 tokens, checkpoints:  8,   362.547 MiB
srv        update:    - prompt 0x5a03cd584ab0:    4868 tokens, checkpoints:  7,   270.134 MiB
srv        update:    - prompt 0x5a03cf145a50:    4159 tokens, checkpoints:  7,   251.891 MiB
srv        update:    - prompt 0x5a03d42d4e40:    5689 tokens, checkpoints:  8,   313.961 MiB
srv        update:    - prompt 0x5a0400041a00:    6280 tokens, checkpoints:  8,   329.062 MiB
srv        update:    - prompt 0x5a03cf19ea50:    4589 tokens, checkpoints:  5,   227.644 MiB
srv  get_availabl: prompt cache update took 191.79 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 22479 | processing task, is_child = 0
slot update_slots: id  2 | task 22479 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4448
slot update_slots: id  2 | task 22479 | n_past = 676, slot.prompt.tokens.size() = 4589, seq_id = 2, pos_min = 3662, n_swa = 128
slot update_slots: id  2 | task 22479 | restored context checkpoint (pos_min = 37, pos_max = 873, size = 19.627 MiB)
slot update_slots: id  2 | task 22479 | erased invalidated context checkpoint (pos_min = 3442, pos_max = 4368, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 22479 | erased invalidated context checkpoint (pos_min = 3604, pos_max = 4521, n_swa = 128, size = 21.526 MiB)
slot update_slots: id  2 | task 22479 | n_tokens = 676, memory_seq_rm [676, end)
slot update_slots: id  2 | task 22479 | prompt processing progress, n_tokens = 2724, batch.n_tokens = 2048, progress = 0.612410
slot update_slots: id  2 | task 22479 | n_tokens = 2724, memory_seq_rm [2724, end)
slot update_slots: id  2 | task 22479 | prompt processing progress, n_tokens = 4384, batch.n_tokens = 1660, progress = 0.985611
slot update_slots: id  2 | task 22479 | n_tokens = 4384, memory_seq_rm [4384, end)
slot update_slots: id  2 | task 22479 | prompt processing progress, n_tokens = 4448, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 22479 | prompt done, n_tokens = 4448, batch.n_tokens = 64
slot init_sampler: id  2 | task 22479 | init sampler, took 0.72 ms, tokens: text = 4448, total = 4448
slot update_slots: id  2 | task 22479 | created context checkpoint 4 of 8 (pos_min = 3457, pos_max = 4383, size = 21.737 MiB)
slot print_timing: id  2 | task 22479 | 
prompt eval time =    4246.60 ms /  3772 tokens (    1.13 ms per token,   888.24 tokens per second)
       eval time =    3658.00 ms /   132 tokens (   27.71 ms per token,    36.09 tokens per second)
      total time =    7904.59 ms /  3904 tokens
slot      release: id  2 | task 22479 | stop processing: n_tokens = 4579, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.838 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 22614 | processing task, is_child = 0
slot update_slots: id  2 | task 22614 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5305
slot update_slots: id  2 | task 22614 | n_tokens = 4448, memory_seq_rm [4448, end)
slot update_slots: id  2 | task 22614 | prompt processing progress, n_tokens = 5241, batch.n_tokens = 793, progress = 0.987936
slot update_slots: id  2 | task 22614 | n_tokens = 5241, memory_seq_rm [5241, end)
slot update_slots: id  2 | task 22614 | prompt processing progress, n_tokens = 5305, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 22614 | prompt done, n_tokens = 5305, batch.n_tokens = 64
slot init_sampler: id  2 | task 22614 | init sampler, took 1.00 ms, tokens: text = 5305, total = 5305
slot update_slots: id  2 | task 22614 | created context checkpoint 5 of 8 (pos_min = 4314, pos_max = 5240, size = 21.737 MiB)
slot print_timing: id  2 | task 22614 | 
prompt eval time =    1189.70 ms /   857 tokens (    1.39 ms per token,   720.35 tokens per second)
       eval time =    5610.60 ms /   192 tokens (   29.22 ms per token,    34.22 tokens per second)
      total time =    6800.30 ms /  1049 tokens
slot      release: id  2 | task 22614 | stop processing: n_tokens = 5496, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.148 (> 0.100 thold), f_keep = 0.126
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5496, total state size = 150.613 MiB
srv          load:  - looking for better prompt, base f_keep = 0.126, sim = 0.148
srv        update:  - cache state: 15 prompts, 4199.953 MiB (limits: 8192.000 MiB, 56064 tokens, 160968 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv        update:    - prompt 0x5a03d440fde0:    7843 tokens, checkpoints:  8,   362.547 MiB
srv        update:    - prompt 0x5a03cd584ab0:    4868 tokens, checkpoints:  7,   270.134 MiB
srv        update:    - prompt 0x5a03cf145a50:    4159 tokens, checkpoints:  7,   251.891 MiB
srv        update:    - prompt 0x5a03d42d4e40:    5689 tokens, checkpoints:  8,   313.961 MiB
srv        update:    - prompt 0x5a0400041a00:    6280 tokens, checkpoints:  8,   329.062 MiB
srv        update:    - prompt 0x5a03cf19ea50:    4589 tokens, checkpoints:  5,   227.644 MiB
srv        update:    - prompt 0x5a03cee753a0:    5496 tokens, checkpoints:  5,   249.124 MiB
srv  get_availabl: prompt cache update took 187.74 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 22808 | processing task, is_child = 0
slot update_slots: id  2 | task 22808 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4656
slot update_slots: id  2 | task 22808 | n_past = 690, slot.prompt.tokens.size() = 5496, seq_id = 2, pos_min = 4569, n_swa = 128
slot update_slots: id  2 | task 22808 | restored context checkpoint (pos_min = 37, pos_max = 873, size = 19.627 MiB)
slot update_slots: id  2 | task 22808 | erased invalidated context checkpoint (pos_min = 3457, pos_max = 4383, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 22808 | erased invalidated context checkpoint (pos_min = 4314, pos_max = 5240, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 22808 | n_tokens = 690, memory_seq_rm [690, end)
slot update_slots: id  2 | task 22808 | prompt processing progress, n_tokens = 2738, batch.n_tokens = 2048, progress = 0.588058
slot update_slots: id  2 | task 22808 | n_tokens = 2738, memory_seq_rm [2738, end)
slot update_slots: id  2 | task 22808 | prompt processing progress, n_tokens = 4592, batch.n_tokens = 1854, progress = 0.986254
slot update_slots: id  2 | task 22808 | n_tokens = 4592, memory_seq_rm [4592, end)
slot update_slots: id  2 | task 22808 | prompt processing progress, n_tokens = 4656, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 22808 | prompt done, n_tokens = 4656, batch.n_tokens = 64
slot init_sampler: id  2 | task 22808 | init sampler, took 0.73 ms, tokens: text = 4656, total = 4656
slot update_slots: id  2 | task 22808 | created context checkpoint 4 of 8 (pos_min = 3665, pos_max = 4591, size = 21.737 MiB)
slot print_timing: id  2 | task 22808 | 
prompt eval time =    4233.24 ms /  3966 tokens (    1.07 ms per token,   936.87 tokens per second)
       eval time =   18075.45 ms /   632 tokens (   28.60 ms per token,    34.96 tokens per second)
      total time =   22308.69 ms /  4598 tokens
slot      release: id  2 | task 22808 | stop processing: n_tokens = 5287, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.891 (> 0.100 thold), f_keep = 0.128
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 5287, total state size = 145.712 MiB
srv          load:  - looking for better prompt, base f_keep = 0.128, sim = 0.891
srv        update:  - cache state: 16 prompts, 4422.439 MiB (limits: 8192.000 MiB, 56064 tokens, 162664 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv        update:    - prompt 0x5a03d440fde0:    7843 tokens, checkpoints:  8,   362.547 MiB
srv        update:    - prompt 0x5a03cd584ab0:    4868 tokens, checkpoints:  7,   270.134 MiB
srv        update:    - prompt 0x5a03cf145a50:    4159 tokens, checkpoints:  7,   251.891 MiB
srv        update:    - prompt 0x5a03d42d4e40:    5689 tokens, checkpoints:  8,   313.961 MiB
srv        update:    - prompt 0x5a0400041a00:    6280 tokens, checkpoints:  8,   329.062 MiB
srv        update:    - prompt 0x5a03cf19ea50:    4589 tokens, checkpoints:  5,   227.644 MiB
srv        update:    - prompt 0x5a03cee753a0:    5496 tokens, checkpoints:  5,   249.124 MiB
srv        update:    - prompt 0x5a03cae884d0:    5287 tokens, checkpoints:  4,   222.485 MiB
srv  get_availabl: prompt cache update took 244.35 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 23443 | processing task, is_child = 0
slot update_slots: id  2 | task 23443 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 758
slot update_slots: id  2 | task 23443 | n_past = 675, slot.prompt.tokens.size() = 5287, seq_id = 2, pos_min = 4360, n_swa = 128
slot update_slots: id  2 | task 23443 | restored context checkpoint (pos_min = 37, pos_max = 873, size = 19.627 MiB)
slot update_slots: id  2 | task 23443 | erased invalidated context checkpoint (pos_min = 3665, pos_max = 4591, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 23443 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  2 | task 23443 | prompt processing progress, n_tokens = 694, batch.n_tokens = 19, progress = 0.915567
slot update_slots: id  2 | task 23443 | n_tokens = 694, memory_seq_rm [694, end)
slot update_slots: id  2 | task 23443 | prompt processing progress, n_tokens = 758, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 23443 | prompt done, n_tokens = 758, batch.n_tokens = 64
slot init_sampler: id  2 | task 23443 | init sampler, took 0.14 ms, tokens: text = 758, total = 758
slot print_timing: id  2 | task 23443 | 
prompt eval time =     350.67 ms /    83 tokens (    4.22 ms per token,   236.69 tokens per second)
       eval time =    1675.67 ms /    63 tokens (   26.60 ms per token,    37.60 tokens per second)
      total time =    2026.34 ms /   146 tokens
slot      release: id  2 | task 23443 | stop processing: n_tokens = 820, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.487 (> 0.100 thold), f_keep = 0.924
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 23508 | processing task, is_child = 0
slot update_slots: id  2 | task 23508 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1558
slot update_slots: id  2 | task 23508 | n_tokens = 758, memory_seq_rm [758, end)
slot update_slots: id  2 | task 23508 | prompt processing progress, n_tokens = 1494, batch.n_tokens = 736, progress = 0.958922
slot update_slots: id  2 | task 23508 | n_tokens = 1494, memory_seq_rm [1494, end)
slot update_slots: id  2 | task 23508 | prompt processing progress, n_tokens = 1558, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 23508 | prompt done, n_tokens = 1558, batch.n_tokens = 64
slot init_sampler: id  2 | task 23508 | init sampler, took 0.33 ms, tokens: text = 1558, total = 1558
slot update_slots: id  2 | task 23508 | created context checkpoint 4 of 8 (pos_min = 675, pos_max = 1493, size = 19.205 MiB)
slot print_timing: id  2 | task 23508 | 
prompt eval time =    1020.12 ms /   800 tokens (    1.28 ms per token,   784.22 tokens per second)
       eval time =    1738.40 ms /    62 tokens (   28.04 ms per token,    35.66 tokens per second)
      total time =    2758.53 ms /   862 tokens
slot      release: id  2 | task 23508 | stop processing: n_tokens = 1619, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.472 (> 0.100 thold), f_keep = 0.962
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 23572 | processing task, is_child = 0
slot update_slots: id  2 | task 23572 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3303
slot update_slots: id  2 | task 23572 | n_tokens = 1558, memory_seq_rm [1558, end)
slot update_slots: id  2 | task 23572 | prompt processing progress, n_tokens = 3239, batch.n_tokens = 1681, progress = 0.980624
slot update_slots: id  2 | task 23572 | n_tokens = 3239, memory_seq_rm [3239, end)
slot update_slots: id  2 | task 23572 | prompt processing progress, n_tokens = 3303, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 23572 | prompt done, n_tokens = 3303, batch.n_tokens = 64
slot init_sampler: id  2 | task 23572 | init sampler, took 0.64 ms, tokens: text = 3303, total = 3303
slot update_slots: id  2 | task 23572 | created context checkpoint 5 of 8 (pos_min = 2312, pos_max = 3238, size = 21.737 MiB)
slot print_timing: id  2 | task 23572 | 
prompt eval time =    2183.66 ms /  1745 tokens (    1.25 ms per token,   799.12 tokens per second)
       eval time =    2130.68 ms /    73 tokens (   29.19 ms per token,    34.26 tokens per second)
      total time =    4314.34 ms /  1818 tokens
slot      release: id  2 | task 23572 | stop processing: n_tokens = 3375, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.721 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 23647 | processing task, is_child = 0
slot update_slots: id  2 | task 23647 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4580
slot update_slots: id  2 | task 23647 | n_tokens = 3303, memory_seq_rm [3303, end)
slot update_slots: id  2 | task 23647 | prompt processing progress, n_tokens = 4516, batch.n_tokens = 1213, progress = 0.986026
slot update_slots: id  2 | task 23647 | n_tokens = 4516, memory_seq_rm [4516, end)
slot update_slots: id  2 | task 23647 | prompt processing progress, n_tokens = 4580, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 23647 | prompt done, n_tokens = 4580, batch.n_tokens = 64
slot init_sampler: id  2 | task 23647 | init sampler, took 0.76 ms, tokens: text = 4580, total = 4580
slot update_slots: id  2 | task 23647 | created context checkpoint 6 of 8 (pos_min = 3589, pos_max = 4515, size = 21.737 MiB)
slot print_timing: id  2 | task 23647 | 
prompt eval time =    1759.70 ms /  1277 tokens (    1.38 ms per token,   725.69 tokens per second)
       eval time =    2766.16 ms /    92 tokens (   30.07 ms per token,    33.26 tokens per second)
      total time =    4525.86 ms /  1369 tokens
slot      release: id  2 | task 23647 | stop processing: n_tokens = 4671, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.786 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 23741 | processing task, is_child = 0
slot update_slots: id  2 | task 23741 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5826
slot update_slots: id  2 | task 23741 | n_tokens = 4580, memory_seq_rm [4580, end)
slot update_slots: id  2 | task 23741 | prompt processing progress, n_tokens = 5762, batch.n_tokens = 1182, progress = 0.989015
slot update_slots: id  2 | task 23741 | n_tokens = 5762, memory_seq_rm [5762, end)
slot update_slots: id  2 | task 23741 | prompt processing progress, n_tokens = 5826, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 23741 | prompt done, n_tokens = 5826, batch.n_tokens = 64
slot init_sampler: id  2 | task 23741 | init sampler, took 0.88 ms, tokens: text = 5826, total = 5826
slot update_slots: id  2 | task 23741 | created context checkpoint 7 of 8 (pos_min = 4835, pos_max = 5761, size = 21.737 MiB)
slot print_timing: id  2 | task 23741 | 
prompt eval time =    1784.37 ms /  1246 tokens (    1.43 ms per token,   698.29 tokens per second)
       eval time =    6672.02 ms /   223 tokens (   29.92 ms per token,    33.42 tokens per second)
      total time =    8456.39 ms /  1469 tokens
slot      release: id  2 | task 23741 | stop processing: n_tokens = 6048, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.912 (> 0.100 thold), f_keep = 0.963
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 23966 | processing task, is_child = 0
slot update_slots: id  2 | task 23966 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6386
slot update_slots: id  2 | task 23966 | n_tokens = 5826, memory_seq_rm [5826, end)
slot update_slots: id  2 | task 23966 | prompt processing progress, n_tokens = 6322, batch.n_tokens = 496, progress = 0.989978
slot update_slots: id  2 | task 23966 | n_tokens = 6322, memory_seq_rm [6322, end)
slot update_slots: id  2 | task 23966 | prompt processing progress, n_tokens = 6386, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 23966 | prompt done, n_tokens = 6386, batch.n_tokens = 64
slot init_sampler: id  2 | task 23966 | init sampler, took 0.94 ms, tokens: text = 6386, total = 6386
slot update_slots: id  2 | task 23966 | created context checkpoint 8 of 8 (pos_min = 5395, pos_max = 6321, size = 21.737 MiB)
slot print_timing: id  2 | task 23966 | 
prompt eval time =     796.67 ms /   560 tokens (    1.42 ms per token,   702.93 tokens per second)
       eval time =   28405.60 ms /  1057 tokens (   26.87 ms per token,    37.21 tokens per second)
      total time =   29202.27 ms /  1617 tokens
slot      release: id  2 | task 23966 | stop processing: n_tokens = 7442, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.897 (> 0.100 thold), f_keep = 0.858
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 25025 | processing task, is_child = 0
slot update_slots: id  2 | task 25025 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7119
slot update_slots: id  2 | task 25025 | n_past = 6386, slot.prompt.tokens.size() = 7442, seq_id = 2, pos_min = 6515, n_swa = 128
slot update_slots: id  2 | task 25025 | restored context checkpoint (pos_min = 5395, pos_max = 6321, size = 21.737 MiB)
slot update_slots: id  2 | task 25025 | n_tokens = 6321, memory_seq_rm [6321, end)
slot update_slots: id  2 | task 25025 | prompt processing progress, n_tokens = 7055, batch.n_tokens = 734, progress = 0.991010
slot update_slots: id  2 | task 25025 | n_tokens = 7055, memory_seq_rm [7055, end)
slot update_slots: id  2 | task 25025 | prompt processing progress, n_tokens = 7119, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 25025 | prompt done, n_tokens = 7119, batch.n_tokens = 64
slot init_sampler: id  2 | task 25025 | init sampler, took 1.75 ms, tokens: text = 7119, total = 7119
slot update_slots: id  2 | task 25025 | erasing old context checkpoint (pos_min = 0, pos_max = 717, size = 16.837 MiB)
slot update_slots: id  2 | task 25025 | created context checkpoint 8 of 8 (pos_min = 6128, pos_max = 7054, size = 21.737 MiB)
slot print_timing: id  2 | task 25025 | 
prompt eval time =    1136.75 ms /   798 tokens (    1.42 ms per token,   702.00 tokens per second)
       eval time =   40717.43 ms /  1515 tokens (   26.88 ms per token,    37.21 tokens per second)
      total time =   41854.18 ms /  2313 tokens
slot      release: id  2 | task 25025 | stop processing: n_tokens = 8633, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.165 (> 0.100 thold), f_keep = 0.078
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8633, total state size = 224.172 MiB
srv          load:  - looking for better prompt, base f_keep = 0.078, sim = 0.165
srv        update:  - cache state: 17 prompts, 4812.702 MiB (limits: 8192.000 MiB, 56064 tokens, 164168 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv        update:    - prompt 0x5a03d440fde0:    7843 tokens, checkpoints:  8,   362.547 MiB
srv        update:    - prompt 0x5a03cd584ab0:    4868 tokens, checkpoints:  7,   270.134 MiB
srv        update:    - prompt 0x5a03cf145a50:    4159 tokens, checkpoints:  7,   251.891 MiB
srv        update:    - prompt 0x5a03d42d4e40:    5689 tokens, checkpoints:  8,   313.961 MiB
srv        update:    - prompt 0x5a0400041a00:    6280 tokens, checkpoints:  8,   329.062 MiB
srv        update:    - prompt 0x5a03cf19ea50:    4589 tokens, checkpoints:  5,   227.644 MiB
srv        update:    - prompt 0x5a03cee753a0:    5496 tokens, checkpoints:  5,   249.124 MiB
srv        update:    - prompt 0x5a03cae884d0:    5287 tokens, checkpoints:  4,   222.485 MiB
srv        update:    - prompt 0x5a03d42f2a80:    8633 tokens, checkpoints:  8,   390.264 MiB
srv  get_availabl: prompt cache update took 347.34 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 26542 | processing task, is_child = 0
slot update_slots: id  2 | task 26542 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4096
slot update_slots: id  2 | task 26542 | n_past = 675, slot.prompt.tokens.size() = 8633, seq_id = 2, pos_min = 7706, n_swa = 128
slot update_slots: id  2 | task 26542 | restored context checkpoint (pos_min = 37, pos_max = 873, size = 19.627 MiB)
slot update_slots: id  2 | task 26542 | erased invalidated context checkpoint (pos_min = 675, pos_max = 1493, n_swa = 128, size = 19.205 MiB)
slot update_slots: id  2 | task 26542 | erased invalidated context checkpoint (pos_min = 2312, pos_max = 3238, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 26542 | erased invalidated context checkpoint (pos_min = 3589, pos_max = 4515, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 26542 | erased invalidated context checkpoint (pos_min = 4835, pos_max = 5761, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 26542 | erased invalidated context checkpoint (pos_min = 5395, pos_max = 6321, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 26542 | erased invalidated context checkpoint (pos_min = 6128, pos_max = 7054, n_swa = 128, size = 21.737 MiB)
slot update_slots: id  2 | task 26542 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  2 | task 26542 | prompt processing progress, n_tokens = 2723, batch.n_tokens = 2048, progress = 0.664795
slot update_slots: id  2 | task 26542 | n_tokens = 2723, memory_seq_rm [2723, end)
slot update_slots: id  2 | task 26542 | prompt processing progress, n_tokens = 4032, batch.n_tokens = 1309, progress = 0.984375
slot update_slots: id  2 | task 26542 | n_tokens = 4032, memory_seq_rm [4032, end)
slot update_slots: id  2 | task 26542 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 26542 | prompt done, n_tokens = 4096, batch.n_tokens = 64
slot init_sampler: id  2 | task 26542 | init sampler, took 0.73 ms, tokens: text = 4096, total = 4096
slot update_slots: id  2 | task 26542 | created context checkpoint 3 of 8 (pos_min = 3105, pos_max = 4031, size = 21.737 MiB)
slot print_timing: id  2 | task 26542 | 
prompt eval time =    3865.49 ms /  3421 tokens (    1.13 ms per token,   885.01 tokens per second)
       eval time =   36205.40 ms /  1258 tokens (   28.78 ms per token,    34.75 tokens per second)
      total time =   40070.89 ms /  4679 tokens
slot      release: id  2 | task 26542 | stop processing: n_tokens = 5353, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.763 (> 0.100 thold), f_keep = 0.765
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 27803 | processing task, is_child = 0
slot update_slots: id  2 | task 27803 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5367
slot update_slots: id  2 | task 27803 | n_past = 4096, slot.prompt.tokens.size() = 5353, seq_id = 2, pos_min = 4426, n_swa = 128
slot update_slots: id  2 | task 27803 | restored context checkpoint (pos_min = 3105, pos_max = 4031, size = 21.737 MiB)
slot update_slots: id  2 | task 27803 | n_tokens = 4031, memory_seq_rm [4031, end)
slot update_slots: id  2 | task 27803 | prompt processing progress, n_tokens = 5303, batch.n_tokens = 1272, progress = 0.988075
slot update_slots: id  2 | task 27803 | n_tokens = 5303, memory_seq_rm [5303, end)
slot update_slots: id  2 | task 27803 | prompt processing progress, n_tokens = 5367, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 27803 | prompt done, n_tokens = 5367, batch.n_tokens = 64
slot init_sampler: id  2 | task 27803 | init sampler, took 1.45 ms, tokens: text = 5367, total = 5367
slot update_slots: id  2 | task 27803 | created context checkpoint 4 of 8 (pos_min = 4376, pos_max = 5302, size = 21.737 MiB)
slot print_timing: id  2 | task 27803 | 
prompt eval time =    1688.44 ms /  1336 tokens (    1.26 ms per token,   791.26 tokens per second)
       eval time =   19930.51 ms /   761 tokens (   26.19 ms per token,    38.18 tokens per second)
      total time =   21618.95 ms /  2097 tokens
slot      release: id  2 | task 27803 | stop processing: n_tokens = 6127, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.879 (> 0.100 thold), f_keep = 0.876
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 28566 | processing task, is_child = 0
slot update_slots: id  2 | task 28566 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6104
slot update_slots: id  2 | task 28566 | n_tokens = 5367, memory_seq_rm [5367, end)
slot update_slots: id  2 | task 28566 | prompt processing progress, n_tokens = 6040, batch.n_tokens = 673, progress = 0.989515
slot update_slots: id  2 | task 28566 | n_tokens = 6040, memory_seq_rm [6040, end)
slot update_slots: id  2 | task 28566 | prompt processing progress, n_tokens = 6104, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 28566 | prompt done, n_tokens = 6104, batch.n_tokens = 64
slot init_sampler: id  2 | task 28566 | init sampler, took 0.91 ms, tokens: text = 6104, total = 6104
slot update_slots: id  2 | task 28566 | created context checkpoint 5 of 8 (pos_min = 5200, pos_max = 6039, size = 19.697 MiB)
slot print_timing: id  2 | task 28566 | 
prompt eval time =    1038.74 ms /   737 tokens (    1.41 ms per token,   709.51 tokens per second)
       eval time =     548.48 ms /    22 tokens (   24.93 ms per token,    40.11 tokens per second)
      total time =    1587.21 ms /   759 tokens
slot      release: id  2 | task 28566 | stop processing: n_tokens = 6125, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28590 | processing task, is_child = 0
slot update_slots: id  0 | task 28590 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7987
slot update_slots: id  0 | task 28590 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 28590 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.256417
slot update_slots: id  0 | task 28590 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  0 | task 28590 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.512833
slot update_slots: id  0 | task 28590 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  0 | task 28590 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.769250
slot update_slots: id  0 | task 28590 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  0 | task 28590 | prompt processing progress, n_tokens = 7923, batch.n_tokens = 1779, progress = 0.991987
slot update_slots: id  0 | task 28590 | n_tokens = 7923, memory_seq_rm [7923, end)
slot update_slots: id  0 | task 28590 | prompt processing progress, n_tokens = 7987, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28590 | prompt done, n_tokens = 7987, batch.n_tokens = 64
slot init_sampler: id  0 | task 28590 | init sampler, took 2.06 ms, tokens: text = 7987, total = 7987
slot update_slots: id  0 | task 28590 | created context checkpoint 1 of 8 (pos_min = 7123, pos_max = 7922, size = 18.759 MiB)
slot print_timing: id  0 | task 28590 | 
prompt eval time =    9454.16 ms /  7987 tokens (    1.18 ms per token,   844.81 tokens per second)
       eval time =    1980.07 ms /    69 tokens (   28.70 ms per token,    34.85 tokens per second)
      total time =   11434.23 ms /  8056 tokens
slot      release: id  0 | task 28590 | stop processing: n_tokens = 8055, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.934 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28664 | processing task, is_child = 0
slot update_slots: id  0 | task 28664 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8547
slot update_slots: id  0 | task 28664 | n_tokens = 7987, memory_seq_rm [7987, end)
slot update_slots: id  0 | task 28664 | prompt processing progress, n_tokens = 8483, batch.n_tokens = 496, progress = 0.992512
slot update_slots: id  0 | task 28664 | n_tokens = 8483, memory_seq_rm [8483, end)
slot update_slots: id  0 | task 28664 | prompt processing progress, n_tokens = 8547, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28664 | prompt done, n_tokens = 8547, batch.n_tokens = 64
slot init_sampler: id  0 | task 28664 | init sampler, took 1.34 ms, tokens: text = 8547, total = 8547
slot update_slots: id  0 | task 28664 | created context checkpoint 2 of 8 (pos_min = 7683, pos_max = 8482, size = 18.759 MiB)
slot print_timing: id  0 | task 28664 | 
prompt eval time =     875.66 ms /   560 tokens (    1.56 ms per token,   639.52 tokens per second)
       eval time =    2696.08 ms /    93 tokens (   28.99 ms per token,    34.49 tokens per second)
      total time =    3571.74 ms /   653 tokens
slot      release: id  0 | task 28664 | stop processing: n_tokens = 8639, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.870 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28759 | processing task, is_child = 0
slot update_slots: id  0 | task 28759 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9824
slot update_slots: id  0 | task 28759 | n_tokens = 8547, memory_seq_rm [8547, end)
slot update_slots: id  0 | task 28759 | prompt processing progress, n_tokens = 9760, batch.n_tokens = 1213, progress = 0.993485
slot update_slots: id  0 | task 28759 | n_tokens = 9760, memory_seq_rm [9760, end)
slot update_slots: id  0 | task 28759 | prompt processing progress, n_tokens = 9824, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28759 | prompt done, n_tokens = 9824, batch.n_tokens = 64
slot init_sampler: id  0 | task 28759 | init sampler, took 1.41 ms, tokens: text = 9824, total = 9824
slot update_slots: id  0 | task 28759 | created context checkpoint 3 of 8 (pos_min = 8960, pos_max = 9759, size = 18.759 MiB)
slot print_timing: id  0 | task 28759 | 
prompt eval time =    1971.80 ms /  1277 tokens (    1.54 ms per token,   647.63 tokens per second)
       eval time =    2740.95 ms /    91 tokens (   30.12 ms per token,    33.20 tokens per second)
      total time =    4712.74 ms /  1368 tokens
slot      release: id  0 | task 28759 | stop processing: n_tokens = 9914, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.887 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 28852 | processing task, is_child = 0
slot update_slots: id  0 | task 28852 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11070
slot update_slots: id  0 | task 28852 | n_tokens = 9824, memory_seq_rm [9824, end)
slot update_slots: id  0 | task 28852 | prompt processing progress, n_tokens = 11006, batch.n_tokens = 1182, progress = 0.994219
slot update_slots: id  0 | task 28852 | n_tokens = 11006, memory_seq_rm [11006, end)
slot update_slots: id  0 | task 28852 | prompt processing progress, n_tokens = 11070, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 28852 | prompt done, n_tokens = 11070, batch.n_tokens = 64
slot init_sampler: id  0 | task 28852 | init sampler, took 2.53 ms, tokens: text = 11070, total = 11070
slot update_slots: id  0 | task 28852 | created context checkpoint 4 of 8 (pos_min = 10206, pos_max = 11005, size = 18.759 MiB)
slot print_timing: id  0 | task 28852 | 
prompt eval time =    2025.44 ms /  1246 tokens (    1.63 ms per token,   615.18 tokens per second)
       eval time =   62553.78 ms /  2193 tokens (   28.52 ms per token,    35.06 tokens per second)
      total time =   64579.21 ms /  3439 tokens
slot      release: id  0 | task 28852 | stop processing: n_tokens = 13262, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.835
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31047 | processing task, is_child = 0
slot update_slots: id  0 | task 31047 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11727
slot update_slots: id  0 | task 31047 | n_past = 11070, slot.prompt.tokens.size() = 13262, seq_id = 0, pos_min = 12462, n_swa = 128
slot update_slots: id  0 | task 31047 | restored context checkpoint (pos_min = 10206, pos_max = 11005, size = 18.759 MiB)
slot update_slots: id  0 | task 31047 | n_tokens = 11005, memory_seq_rm [11005, end)
slot update_slots: id  0 | task 31047 | prompt processing progress, n_tokens = 11663, batch.n_tokens = 658, progress = 0.994542
slot update_slots: id  0 | task 31047 | n_tokens = 11663, memory_seq_rm [11663, end)
slot update_slots: id  0 | task 31047 | prompt processing progress, n_tokens = 11727, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31047 | prompt done, n_tokens = 11727, batch.n_tokens = 64
slot init_sampler: id  0 | task 31047 | init sampler, took 1.70 ms, tokens: text = 11727, total = 11727
slot update_slots: id  0 | task 31047 | created context checkpoint 5 of 8 (pos_min = 10863, pos_max = 11662, size = 18.759 MiB)
slot print_timing: id  0 | task 31047 | 
prompt eval time =    1341.83 ms /   722 tokens (    1.86 ms per token,   538.07 tokens per second)
       eval time =   10259.60 ms /   361 tokens (   28.42 ms per token,    35.19 tokens per second)
      total time =   11601.43 ms /  1083 tokens
slot      release: id  0 | task 31047 | stop processing: n_tokens = 12087, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.970
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31410 | processing task, is_child = 0
slot update_slots: id  0 | task 31410 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12060
slot update_slots: id  0 | task 31410 | n_tokens = 11727, memory_seq_rm [11727, end)
slot update_slots: id  0 | task 31410 | prompt processing progress, n_tokens = 11996, batch.n_tokens = 269, progress = 0.994693
slot update_slots: id  0 | task 31410 | n_tokens = 11996, memory_seq_rm [11996, end)
slot update_slots: id  0 | task 31410 | prompt processing progress, n_tokens = 12060, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31410 | prompt done, n_tokens = 12060, batch.n_tokens = 64
slot init_sampler: id  0 | task 31410 | init sampler, took 1.87 ms, tokens: text = 12060, total = 12060
slot update_slots: id  0 | task 31410 | created context checkpoint 6 of 8 (pos_min = 11287, pos_max = 11995, size = 16.626 MiB)
slot print_timing: id  0 | task 31410 | 
prompt eval time =     653.62 ms /   333 tokens (    1.96 ms per token,   509.47 tokens per second)
       eval time =   13717.82 ms /   465 tokens (   29.50 ms per token,    33.90 tokens per second)
      total time =   14371.45 ms /   798 tokens
slot      release: id  0 | task 31410 | stop processing: n_tokens = 12524, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.963
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 31877 | processing task, is_child = 0
slot update_slots: id  0 | task 31877 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12351
slot update_slots: id  0 | task 31877 | n_tokens = 12060, memory_seq_rm [12060, end)
slot update_slots: id  0 | task 31877 | prompt processing progress, n_tokens = 12287, batch.n_tokens = 227, progress = 0.994818
slot update_slots: id  0 | task 31877 | n_tokens = 12287, memory_seq_rm [12287, end)
slot update_slots: id  0 | task 31877 | prompt processing progress, n_tokens = 12351, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 31877 | prompt done, n_tokens = 12351, batch.n_tokens = 64
slot init_sampler: id  0 | task 31877 | init sampler, took 1.80 ms, tokens: text = 12351, total = 12351
slot update_slots: id  0 | task 31877 | created context checkpoint 7 of 8 (pos_min = 11724, pos_max = 12286, size = 13.202 MiB)
slot print_timing: id  0 | task 31877 | 
prompt eval time =     657.48 ms /   291 tokens (    2.26 ms per token,   442.60 tokens per second)
       eval time =    6287.05 ms /   211 tokens (   29.80 ms per token,    33.56 tokens per second)
      total time =    6944.53 ms /   502 tokens
slot      release: id  0 | task 31877 | stop processing: n_tokens = 12561, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.983
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 32090 | processing task, is_child = 0
slot update_slots: id  0 | task 32090 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12424
slot update_slots: id  0 | task 32090 | n_tokens = 12351, memory_seq_rm [12351, end)
slot update_slots: id  0 | task 32090 | prompt processing progress, n_tokens = 12360, batch.n_tokens = 9, progress = 0.994849
slot update_slots: id  0 | task 32090 | n_tokens = 12360, memory_seq_rm [12360, end)
slot update_slots: id  0 | task 32090 | prompt processing progress, n_tokens = 12424, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 32090 | prompt done, n_tokens = 12424, batch.n_tokens = 64
slot init_sampler: id  0 | task 32090 | init sampler, took 2.35 ms, tokens: text = 12424, total = 12424
slot update_slots: id  0 | task 32090 | created context checkpoint 8 of 8 (pos_min = 11813, pos_max = 12359, size = 12.827 MiB)
slot print_timing: id  0 | task 32090 | 
prompt eval time =     289.30 ms /    73 tokens (    3.96 ms per token,   252.33 tokens per second)
       eval time =     919.89 ms /    32 tokens (   28.75 ms per token,    34.79 tokens per second)
      total time =    1209.19 ms /   105 tokens
slot      release: id  0 | task 32090 | stop processing: n_tokens = 12455, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.939 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 32124 | processing task, is_child = 0
slot update_slots: id  0 | task 32124 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13227
slot update_slots: id  0 | task 32124 | n_tokens = 12424, memory_seq_rm [12424, end)
slot update_slots: id  0 | task 32124 | prompt processing progress, n_tokens = 13163, batch.n_tokens = 739, progress = 0.995161
slot update_slots: id  0 | task 32124 | n_tokens = 13163, memory_seq_rm [13163, end)
slot update_slots: id  0 | task 32124 | prompt processing progress, n_tokens = 13227, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 32124 | prompt done, n_tokens = 13227, batch.n_tokens = 64
slot init_sampler: id  0 | task 32124 | init sampler, took 1.93 ms, tokens: text = 13227, total = 13227
slot update_slots: id  0 | task 32124 | erasing old context checkpoint (pos_min = 7123, pos_max = 7922, size = 18.759 MiB)
slot update_slots: id  0 | task 32124 | created context checkpoint 8 of 8 (pos_min = 12363, pos_max = 13162, size = 18.759 MiB)
slot print_timing: id  0 | task 32124 | 
prompt eval time =    1312.18 ms /   803 tokens (    1.63 ms per token,   611.96 tokens per second)
       eval time =    1831.29 ms /    63 tokens (   29.07 ms per token,    34.40 tokens per second)
      total time =    3143.47 ms /   866 tokens
slot      release: id  0 | task 32124 | stop processing: n_tokens = 13289, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 32189 | processing task, is_child = 0
slot update_slots: id  0 | task 32189 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13473
slot update_slots: id  0 | task 32189 | n_tokens = 13227, memory_seq_rm [13227, end)
slot update_slots: id  0 | task 32189 | prompt processing progress, n_tokens = 13409, batch.n_tokens = 182, progress = 0.995250
slot update_slots: id  0 | task 32189 | n_tokens = 13409, memory_seq_rm [13409, end)
slot update_slots: id  0 | task 32189 | prompt processing progress, n_tokens = 13473, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 32189 | prompt done, n_tokens = 13473, batch.n_tokens = 64
slot init_sampler: id  0 | task 32189 | init sampler, took 1.93 ms, tokens: text = 13473, total = 13473
slot update_slots: id  0 | task 32189 | erasing old context checkpoint (pos_min = 7683, pos_max = 8482, size = 18.759 MiB)
slot update_slots: id  0 | task 32189 | created context checkpoint 8 of 8 (pos_min = 12609, pos_max = 13408, size = 18.759 MiB)
slot print_timing: id  0 | task 32189 | 
prompt eval time =     597.74 ms /   246 tokens (    2.43 ms per token,   411.55 tokens per second)
       eval time =    8714.04 ms /   300 tokens (   29.05 ms per token,    34.43 tokens per second)
      total time =    9311.78 ms /   546 tokens
slot      release: id  0 | task 32189 | stop processing: n_tokens = 13772, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 32491 | processing task, is_child = 0
slot update_slots: id  0 | task 32491 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14163
slot update_slots: id  0 | task 32491 | n_tokens = 13473, memory_seq_rm [13473, end)
slot update_slots: id  0 | task 32491 | prompt processing progress, n_tokens = 14099, batch.n_tokens = 626, progress = 0.995481
slot update_slots: id  0 | task 32491 | n_tokens = 14099, memory_seq_rm [14099, end)
slot update_slots: id  0 | task 32491 | prompt processing progress, n_tokens = 14163, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 32491 | prompt done, n_tokens = 14163, batch.n_tokens = 64
slot init_sampler: id  0 | task 32491 | init sampler, took 2.01 ms, tokens: text = 14163, total = 14163
slot update_slots: id  0 | task 32491 | erasing old context checkpoint (pos_min = 8960, pos_max = 9759, size = 18.759 MiB)
slot update_slots: id  0 | task 32491 | created context checkpoint 8 of 8 (pos_min = 13346, pos_max = 14098, size = 17.657 MiB)
slot print_timing: id  0 | task 32491 | 
prompt eval time =    1318.17 ms /   690 tokens (    1.91 ms per token,   523.45 tokens per second)
       eval time =   95709.98 ms /  3306 tokens (   28.95 ms per token,    34.54 tokens per second)
      total time =   97028.15 ms /  3996 tokens
slot      release: id  0 | task 32491 | stop processing: n_tokens = 17468, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.811
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35799 | processing task, is_child = 0
slot update_slots: id  0 | task 35799 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14696
slot update_slots: id  0 | task 35799 | n_past = 14163, slot.prompt.tokens.size() = 17468, seq_id = 0, pos_min = 16668, n_swa = 128
slot update_slots: id  0 | task 35799 | restored context checkpoint (pos_min = 13346, pos_max = 14098, size = 17.657 MiB)
slot update_slots: id  0 | task 35799 | n_tokens = 14098, memory_seq_rm [14098, end)
slot update_slots: id  0 | task 35799 | prompt processing progress, n_tokens = 14632, batch.n_tokens = 534, progress = 0.995645
slot update_slots: id  0 | task 35799 | n_tokens = 14632, memory_seq_rm [14632, end)
slot update_slots: id  0 | task 35799 | prompt processing progress, n_tokens = 14696, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35799 | prompt done, n_tokens = 14696, batch.n_tokens = 64
slot init_sampler: id  0 | task 35799 | init sampler, took 2.07 ms, tokens: text = 14696, total = 14696
slot update_slots: id  0 | task 35799 | erasing old context checkpoint (pos_min = 10206, pos_max = 11005, size = 18.759 MiB)
slot update_slots: id  0 | task 35799 | created context checkpoint 8 of 8 (pos_min = 13959, pos_max = 14631, size = 15.781 MiB)
slot print_timing: id  0 | task 35799 | 
prompt eval time =    1200.07 ms /   598 tokens (    2.01 ms per token,   498.30 tokens per second)
       eval time =    1349.84 ms /    47 tokens (   28.72 ms per token,    34.82 tokens per second)
      total time =    2549.91 ms /   645 tokens
slot      release: id  0 | task 35799 | stop processing: n_tokens = 14742, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.950 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35848 | processing task, is_child = 0
slot update_slots: id  0 | task 35848 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15477
slot update_slots: id  0 | task 35848 | n_tokens = 14696, memory_seq_rm [14696, end)
slot update_slots: id  0 | task 35848 | prompt processing progress, n_tokens = 15413, batch.n_tokens = 717, progress = 0.995865
slot update_slots: id  0 | task 35848 | n_tokens = 15413, memory_seq_rm [15413, end)
slot update_slots: id  0 | task 35848 | prompt processing progress, n_tokens = 15477, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35848 | prompt done, n_tokens = 15477, batch.n_tokens = 64
slot init_sampler: id  0 | task 35848 | init sampler, took 2.33 ms, tokens: text = 15477, total = 15477
slot update_slots: id  0 | task 35848 | erasing old context checkpoint (pos_min = 10863, pos_max = 11662, size = 18.759 MiB)
slot update_slots: id  0 | task 35848 | created context checkpoint 8 of 8 (pos_min = 14613, pos_max = 15412, size = 18.759 MiB)
slot print_timing: id  0 | task 35848 | 
prompt eval time =    1355.38 ms /   781 tokens (    1.74 ms per token,   576.22 tokens per second)
       eval time =     975.04 ms /    34 tokens (   28.68 ms per token,    34.87 tokens per second)
      total time =    2330.41 ms /   815 tokens
slot      release: id  0 | task 35848 | stop processing: n_tokens = 15510, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 35884 | processing task, is_child = 0
slot update_slots: id  0 | task 35884 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15644
slot update_slots: id  0 | task 35884 | n_tokens = 15477, memory_seq_rm [15477, end)
slot update_slots: id  0 | task 35884 | prompt processing progress, n_tokens = 15580, batch.n_tokens = 103, progress = 0.995909
slot update_slots: id  0 | task 35884 | n_tokens = 15580, memory_seq_rm [15580, end)
slot update_slots: id  0 | task 35884 | prompt processing progress, n_tokens = 15644, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 35884 | prompt done, n_tokens = 15644, batch.n_tokens = 64
slot init_sampler: id  0 | task 35884 | init sampler, took 2.18 ms, tokens: text = 15644, total = 15644
slot update_slots: id  0 | task 35884 | erasing old context checkpoint (pos_min = 11287, pos_max = 11995, size = 16.626 MiB)
slot update_slots: id  0 | task 35884 | created context checkpoint 8 of 8 (pos_min = 14780, pos_max = 15579, size = 18.759 MiB)
slot print_timing: id  0 | task 35884 | 
prompt eval time =     514.97 ms /   167 tokens (    3.08 ms per token,   324.29 tokens per second)
       eval time =    6089.48 ms /   209 tokens (   29.14 ms per token,    34.32 tokens per second)
      total time =    6604.45 ms /   376 tokens
slot      release: id  0 | task 35884 | stop processing: n_tokens = 15852, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.987
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36095 | processing task, is_child = 0
slot update_slots: id  0 | task 36095 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15838
slot update_slots: id  0 | task 36095 | n_tokens = 15644, memory_seq_rm [15644, end)
slot update_slots: id  0 | task 36095 | prompt processing progress, n_tokens = 15774, batch.n_tokens = 130, progress = 0.995959
slot update_slots: id  0 | task 36095 | n_tokens = 15774, memory_seq_rm [15774, end)
slot update_slots: id  0 | task 36095 | prompt processing progress, n_tokens = 15838, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36095 | prompt done, n_tokens = 15838, batch.n_tokens = 64
slot init_sampler: id  0 | task 36095 | init sampler, took 2.26 ms, tokens: text = 15838, total = 15838
slot update_slots: id  0 | task 36095 | erasing old context checkpoint (pos_min = 11724, pos_max = 12286, size = 13.202 MiB)
slot update_slots: id  0 | task 36095 | created context checkpoint 8 of 8 (pos_min = 15076, pos_max = 15773, size = 16.368 MiB)
slot print_timing: id  0 | task 36095 | 
prompt eval time =     532.70 ms /   194 tokens (    2.75 ms per token,   364.18 tokens per second)
       eval time =   13524.27 ms /   462 tokens (   29.27 ms per token,    34.16 tokens per second)
      total time =   14056.97 ms /   656 tokens
slot      release: id  0 | task 36095 | stop processing: n_tokens = 16299, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36559 | processing task, is_child = 0
slot update_slots: id  0 | task 36559 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16032
slot update_slots: id  0 | task 36559 | n_tokens = 15838, memory_seq_rm [15838, end)
slot update_slots: id  0 | task 36559 | prompt processing progress, n_tokens = 15968, batch.n_tokens = 130, progress = 0.996008
slot update_slots: id  0 | task 36559 | n_tokens = 15968, memory_seq_rm [15968, end)
slot update_slots: id  0 | task 36559 | prompt processing progress, n_tokens = 16032, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36559 | prompt done, n_tokens = 16032, batch.n_tokens = 64
slot init_sampler: id  0 | task 36559 | init sampler, took 2.40 ms, tokens: text = 16032, total = 16032
slot update_slots: id  0 | task 36559 | erasing old context checkpoint (pos_min = 11813, pos_max = 12359, size = 12.827 MiB)
slot update_slots: id  0 | task 36559 | created context checkpoint 8 of 8 (pos_min = 15601, pos_max = 15967, size = 8.606 MiB)
slot print_timing: id  0 | task 36559 | 
prompt eval time =     530.82 ms /   194 tokens (    2.74 ms per token,   365.47 tokens per second)
       eval time =   12742.10 ms /   434 tokens (   29.36 ms per token,    34.06 tokens per second)
      total time =   13272.92 ms /   628 tokens
slot      release: id  0 | task 36559 | stop processing: n_tokens = 16465, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.986 (> 0.100 thold), f_keep = 0.974
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 36995 | processing task, is_child = 0
slot update_slots: id  0 | task 36995 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16256
slot update_slots: id  0 | task 36995 | n_tokens = 16032, memory_seq_rm [16032, end)
slot update_slots: id  0 | task 36995 | prompt processing progress, n_tokens = 16192, batch.n_tokens = 160, progress = 0.996063
slot update_slots: id  0 | task 36995 | n_tokens = 16192, memory_seq_rm [16192, end)
slot update_slots: id  0 | task 36995 | prompt processing progress, n_tokens = 16256, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 36995 | prompt done, n_tokens = 16256, batch.n_tokens = 64
slot init_sampler: id  0 | task 36995 | init sampler, took 2.29 ms, tokens: text = 16256, total = 16256
slot update_slots: id  0 | task 36995 | erasing old context checkpoint (pos_min = 12363, pos_max = 13162, size = 18.759 MiB)
slot update_slots: id  0 | task 36995 | created context checkpoint 8 of 8 (pos_min = 15804, pos_max = 16191, size = 9.098 MiB)
slot print_timing: id  0 | task 36995 | 
prompt eval time =     576.89 ms /   224 tokens (    2.58 ms per token,   388.29 tokens per second)
       eval time =    1254.95 ms /    43 tokens (   29.18 ms per token,    34.26 tokens per second)
      total time =    1831.84 ms /   267 tokens
slot      release: id  0 | task 36995 | stop processing: n_tokens = 16298, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.963 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37040 | processing task, is_child = 0
slot update_slots: id  0 | task 37040 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16878
slot update_slots: id  0 | task 37040 | n_tokens = 16256, memory_seq_rm [16256, end)
slot update_slots: id  0 | task 37040 | prompt processing progress, n_tokens = 16814, batch.n_tokens = 558, progress = 0.996208
slot update_slots: id  0 | task 37040 | n_tokens = 16814, memory_seq_rm [16814, end)
slot update_slots: id  0 | task 37040 | prompt processing progress, n_tokens = 16878, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37040 | prompt done, n_tokens = 16878, batch.n_tokens = 64
slot init_sampler: id  0 | task 37040 | init sampler, took 2.38 ms, tokens: text = 16878, total = 16878
slot update_slots: id  0 | task 37040 | erasing old context checkpoint (pos_min = 12609, pos_max = 13408, size = 18.759 MiB)
slot update_slots: id  0 | task 37040 | created context checkpoint 8 of 8 (pos_min = 16032, pos_max = 16813, size = 18.337 MiB)
slot print_timing: id  0 | task 37040 | 
prompt eval time =    1145.57 ms /   622 tokens (    1.84 ms per token,   542.96 tokens per second)
       eval time =    4983.35 ms /   168 tokens (   29.66 ms per token,    33.71 tokens per second)
      total time =    6128.92 ms /   790 tokens
slot      release: id  0 | task 37040 | stop processing: n_tokens = 17045, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37210 | processing task, is_child = 0
slot update_slots: id  0 | task 37210 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17100
slot update_slots: id  0 | task 37210 | n_tokens = 16878, memory_seq_rm [16878, end)
slot update_slots: id  0 | task 37210 | prompt processing progress, n_tokens = 17036, batch.n_tokens = 158, progress = 0.996257
slot update_slots: id  0 | task 37210 | n_tokens = 17036, memory_seq_rm [17036, end)
slot update_slots: id  0 | task 37210 | prompt processing progress, n_tokens = 17100, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37210 | prompt done, n_tokens = 17100, batch.n_tokens = 64
slot init_sampler: id  0 | task 37210 | init sampler, took 3.42 ms, tokens: text = 17100, total = 17100
slot update_slots: id  0 | task 37210 | erasing old context checkpoint (pos_min = 13346, pos_max = 14098, size = 17.657 MiB)
slot update_slots: id  0 | task 37210 | created context checkpoint 8 of 8 (pos_min = 16245, pos_max = 17035, size = 18.548 MiB)
slot print_timing: id  0 | task 37210 | 
prompt eval time =     580.12 ms /   222 tokens (    2.61 ms per token,   382.68 tokens per second)
       eval time =    1350.47 ms /    45 tokens (   30.01 ms per token,    33.32 tokens per second)
      total time =    1930.58 ms /   267 tokens
slot      release: id  0 | task 37210 | stop processing: n_tokens = 17144, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37257 | processing task, is_child = 0
slot update_slots: id  0 | task 37257 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17461
slot update_slots: id  0 | task 37257 | n_tokens = 17100, memory_seq_rm [17100, end)
slot update_slots: id  0 | task 37257 | prompt processing progress, n_tokens = 17397, batch.n_tokens = 297, progress = 0.996335
slot update_slots: id  0 | task 37257 | n_tokens = 17397, memory_seq_rm [17397, end)
slot update_slots: id  0 | task 37257 | prompt processing progress, n_tokens = 17461, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37257 | prompt done, n_tokens = 17461, batch.n_tokens = 64
slot init_sampler: id  0 | task 37257 | init sampler, took 2.63 ms, tokens: text = 17461, total = 17461
slot update_slots: id  0 | task 37257 | erasing old context checkpoint (pos_min = 13959, pos_max = 14631, size = 15.781 MiB)
slot update_slots: id  0 | task 37257 | created context checkpoint 8 of 8 (pos_min = 16597, pos_max = 17396, size = 18.759 MiB)
slot print_timing: id  0 | task 37257 | 
prompt eval time =     726.12 ms /   361 tokens (    2.01 ms per token,   497.16 tokens per second)
       eval time =    1684.72 ms /    57 tokens (   29.56 ms per token,    33.83 tokens per second)
      total time =    2410.84 ms /   418 tokens
slot      release: id  0 | task 37257 | stop processing: n_tokens = 17517, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37316 | processing task, is_child = 0
slot update_slots: id  0 | task 37316 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18083
slot update_slots: id  0 | task 37316 | n_tokens = 17461, memory_seq_rm [17461, end)
slot update_slots: id  0 | task 37316 | prompt processing progress, n_tokens = 18019, batch.n_tokens = 558, progress = 0.996461
slot update_slots: id  0 | task 37316 | n_tokens = 18019, memory_seq_rm [18019, end)
slot update_slots: id  0 | task 37316 | prompt processing progress, n_tokens = 18083, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37316 | prompt done, n_tokens = 18083, batch.n_tokens = 64
slot init_sampler: id  0 | task 37316 | init sampler, took 2.61 ms, tokens: text = 18083, total = 18083
slot update_slots: id  0 | task 37316 | erasing old context checkpoint (pos_min = 14613, pos_max = 15412, size = 18.759 MiB)
slot update_slots: id  0 | task 37316 | created context checkpoint 8 of 8 (pos_min = 17219, pos_max = 18018, size = 18.759 MiB)
slot print_timing: id  0 | task 37316 | 
prompt eval time =    1141.22 ms /   622 tokens (    1.83 ms per token,   545.03 tokens per second)
       eval time =    5853.19 ms /   195 tokens (   30.02 ms per token,    33.32 tokens per second)
      total time =    6994.41 ms /   817 tokens
slot      release: id  0 | task 37316 | stop processing: n_tokens = 18277, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.987 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37513 | processing task, is_child = 0
slot update_slots: id  0 | task 37513 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18315
slot update_slots: id  0 | task 37513 | n_tokens = 18083, memory_seq_rm [18083, end)
slot update_slots: id  0 | task 37513 | prompt processing progress, n_tokens = 18251, batch.n_tokens = 168, progress = 0.996506
slot update_slots: id  0 | task 37513 | n_tokens = 18251, memory_seq_rm [18251, end)
slot update_slots: id  0 | task 37513 | prompt processing progress, n_tokens = 18315, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37513 | prompt done, n_tokens = 18315, batch.n_tokens = 64
slot init_sampler: id  0 | task 37513 | init sampler, took 3.82 ms, tokens: text = 18315, total = 18315
slot update_slots: id  0 | task 37513 | erasing old context checkpoint (pos_min = 14780, pos_max = 15579, size = 18.759 MiB)
slot update_slots: id  0 | task 37513 | created context checkpoint 8 of 8 (pos_min = 17477, pos_max = 18250, size = 18.150 MiB)
slot print_timing: id  0 | task 37513 | 
prompt eval time =     648.78 ms /   232 tokens (    2.80 ms per token,   357.60 tokens per second)
       eval time =    1954.50 ms /    65 tokens (   30.07 ms per token,    33.26 tokens per second)
      total time =    2603.27 ms /   297 tokens
slot      release: id  0 | task 37513 | stop processing: n_tokens = 18379, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37580 | processing task, is_child = 0
slot update_slots: id  0 | task 37580 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18455
slot update_slots: id  0 | task 37580 | n_tokens = 18315, memory_seq_rm [18315, end)
slot update_slots: id  0 | task 37580 | prompt processing progress, n_tokens = 18391, batch.n_tokens = 76, progress = 0.996532
slot update_slots: id  0 | task 37580 | n_tokens = 18391, memory_seq_rm [18391, end)
slot update_slots: id  0 | task 37580 | prompt processing progress, n_tokens = 18455, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37580 | prompt done, n_tokens = 18455, batch.n_tokens = 64
slot init_sampler: id  0 | task 37580 | init sampler, took 2.67 ms, tokens: text = 18455, total = 18455
slot update_slots: id  0 | task 37580 | erasing old context checkpoint (pos_min = 15076, pos_max = 15773, size = 16.368 MiB)
slot update_slots: id  0 | task 37580 | created context checkpoint 8 of 8 (pos_min = 17591, pos_max = 18390, size = 18.759 MiB)
slot print_timing: id  0 | task 37580 | 
prompt eval time =     509.06 ms /   140 tokens (    3.64 ms per token,   275.02 tokens per second)
       eval time =    1624.69 ms /    55 tokens (   29.54 ms per token,    33.85 tokens per second)
      total time =    2133.75 ms /   195 tokens
slot      release: id  0 | task 37580 | stop processing: n_tokens = 18509, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37637 | processing task, is_child = 0
slot update_slots: id  0 | task 37637 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 18595
slot update_slots: id  0 | task 37637 | n_tokens = 18455, memory_seq_rm [18455, end)
slot update_slots: id  0 | task 37637 | prompt processing progress, n_tokens = 18531, batch.n_tokens = 76, progress = 0.996558
slot update_slots: id  0 | task 37637 | n_tokens = 18531, memory_seq_rm [18531, end)
slot update_slots: id  0 | task 37637 | prompt processing progress, n_tokens = 18595, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37637 | prompt done, n_tokens = 18595, batch.n_tokens = 64
slot init_sampler: id  0 | task 37637 | init sampler, took 2.74 ms, tokens: text = 18595, total = 18595
slot update_slots: id  0 | task 37637 | erasing old context checkpoint (pos_min = 15601, pos_max = 15967, size = 8.606 MiB)
slot update_slots: id  0 | task 37637 | created context checkpoint 8 of 8 (pos_min = 17731, pos_max = 18530, size = 18.759 MiB)
slot print_timing: id  0 | task 37637 | 
prompt eval time =     500.38 ms /   140 tokens (    3.57 ms per token,   279.79 tokens per second)
       eval time =    3052.64 ms /   102 tokens (   29.93 ms per token,    33.41 tokens per second)
      total time =    3553.02 ms /   242 tokens
slot      release: id  0 | task 37637 | stop processing: n_tokens = 18696, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 37741 | processing task, is_child = 0
slot update_slots: id  0 | task 37741 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19297
slot update_slots: id  0 | task 37741 | n_tokens = 18595, memory_seq_rm [18595, end)
slot update_slots: id  0 | task 37741 | prompt processing progress, n_tokens = 19233, batch.n_tokens = 638, progress = 0.996683
slot update_slots: id  0 | task 37741 | n_tokens = 19233, memory_seq_rm [19233, end)
slot update_slots: id  0 | task 37741 | prompt processing progress, n_tokens = 19297, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 37741 | prompt done, n_tokens = 19297, batch.n_tokens = 64
slot init_sampler: id  0 | task 37741 | init sampler, took 2.75 ms, tokens: text = 19297, total = 19297
slot update_slots: id  0 | task 37741 | erasing old context checkpoint (pos_min = 15804, pos_max = 16191, size = 9.098 MiB)
slot update_slots: id  0 | task 37741 | created context checkpoint 8 of 8 (pos_min = 18433, pos_max = 19232, size = 18.759 MiB)
slot print_timing: id  0 | task 37741 | 
prompt eval time =    1394.00 ms /   702 tokens (    1.99 ms per token,   503.59 tokens per second)
       eval time =   48388.13 ms /  1593 tokens (   30.38 ms per token,    32.92 tokens per second)
      total time =   49782.13 ms /  2295 tokens
slot      release: id  0 | task 37741 | stop processing: n_tokens = 20889, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.924
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 39336 | processing task, is_child = 0
slot update_slots: id  0 | task 39336 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 19684
slot update_slots: id  0 | task 39336 | n_past = 19297, slot.prompt.tokens.size() = 20889, seq_id = 0, pos_min = 20089, n_swa = 128
slot update_slots: id  0 | task 39336 | restored context checkpoint (pos_min = 18433, pos_max = 19232, size = 18.759 MiB)
slot update_slots: id  0 | task 39336 | n_tokens = 19232, memory_seq_rm [19232, end)
slot update_slots: id  0 | task 39336 | prompt processing progress, n_tokens = 19620, batch.n_tokens = 388, progress = 0.996749
slot update_slots: id  0 | task 39336 | n_tokens = 19620, memory_seq_rm [19620, end)
slot update_slots: id  0 | task 39336 | prompt processing progress, n_tokens = 19684, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 39336 | prompt done, n_tokens = 19684, batch.n_tokens = 64
slot init_sampler: id  0 | task 39336 | init sampler, took 3.14 ms, tokens: text = 19684, total = 19684
slot update_slots: id  0 | task 39336 | erasing old context checkpoint (pos_min = 16032, pos_max = 16813, size = 18.337 MiB)
slot update_slots: id  0 | task 39336 | created context checkpoint 8 of 8 (pos_min = 18820, pos_max = 19619, size = 18.759 MiB)
slot print_timing: id  0 | task 39336 | 
prompt eval time =     987.14 ms /   452 tokens (    2.18 ms per token,   457.89 tokens per second)
       eval time =    1355.04 ms /    45 tokens (   30.11 ms per token,    33.21 tokens per second)
      total time =    2342.18 ms /   497 tokens
slot      release: id  0 | task 39336 | stop processing: n_tokens = 19728, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 39383 | processing task, is_child = 0
slot update_slots: id  0 | task 39383 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 20087
slot update_slots: id  0 | task 39383 | n_tokens = 19684, memory_seq_rm [19684, end)
slot update_slots: id  0 | task 39383 | prompt processing progress, n_tokens = 20023, batch.n_tokens = 339, progress = 0.996814
slot update_slots: id  0 | task 39383 | n_tokens = 20023, memory_seq_rm [20023, end)
slot update_slots: id  0 | task 39383 | prompt processing progress, n_tokens = 20087, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 39383 | prompt done, n_tokens = 20087, batch.n_tokens = 64
slot init_sampler: id  0 | task 39383 | init sampler, took 4.53 ms, tokens: text = 20087, total = 20087
slot update_slots: id  0 | task 39383 | erasing old context checkpoint (pos_min = 16245, pos_max = 17035, size = 18.548 MiB)
slot update_slots: id  0 | task 39383 | created context checkpoint 8 of 8 (pos_min = 19223, pos_max = 20022, size = 18.759 MiB)
slot print_timing: id  0 | task 39383 | 
prompt eval time =     815.04 ms /   403 tokens (    2.02 ms per token,   494.46 tokens per second)
       eval time =   10159.90 ms /   336 tokens (   30.24 ms per token,    33.07 tokens per second)
      total time =   10974.94 ms /   739 tokens
slot      release: id  0 | task 39383 | stop processing: n_tokens = 20422, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 39721 | processing task, is_child = 0
slot update_slots: id  0 | task 39721 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 20436
slot update_slots: id  0 | task 39721 | n_tokens = 20087, memory_seq_rm [20087, end)
slot update_slots: id  0 | task 39721 | prompt processing progress, n_tokens = 20372, batch.n_tokens = 285, progress = 0.996868
slot update_slots: id  0 | task 39721 | n_tokens = 20372, memory_seq_rm [20372, end)
slot update_slots: id  0 | task 39721 | prompt processing progress, n_tokens = 20436, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 39721 | prompt done, n_tokens = 20436, batch.n_tokens = 64
slot init_sampler: id  0 | task 39721 | init sampler, took 2.88 ms, tokens: text = 20436, total = 20436
slot update_slots: id  0 | task 39721 | erasing old context checkpoint (pos_min = 16597, pos_max = 17396, size = 18.759 MiB)
slot update_slots: id  0 | task 39721 | created context checkpoint 8 of 8 (pos_min = 19622, pos_max = 20371, size = 17.587 MiB)
slot print_timing: id  0 | task 39721 | 
prompt eval time =     703.06 ms /   349 tokens (    2.01 ms per token,   496.40 tokens per second)
       eval time =    6016.75 ms /   199 tokens (   30.23 ms per token,    33.07 tokens per second)
      total time =    6719.81 ms /   548 tokens
slot      release: id  0 | task 39721 | stop processing: n_tokens = 20634, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 39922 | processing task, is_child = 0
slot update_slots: id  0 | task 39922 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 20677
slot update_slots: id  0 | task 39922 | n_tokens = 20436, memory_seq_rm [20436, end)
slot update_slots: id  0 | task 39922 | prompt processing progress, n_tokens = 20613, batch.n_tokens = 177, progress = 0.996905
slot update_slots: id  0 | task 39922 | n_tokens = 20613, memory_seq_rm [20613, end)
slot update_slots: id  0 | task 39922 | prompt processing progress, n_tokens = 20677, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 39922 | prompt done, n_tokens = 20677, batch.n_tokens = 64
slot init_sampler: id  0 | task 39922 | init sampler, took 2.97 ms, tokens: text = 20677, total = 20677
slot update_slots: id  0 | task 39922 | erasing old context checkpoint (pos_min = 17219, pos_max = 18018, size = 18.759 MiB)
slot update_slots: id  0 | task 39922 | created context checkpoint 8 of 8 (pos_min = 19834, pos_max = 20612, size = 18.267 MiB)
slot print_timing: id  0 | task 39922 | 
prompt eval time =     628.93 ms /   241 tokens (    2.61 ms per token,   383.19 tokens per second)
       eval time =    2146.43 ms /    71 tokens (   30.23 ms per token,    33.08 tokens per second)
      total time =    2775.36 ms /   312 tokens
slot      release: id  0 | task 39922 | stop processing: n_tokens = 20747, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 39995 | processing task, is_child = 0
slot update_slots: id  0 | task 39995 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21210
slot update_slots: id  0 | task 39995 | n_tokens = 20677, memory_seq_rm [20677, end)
slot update_slots: id  0 | task 39995 | prompt processing progress, n_tokens = 21146, batch.n_tokens = 469, progress = 0.996983
slot update_slots: id  0 | task 39995 | n_tokens = 21146, memory_seq_rm [21146, end)
slot update_slots: id  0 | task 39995 | prompt processing progress, n_tokens = 21210, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 39995 | prompt done, n_tokens = 21210, batch.n_tokens = 64
slot init_sampler: id  0 | task 39995 | init sampler, took 3.96 ms, tokens: text = 21210, total = 21210
slot update_slots: id  0 | task 39995 | erasing old context checkpoint (pos_min = 17477, pos_max = 18250, size = 18.150 MiB)
slot update_slots: id  0 | task 39995 | created context checkpoint 8 of 8 (pos_min = 20346, pos_max = 21145, size = 18.759 MiB)
slot print_timing: id  0 | task 39995 | 
prompt eval time =     968.91 ms /   533 tokens (    1.82 ms per token,   550.10 tokens per second)
       eval time =    4975.73 ms /   164 tokens (   30.34 ms per token,    32.96 tokens per second)
      total time =    5944.64 ms /   697 tokens
slot      release: id  0 | task 39995 | stop processing: n_tokens = 21373, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 40161 | processing task, is_child = 0
slot update_slots: id  0 | task 40161 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21432
slot update_slots: id  0 | task 40161 | n_tokens = 21210, memory_seq_rm [21210, end)
slot update_slots: id  0 | task 40161 | prompt processing progress, n_tokens = 21368, batch.n_tokens = 158, progress = 0.997014
slot update_slots: id  0 | task 40161 | n_tokens = 21368, memory_seq_rm [21368, end)
slot update_slots: id  0 | task 40161 | prompt processing progress, n_tokens = 21432, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 40161 | prompt done, n_tokens = 21432, batch.n_tokens = 64
slot init_sampler: id  0 | task 40161 | init sampler, took 3.27 ms, tokens: text = 21432, total = 21432
slot update_slots: id  0 | task 40161 | erasing old context checkpoint (pos_min = 17591, pos_max = 18390, size = 18.759 MiB)
slot update_slots: id  0 | task 40161 | created context checkpoint 8 of 8 (pos_min = 20573, pos_max = 21367, size = 18.642 MiB)
slot print_timing: id  0 | task 40161 | 
prompt eval time =     589.54 ms /   222 tokens (    2.66 ms per token,   376.56 tokens per second)
       eval time =    1802.04 ms /    60 tokens (   30.03 ms per token,    33.30 tokens per second)
      total time =    2391.58 ms /   282 tokens
slot      release: id  0 | task 40161 | stop processing: n_tokens = 21491, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 40223 | processing task, is_child = 0
slot update_slots: id  0 | task 40223 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 21680
slot update_slots: id  0 | task 40223 | n_tokens = 21432, memory_seq_rm [21432, end)
slot update_slots: id  0 | task 40223 | prompt processing progress, n_tokens = 21616, batch.n_tokens = 184, progress = 0.997048
slot update_slots: id  0 | task 40223 | n_tokens = 21616, memory_seq_rm [21616, end)
slot update_slots: id  0 | task 40223 | prompt processing progress, n_tokens = 21680, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 40223 | prompt done, n_tokens = 21680, batch.n_tokens = 64
slot init_sampler: id  0 | task 40223 | init sampler, took 3.27 ms, tokens: text = 21680, total = 21680
slot update_slots: id  0 | task 40223 | erasing old context checkpoint (pos_min = 17731, pos_max = 18530, size = 18.759 MiB)
slot update_slots: id  0 | task 40223 | created context checkpoint 8 of 8 (pos_min = 20816, pos_max = 21615, size = 18.759 MiB)
slot print_timing: id  0 | task 40223 | 
prompt eval time =     630.78 ms /   248 tokens (    2.54 ms per token,   393.16 tokens per second)
       eval time =   39745.08 ms /  1309 tokens (   30.36 ms per token,    32.93 tokens per second)
      total time =   40375.86 ms /  1557 tokens
slot      release: id  0 | task 40223 | stop processing: n_tokens = 22988, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.943
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41534 | processing task, is_child = 0
slot update_slots: id  0 | task 41534 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 22976
slot update_slots: id  0 | task 41534 | n_past = 21680, slot.prompt.tokens.size() = 22988, seq_id = 0, pos_min = 22188, n_swa = 128
slot update_slots: id  0 | task 41534 | restored context checkpoint (pos_min = 20816, pos_max = 21615, size = 18.759 MiB)
slot update_slots: id  0 | task 41534 | n_tokens = 21615, memory_seq_rm [21615, end)
slot update_slots: id  0 | task 41534 | prompt processing progress, n_tokens = 22912, batch.n_tokens = 1297, progress = 0.997214
slot update_slots: id  0 | task 41534 | n_tokens = 22912, memory_seq_rm [22912, end)
slot update_slots: id  0 | task 41534 | prompt processing progress, n_tokens = 22976, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41534 | prompt done, n_tokens = 22976, batch.n_tokens = 64
slot init_sampler: id  0 | task 41534 | init sampler, took 6.77 ms, tokens: text = 22976, total = 22976
slot update_slots: id  0 | task 41534 | erasing old context checkpoint (pos_min = 18433, pos_max = 19232, size = 18.759 MiB)
slot update_slots: id  0 | task 41534 | created context checkpoint 8 of 8 (pos_min = 22112, pos_max = 22911, size = 18.759 MiB)
slot print_timing: id  0 | task 41534 | 
prompt eval time =    2442.74 ms /  1361 tokens (    1.79 ms per token,   557.16 tokens per second)
       eval time =    1803.33 ms /    59 tokens (   30.56 ms per token,    32.72 tokens per second)
      total time =    4246.07 ms /  1420 tokens
slot      release: id  0 | task 41534 | stop processing: n_tokens = 23034, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.974 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 41595 | processing task, is_child = 0
slot update_slots: id  0 | task 41595 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 23598
slot update_slots: id  0 | task 41595 | n_tokens = 22976, memory_seq_rm [22976, end)
slot update_slots: id  0 | task 41595 | prompt processing progress, n_tokens = 23534, batch.n_tokens = 558, progress = 0.997288
slot update_slots: id  0 | task 41595 | n_tokens = 23534, memory_seq_rm [23534, end)
slot update_slots: id  0 | task 41595 | prompt processing progress, n_tokens = 23598, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 41595 | prompt done, n_tokens = 23598, batch.n_tokens = 64
slot init_sampler: id  0 | task 41595 | init sampler, took 3.34 ms, tokens: text = 23598, total = 23598
slot update_slots: id  0 | task 41595 | erasing old context checkpoint (pos_min = 18820, pos_max = 19619, size = 18.759 MiB)
slot update_slots: id  0 | task 41595 | created context checkpoint 8 of 8 (pos_min = 22734, pos_max = 23533, size = 18.759 MiB)
slot print_timing: id  0 | task 41595 | 
prompt eval time =    1205.38 ms /   622 tokens (    1.94 ms per token,   516.02 tokens per second)
       eval time =   21216.93 ms /   698 tokens (   30.40 ms per token,    32.90 tokens per second)
      total time =   22422.32 ms /  1320 tokens
slot      release: id  0 | task 41595 | stop processing: n_tokens = 24295, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.971
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 42295 | processing task, is_child = 0
slot update_slots: id  0 | task 42295 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 24283
slot update_slots: id  0 | task 42295 | n_past = 23598, slot.prompt.tokens.size() = 24295, seq_id = 0, pos_min = 23495, n_swa = 128
slot update_slots: id  0 | task 42295 | restored context checkpoint (pos_min = 22734, pos_max = 23533, size = 18.759 MiB)
slot update_slots: id  0 | task 42295 | n_tokens = 23533, memory_seq_rm [23533, end)
slot update_slots: id  0 | task 42295 | prompt processing progress, n_tokens = 24219, batch.n_tokens = 686, progress = 0.997364
slot update_slots: id  0 | task 42295 | n_tokens = 24219, memory_seq_rm [24219, end)
slot update_slots: id  0 | task 42295 | prompt processing progress, n_tokens = 24283, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 42295 | prompt done, n_tokens = 24283, batch.n_tokens = 64
slot init_sampler: id  0 | task 42295 | init sampler, took 4.99 ms, tokens: text = 24283, total = 24283
slot update_slots: id  0 | task 42295 | erasing old context checkpoint (pos_min = 19223, pos_max = 20022, size = 18.759 MiB)
slot update_slots: id  0 | task 42295 | created context checkpoint 8 of 8 (pos_min = 23419, pos_max = 24218, size = 18.759 MiB)
slot print_timing: id  0 | task 42295 | 
prompt eval time =    1572.69 ms /   750 tokens (    2.10 ms per token,   476.89 tokens per second)
       eval time =    2017.64 ms /    66 tokens (   30.57 ms per token,    32.71 tokens per second)
      total time =    3590.32 ms /   816 tokens
slot      release: id  0 | task 42295 | stop processing: n_tokens = 24348, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 42363 | processing task, is_child = 0
slot update_slots: id  0 | task 42363 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 24773
slot update_slots: id  0 | task 42363 | n_tokens = 24283, memory_seq_rm [24283, end)
slot update_slots: id  0 | task 42363 | prompt processing progress, n_tokens = 24709, batch.n_tokens = 426, progress = 0.997417
slot update_slots: id  0 | task 42363 | n_tokens = 24709, memory_seq_rm [24709, end)
slot update_slots: id  0 | task 42363 | prompt processing progress, n_tokens = 24773, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 42363 | prompt done, n_tokens = 24773, batch.n_tokens = 64
slot init_sampler: id  0 | task 42363 | init sampler, took 6.07 ms, tokens: text = 24773, total = 24773
slot update_slots: id  0 | task 42363 | erasing old context checkpoint (pos_min = 19622, pos_max = 20371, size = 17.587 MiB)
slot update_slots: id  0 | task 42363 | created context checkpoint 8 of 8 (pos_min = 23909, pos_max = 24708, size = 18.759 MiB)
slot print_timing: id  0 | task 42363 | 
prompt eval time =     909.83 ms /   490 tokens (    1.86 ms per token,   538.57 tokens per second)
       eval time =   33756.07 ms /  1111 tokens (   30.38 ms per token,    32.91 tokens per second)
      total time =   34665.90 ms /  1601 tokens
slot      release: id  0 | task 42363 | stop processing: n_tokens = 25883, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.990 (> 0.100 thold), f_keep = 0.957
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 43476 | processing task, is_child = 0
slot update_slots: id  0 | task 43476 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 25021
slot update_slots: id  0 | task 43476 | n_past = 24773, slot.prompt.tokens.size() = 25883, seq_id = 0, pos_min = 25083, n_swa = 128
slot update_slots: id  0 | task 43476 | restored context checkpoint (pos_min = 23909, pos_max = 24708, size = 18.759 MiB)
slot update_slots: id  0 | task 43476 | n_tokens = 24708, memory_seq_rm [24708, end)
slot update_slots: id  0 | task 43476 | prompt processing progress, n_tokens = 24957, batch.n_tokens = 249, progress = 0.997442
slot update_slots: id  0 | task 43476 | n_tokens = 24957, memory_seq_rm [24957, end)
slot update_slots: id  0 | task 43476 | prompt processing progress, n_tokens = 25021, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 43476 | prompt done, n_tokens = 25021, batch.n_tokens = 64
slot init_sampler: id  0 | task 43476 | init sampler, took 3.50 ms, tokens: text = 25021, total = 25021
slot update_slots: id  0 | task 43476 | erasing old context checkpoint (pos_min = 19834, pos_max = 20612, size = 18.267 MiB)
slot update_slots: id  0 | task 43476 | created context checkpoint 8 of 8 (pos_min = 24157, pos_max = 24956, size = 18.759 MiB)
slot print_timing: id  0 | task 43476 | 
prompt eval time =     899.31 ms /   313 tokens (    2.87 ms per token,   348.05 tokens per second)
       eval time =   37297.40 ms /  1227 tokens (   30.40 ms per token,    32.90 tokens per second)
      total time =   38196.71 ms /  1540 tokens
slot      release: id  0 | task 43476 | stop processing: n_tokens = 26247, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 44705 | processing task, is_child = 0
slot update_slots: id  0 | task 44705 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 25113
slot update_slots: id  0 | task 44705 | n_past = 25021, slot.prompt.tokens.size() = 26247, seq_id = 0, pos_min = 25447, n_swa = 128
slot update_slots: id  0 | task 44705 | restored context checkpoint (pos_min = 24157, pos_max = 24956, size = 18.759 MiB)
slot update_slots: id  0 | task 44705 | n_tokens = 24956, memory_seq_rm [24956, end)
slot update_slots: id  0 | task 44705 | prompt processing progress, n_tokens = 25049, batch.n_tokens = 93, progress = 0.997452
slot update_slots: id  0 | task 44705 | n_tokens = 25049, memory_seq_rm [25049, end)
slot update_slots: id  0 | task 44705 | prompt processing progress, n_tokens = 25113, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 44705 | prompt done, n_tokens = 25113, batch.n_tokens = 64
slot init_sampler: id  0 | task 44705 | init sampler, took 5.15 ms, tokens: text = 25113, total = 25113
slot update_slots: id  0 | task 44705 | erasing old context checkpoint (pos_min = 20346, pos_max = 21145, size = 18.759 MiB)
slot update_slots: id  0 | task 44705 | created context checkpoint 8 of 8 (pos_min = 24249, pos_max = 25048, size = 18.759 MiB)
slot print_timing: id  0 | task 44705 | 
prompt eval time =     661.12 ms /   157 tokens (    4.21 ms per token,   237.48 tokens per second)
       eval time =   19409.69 ms /   638 tokens (   30.42 ms per token,    32.87 tokens per second)
      total time =   20070.81 ms /   795 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 44705 | stop processing: n_tokens = 25750, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.975
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 45345 | processing task, is_child = 0
slot update_slots: id  0 | task 45345 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 25721
slot update_slots: id  0 | task 45345 | n_tokens = 25113, memory_seq_rm [25113, end)
slot update_slots: id  0 | task 45345 | prompt processing progress, n_tokens = 25657, batch.n_tokens = 544, progress = 0.997512
slot update_slots: id  0 | task 45345 | n_tokens = 25657, memory_seq_rm [25657, end)
slot update_slots: id  0 | task 45345 | prompt processing progress, n_tokens = 25721, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 45345 | prompt done, n_tokens = 25721, batch.n_tokens = 64
slot init_sampler: id  0 | task 45345 | init sampler, took 3.66 ms, tokens: text = 25721, total = 25721
slot update_slots: id  0 | task 45345 | erasing old context checkpoint (pos_min = 20573, pos_max = 21367, size = 18.642 MiB)
slot update_slots: id  0 | task 45345 | created context checkpoint 8 of 8 (pos_min = 24986, pos_max = 25656, size = 15.735 MiB)
slot print_timing: id  0 | task 45345 | 
prompt eval time =    1270.54 ms /   608 tokens (    2.09 ms per token,   478.54 tokens per second)
       eval time =   58232.22 ms /  1916 tokens (   30.39 ms per token,    32.90 tokens per second)
      total time =   59502.76 ms /  2524 tokens
slot      release: id  0 | task 45345 | stop processing: n_tokens = 27636, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.891 (> 0.100 thold), f_keep = 0.024
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 27636, total state size = 666.795 MiB
srv  params_from_: Chat format: GPT-OSS
srv          load:  - looking for better prompt, base f_keep = 0.024, sim = 0.891
srv        update:  - cache state: 18 prompts, 5626.548 MiB (limits: 8192.000 MiB, 56064 tokens, 180659 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv        update:    - prompt 0x5a03d440fde0:    7843 tokens, checkpoints:  8,   362.547 MiB
srv        update:    - prompt 0x5a03cd584ab0:    4868 tokens, checkpoints:  7,   270.134 MiB
srv        update:    - prompt 0x5a03cf145a50:    4159 tokens, checkpoints:  7,   251.891 MiB
srv        update:    - prompt 0x5a03d42d4e40:    5689 tokens, checkpoints:  8,   313.961 MiB
srv        update:    - prompt 0x5a0400041a00:    6280 tokens, checkpoints:  8,   329.062 MiB
srv        update:    - prompt 0x5a03cf19ea50:    4589 tokens, checkpoints:  5,   227.644 MiB
srv        update:    - prompt 0x5a03cee753a0:    5496 tokens, checkpoints:  5,   249.124 MiB
srv        update:    - prompt 0x5a03cae884d0:    5287 tokens, checkpoints:  4,   222.485 MiB
srv        update:    - prompt 0x5a03d42f2a80:    8633 tokens, checkpoints:  8,   390.264 MiB
srv        update:    - prompt 0x5a041341c550:   27636 tokens, checkpoints:  8,   813.845 MiB
srv  get_availabl: prompt cache update took 693.88 ms
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 47263 | processing task, is_child = 0
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = 1593017584
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 97, total state size = 4.550 MiB
srv          load:  - looking for better prompt, base f_keep = 0.608, sim = 0.002
srv          load:  - found better prompt with f_keep = 0.931, sim = 0.979
state_read_meta: failed to find available cells in kv cache
state_seq_set_data: error loading state: failed to restore kv cache
srv          load: failed to restore state with size 699184976
slot  prompt_load: id  3 | task -1 | failed to load prompt from cache
slot prompt_clear: id  3 | task -1 | clearing prompt with 97 tokens
srv        update:  - cache state: 19 prompts, 5631.098 MiB (limits: 8192.000 MiB, 56064 tokens, 180654 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv        update:    - prompt 0x5a03cd690d10:   17282 tokens, checkpoints:  8,   593.097 MiB
srv        update:    - prompt 0x5a03cc651880:    1119 tokens, checkpoints:  5,   128.268 MiB
srv        update:    - prompt 0x5a03d440fde0:    7843 tokens, checkpoints:  8,   362.547 MiB
srv        update:    - prompt 0x5a03cd584ab0:    4868 tokens, checkpoints:  7,   270.134 MiB
srv        update:    - prompt 0x5a03cf145a50:    4159 tokens, checkpoints:  7,   251.891 MiB
srv        update:    - prompt 0x5a03d42d4e40:    5689 tokens, checkpoints:  8,   313.961 MiB
srv        update:    - prompt 0x5a0400041a00:    6280 tokens, checkpoints:  8,   329.062 MiB
srv        update:    - prompt 0x5a03cf19ea50:    4589 tokens, checkpoints:  5,   227.644 MiB
srv        update:    - prompt 0x5a03cee753a0:    5496 tokens, checkpoints:  5,   249.124 MiB
srv        update:    - prompt 0x5a03cae884d0:    5287 tokens, checkpoints:  4,   222.485 MiB
srv        update:    - prompt 0x5a03d42f2a80:    8633 tokens, checkpoints:  8,   390.264 MiB
srv        update:    - prompt 0x5a041341c550:   27636 tokens, checkpoints:  8,   813.845 MiB
srv        update:    - prompt 0x5a042f452e10:      97 tokens, checkpoints:  0,     4.550 MiB
srv  get_availabl: prompt cache update took 5.50 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 47264 | processing task, is_child = 0
slot update_slots: id  0 | task 47263 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 758
slot update_slots: id  0 | task 47263 | n_past = 675, slot.prompt.tokens.size() = 27636, seq_id = 0, pos_min = 26836, n_swa = 128
slot update_slots: id  0 | task 47263 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  0 | task 47263 | erased invalidated context checkpoint (pos_min = 20816, pos_max = 21615, n_swa = 128, size = 18.759 MiB)
slot update_slots: id  0 | task 47263 | erased invalidated context checkpoint (pos_min = 22112, pos_max = 22911, n_swa = 128, size = 18.759 MiB)
slot update_slots: id  0 | task 47263 | erased invalidated context checkpoint (pos_min = 22734, pos_max = 23533, n_swa = 128, size = 18.759 MiB)
slot update_slots: id  0 | task 47263 | erased invalidated context checkpoint (pos_min = 23419, pos_max = 24218, n_swa = 128, size = 18.759 MiB)
slot update_slots: id  0 | task 47263 | erased invalidated context checkpoint (pos_min = 23909, pos_max = 24708, n_swa = 128, size = 18.759 MiB)
slot update_slots: id  0 | task 47263 | erased invalidated context checkpoint (pos_min = 24157, pos_max = 24956, n_swa = 128, size = 18.759 MiB)
slot update_slots: id  0 | task 47263 | erased invalidated context checkpoint (pos_min = 24249, pos_max = 25048, n_swa = 128, size = 18.759 MiB)
slot update_slots: id  0 | task 47263 | erased invalidated context checkpoint (pos_min = 24986, pos_max = 25656, n_swa = 128, size = 15.735 MiB)
slot update_slots: id  0 | task 47263 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  0 | task 47263 | prompt processing progress, n_tokens = 694, batch.n_tokens = 694, progress = 0.915567
slot update_slots: id  3 | task 47264 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 26281
slot update_slots: id  3 | task 47264 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 1354, batch.n_tokens = 2048, progress = 0.051520
slot update_slots: id  0 | task 47263 | n_tokens = 694, memory_seq_rm [694, end)
slot update_slots: id  0 | task 47263 | prompt processing progress, n_tokens = 758, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 47263 | prompt done, n_tokens = 758, batch.n_tokens = 64
slot init_sampler: id  0 | task 47263 | init sampler, took 0.15 ms, tokens: text = 758, total = 758
slot update_slots: id  0 | task 47263 | created context checkpoint 1 of 8 (pos_min = 567, pos_max = 693, size = 2.978 MiB)
slot update_slots: id  3 | task 47264 | n_tokens = 1354, memory_seq_rm [1354, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 3338, batch.n_tokens = 2048, progress = 0.127012
slot update_slots: id  3 | task 47264 | n_tokens = 3338, memory_seq_rm [3338, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 5385, batch.n_tokens = 2048, progress = 0.204901
slot update_slots: id  3 | task 47264 | n_tokens = 5385, memory_seq_rm [5385, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 7432, batch.n_tokens = 2048, progress = 0.282790
slot update_slots: id  3 | task 47264 | n_tokens = 7432, memory_seq_rm [7432, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 9479, batch.n_tokens = 2048, progress = 0.360679
slot update_slots: id  3 | task 47264 | n_tokens = 9479, memory_seq_rm [9479, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 11526, batch.n_tokens = 2048, progress = 0.438568
slot update_slots: id  3 | task 47264 | n_tokens = 11526, memory_seq_rm [11526, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 13573, batch.n_tokens = 2048, progress = 0.516457
slot update_slots: id  3 | task 47264 | n_tokens = 13573, memory_seq_rm [13573, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 15620, batch.n_tokens = 2048, progress = 0.594346
slot update_slots: id  3 | task 47264 | n_tokens = 15620, memory_seq_rm [15620, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 17667, batch.n_tokens = 2048, progress = 0.672235
slot update_slots: id  3 | task 47264 | n_tokens = 17667, memory_seq_rm [17667, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 19714, batch.n_tokens = 2048, progress = 0.750124
slot update_slots: id  3 | task 47264 | n_tokens = 19714, memory_seq_rm [19714, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 21761, batch.n_tokens = 2048, progress = 0.828013
slot update_slots: id  3 | task 47264 | n_tokens = 21761, memory_seq_rm [21761, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 23808, batch.n_tokens = 2048, progress = 0.905902
slot update_slots: id  3 | task 47264 | n_tokens = 23808, memory_seq_rm [23808, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 25855, batch.n_tokens = 2048, progress = 0.983791
slot update_slots: id  3 | task 47264 | n_tokens = 25855, memory_seq_rm [25855, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 26217, batch.n_tokens = 363, progress = 0.997565
slot update_slots: id  3 | task 47264 | n_tokens = 26217, memory_seq_rm [26217, end)
slot update_slots: id  3 | task 47264 | prompt processing progress, n_tokens = 26281, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 47264 | prompt done, n_tokens = 26281, batch.n_tokens = 65
slot init_sampler: id  3 | task 47264 | init sampler, took 6.17 ms, tokens: text = 26281, total = 26281
slot update_slots: id  3 | task 47264 | created context checkpoint 1 of 8 (pos_min = 25448, pos_max = 26216, size = 18.033 MiB)
slot print_timing: id  0 | task 47263 | 
prompt eval time =    4600.73 ms /   758 tokens (    6.07 ms per token,   164.76 tokens per second)
       eval time =   36497.98 ms /    91 tokens (  401.08 ms per token,     2.49 tokens per second)
      total time =   41098.70 ms /   849 tokens
slot      release: id  0 | task 47263 | stop processing: n_tokens = 848, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.542 (> 0.100 thold), f_keep = 0.894
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 47361 | processing task, is_child = 0
slot update_slots: id  0 | task 47361 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1399
slot update_slots: id  0 | task 47361 | n_past = 758, slot.prompt.tokens.size() = 848, seq_id = 0, pos_min = 642, n_swa = 128
slot update_slots: id  0 | task 47361 | restored context checkpoint (pos_min = 567, pos_max = 693, size = 2.978 MiB)
slot update_slots: id  0 | task 47361 | n_tokens = 693, memory_seq_rm [693, end)
slot update_slots: id  0 | task 47361 | prompt processing progress, n_tokens = 1335, batch.n_tokens = 643, progress = 0.954253
slot update_slots: id  0 | task 47361 | n_tokens = 1335, memory_seq_rm [1335, end)
slot update_slots: id  0 | task 47361 | prompt processing progress, n_tokens = 1399, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  0 | task 47361 | prompt done, n_tokens = 1399, batch.n_tokens = 65
slot init_sampler: id  0 | task 47361 | init sampler, took 0.27 ms, tokens: text = 1399, total = 1399
slot update_slots: id  0 | task 47361 | created context checkpoint 2 of 8 (pos_min = 567, pos_max = 1334, size = 18.009 MiB)
slot print_timing: id  0 | task 47361 | 
prompt eval time =    1526.90 ms /   706 tokens (    2.16 ms per token,   462.38 tokens per second)
       eval time =    4138.74 ms /    78 tokens (   53.06 ms per token,    18.85 tokens per second)
      total time =    5665.63 ms /   784 tokens
slot      release: id  0 | task 47361 | stop processing: n_tokens = 1476, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.503 (> 0.100 thold), f_keep = 0.948
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 47447 | processing task, is_child = 0
slot update_slots: id  0 | task 47447 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2780
slot update_slots: id  0 | task 47447 | n_tokens = 1399, memory_seq_rm [1399, end)
slot update_slots: id  0 | task 47447 | prompt processing progress, n_tokens = 2716, batch.n_tokens = 1318, progress = 0.976978
slot update_slots: id  0 | task 47447 | n_tokens = 2716, memory_seq_rm [2716, end)
slot update_slots: id  0 | task 47447 | prompt processing progress, n_tokens = 2780, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  0 | task 47447 | prompt done, n_tokens = 2780, batch.n_tokens = 65
slot init_sampler: id  0 | task 47447 | init sampler, took 0.50 ms, tokens: text = 2780, total = 2780
slot update_slots: id  0 | task 47447 | created context checkpoint 3 of 8 (pos_min = 1946, pos_max = 2715, size = 18.056 MiB)
slot print_timing: id  0 | task 47447 | 
prompt eval time =    2551.60 ms /  1381 tokens (    1.85 ms per token,   541.23 tokens per second)
       eval time =   10231.22 ms /   198 tokens (   51.67 ms per token,    19.35 tokens per second)
      total time =   12782.83 ms /  1579 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  0 | task 47447 | stop processing: n_tokens = 2977, truncated = 0
slot print_timing: id  3 | task 47264 | 
prompt eval time =   37043.67 ms / 26281 tokens (    1.41 ms per token,   709.46 tokens per second)
       eval time =   40488.48 ms /   938 tokens (   43.16 ms per token,    23.17 tokens per second)
      total time =   77532.15 ms / 27219 tokens
slot      release: id  3 | task 47264 | stop processing: n_tokens = 27218, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.985 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 48219 | processing task, is_child = 0
slot update_slots: id  3 | task 48219 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 26693
slot update_slots: id  3 | task 48219 | n_past = 26281, slot.prompt.tokens.size() = 27218, seq_id = 3, pos_min = 26484, n_swa = 128
slot update_slots: id  3 | task 48219 | restored context checkpoint (pos_min = 25448, pos_max = 26216, size = 18.033 MiB)
slot update_slots: id  3 | task 48219 | n_tokens = 26216, memory_seq_rm [26216, end)
slot update_slots: id  3 | task 48219 | prompt processing progress, n_tokens = 26629, batch.n_tokens = 413, progress = 0.997602
slot update_slots: id  3 | task 48219 | n_tokens = 26629, memory_seq_rm [26629, end)
slot update_slots: id  3 | task 48219 | prompt processing progress, n_tokens = 26693, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 48219 | prompt done, n_tokens = 26693, batch.n_tokens = 64
slot init_sampler: id  3 | task 48219 | init sampler, took 5.46 ms, tokens: text = 26693, total = 26693
slot update_slots: id  3 | task 48219 | created context checkpoint 2 of 8 (pos_min = 25859, pos_max = 26628, size = 18.056 MiB)
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.934
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 48444 | processing task, is_child = 0
slot update_slots: id  0 | task 48444 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2852
slot update_slots: id  0 | task 48444 | n_past = 2780, slot.prompt.tokens.size() = 2977, seq_id = 0, pos_min = 2850, n_swa = 128
slot update_slots: id  0 | task 48444 | restored context checkpoint (pos_min = 1946, pos_max = 2715, size = 18.056 MiB)
slot update_slots: id  0 | task 48444 | n_tokens = 2715, memory_seq_rm [2715, end)
slot update_slots: id  0 | task 48444 | prompt processing progress, n_tokens = 2788, batch.n_tokens = 74, progress = 0.977560
slot update_slots: id  0 | task 48444 | n_tokens = 2788, memory_seq_rm [2788, end)
slot update_slots: id  0 | task 48444 | prompt processing progress, n_tokens = 2852, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  0 | task 48444 | prompt done, n_tokens = 2852, batch.n_tokens = 65
slot init_sampler: id  0 | task 48444 | init sampler, took 0.55 ms, tokens: text = 2852, total = 2852
slot update_slots: id  0 | task 48444 | created context checkpoint 4 of 8 (pos_min = 2152, pos_max = 2787, size = 14.914 MiB)
slot print_timing: id  3 | task 48219 | 
prompt eval time =    1201.03 ms /   477 tokens (    2.52 ms per token,   397.16 tokens per second)
       eval time =   10064.57 ms /   268 tokens (   37.55 ms per token,    26.63 tokens per second)
      total time =   11265.60 ms /   745 tokens
slot      release: id  3 | task 48219 | stop processing: n_tokens = 26960, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.990
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 48541 | processing task, is_child = 0
slot update_slots: id  3 | task 48541 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 26879
slot update_slots: id  3 | task 48541 | n_past = 26693, slot.prompt.tokens.size() = 26960, seq_id = 3, pos_min = 26788, n_swa = 128
slot update_slots: id  3 | task 48541 | restored context checkpoint (pos_min = 25859, pos_max = 26628, size = 18.056 MiB)
slot update_slots: id  3 | task 48541 | n_tokens = 26628, memory_seq_rm [26628, end)
slot update_slots: id  3 | task 48541 | prompt processing progress, n_tokens = 26815, batch.n_tokens = 188, progress = 0.997619
slot update_slots: id  3 | task 48541 | n_tokens = 26815, memory_seq_rm [26815, end)
slot update_slots: id  3 | task 48541 | prompt processing progress, n_tokens = 26879, batch.n_tokens = 65, progress = 1.000000
slot update_slots: id  3 | task 48541 | prompt done, n_tokens = 26879, batch.n_tokens = 65
slot init_sampler: id  3 | task 48541 | init sampler, took 5.53 ms, tokens: text = 26879, total = 26879
slot update_slots: id  3 | task 48541 | created context checkpoint 3 of 8 (pos_min = 26496, pos_max = 26814, size = 7.481 MiB)
slot print_timing: id  3 | task 48541 | 
prompt eval time =     884.02 ms /   251 tokens (    3.52 ms per token,   283.93 tokens per second)
       eval time =    3817.93 ms /    74 tokens (   51.59 ms per token,    19.38 tokens per second)
      total time =    4701.94 ms /   325 tokens
slot      release: id  3 | task 48541 | stop processing: n_tokens = 26952, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot print_timing: id  0 | task 48444 | 
prompt eval time =     831.18 ms /   137 tokens (    6.07 ms per token,   164.83 tokens per second)
       eval time =   10346.39 ms /   223 tokens (   46.40 ms per token,    21.55 tokens per second)
      total time =   11177.58 ms /   360 tokens
slot      release: id  0 | task 48444 | stop processing: n_tokens = 3074, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  0 | task -1 | selected slot by LCP similarity, sim_best = 0.252 (> 0.100 thold), f_keep = 0.928
slot launch_slot_: id  0 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  0 | task 48670 | processing task, is_child = 0
slot update_slots: id  0 | task 48670 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11331
slot update_slots: id  0 | task 48670 | n_past = 2852, slot.prompt.tokens.size() = 3074, seq_id = 0, pos_min = 2819, n_swa = 128
slot update_slots: id  0 | task 48670 | restored context checkpoint (pos_min = 2152, pos_max = 2787, size = 14.914 MiB)
slot update_slots: id  0 | task 48670 | n_tokens = 2787, memory_seq_rm [2787, end)
slot update_slots: id  0 | task 48670 | prompt processing progress, n_tokens = 4835, batch.n_tokens = 2048, progress = 0.426706
slot update_slots: id  0 | task 48670 | n_tokens = 4835, memory_seq_rm [4835, end)
slot update_slots: id  0 | task 48670 | prompt processing progress, n_tokens = 6883, batch.n_tokens = 2048, progress = 0.607449
slot update_slots: id  0 | task 48670 | n_tokens = 6883, memory_seq_rm [6883, end)
slot update_slots: id  0 | task 48670 | prompt processing progress, n_tokens = 8931, batch.n_tokens = 2048, progress = 0.788192
slot update_slots: id  0 | task 48670 | n_tokens = 8931, memory_seq_rm [8931, end)
slot update_slots: id  0 | task 48670 | prompt processing progress, n_tokens = 10979, batch.n_tokens = 2048, progress = 0.968935
slot update_slots: id  0 | task 48670 | n_tokens = 10979, memory_seq_rm [10979, end)
slot update_slots: id  0 | task 48670 | prompt processing progress, n_tokens = 11267, batch.n_tokens = 288, progress = 0.994352
slot update_slots: id  0 | task 48670 | n_tokens = 11267, memory_seq_rm [11267, end)
slot update_slots: id  0 | task 48670 | prompt processing progress, n_tokens = 11331, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  0 | task 48670 | prompt done, n_tokens = 11331, batch.n_tokens = 64
slot init_sampler: id  0 | task 48670 | init sampler, took 2.38 ms, tokens: text = 11331, total = 11331
slot update_slots: id  0 | task 48670 | created context checkpoint 5 of 8 (pos_min = 10497, pos_max = 11266, size = 18.056 MiB)
