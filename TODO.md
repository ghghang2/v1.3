# RSI Research Surface Expansion

## Overview
This repository contains a proofâ€‘ofâ€‘concept to broaden the RSI research surface beyond arXiv and GitHub. The goal is to gather metadata from a variety of public sources (search engines, preprint servers, patents, blogs, code hosting platforms) while keeping the stack free and accountâ€‘free.

---

## Implementation Plan
Each task is marked with a status:
- `âŒ Pending`
- `âœ… Completed`
- `ğŸš§ Blocked`

---

### Phase 0 â€“ Project Setup
| # | Task | Status |
|---|------|--------|
| 0.1 | Initialise a new Python project (virtualenv, requirements.txt) | âŒ Pending |
| 0.2 | Create a basic directory layout (`src/`, `results/`, `tests/`) | âŒ Pending |
| 0.3 | Add a `config.yaml` template with default settings | âŒ Pending |

### Phase 1 â€“ Search Engine & Classification
| # | Task | Status |
|---|------|--------|
| 1.1 | Write `search_engine.py` to perform Google/DuckDuckGo search using `browser` tool | âŒ Pending |
| 1.2 | Implement result extraction (title, URL, snippet) | âŒ Pending |
| 1.3 | Create a simple domainâ€‘based classifier to tag URLs as `paper`, `patent`, `repo`, `blog` | âŒ Pending |
| 1.4 | Unitâ€‘test the search engine with mock responses | âŒ Pending |

### Phase 2 â€“ Academic & Patent Harvesters
| # | Task | Status |
|---|------|--------|
| 2.1 | Implement `harvest_arxiv.py` using arXiv API | âŒ Pending |
| 2.2 | Implement `harvest_semanticscholar.py` (free tier) | âŒ Pending |
| 2.3 | Implement `harvest_crossref.py` for nonâ€‘arXiv preprints | âŒ Pending |
| 2.4 | Implement `harvest_patents.py` using USPTO Open Patent Services | âŒ Pending |
| 2.5 | Unitâ€‘test each harvester with sample IDs | âŒ Pending |

### Phase 3 â€“ Code Repository Harvesters
| # | Task | Status |
|---|------|--------|
| 3.1 | Implement `harvest_gitlab.py` using GitLab public API | âŒ Pending |
| 3.2 | Implement `harvest_bitbucket.py` using Bitbucket public API | âŒ Pending |
| 3.3 | Unitâ€‘test repository harvesters | âŒ Pending |

### Phase 4 â€“ Result Storage & Logging
| # | Task | Status |
|---|------|--------|
| 4.1 | Design a JSON schema for harvested metadata | âŒ Pending |
| 4.2 | Write `store_results.py` to persist query, raw search, and harvested data | âŒ Pending |
| 4.3 | Implement simple duplicate detection (URL hash) | âŒ Pending |
| 4.4 | Add timestamp & logging to each run | âŒ Pending |

### Phase 5 â€“ Packaging & Demo
| # | Task | Status |
|---|------|--------|
| 5.1 | Create a commandâ€‘line interface (`rsi_search.py`) that orchestrates all modules | âŒ Pending |
| 5.2 | Generate a Markdown summary (`summary.md`) per query | âŒ Pending |
| 5.3 | Write a README with usage instructions | âŒ Pending |
| 5.4 | Add a sample run output in the repository | âŒ Pending |

### Phase 6 â€“ Validation & Testing
| # | Task | Status |
|---|------|--------|
| 6.1 | Run a full pipeline on the test keyword `"recursive selfâ€‘improvement"` | âŒ Pending |
| 6.2 | Verify that at least one result is harvested from each source type | âŒ Pending |
| 6.3 | Ensure no rateâ€‘limit errors occur | âŒ Pending |

### Phase 7 â€“ Finalisation
| # | Task | Status |
|---|------|--------|
| 7.1 | Mark all completed tasks and remove `TODO.md` or archive it | âŒ Pending |
| 7.2 | Notify via email that implementation is finished | âŒ Pending |

---

## Notes
- All external calls are rateâ€‘limited; implement exponential backâ€‘off where necessary.
- The project is intentionally minimalistic to keep dependencies light (requests, beautifulsoup4, pyyaml, tqdm).
- For privacy, the script will only scrape publicly accessible pages.
- Future enhancements (translation, PDF extraction) will be added in later phases.
